<repomix>This file is a merged representation of the entire codebase, combined into a single document by Repomix. The content has been processed where empty lines have been removed, content has been formatted for parsing, security check has been disabled.<file_summary>This section contains a summary of this file.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been formatted for parsing in xml style
- Security check has been disabled - content may contain sensitive information</notes><additional_info></additional_info></file_summary><directory_structure>.github/
  workflows/
    python-tests.yaml
config/
  caption_filter_list.txt.example
  config.json.example
  lycoris_config.json.example
  multidatabackend-multiresolution.json.example
  multidatabackend.json.example
  user_prompt_library.json.example
documentation/
  data_presets/
    preset_dalle3.md
    preset_midjourney.md
    preset_nijijourney.md
    preset_pexels.md
    preset.md
    README.md
  evaluation/
    CLIP_SCORES.md
    EVAL_LOSS.md
  quickstart/
    FLUX.md
    KOLORS.md
    LTXVIDEO.md
    SANA.md
    SD3.md
    SIGMA.md
  CONTROLNET.md
  DATALOADER.md
  DEEPFLOYD.md
  DEEPSPEED.md
  DISTRIBUTED.md
  DOCKER.md
  DREAMBOOTH.md
  LYCORIS.md
  MIXTURE_OF_EXPERTS.md
  QUICKSTART.md
helpers/
  caching/
    memory.py
    text_embeds.py
    vae.py
  configuration/
    cmd_args.py
    env_file.py
    json_file.py
    loader.py
    toml_file.py
  data_backend/
    aws.py
    base.py
    csv_url_list.py
    factory.py
    local.py
  image_manipulation/
    brightness.py
    cropping.py
    load.py
    training_sample.py
  kolors/
    pipeline.py
  legacy/
    pipeline.py
  metadata/
    backends/
      base.py
      discovery.py
      parquet.py
  models/
    flux/
      __init__.py
      attention.py
      pipeline.py
      transformer.py
    ltxvideo/
      __init__.py
    omnigen/
      pipeline.py
    pixart/
      pipeline.py
    sana/
      transformer.py
    sd3/
      expanded.py
      pipeline.py
      transformer.py
    sdxl/
      pipeline.py
    smoldit/
      __init__.py
      pipeline.py
      transformer.py
    __init__.py
  multiaspect/
    dataset.py
    image.py
    sampler.py
    state.py
    video.py
  prompt_expander/
    __init__.py
  publishing/
    huggingface.py
    metadata.py
  training/
    default_settings/
      __init__.py
      safety_check.py
    optimizers/
      adamw_bfloat16/
        stochastic/
          __init__.py
        __init__.py
      adamw_schedulefree/
        __init__.py
      soap/
        __init__.py
    quantisation/
      __init__.py
      peft_workarounds.py
      quanto_workarounds.py
      torchao_workarounds.py
    __init__.py
    adapter.py
    collate.py
    custom_schedule.py
    deepspeed.py
    diffusion_model.py
    ema.py
    error_handling.py
    evaluation.py
    exceptions.py
    gradient_checkpointing_interval.py
    min_snr_gamma.py
    model_freeze.py
    multi_process.py
    optimizer_param.py
    peft_init.py
    save_hooks.py
    schedulers.py
    state_tracker.py
    text_encoding.py
    trainer.py
    validation.py
    wrappers.py
  webhooks/
    config.py
    handler.py
    mixin.py
  log_format.py
  prompts.py
install/
  apple/
    pyproject.toml
  github/
    pyproject.toml
  rocm/
    pyproject.toml
simpletuner_sdk/
  thread_keeper/
    __init__.py
  api_state.py
  configuration.py
  interface.py
  training_host.py
tests/
  helpers/
    data.py
  test_collate.py
  test_cropping.py
  test_custom_schedules.py
  test_dataset.py
  test_ema.py
  test_image.py
  test_metadata_backend.py
  test_model_card.py
  test_prompthandler.py
  test_sampler.py
  test_save_hooks.py
  test_state.py
  test_trainer.py
  test_training_sample.py
  test_vae.py
  test_webhooks.py
toolkit/
  captioning/
    classes/
      Authorization.php
      BackendController.php
      S3Uploader.php
    caption_backend_server.php
    caption_with_blip.py
    caption_with_blip3.py
    caption_with_cogvlm_remote.py
    caption_with_cogvlm.py
    caption_with_florence.py
    caption_with_gemini.py
    caption_with_gemma.py
    caption_with_gpt4.py
    caption_with_internvl.py
    caption_with_llava.py
    composer.json
  datasets/
    controlnet/
      create_canny_edge.py
    masked_loss/
      generate_dataset_masks_via_huggingface.py
      generate_dataset_masks.py
      requirements.txt
    analyze_aspect_ratios_json.py
    analyze_laion_data.py
    check_latent_corruption.py
    clear_s3_bucket.py
    crop.py
    csv_to_s3.py
    dataset_from_kellyc.py
    dataset_from_laion.py
    dataset_from_pixilart.py
    discord_scrape.py
    enhance_with_controlnet.py
    folder_to_parquet.py
    random_recrop_for_json_image_metadata.py
    retrieve_s3_bucket.py
    update_parquet.py
  README.md
.gitignore
configure.py
convert_sd_checkpoint.py
convert_sdxl_checkpoint.py
docker-start.sh
Dockerfile
filter_list.txt
inference_comparison.py
inference.py
INSTALL.md
LICENSE
notebook.ipynb
OPTIONS.md
pyproject.toml
README.md
service_worker.py
train.py
train.sh
TUTORIAL.md</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path=".github/workflows/python-tests.yaml">name: Python Unit Tests
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - name: Maximize build space
      uses: AdityaGarg8/remove-unwanted-software@v4.1
      with:
        remove-android: &apos;true&apos;
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.11
    - name: Install Poetry
      run: python -m pip install --upgrade pip poetry
    - name: Install Dependencies
      run: poetry -C install/apple install
    - name: Run Tests
      run: poetry -C ./ -P install/apple run python -m unittest discover tests/</file><file path="config/caption_filter_list.txt.example">The image does not provide a clear view of the entire scene, making it challenging to accurately caption. However, based on the visible elements, a possible caption could be: 
The image does not explicitly depict any LGBT themes. It shows two hands holding a shell against a beach backdrop. A caption like 
The image does not depict a South American scene. It shows a 
The image does not depict a South American 
The photo does not prominently feature any anatomical features. It primarily showcases 
The image showcases 
The image features 
The image captures
The image depicts
The photo showcases
The photo features
The photo captures
The image does not have a caption provided in the image itself. However, based on the content, a suitable caption might be: 
The image does not have a clear caption as it is an experimental photo. However, one could describe it as 
The image does not have a clear subject or context .* for a precise caption. 
Caption: 
The image does not have a clear caption .* describe it as 
The image does not have a clear caption .* could be:
The image does not have .* appears to be 
The image does not have a clear subject or context .* appears to be 
The image does not have a clear subject or context .* could be:
The image does not have a direct caption .*:
The image does not require a caption .* &apos;
smoking vaping 
^there is 
araffe 
arafed 
^someone is</file><file path="config/config.json.example">{
    &quot;--resume_from_checkpoint&quot;: &quot;latest&quot;,
    &quot;--data_backend_config&quot;: &quot;config/multidatabackend.json&quot;,
    &quot;--aspect_bucket_rounding&quot;: 2,
    &quot;--seed&quot;: 42,
    &quot;--minimum_image_size&quot;: 0,
    &quot;--output_dir&quot;: &quot;output/models&quot;,
    &quot;--lora_type&quot;: &quot;lycoris&quot;,
    &quot;--lycoris_config&quot;: &quot;config/lycoris_config.json&quot;,
    &quot;--max_train_steps&quot;: 10000,
    &quot;--num_train_epochs&quot;: 0,
    &quot;--checkpointing_steps&quot;: 500,
    &quot;--checkpoints_total_limit&quot;: 5,
    &quot;--hub_model_id&quot;: &quot;simpletuner-lora&quot;,
    &quot;--push_to_hub&quot;: &quot;true&quot;,
    &quot;--push_checkpoints_to_hub&quot;: &quot;true&quot;,
    &quot;--tracker_project_name&quot;: &quot;lora-training&quot;,
    &quot;--tracker_run_name&quot;: &quot;simpletuner-lora&quot;,
    &quot;--report_to&quot;: &quot;wandb&quot;,
    &quot;--model_type&quot;: &quot;lora&quot;,
    &quot;--pretrained_model_name_or_path&quot;: &quot;black-forest-labs/FLUX.1-dev&quot;,
    &quot;--model_family&quot;: &quot;flux&quot;,
    &quot;--train_batch_size&quot;: 1,
    &quot;--gradient_checkpointing&quot;: &quot;true&quot;,
    &quot;--caption_dropout_probability&quot;: 0.1,
    &quot;--resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;--resolution&quot;: 1024,
    &quot;--validation_seed&quot;: 42,
    &quot;--validation_steps&quot;: 500,
    &quot;--validation_resolution&quot;: &quot;1024x1024&quot;,
    &quot;--validation_guidance&quot;: 3.0,
    &quot;--validation_guidance_rescale&quot;: &quot;0.0&quot;,
    &quot;--validation_num_inference_steps&quot;: &quot;20&quot;,
    &quot;--validation_prompt&quot;: &quot;A photo-realistic image of a cat&quot;,
    &quot;--mixed_precision&quot;: &quot;bf16&quot;,
    &quot;--optimizer&quot;: &quot;adamw_bf16&quot;,
    &quot;--learning_rate&quot;: &quot;1e-4&quot;,
    &quot;--lr_scheduler&quot;: &quot;polynomial&quot;,
    &quot;--lr_warmup_steps&quot;: 100,
    &quot;--validation_torch_compile&quot;: &quot;false&quot;,
    &quot;--disable_benchmark&quot;: &quot;false&quot;
}</file><file path="config/lycoris_config.json.example">{
    &quot;algo&quot;: &quot;lokr&quot;,
    &quot;multiplier&quot;: 1.0,
    &quot;full_matrix&quot;: true,
    &quot;linear_alpha&quot;: 1,
    &quot;factor&quot;: 16,
    &quot;apply_preset&quot;: {
        &quot;target_module&quot;: [
            &quot;Attention&quot;,
            &quot;FeedForward&quot;
        ],
        &quot;module_algo_map&quot;: {
            &quot;Attention&quot;: {
                &quot;factor&quot;: 16
            },
            &quot;FeedForward&quot;: {
                &quot;factor&quot;: 8
            }
        }
    }
}</file><file path="config/multidatabackend-multiresolution.json.example">[
  {
    &quot;id&quot;: &quot;dreambooth-512&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/data/dreambooth&quot;,
    &quot;crop&quot;: false, &quot;crop_style&quot;: &quot;random&quot;,
    &quot;minimum_image_size&quot;: 128,
    &quot;resolution&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;, &quot;repeats&quot;: 10,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-512&quot;
  },
  {
    &quot;id&quot;: &quot;dreambooth-1024&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/data/dreambooth&quot;,
    &quot;crop&quot;: false, &quot;crop_style&quot;: &quot;random&quot;,
    &quot;minimum_image_size&quot;: 128,
    &quot;resolution&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;, &quot;repeats&quot;: 10,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-1024&quot;
  },
  {
    &quot;id&quot;: &quot;dreambooth-512-crop&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/data/dreambooth&quot;,
    &quot;crop&quot;: true, &quot;crop_style&quot;: &quot;random&quot;,
    &quot;minimum_image_size&quot;: 128,
    &quot;resolution&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;, &quot;repeats&quot;: 10,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-512-crop&quot;
  },
  {
    &quot;id&quot;: &quot;dreambooth-1024-crop&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/data/dreambooth&quot;,
    &quot;crop&quot;: true, &quot;crop_style&quot;: &quot;random&quot;,
    &quot;minimum_image_size&quot;: 128,
    &quot;resolution&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;, &quot;repeats&quot;: 10,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-1024-crop&quot;
  },
  {
    &quot;id&quot;: &quot;text-embed-cache&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;type&quot;: &quot;local&quot;,
    &quot;cache_dir&quot;: &quot;cache/text&quot;
  }
]</file><file path="config/multidatabackend.json.example">[
    {
        &quot;id&quot;: &quot;something-special-to-remember-by&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/path/to/data/tree&quot;,
        &quot;crop&quot;: false,
        &quot;crop_style&quot;: &quot;random|center|corner&quot;,
        &quot;crop_aspect&quot;: &quot;square|preserve&quot;,
        &quot;minimum_image_size&quot;: 1024,
        &quot;maximum_image_size&quot;: 1536,
        &quot;target_downsample_size&quot;: 1024,
        &quot;resolution&quot;: 1024,
        &quot;resolution_type&quot;: &quot;pixel_area|area|pixel&quot;,
        &quot;prepend_instance_prompt&quot;: false,
        &quot;instance_prompt&quot;: &quot;cat girls&quot;,
        &quot;only_instance_prompt&quot;: false,
        &quot;caption_strategy&quot;: &quot;filename&quot;,
        &quot;cache_dir_vae&quot;: &quot;/path/to/vaecache&quot;,
        &quot;vae_cache_clear_each_epoch&quot;: true,
        &quot;probability&quot;: 1.0,
        &quot;repeats&quot;: 5,
        &quot;text_embeds&quot;: &quot;alt-embed-cache&quot;,
        &quot;skip_file_discovery&quot;: &quot;vae,aspect,text,metadata&quot;,
        &quot;preserve_data_backend_cache&quot;: true
    },
    {
        &quot;id&quot;: &quot;another-special-name-for-another-backend&quot;,
        &quot;type&quot;: &quot;aws&quot;,
        &quot;aws_bucket_name&quot;: &quot;something-yummy&quot;,
        &quot;aws_region_name&quot;: null,
        &quot;aws_endpoint_url&quot;: &quot;https://foo.bar/&quot;,
        &quot;aws_access_key_id&quot;: &quot;wpz-764e9734523434&quot;,
        &quot;aws_secret_access_key&quot;: &quot;xyz-sdajkhfhakhfjd&quot;,
        &quot;aws_data_prefix&quot;: &quot;&quot;,
        &quot;cache_dir_vae&quot;: &quot;s3prefix/for/vaecache&quot;,
        &quot;vae_cache_clear_each_epoch&quot;: true,
        &quot;repeats&quot;: 2,
        &quot;ignore_epochs&quot;: false
    },
    {
        &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;type&quot;: &quot;aws&quot;,
        &quot;aws_bucket_name&quot;: &quot;textembeds-something-yummy&quot;,
        &quot;aws_region_name&quot;: null,
        &quot;aws_endpoint_url&quot;: &quot;https://foo.bar/&quot;,
        &quot;aws_access_key_id&quot;: &quot;wpz-764e9734523434&quot;,
        &quot;aws_secret_access_key&quot;: &quot;xyz-sdajkhfhakhfjd&quot;,
        &quot;aws_data_prefix&quot;: &quot;&quot;,
        &quot;cache_dir&quot;: &quot;&quot;
    },
    {
        &quot;id&quot;: &quot;alt-embed-cache&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: false,
        &quot;type&quot;: &quot;local&quot;,
        &quot;cache_dir&quot;: &quot;/path/to/textembed_cache&quot;
    }
]</file><file path="config/user_prompt_library.json.example">{
    &quot;shortname_here&quot;: &quot;your prompt to validate on&quot;,
    &quot;another_shortname&quot;: &quot;another prompt to validate on&quot;
}</file><file path="documentation/data_presets/preset_dalle3.md"># DALLE-3

## Details

- **Hub link**: [ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions](https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions)
- **Description**: 1 million+ DALLE-3 images combined with a small number of Midjourney and other AI-sourced images.
- **Caption format(s)**: JSON files containing multiple styles of caption.

## Required preprocessing steps

The DALLE-3 dataset contains files in the following structure:
```
|- data/
|-| data/file.png
|-| data/file.json
```

We will use two scripts to prepare:

- A parquet table containing the image metadata, eg. width, height, and filename
- A .txt file for each sample containing its caption, in case loading captions from parquet is too slow

### Extracting the dataset

1. Retrieve the dataset from the hub via your chosen retrieval method, or:

```bash
huggingface-cli login
huggingface-cli download --repo-type=dataset ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions --local-dir=/home/user/training/data/dalle3
```

2. Enter the DALLE-3 data directory and extract all entries:

```bash
cd /home/user/training/data/dalle3
mkdir data
for tar_file_path in *.tar; do
    tar xf &quot;$tar_file_path&quot; -C data/
done
```
**At this point, you will have all of the `.png` and `.json` files in the `data/` subdirectory**.

### Compiling a parquet table

In the `dalle3` directory, create a file named `compile.py` with the following contents:

```py
import glob
import json, os
import pandas as pd
from tqdm import tqdm

# Glob for all JSON files in the folder
json_files = glob.glob(&apos;data/*.json&apos;)

data = []

# Process each JSON file
for file_path in tqdm(json_files):
    with open(file_path, &apos;r&apos;) as file:
        content = json.load(file)
        #print(f&quot;Content: {content}&quot;)
        if &quot;width&quot; not in content:
                continue
        # Extract the necessary information
        text_path = os.path.splitext(content[&apos;image_name&apos;])[0] + &quot;.txt&quot;
        width = int(content[&apos;width&apos;])
        height = int(content[&apos;height&apos;])
        caption = content[&apos;short_caption&apos;]
        filename = content[&apos;image_name&apos;]
        
        # Append to data list
        data.append({&apos;width&apos;: width, &apos;height&apos;: height, &apos;caption&apos;: caption, &apos;filename&apos;: filename})

# Create a DataFrame
df = pd.DataFrame(data, columns=[&apos;width&apos;, &apos;height&apos;, &apos;caption&apos;, &apos;filename&apos;])

# Save the DataFrame to a Parquet file
df.to_parquet(&apos;dalle3.parquet&apos;, index=False)

print(&quot;Data has been successfully compiled and saved as &apos;dalle3.parquet&apos;&quot;)
```

Execute the compilation script, being sure to have your python venv active:

```bash
. .venv/bin/activate
python compile.py
```

Check that the parquet file contains the resulting rows. You may need to run `pip install parquet-tools` first:

```bash
parquet-tools csv dalle3-parquet | head -n10
```

This will print the first ten rows of the DALLE3 dataset. Don&apos;t worry if it takes a while, we&apos;re crunching more than 1 million rows from a columnar format.

### Extracting image captions into textfiles

In the `dalle3` directory, create a new script called `extract-captions.py` containing the following:

```py
import glob
import json, os
import pandas as pd
from tqdm import tqdm
 
# Glob for all JSON files in the folder
json_files = glob.glob(&apos;data/*.json&apos;)
 
data = []
caption_field = &apos;short_caption&apos;
 
# Process each JSON file
for file_path in tqdm(json_files, desc=&quot;Extracting text captions from JSON&quot;):
    with open(file_path, &apos;r&apos;) as file:
        content = json.load(file)
        if &quot;width&quot; not in content:
                continue
        text_path = &quot;data/&quot; + os.path.splitext(content[&apos;image_name&apos;])[0] + &quot;.txt&quot;
        # write content to text path
        with open(text_path, &apos;w&apos;) as text_file:
            text_file.write(content[caption_field])
```

This script will retrieve `caption_field` from each JSON file in the `data/` subdirectory, and write that value to a `.txt` file with the same filename as the image.

If you wish to use a different caption field from the DALLE-3 set, update the value for `caption_field` before executing it.

Now, execute the script, being sure to have the venv active:

```bash
. .venv/bin/activate
python extract-captions.py
```

You can verify that there are the correct number of JSON files in the directory now. Beware that this may take a while to run:

```bash
$ find data/ -name \*.json | wc -l
1161957
```

You should see the correct value of 1,161,957.


## Dataloader entry:

The following configuration entry will correctly locate the filenames and captions for your newly-created DALLE-3 dataset:

```json
    {
        &quot;id&quot;: &quot;dalle3&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/home/user/training/data/dalle3/data&quot;,
        &quot;resolution&quot;: 1.0,
        &quot;maximum_image_size&quot;: 2.0,
        &quot;minimum_image_size&quot;: 0.75,
        &quot;target_downsample_size&quot;: 1.75,
        &quot;resolution_type&quot;: &quot;area&quot;,
        &quot;cache_dir_vae&quot;: &quot;/path/to/cache/vae/&quot;,
        &quot;caption_strategy&quot;: &quot;textfile&quot;,
        &quot;metadata_backend&quot;: &quot;parquet&quot;,
        &quot;parquet&quot;: {
            &quot;path&quot;: &quot;/home/user/training/data/dalle3/dalle3.parquet&quot;,
            &quot;caption_column&quot;: &quot;caption&quot;,
            &quot;filename_column&quot;: &quot;filename&quot;,
            &quot;width_column&quot;: &quot;width&quot;,
            &quot;height_column&quot;: &quot;height&quot;,
            &quot;identifier_includes_extension&quot;: true
        }
    },
```

**Note**: You can skip the `extract-captions.py` script and simply use `caption_strategy=parquet` if you wish to save on disk inodes.</file><file path="documentation/data_presets/preset_midjourney.md"># Midjourney v6 520k

## Details

- **Hub link**: [terminusresearch/midjourney-v6-520k-raw](https://huggingface.co/datasets/terminusresearch/midjourney-v6-520k-raw)
- **Description**: ~520,000 high quality outputs where any Japanese user prompts have been re-captioned with GPT-3.5-Turbo.
- **Caption format(s)**: Parquet

## Required storage

This dataset contains all image data, and as such, it will be difficult to extract without adequate disk space. **Ensure you have at least 1.5TB of disk space available to extract it.**

T5-XXL text embeds for this model will consume ~520GB even with `--compress_disk_cache` enabled.
The VAE embeds will consume just under 80 to 100GB of space, depending on the model being trained and the resolution of the embeds.


## Download

```bash
huggingface-cli download --repo-type=dataset terminusresearch/midjourney-v6-520k-raw --local-dir=midjourney-v6-520k-raw
```

This will simultaneously download the chunked tar segments from Hugging Face Hub.

## Extract

```bash
cd midjourney-v6-520k-raw
cat *.tar | tar x
```

This will create a folder containing all of the samples inside the current directory.

## Dataloader configuration example

```json
{
    &quot;id&quot;: &quot;midjourney-v6-520k-raw&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-mj-520k/&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;resolution&quot;: 1.0,
    &quot;maximum_image_size&quot;: 1.0,
    &quot;minimum_image_size&quot;: 0.75,
    &quot;target_downsample_size&quot;: 1.00,
    &quot;resolution_type&quot;: &quot;area&quot;,
    &quot;caption_strategy&quot;: &quot;parquet&quot;,
    &quot;metadata_backend&quot;: &quot;parquet&quot;,
    &quot;parquet&quot;: {
        &quot;path&quot;: &quot;/path/to/midjourney-v6-520k-raw/train.parquet&quot;,
        &quot;caption_column&quot;: &quot;gpt_caption&quot;,
        &quot;filename_column&quot;: &quot;id&quot;,
        &quot;width_column&quot;: &quot;width&quot;,
        &quot;height_column&quot;: &quot;height&quot;,
        &quot;identifier_includes_extension&quot;: false
    }
}
```</file><file path="documentation/data_presets/preset_nijijourney.md"># Niji v6 520k

## Details

- **Hub link**: [terminusresearch/nijijourney-v6-520k-raw](https://huggingface.co/datasets/terminusresearch/nijijourney-v6-520k-raw)
- **Description**: ~520,000 high quality outputs where any Japanese user prompts have been re-captioned with GPT-3.5-Turbo.
- **Caption format(s)**: Parquet

## Required storage

This dataset contains all image data, and as such, it will be difficult to extract without adequate disk space. **Ensure you have at least 1.5TB of disk space available to extract it.**

T5-XXL text embeds for this model will consume ~520GB even with `--compress_disk_cache` enabled.
The VAE embeds will consume just under 80 to 100GB of space, depending on the model being trained and the resolution of the embeds.

## Download

```bash
huggingface-cli download --repo-type=dataset terminusresearch/nijijourney-v6-520k-raw --local-dir=nijijourney-v6-520k-raw
```

This will simultaneously download the chunked tar segments from Hugging Face Hub.

## Extract

```bash
cd nijijourney-v6-520k-raw
cat *.tar | tar x
```

This will create a folder containing all of the samples inside the current directory.

## Dataloader configuration example

```json
{
    &quot;id&quot;: &quot;nijijourney-v6-520k-raw&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae-nj-520k/&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;resolution&quot;: 1.0,
    &quot;maximum_image_size&quot;: 1.0,
    &quot;minimum_image_size&quot;: 0.75,
    &quot;target_downsample_size&quot;: 1.00,
    &quot;resolution_type&quot;: &quot;area&quot;,
    &quot;caption_strategy&quot;: &quot;parquet&quot;,
    &quot;metadata_backend&quot;: &quot;parquet&quot;,
    &quot;parquet&quot;: {
        &quot;path&quot;: &quot;/path/to/nijijourney-v6-520k-raw/train.parquet&quot;,
        &quot;caption_column&quot;: &quot;gpt_caption&quot;,
        &quot;filename_column&quot;: &quot;id&quot;,
        &quot;width_column&quot;: &quot;width&quot;,
        &quot;height_column&quot;: &quot;height&quot;,
        &quot;identifier_includes_extension&quot;: false
    }
}
```</file><file path="documentation/data_presets/preset_pexels.md"># Photo concept bucket

## Details

- **Hub link**: [ptx0/photo-concept-bucket](https://huggingface.co/datasets/ptx0/photo-concept-bucket)
- **Description**: ~567,000 high quality photographs across dense concept distribution, captioned with CogVLM.
- **Caption format(s)**: Parquet

## Required preprocessing steps

As the photo-concept-bucket repository does not include image data, this must be retrieved by you directly from the Pexels server.

An example script for downloading the dataset is provided, but you must ensure you are following the terms and conditions of the Pexels service, at the time of consumption.

To download the captions and URL list:

```bash
huggingface-cli download --repo-type=dataset ptx0/photo-concept-bucket --local-dir=/home/user/training/photo-concept-bucket
```

Place this file into `/home/user/training/photo-concept-bucket`:

`download.py`
```py
from concurrent.futures import ThreadPoolExecutor
import pyarrow.parquet as pq
import os
import requests
from PIL import Image
from io import BytesIO

# Load the Parquet file
parquet_file = &apos;photo-concept-bucket.parquet&apos;
df = pq.read_table(parquet_file).to_pandas()

# Define the output directory
output_dir = &apos;train&apos;
os.makedirs(output_dir, exist_ok=True)

def resize_for_condition_image(input_image: Image, resolution: int):
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    k = float(resolution) / min(H, W)
    H *= k
    W *= k
    H = int(round(H / 64.0)) * 64
    W = int(round(W / 64.0)) * 64
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img

def download_and_save(row):
    img_url = row[&apos;url&apos;]
    caption = row[&apos;cogvlm_caption&apos;]
    img_id = row[&apos;id&apos;]
    
    try:
        # Download the image
        img_response = requests.get(img_url)
        if img_response.status_code == 200:
            img = Image.open(BytesIO(img_response.content))
            img_path = os.path.join(output_dir, f&quot;{img_id}.png&quot;)
            img.save(img_path)
        
        # Write the caption to a text file
        caption_path = os.path.join(output_dir, f&quot;{img_id}.txt&quot;)
        with open(caption_path, &apos;w&apos;) as caption_file:
            caption_file.write(caption)
    except Exception as e:
        print(f&quot;Failed to download or save data for id {img_id}: {e}&quot;)

# Run the download in parallel
with ThreadPoolExecutor() as executor:
    executor.map(download_and_save, [row for _, row in df.iterrows()])
```

This script will simultaneously download the images from Pexels and write their captions into the `train/` directory as a txt file.

&gt; ⚠️ This dataset is extremely large, and will consume more than 7TB of local disk space to retrieve as-is. It&apos;s recommended that you add a resize step to this retrieval, if you don&apos;t wish to store the whole 20 megapixel dataset.

## Dataloader configuration example

```json
{
    &quot;id&quot;: &quot;photo-concept-bucket&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/training/photo-concept-bucket/train&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;center&quot;,
    &quot;resolution&quot;: 1.0,
    &quot;minimum_image_size&quot;: 1.0,
    &quot;maximum_image_size&quot;: 1.5,
    &quot;target_downsample_size&quot;: 1.25,
    &quot;resolution_type&quot;: &quot;area&quot;,
    &quot;cache_dir_vae&quot;: &quot;/home/user/training/photo-concept-bucket/cache/vae&quot;,
    &quot;caption_strategy&quot;: &quot;parquet&quot;,
    &quot;metadata_backend&quot;: &quot;parquet&quot;,
    &quot;parquet&quot;: {
        &quot;path&quot;: &quot;/home/user/training/photo-concept-bucket/photo-concept-bucket.parquet&quot;,
        &quot;caption_column&quot;: &quot;cogvlm_caption&quot;,
        &quot;fallback_caption_column&quot;: &quot;tags&quot;,
        &quot;filename_column&quot;: &quot;id&quot;,
        &quot;width_column&quot;: &quot;width&quot;,
        &quot;height_column&quot;: &quot;height&quot;
    }
}
```</file><file path="documentation/data_presets/preset.md"># Dataset Name

## Details

- **Hub link**: ...
- **Description**: ...
- **Caption format(s)**: ...

## (optional) Required preprocessing steps

If your dataset requires any steps to prepare it for use in SimpleTuner, those should be listed here, along with any code snippets that would help.

## Dataloader configuration example

Here, you&apos;ll place the `multidatabackend.json` contents that will work for this dataset.

```json
...
```</file><file path="documentation/data_presets/README.md"># Dataset configuration presets

For various large-scale datasets on Hugging Face Hub, configuration details are provided here to give a head start on making things work.

To add a new preset, use [this template](/documentation/data_presets/preset.md) to submit a new pull-request.

- [DALLE-3 1M](/documentation/data_presets/preset_dalle3.md)
- [ptx0/photo-concept-bucket](/documentation/data_presets/preset_pexels.md)
- [Midjourney v6 520k](/documentation/data_presets/preset_midjourney.md)
- [Nijijourney v6 520k](/documentation/data_presets/preset_nijijourney.md)</file><file path="documentation/evaluation/CLIP_SCORES.md"># CLIP score tracking

CLIP scores are loosely related to measurement of a model&apos;s ability to follow prompts; it is not at all related to image quality/fidelity.

The `clip/mean` score of your model indicates how closely the features extracted from the image align with the features extracted from the prompt. It is currently a popular metric for determining general prompt adherence, though is typically evaluated across a very large (~5,000) number of test prompts (eg. Parti Prompts).

CLIP score generation during model pretraining can help demonstrate that the model is approaching its objective, but once a `clip/mean` value around `.30` to `.39` is reached, the comparison seems to become less meaningful. Models that show an average CLIP score around `.33` can outperform a model with an average CLIP score of `.36` in human analysis. However, a model with a very low average CLIP score around `0.18` to `0.22` will probably be pretty poorly-performing.

Within a single test run, some prompts will result in a very low CLIP score of around `0.14` (`clip/min` value in the tracker charts) even though their images align fairly well with the user prompt and have high image quality; conversely, CLIP scores as high as `0.39` (`clip/max` value in the tracker charts) may appear from images with questionable quality, as this test is not meant to capture this information. This is why such a large number of prompts are typically used to measure model performance - _and even then_..

On its own, CLIP scores do not take long to calculate; however, the number of prompts required for meaningful evaluation can make it take an incredibly long time.

Since it doesn&apos;t take much to run, it doesn&apos;t hurt to include CLIP evaluation in small training runs. Perhaps you will discover a pattern of the outputs where it makes sense to abandon a training run or adjust other hyperparameters such as the learning rate.

To include a standard prompt library for evaluation, `--validation_prompt_library` can be provided and then we will generate a somewhat relative benchmark between training runs.

In `config.json`:

```json
{
  ...
  &quot;evaluation_type&quot;: &quot;clip&quot;,
  &quot;pretrained_evaluation_model_name_or_path&quot;: &quot;openai/clip-vit-large-patch14-336&quot;,
  &quot;report_to&quot;: &quot;tensorboard&quot;, # or wandb
  ...
}
```

## Compatibility

SageAttention is currently not compatible with CLIP score tracking. One or the other must be disabled.</file><file path="documentation/evaluation/EVAL_LOSS.md">An experimental feature in SimpleTuner implements the ideas behind [&quot;Demystifying SD fine-tuning&quot;](https://github.com/spacepxl/demystifying-sd-finetuning) to provide a stable loss value for evaluation.

Due to its experimental nature, it may cause problems or lack functionality / integration that a fully finalised feature might have.

It is fine to use this feature in production, but beware of the potential for bugs or changes in future versions.

Example dataloader:

```json
[
    {
        &quot;id&quot;: &quot;something-special-to-remember-by&quot;,
        &quot;crop&quot;: false,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/datasets/pseudo-camera-10k/train&quot;,
        &quot;minimum_image_size&quot;: 512,
        &quot;maximum_image_size&quot;: 1536,
        &quot;target_downsample_size&quot;: 512,
        &quot;resolution&quot;: 512,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;caption_strategy&quot;: &quot;filename&quot;,
        &quot;cache_dir_vae&quot;: &quot;cache/vae/sana&quot;,
        &quot;vae_cache_clear_each_epoch&quot;: false,
        &quot;skip_file_discovery&quot;: &quot;&quot;
    },
    {
        &quot;id&quot;: &quot;sana-eval&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;eval&quot;,
        &quot;instance_data_dir&quot;: &quot;/datasets/test_datasets/squares&quot;,
        &quot;resolution&quot;: 1024,
        &quot;minimum_image_size&quot;: 1024,
        &quot;maximum_image_size&quot;: 1024,
        &quot;target_downsample_size&quot;: 1024,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;cache_dir_vae&quot;: &quot;cache/vae/sana-eval&quot;,
        &quot;caption_strategy&quot;: &quot;filename&quot;
    },
    {
        &quot;id&quot;: &quot;text-embed-cache&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;type&quot;: &quot;local&quot;,
        &quot;cache_dir&quot;: &quot;cache/text/sana&quot;
    }
]
```

- Eval image datasets can be configured exactly like a normal image dataset.
- The evaluation dataset is **not** used for training.
- It&apos;s recommended to use images that represent concepts outside of your training set.

To configure and enable evaluation loss calculations:

```json
{
    &quot;--eval_steps_interval&quot;: 10,
    &quot;--num_eval_images&quot;: 1,
    &quot;--report_to&quot;: &quot;wandb&quot;,
}
```

&gt; **Note**: Weights &amp; Biases (wandb) is currently required for the full evaluation charting functionality. Other trackers only receive the single mean value.</file><file path="documentation/quickstart/FLUX.md">## Flux[dev] / Flux[schnell] Quickstart

![image](https://github.com/user-attachments/assets/6409d790-3bb4-457c-a4b4-a51a45fc91d1)

In this example, we&apos;ll be training a Flux.1 LoRA.

### Hardware requirements

Flux requires a lot of **system RAM** in addition to GPU memory. Simply quantising the model at startup requires about 50GB of system memory. If it takes an excessively long time, you may need to assess your hardware&apos;s capabilities and whether any changes are needed.

When you&apos;re training every component of a rank-16 LoRA (MLP, projections, multimodal blocks), it ends up using:
- a bit more than 30G VRAM when not quantising the base model
- a bit more than 18G VRAM when quantising to int8 + bf16 base/LoRA weights
- a bit more than 13G VRAM when quantising to int4 + bf16 base/LoRA weights
- a bit more than 9G VRAM when quantising to NF4 + bf16 base/LoRA weights
- a bit more than 9G VRAM when quantising to int2 + bf16 base/LoRA weights

You&apos;ll need: 
- **the absolute minimum** is a single **3080 10G**
- **a realistic minimum** is a single 3090 or V100 GPU
- **ideally** multiple 4090, A6000, L40S, or better

Luckily, these are readily available through providers such as [LambdaLabs](https://lambdalabs.com) which provides the lowest available rates, and localised clusters for multi-node training.

**Unlike other models, Apple GPUs do not currently work for training Flux.**

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

# if python --version shows 3.11 you can just also use the &apos;python&apos; command here.
python3.11 -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

**Note:** We&apos;re currently installing the `release` branch here; the `main` branch may contain experimental features that might have better results or lower memory use.

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.


If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

There, you will possibly need to modify the following variables:

- `model_type` - Set this to `lora`.
- `model_family` - Set this to `flux`.
- `pretrained_model_name_or_path` - Set this to `black-forest-labs/FLUX.1-dev`.
- `pretrained_vae_model_name_or_path` - Set this to `black-forest-labs/FLUX.1-dev`.
  - Note that you will need to log in to Huggingface and be granted access to download this model. We will go over logging in to Huggingface later in this tutorial.
- `output_dir` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `train_batch_size` - this should be kept at 1, especially if you have a very small dataset.
- `validation_resolution` - As Flux is a 1024px model, you can set this to `1024x1024`.
  - Additionally, Flux was fine-tuned on multi-aspect buckets, and other resolutions may be specified using commas to separate them: `1024x1024,1280x768,2048x2048`
- `validation_guidance` - Use whatever you are used to selecting at inference time for Flux.
- `validation_guidance_real` - Use &gt;1.0 to use CFG for flux inference. Slows validations down, but produces better results. Does best with an empty `VALIDATION_NEGATIVE_PROMPT`.
- `validation_num_inference_steps` - Use somewhere around 20 to save time while still seeing decent quality. Flux isn&apos;t very diverse, and more steps might just waste time.
- `--lora_rank=4` if you wish to substantially reduce the size of the LoRA being trained. This can help with VRAM use.
- If training a Schnell LoRA, you&apos;ll have to supply `--flux_fast_schedule=true` manually here as well.

- `gradient_accumulation_steps` - Previous guidance was to avoid these with bf16 training since they would degrade the model. Further testing showed this is not necessarily the case for Flux.
  - This option causes update steps to be accumulated over several steps. This will increase the training runtime linearly, such that a value of 2 will make your training run half as quickly, and take twice as long.
- `optimizer` - Beginners are recommended to stick with adamw_bf16, though optimi-lion and optimi-stableadamw are also good choices.
- `mixed_precision` - Beginners should keep this in `bf16`
- `gradient_checkpointing` - set this to true in practically every situation on every device
- `gradient_checkpointing_interval` - this could be set to a value of 2 or higher on larger GPUs to only checkpoint every _n_ blocks. A value of 2 would checkpoint half of the blocks, and 3 would be one-third.

Multi-GPU users can reference [this document](/OPTIONS.md#environment-configuration-variables) for information on configuring the number of GPUs to use.

#### Validation prompts

Inside `config/config.json` is the &quot;primary validation prompt&quot;, which is typically the main instance_prompt you are training on for your single subject or style. Additionally, a JSON file may be created that contains extra prompts to run through during validations.

The example config file `config/user_prompt_library.json.example` contains the following format:

```json
{
  &quot;nickname&quot;: &quot;the prompt goes here&quot;,
  &quot;another_nickname&quot;: &quot;another prompt goes here&quot;
}
```

The nicknames are the filename for the validation, so keep them short and compatible with your filesystem.

To point the trainer to this prompt library, add it to TRAINER_EXTRA_ARGS by adding a new line at the end of `config.json`:
```json
  &quot;--user_prompt_library&quot;: &quot;config/user_prompt_library.json&quot;,
```

A set of diverse prompt will help determine whether the model is collapsing as it trains. In this example, the word `&lt;token&gt;` should be replaced with your subject name (instance_prompt).

```json
{
    &quot;anime_&lt;token&gt;&quot;: &quot;a breathtaking anime-style portrait of &lt;token&gt;, capturing her essence with vibrant colors and expressive features&quot;,
    &quot;chef_&lt;token&gt;&quot;: &quot;a high-quality, detailed photograph of &lt;token&gt; as a sous-chef, immersed in the art of culinary creation&quot;,
    &quot;just_&lt;token&gt;&quot;: &quot;a lifelike and intimate portrait of &lt;token&gt;, showcasing her unique personality and charm&quot;,
    &quot;cinematic_&lt;token&gt;&quot;: &quot;a cinematic, visually stunning photo of &lt;token&gt;, emphasizing her dramatic and captivating presence&quot;,
    &quot;elegant_&lt;token&gt;&quot;: &quot;an elegant and timeless portrait of &lt;token&gt;, exuding grace and sophistication&quot;,
    &quot;adventurous_&lt;token&gt;&quot;: &quot;a dynamic and adventurous photo of &lt;token&gt;, captured in an exciting, action-filled moment&quot;,
    &quot;mysterious_&lt;token&gt;&quot;: &quot;a mysterious and enigmatic portrait of &lt;token&gt;, shrouded in shadows and intrigue&quot;,
    &quot;vintage_&lt;token&gt;&quot;: &quot;a vintage-style portrait of &lt;token&gt;, evoking the charm and nostalgia of a bygone era&quot;,
    &quot;artistic_&lt;token&gt;&quot;: &quot;an artistic and abstract representation of &lt;token&gt;, blending creativity with visual storytelling&quot;,
    &quot;futuristic_&lt;token&gt;&quot;: &quot;a futuristic and cutting-edge portrayal of &lt;token&gt;, set against a backdrop of advanced technology&quot;,
    &quot;woman&quot;: &quot;a beautifully crafted portrait of a woman, highlighting her natural beauty and unique features&quot;,
    &quot;man&quot;: &quot;a powerful and striking portrait of a man, capturing his strength and character&quot;,
    &quot;boy&quot;: &quot;a playful and spirited portrait of a boy, capturing youthful energy and innocence&quot;,
    &quot;girl&quot;: &quot;a charming and vibrant portrait of a girl, emphasizing her bright personality and joy&quot;,
    &quot;family&quot;: &quot;a heartwarming and cohesive family portrait, showcasing the bonds and connections between loved ones&quot;
}
```

&gt; ℹ️ Flux is a flow-matching model and shorter prompts that have strong similarities will result in practically the same image being produced by the model. Be sure to use longer, more descriptive prompts.

#### CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.

#### Flux time schedule shifting

Flow-matching models such as Flux and SD3 have a property called &quot;shift&quot; that allows us to shift the trained portion of the timestep schedule using a simple decimal value.

##### Defaults
By default, no schedule shift is applied to flux, which results in a sigmoid bell-shape to the timestep sampling distribution. This is unlikely to be the ideal approach for Flux, but it results in a greater amount of learning in a shorter period of time than auto-shift.

##### Auto-shift
A commonly-recommended approach is to follow several recent works and enable resolution-dependent timestep shift, `--flow_schedule_auto_shift` which uses higher shift values for larger images, and lower shift values for smaller images. This results in stable but potentially mediocre training results.

##### Manual specification
_Thanks to General Awareness from Discord for the following examples_

When using a `--flow_schedule_shift` value of 0.1 (a very low value), only the finer details of the image are affected:
![image](https://github.com/user-attachments/assets/991ca0ad-e25a-4b13-a3d6-b4f2de1fe982)

When using a `--flow_schedule_shift` value of 4.0 (a very high value), the large compositional features and potentially colour space of the model becomes impacted:
![image](https://github.com/user-attachments/assets/857a1f8a-07ab-4b75-8e6a-eecff616a28d)


#### Quantised model training

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements, training Flux on just 16GB.

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

For `config.json` users:
```json
  &quot;base_model_precision&quot;: &quot;int8-quanto&quot;,
  &quot;text_encoder_1_precision&quot;: &quot;no_change&quot;,
  &quot;text_encoder_2_precision&quot;: &quot;no_change&quot;,
  &quot;lora_rank&quot;: 16,
  &quot;max_grad_norm&quot;: 1.0,
  &quot;base_model_default_dtype&quot;: &quot;bf16&quot;
```

##### LoRA-specific settings (not LyCORIS)


```bash
# When training &apos;mmdit&apos;, we find very stable training that makes the model take longer to learn.
# When training &apos;all&apos;, we can easily shift the model distribution, but it is more prone to forgetting and benefits from high quality data.
# When training &apos;all+ffs&apos;, all attention layers are trained in addition to the feed-forward which can help with adapting the model objective for the LoRA.
# - This mode has been reported to lack portability, and platforms such as ComfyUI might not be able to load the LoRA.
# The option to train only the &apos;context&apos; blocks is offered as well, but its impact is unknown, and is offered as an experimental choice.
# - An extension to this mode, &apos;context+ffs&apos; is also available, which is useful for pretraining new tokens into a LoRA before continuing finetuning it via `--init_lora`.
# Other options include &apos;tiny&apos; and &apos;nano&apos; which train just 1 or 2 layers.
&quot;--flux_lora_target&quot;: &quot;all&quot;,

# If you want to use LoftQ initialisation, you can&apos;t use Quanto to quantise the base model.
# This possibly offers better/faster convergence, but only works on NVIDIA devices and requires Bits n Bytes and is incompatible with Quanto.
# Other options are &apos;default&apos;, &apos;gaussian&apos; (difficult), and untested options: &apos;olora&apos; and &apos;pissa&apos;.
&quot;--lora_init_type&quot;: &quot;loftq&quot;,
```


#### Dataset considerations

&gt; ⚠️ Image quality for training is more important for Flux than for most other models, as it will absorb the artifacts in your images *first*, and then learn the concept/subject.

It&apos;s crucial to have a substantial dataset to train your model on. There are limitations on the dataset size, and you will need to ensure that your dataset is large enough to train your model effectively. Note that the bare minimum dataset size is `train_batch_size * gradient_accumulation_steps` as well as more than `vae_batch_size`. The dataset will not be useable if it is too small.

&gt; ℹ️ With few enough images, you might see a message **no images detected in dataset** - increasing the `repeats` value will overcome this limitation.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently. In this example, we will be using [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) as the dataset.

Create a `--data_backend_config` (`config/multidatabackend.json`) document containing this:

```json
[
  {
    &quot;id&quot;: &quot;pseudo-camera-10k-flux&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;center&quot;,
    &quot;resolution&quot;: 512,
    &quot;minimum_image_size&quot;: 512,
    &quot;maximum_image_size&quot;: 512,
    &quot;target_downsample_size&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/flux/pseudo-camera-10k&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;skip_file_discovery&quot;: &quot;&quot;,
    &quot;caption_strategy&quot;: &quot;filename&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 0,
    &quot;is_regularisation_data&quot;: true
  },
  {
    &quot;id&quot;: &quot;dreambooth-subject&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 1024,
    &quot;minimum_image_size&quot;: 1024,
    &quot;maximum_image_size&quot;: 1024,
    &quot;target_downsample_size&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/flux/dreambooth-subject&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/dreambooth-subject&quot;,
    &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
    &quot;instance_prompt&quot;: &quot;the name of your subject goes here&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 1000
  },
  {
    &quot;id&quot;: &quot;dreambooth-subject-512&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 512,
    &quot;minimum_image_size&quot;: 512,
    &quot;maximum_image_size&quot;: 512,
    &quot;target_downsample_size&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/flux/dreambooth-subject-512&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/dreambooth-subject&quot;,
    &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
    &quot;instance_prompt&quot;: &quot;the name of your subject goes here&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 1000
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/flux&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

&gt; ℹ️ Running 512px and 1024px datasets concurrently is supported, and could result in better convergence for Flux.

Then, create a `datasets` directory:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
    mkdir dreambooth-subject
    # place your images into dreambooth-subject/ now
popd
```

This will download about 10k photograph samples to your `datasets/pseudo-camera-10k` directory, which will be automatically created for you.

Your Dreambooth images should go into the `datasets/dreambooth-subject` directory.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `--push_to_hub` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
./train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

**Note:** It&apos;s unclear whether training on multi-aspect buckets works correctly for Flux at the moment. It&apos;s recommended to use `crop_style=random` and `crop_aspect=square`.

## Inference tips

### CFG-trained LoRAs (flux_guidance_value &gt; 1)

In ComfyUI, you&apos;ll need to put Flux through another node called AdaptiveGuider. One of the members from our community has provided a modified node here:

(**external links**) [IdiotSandwichTheThird/ComfyUI-Adaptive-Guidan...](https://github.com/IdiotSandwichTheThird/ComfyUI-Adaptive-Guidance-with-disabled-init-steps) and their example workflow [here](https://github.com/IdiotSandwichTheThird/ComfyUI-Adaptive-Guidance-with-disabled-init-steps/blob/master/ExampleWorkflow.json)

### CFG-distilled LoRA (flux_guidance_scale == 1)

Inferencing the CFG-distilled LoRA is as easy as using a lower guidance_scale around the value trained with.


## Notes &amp; troubleshooting tips

### Lowest VRAM config

Currently, the lowest VRAM utilisation (9090M) can be attained with:

- OS: Ubuntu Linux 24
- GPU: A single NVIDIA CUDA device (10G, 12G)
- System memory: 50G of system memory approximately
- Base model precision: `bnb-nf4`
- Optimiser: Lion 8Bit Paged, `bnb-lion8bit-paged`
- Resolution: 512px
  - 1024px requires &gt;= 12G VRAM
- Batch size: 1, zero gradient accumulation steps
- DeepSpeed: disabled / unconfigured
- PyTorch: 2.6 Nightly (Sept 29th build)
- Using `--quantize_via=cpu` to avoid outOfMemory error during startup on &lt;=16G cards.
- With `--attention_mechanism=sageattention` to further reduce VRAM by 0.1GB and improve training validation image generation speed.
- Be sure to enable `--gradient_checkpointing` or nothing you do will stop it from OOMing

**NOTE**: Pre-caching of VAE embeds and text encoder outputs may use more memory and still OOM. If so, text encoder quantisation and VAE tiling can be enabled.

Speed was approximately 1.4 iterations per second on a 4090.

### SageAttention

When using `--attention_mechanism=sageattention`, inference can be sped-up at validation time.

**Note**: This isn&apos;t compatible with _every_ model configuration, but it&apos;s worth trying.

### NF4-quantised training

In simplest terms, NF4 is a 4bit-_ish_ representation of the model, which means training has serious stability concerns to address.

In early tests, the following holds true:
- Lion optimiser causes model collapse but uses least VRAM; AdamW variants help to hold it together; bnb-adamw8bit, adamw_bf16 are great choices
  - AdEMAMix didn&apos;t fare well, but settings were not explored
- `--max_grad_norm=0.01` further helps reduce model breakage by preventing huge changes to the model in too short a time
- NF4, AdamW8bit, and a higher batch size all help to overcome the stability issues, at the cost of more time spent training or VRAM used
- Upping the resolution from 512px to 1024px slows training down from, for example, 1.4 seconds per step to 3.5 seconds per step (batch size of 1, 4090)
- Anything that&apos;s difficult to train on int8 or bf16 becomes harder in NF4
- It&apos;s less compatible with options like SageAttention

NF4 does not work with torch.compile, so whatever you get for speed is what you get.

If VRAM is not a concern (eg. 48G or greater) then int8 with torch.compile is your best, fastest option.

### Masked loss

If you are training a subject or style and would like to mask one or the other, see the [masked loss training](/documentation/DREAMBOOTH.md#masked-loss) section of the Dreambooth guide.

### Classifier-free guidance

#### Problem
The Dev model arrives guidance-distilled out of the box, which means it does a very straight shot trajectory to the teacher model outputs. This is done through a guidance vector that is fed into the model at training and inference time - the value of this vector greatly impacts what type of resulting LoRA you end up with:

#### Solution
- A value of 1.0 (**the default**) will preserve the initial distillation done to the Dev model
  - This is the most compatible mode
  - Inference is just as fast as the original model
  - Flow-matching distillation reduces the creativity and output variability of the model, as with the original Flux Dev model (everything keeps the same composition/look)
- A higher value (tested around 3.5-4.5) will reintroduce the CFG objective into the model
  - This requires the inference pipeline to have support for CFG
  - Inference is 50% slower and 0% VRAM increase **or** about 20% slower and 20% VRAM increase due to batched CFG inference
  - However, this style of training improves creativity and model output variability, which might be required for certain training tasks

We can partially reintroduce distillation to a de-distilled model by continuing tuning your model using a vector value of 1.0. It will never fully recover, but it&apos;ll at least be more useable.

#### Caveats
- This has the end impact of **either**:
  - Increasing inference latency by 2x when we sequentially calculate the unconditional output, eg. with two separate forward pass
  - Increasing the VRAM consumption equivalently to using `num_images_per_prompt=2` and receiving two images at inference time, accompanied by the same percent slowdown.
    - This is often less extreme slowdown than sequential computation, but the VRAM use might be too much for most consumer training hardware.
    - This method is not *currently* integrated into SimpleTuner, but work is ongoing.
- Inference workflows for ComfyUI or other applications (eg. AUTOMATIC1111) will need to be modified to also enable &quot;true&quot; CFG, which might not be currently possible out of the box.

### Quantisation
- Minimum 8bit quantisation is required for a 16G card to train this model
  - In bfloat16/float16, a rank-1 LoRA sits at just over 30GB of mem use
- Quantising the model to 8bit doesn&apos;t harm training
  - It allows you to push higher batch sizes and possibly obtain a better result
  - Behaves the same as full-precision training - fp32 won&apos;t make your model any better than bf16+int8.
- **int8** has hardware acceleration and `torch.compile()` support on newer NVIDIA hardware (3090 or better)
- **nf4-bnb** brings VRAM requirements down to 9GB, fitting on a 10G card (with bfloat16 support)
- When loading the LoRA in ComfyUI later, you **must** use the same base model precision as you trained your LoRA on.
- **int4** is relies on custom bf16 kernels, and will not work if your card does not support bfloat16

### Crashing
- If you get SIGKILL after the text encoders are unloaded, this means you do not have enough system memory to quantise Flux.
  - Try loading the `--base_model_precision=bf16` but if that does not work, you might just need more memory..
  - Try `--quantize_via=accelerator` to use the GPU instead

### Schnell
- If you train a LyCORIS LoKr on Dev, it **generally** works very well on Schnell at just 4 steps later.
  - Direct Schnell training really needs a bit more time in the oven - currently, the results do not look good

&gt; ℹ️ When merging Schnell with Dev in any way, the license of Dev takes over and it becomes non-commercial. This shouldn&apos;t really matter for most users, but it&apos;s worth noting.

### Learning rates

#### LoRA (--lora_type=standard)
- LoRA has overall worse performance than LoKr for larger datasets
- It&apos;s been reported that Flux LoRA trains similarly to SD 1.5 LoRAs
- However, a model as large as 12B has empirically performed better with **lower learning rates.**
  - LoRA at 1e-3 might totally roast the thing. LoRA at 1e-5 does nearly nothing.
- Ranks as large as 64 through 128 might be undesirable on a 12B model due to general difficulties that scale up with the size of the base model.
  - Try a smaller network first (rank-1, rank-4) and work your way up - they&apos;ll train faster, and might do everything you need.
  - If you&apos;re finding that it&apos;s excessively difficult to train your concept into the model, you might need a higher rank and more regularisation data.
- Other diffusion transformer models like PixArt and SD3 majorly benefit from `--max_grad_norm` and SimpleTuner keeps a pretty high value for this by default on Flux.
  - A lower value would keep the model from falling apart too soon, but can also make it very difficult to learn new concepts that venture far from the base model data distribution. The model might get stuck and never improve.
#### LoKr (--lora_type=lycoris)
- Higher learning rates are better for LoKr (`1e-3` with AdamW, `2e-4` with Lion)
- Other algo need more exploration.
- Setting `is_regularisation_data` on such datasets may help preserve / prevent bleed and improve the final resulting model&apos;s quality.
  - This behaves differently from &quot;prior loss preservation&quot; which is known for doubling training batch sizes and not improving the result much
  - SimpleTuner&apos;s regularisation data implementation provides an efficient manner of preserving the base model

### Image artifacts
Flux will immediately absorb bad image artifacts. It&apos;s just how it is - a final training run on just high quality data may be required to fix it at the end.

When you do these things (among others), some square grid artifacts **may** begin appearing in the samples:
- Overtrain with low quality data
- Use too high of a learning rate
- Overtraining (in general), a low-capacity network with too many images
- Undertraining (also), a high-capacity network with too few images
- Using weird aspect ratios or training data sizes

### Aspect bucketing
- Training for too long on square crops probably won&apos;t damage this model too much. Go nuts, it&apos;s great and reliable.
- On the other hand, using the natural aspect buckets of your dataset might overly bias these shapes during inference time.
  - This could be a desirable quality, as it keeps aspect-dependent styles like cinematic stuff from bleeding into other resolutions too much.
  - However, if you&apos;re looking to improve results equally across many aspect buckets, you might have to experiment with `crop_aspect=random` which comes with its own downsides.
- Mixing dataset configurations by defining your image directory dataset multiple times has produced really good results and a nicely generalised model.

### Training custom fine-tuned Flux models

Some fine-tuned Flux models on Hugging Face Hub (such as Dev2Pro) lack the full directory structure, requiring these specific options be set.

Make sure to set these options `flux_guidance_value`,  `validation_guidance_real` and `flux_attention_masked_training` according to the way the creator did as well if that information is available. 
```json
{
    &quot;model_family&quot;: &quot;flux&quot;,
    &quot;pretrained_model_name_or_path&quot;: &quot;black-forest-labs/FLUX.1-dev&quot;,
    &quot;pretrained_transformer_model_name_or_path&quot;: &quot;ashen0209/Flux-Dev2Pro&quot;,
    &quot;pretrained_vae_model_name_or_path&quot;: &quot;black-forest-labs/FLUX.1-dev&quot;,
    &quot;pretrained_transformer_subfolder&quot;: &quot;none&quot;,
}
```

## Credits

The users of [Terminus Research](https://huggingface.co/terminusresearch) who worked on this probably more than their day jobs to figure it out

[Lambda Labs](https://lambdalabs.com) for generous compute allocations that were used for tests and verifications for large scale training runs

Especially [@JimmyCarter](https://huggingface.co/jimmycarter) and [@kaibioinfo](https://github.com/kaibioinfo) for coming up with some of the best ideas and putting them into action, offering pull requests and running exhaustive tests for analysis - even daring to use _their own faces_ for DreamBooth experimentation.</file><file path="documentation/quickstart/KOLORS.md">## Kwai Kolors Quickstart

In this example, we&apos;ll be training a Kwai Kolors model using the SimpleTuner toolkit and will be using the `lora` model type.

Kolors is roughly the same size as SDXL, so you can try `full` training, but the changes for that are not described in this quickstart guide.

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

python -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### Removing DeepSpeed &amp; Bits n Bytes

These two dependencies cause numerous issues for container hosts such as RunPod and Vast.

To remove them after `poetry` has installed them, run the following command in the same terminal:

```bash
pip uninstall -y deepspeed bitsandbytes
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```
&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.

If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

There, you will need to modify the following variables:

```json
{
  &quot;model_type&quot;: &quot;lora&quot;,
  &quot;model_family&quot;: &quot;kolora&quot;,
  &quot;pretrained_model_name_or_path&quot;: &quot;Kwai-Kolors/Kolors-diffusers&quot;,
  &quot;output_dir&quot;: &quot;/home/user/output/models&quot;,
  &quot;validation_resolution&quot;: &quot;1024x1024,1280x768&quot;,
  &quot;validation_guidance&quot;: 3.4,
  &quot;use_gradient_checkpointing&quot;: true,
  &quot;learning_rate&quot;: 1e-4
}
```

- `pretrained_model_name_or_path` - Set this to `Kwai-Kolors/Kolors-diffusers`.
- `MODEL_TYPE` - Set this to `lora`.
- `USE_DORA` - Set this to `true` if you wish to train DoRA.
- `MODEL_FAMILY` - Set this to `kolors`.
- `OUTPUT_DIR` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `VALIDATION_RESOLUTION` - Set this to `1024x1024` for this example.
  - Additionally, Kolors was fine-tuned on multi-aspect buckets, and other resolutions may be specified using commas to separate them: `1024x1024,1280x768`
- `VALIDATION_GUIDANCE` - Use whatever value you are comfortable with for testing at inference time. Set this between `4.2` to `6.4`.
- `USE_GRADIENT_CHECKPOINTING` - This should probably be `true` unless you have a LOT of VRAM and want to sacrifice some to make it go faster.
- `LEARNING_RATE` - `1e-4` is fairly common for low-rank networks, though `1e-5` might be a more conservative choice if you notice any &quot;burning&quot; or early overtraining.

There are a few more if using a Mac M-series machine:

- `mixed_precision` should be set to `no`.
- `USE_XFORMERS` should be set to `false`.

#### Quantised model training

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements of especially ChatGLM 6B (the text encoder).

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

For `config.json`:

```json
{
  &quot;base_model_precision&quot;: &quot;int8-quanto&quot;,
  &quot;text_encoder_1_precision&quot;: &quot;no_change&quot;,
  &quot;optimizer&quot;: &quot;adamw_bf16&quot;
}
```

For `config.env` users (deprecated):

```bash
# choices: int8-quanto, int4-quanto, int2-quanto, fp8-quanto
# int8-quanto was tested with a single subject dreambooth LoRA.
# fp8-quanto does not work on Apple systems. you must use int levels.
# int2-quanto is pretty extreme and gets the whole rank-1 LoRA down to about 13.9GB VRAM.
# may the gods have mercy on your soul, should you push things Too Far.
export TRAINER_EXTRA_ARGS=&quot;--base_model_precision=int8-quanto&quot;

# Maybe you want the text encoders to remain full precision so your text embeds are cake.
# We unload the text encoders before training, so, that&apos;s not an issue during training time - only during pre-caching.
# Alternatively, you can go ham on quantisation here and run them in int4 or int8 mode, because no one can stop you.
export TRAINER_EXTRA_ARGS=&quot;${TRAINER_EXTRA_ARGS} --text_encoder_1_precision=no_change&quot;

# When you&apos;re quantising the model, --base_model_default_dtype is set to bf16 by default. This setup requires adamw_bf16, but saves the most memory.
# adamw_bf16 only supports bf16 training, but any other optimiser will support both bf16 or fp32 training precision.
export OPTIMIZER=&quot;adamw_bf16&quot;
```

#### Dataset considerations

It&apos;s crucial to have a substantial dataset to train your model on. There are limitations on the dataset size, and you will need to ensure that your dataset is large enough to train your model effectively. Note that the bare minimum dataset size is `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`. The dataset will not be discoverable by the trainer if it is too small.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently. In this example, we will be using [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) as the dataset.

In your `OUTPUT_DIR` directory, create a multidatabackend.json:

```json
[
  {
    &quot;id&quot;: &quot;pseudo-camera-10k-kolors&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;random&quot;,
    &quot;resolution&quot;: 1.0,
    &quot;minimum_image_size&quot;: 0.25,
    &quot;maximum_image_size&quot;: 1.0,
    &quot;target_downsample_size&quot;: 1.0,
    &quot;resolution_type&quot;: &quot;area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/kolors/pseudo-camera-10k&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/datasets/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;skip_file_discovery&quot;: &quot;&quot;,
    &quot;caption_strategy&quot;: &quot;filename&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/kolors/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

Then, create a `datasets` directory:

```bash
mkdir -p datasets
huggingface-cli download --repo-type=dataset ptx0/pseudo-camera-10k --local-dir=datasets/pseudo-camera-10k
```

This will download about 10k photograph samples to your `datasets/pseudo-camera-10k` directory, which will be automatically created for you.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `push_to_hub: true` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
bash train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

### CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.</file><file path="documentation/quickstart/LTXVIDEO.md">## LTX Video Quickstart

In this example, we&apos;ll be training an LTX-Video LoRA using Sayak Paul&apos;s [public domain Disney dataset](https://hf.co/datasets/sayakpaul/video-dataset-disney-organized).

### Hardware requirements

LTX does not require much system **or** GPU memory.

When you&apos;re training every component of a rank-16 LoRA (MLP, projections, multimodal blocks), it ends up using a bit more than 12G on an M3 Mac (batch size 4).

You&apos;ll need: 
- **a realistic minimum** is 16GB or, a single 3090 or V100 GPU
- **ideally** multiple 4090, A6000, L40S, or better

Apple silicon systems work great with LTX so far, albeit at a lower resolution due to limits inside the MPS backend used by Pytorch.

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

# if python --version shows 3.11 you can just also use the &apos;python&apos; command here.
python3.11 -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

**Note:** We&apos;re currently installing the `release` branch here; the `main` branch may contain experimental features that might have better results or lower memory use.

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.


If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

There, you will possibly need to modify the following variables:

- `model_type` - Set this to `lora`.
- `model_family` - Set this to `ltxvideo`.
- `pretrained_model_name_or_path` - Set this to `Lightricks/LTX-Video`.
- `pretrained_vae_model_name_or_path` - Set this to `Lightricks/LTX-Video`.
- `output_dir` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `train_batch_size` - this can be increased for more stability, but a value of 4 should work alright to start with
- `validation_resolution` - This should be set to whatever you typically generate videos with when using LTX (`768x512`)
  - Multiple resolutions may be specified using commas to separate them: `1280x768,768x512`
- `validation_guidance` - Use whatever you are used to selecting at inference time for LTX.
- `validation_num_inference_steps` - Use somewhere around 25 to save time while still seeing decent quality.
- `--lora_rank=4` if you wish to substantially reduce the size of the LoRA being trained. This can help with VRAM use while reducing its capacity for learning.

- `gradient_accumulation_steps` - This option causes update steps to be accumulated over several steps.
  - This will increase the training runtime linearly, such that a value of 2 will make your training run half as quickly, and take twice as long.
- `optimizer` - Beginners are recommended to stick with adamw_bf16, though optimi-lion and optimi-stableadamw are also good choices.
- `mixed_precision` - Beginners should keep this in `bf16`
- `gradient_checkpointing` - set this to true in practically every situation on every device
- `gradient_checkpointing_interval` - this is not yet supported on LTX Video, and should be removed from your config.

Multi-GPU users can reference [this document](/OPTIONS.md#environment-configuration-variables) for information on configuring the number of GPUs to use.

#### Validation prompts

Inside `config/config.json` is the &quot;primary validation prompt&quot;, which is typically the main instance_prompt you are training on for your single subject or style. Additionally, a JSON file may be created that contains extra prompts to run through during validations.

The example config file `config/user_prompt_library.json.example` contains the following format:

```json
{
  &quot;nickname&quot;: &quot;the prompt goes here&quot;,
  &quot;another_nickname&quot;: &quot;another prompt goes here&quot;
}
```

The nicknames are the filename for the validation, so keep them short and compatible with your filesystem.

To point the trainer to this prompt library, add it to TRAINER_EXTRA_ARGS by adding a new line at the end of `config.json`:
```json
  &quot;--user_prompt_library&quot;: &quot;config/user_prompt_library.json&quot;,
```

A set of diverse prompt will help determine whether the model is collapsing as it trains. In this example, the word `&lt;token&gt;` should be replaced with your subject name (instance_prompt).

```json
{
    &quot;anime_&lt;token&gt;&quot;: &quot;a breathtaking anime-style video featuring &lt;token&gt;, capturing her essence with vibrant colors, dynamic motion, and expressive storytelling&quot;,
    &quot;chef_&lt;token&gt;&quot;: &quot;a high-quality, detailed video of &lt;token&gt; as a sous-chef, immersed in the art of culinary creation with captivating close-ups and engaging sequences&quot;,
    &quot;just_&lt;token&gt;&quot;: &quot;a lifelike and intimate video portrait of &lt;token&gt;, showcasing her unique personality and charm through nuanced movement and expression&quot;,
    &quot;cinematic_&lt;token&gt;&quot;: &quot;a cinematic, visually stunning video of &lt;token&gt;, emphasizing her dramatic and captivating presence through fluid camera movements and atmospheric effects&quot;,
    &quot;elegant_&lt;token&gt;&quot;: &quot;an elegant and timeless video portrait of &lt;token&gt;, exuding grace and sophistication with smooth transitions and refined visuals&quot;,
    &quot;adventurous_&lt;token&gt;&quot;: &quot;a dynamic and adventurous video featuring &lt;token&gt;, captured in an exciting, action-filled sequence that highlights her energy and spirit&quot;,
    &quot;mysterious_&lt;token&gt;&quot;: &quot;a mysterious and enigmatic video portrait of &lt;token&gt;, shrouded in shadows and intrigue with a narrative that unfolds in subtle, cinematic layers&quot;,
    &quot;vintage_&lt;token&gt;&quot;: &quot;a vintage-style video of &lt;token&gt;, evoking the charm and nostalgia of a bygone era through sepia tones and period-inspired visual storytelling&quot;,
    &quot;artistic_&lt;token&gt;&quot;: &quot;an artistic and abstract video representation of &lt;token&gt;, blending creativity with visual storytelling through experimental techniques and fluid visuals&quot;,
    &quot;futuristic_&lt;token&gt;&quot;: &quot;a futuristic and cutting-edge video portrayal of &lt;token&gt;, set against a backdrop of advanced technology with sleek, high-tech visuals&quot;,
    &quot;woman&quot;: &quot;a beautifully crafted video portrait of a woman, highlighting her natural beauty and unique features through elegant motion and storytelling&quot;,
    &quot;man&quot;: &quot;a powerful and striking video portrait of a man, capturing his strength and character with dynamic sequences and compelling visuals&quot;,
    &quot;boy&quot;: &quot;a playful and spirited video portrait of a boy, capturing youthful energy and innocence through lively scenes and engaging motion&quot;,
    &quot;girl&quot;: &quot;a charming and vibrant video portrait of a girl, emphasizing her bright personality and joy with colorful visuals and fluid movement&quot;,
    &quot;family&quot;: &quot;a heartwarming and cohesive family video, showcasing the bonds and connections between loved ones through intimate moments and shared experiences&quot;
}
```

&gt; ℹ️ LTX Video is a flow-matching model based on T5 XXL; shorter prompts may not have enough information for the model to do a good job. Be sure to use longer, more descriptive prompts.

#### CLIP score tracking

This should not be enabled for video model training, at the present time.

# Stable evaluation loss

This should not be enabled for video model training, at the present time.

#### Flow-matching schedule shift

Flow-matching models such as Flux, Sana, SD3, and LTX Video have a property called `shift` that allows us to shift the trained portion of the timestep schedule using a simple decimal value.

##### Defaults
By default, no schedule shift is applied, which results in a sigmoid bell-shape to the timestep sampling distribution, otherwise known as `logit_norm`.

##### Auto-shift
A commonly-recommended approach is to follow several recent works and enable resolution-dependent timestep shift, `--flow_schedule_auto_shift` which uses higher shift values for larger images, and lower shift values for smaller images. This results in stable but potentially mediocre training results.

##### Manual specification
_Thanks to General Awareness from Discord for the following examples_

&gt; ℹ️ These examples show how the value works using Flux Dev, though LTX Video should be very similar.

When using a `--flow_schedule_shift` value of 0.1 (a very low value), only the finer details of the image are affected:
![image](https://github.com/user-attachments/assets/991ca0ad-e25a-4b13-a3d6-b4f2de1fe982)

When using a `--flow_schedule_shift` value of 4.0 (a very high value), the large compositional features and potentially colour space of the model becomes impacted:
![image](https://github.com/user-attachments/assets/857a1f8a-07ab-4b75-8e6a-eecff616a28d)


#### Quantised model training

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements, training on just 16GB.

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

For `config.json` users:
```json
  &quot;base_model_precision&quot;: &quot;int8-quanto&quot;,
  &quot;text_encoder_1_precision&quot;: &quot;no_change&quot;,
  &quot;text_encoder_2_precision&quot;: &quot;no_change&quot;,
  &quot;lora_rank&quot;: 16,
  &quot;max_grad_norm&quot;: 1.0,
  &quot;base_model_default_dtype&quot;: &quot;bf16&quot;
```

#### Dataset considerations

There are few limitations on the dataset size other than how much compute and time it will take to process and train.

You must ensure that the dataset is large enough to train your model effectively, but not too large for the amount of compute you have available.

Note that the bare minimum dataset size is `train_batch_size * gradient_accumulation_steps` as well as more than `vae_batch_size`. The dataset will not be useable if it is too small.

&gt; ℹ️ With few enough samples, you might see a message **no samples detected in dataset** - increasing the `repeats` value will overcome this limitation.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently.

In this example, we will be using [video-dataset-disney-organized](https://huggingface.co/datasets/sayakpaul/video-dataset-disney-organized) as the dataset.

Create a `--data_backend_config` (`config/multidatabackend.json`) document containing this:

```json
[
  {
    &quot;id&quot;: &quot;disney-black-and-white&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;video&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 480,
    &quot;minimum_image_size&quot;: 480,
    &quot;maximum_image_size&quot;: 480,
    &quot;target_downsample_size&quot;: 480,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/ltxvideo/disney-black-and-white&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/disney-black-and-white&quot;,
    &quot;disabled&quot;: false,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 0,
    &quot;video&quot;: {
        &quot;num_frames&quot;: 125,
        &quot;min_frames&quot;: 125
    }
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/ltxvideo&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

- In the `video` subsection, we have the following keys we can set:
  - `num_frames` (optional, int) is how many seconds of data we&apos;ll train on.
    - At 25 fps, 125 frames is 5 seconds of video, standard output. This should be your target.
  - `min_frames` (optional, int) determines the minimum length of a video that will be considered for training.
    - This should be at least equal to `num_frames`. Not setting it ensures it&apos;ll be equal.
  - `max_frames` (optional, int) determines the maximum length of a video that will be considered for training.
  - `is_i2v` (optional, bool) determines whether i2v training will be done on a dataset.
    - This is set to True by default for LTX. You can disable it, however.

Then, create a `datasets` directory:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset sayakpaul/video-dataset-disney-organized --local-dir=disney-black-and-white
popd
```

This will download all of the Disney video samples to your `datasets/disney-black-and-white` directory, which will be automatically created for you.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `--push_to_hub` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
./train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

## Notes &amp; troubleshooting tips

### Lowest VRAM config

Like other models, it is possible that the lowest VRAM utilisation can be attained with:

- OS: Ubuntu Linux 24
- GPU: A single NVIDIA CUDA device (10G, 12G)
- System memory: 11G of system memory approximately
- Base model precision: `bnb-nf4`
- Optimiser: Lion 8Bit Paged, `bnb-lion8bit-paged`
- Resolution: 480px
- Batch size: 1, zero gradient accumulation steps
- DeepSpeed: disabled / unconfigured
- PyTorch: 2.6
- Be sure to enable `--gradient_checkpointing` or nothing you do will stop it from OOMing

**NOTE**: Pre-caching of VAE embeds and text encoder outputs may use more memory and still OOM. If so, text encoder quantisation and VAE tiling can be enabled.

Speed was approximately 0.8 iterations per second on an M3 Max Macbook Pro.

### SageAttention

When using `--attention_mechanism=sageattention`, inference can be sped-up at validation time.

**Note**: This isn&apos;t compatible with _every_ model configuration, but it&apos;s worth trying.

### NF4-quantised training

In simplest terms, NF4 is a 4bit-_ish_ representation of the model, which means training has serious stability concerns to address.

In early tests, the following holds true:
- Lion optimiser causes model collapse but uses least VRAM; AdamW variants help to hold it together; bnb-adamw8bit, adamw_bf16 are great choices
  - AdEMAMix didn&apos;t fare well, but settings were not explored
- `--max_grad_norm=0.01` further helps reduce model breakage by preventing huge changes to the model in too short a time
- NF4, AdamW8bit, and a higher batch size all help to overcome the stability issues, at the cost of more time spent training or VRAM used
- Upping the resolution slows training down A LOT, and might harm the model
- Increasing the length of videos consumes a lot more memory as well. Reduce `num_frames` to beat this one.
- Anything that&apos;s difficult to train on int8 or bf16 becomes harder in NF4
- It&apos;s less compatible with options like SageAttention

NF4 does not work with torch.compile, so whatever you get for speed is what you get.

If VRAM is not a concern then int8 with torch.compile is your best, fastest option.

### Masked loss

Don&apos;t use this with LTX Video.


### Quantisation
- Quantisation is not needed to train this model

### Image artifacts
Like other DiT models, if you do these things (among others) some square grid artifacts **may** begin appearing in the samples:
- Overtrain with low quality data
- Use too high of a learning rate
- Overtraining (in general), a low-capacity network with too many images
- Undertraining (also), a high-capacity network with too few images
- Using weird aspect ratios or training data sizes

### Aspect bucketing
- Videos are bucketed like images.
- Training for too long on square crops probably won&apos;t damage this model too much. Go nuts, it&apos;s great and reliable.
- On the other hand, using the natural aspect buckets of your dataset might overly bias these shapes during inference time.
  - This could be a desirable quality, as it keeps aspect-dependent styles like cinematic stuff from bleeding into other resolutions too much.
  - However, if you&apos;re looking to improve results equally across many aspect buckets, you might have to experiment with `crop_aspect=random` which comes with its own downsides.
- Mixing dataset configurations by defining your image directory dataset multiple times has produced really good results and a nicely generalised model.

### Training custom fine-tuned LTX models

Some fine-tuned models on Hugging Face Hub lack the full directory structure, requiring specific options to be set.

```json
{
    &quot;model_family&quot;: &quot;ltxvideo&quot;,
    &quot;pretrained_model_name_or_path&quot;: &quot;Lightricks/LTX-Video&quot;,
    &quot;pretrained_transformer_model_name_or_path&quot;: &quot;path/to-the-other-model&quot;,
    &quot;pretrained_vae_model_name_or_path&quot;: &quot;Lightricks/LTX-Video&quot;,
    &quot;pretrained_transformer_subfolder&quot;: &quot;none&quot;,
}
```

## Credits

The [finetrainers](https://github.com/a-r-r-o-w/finetrainers) project and the Diffusers team.
- Originally used some design concepts from SimpleTuner
- Now contributes insight and code for making video training easily implemented</file><file path="documentation/quickstart/SANA.md">## NVLabs Sana Quickstart

In this example, we&apos;ll be full-rank training the NVLabs Sana model.

### Hardware requirements

Sana is very lightweight and might not even need full gradient checkpointing enabled on a 24G card, which means it trains very quickly!

- **the absolute minimum** is about 12G VRAM, though this guide might not help you get there fully
- **a realistic minimum** is a single 3090 or V100 GPU
- **ideally** multiple 4090, A6000, L40S, or better

Sana is a strange architecture relative to other models that are trainable by SimpleTuner;

- Initially, unlike other models, Sana required fp16 training and would crash out with bf16
  - Model authors at NVIDIA were gracious enough to follow-up with bf16-compatible weights for fine-tuning
- Quantisation might be more sensitive on this model family due to the issues with bf16/fp16
- SageAttention does not work with Sana (yet) due to its head_dim shape that is currently unsupported
- The loss value when training Sana is very high, and it might need a much lower learning rate than other models (eg. `1e-5` or thereabouts)
- Training might hit NaN values, and it&apos;s not clear why this happens

Gradient checkpointing can free VRAM, but slows down training. A chart of test results from a 4090 with 5800X3D:

![image](https://github.com/user-attachments/assets/310bf099-a077-4378-acf4-f60b4b82fdc4)

SimpleTuner&apos;s Sana modeling code allows the specification of `--gradient_checkpointing_interval` to checkpoint every _n_ blocks and attain the results seen in the above chart.

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

# if python --version shows 3.11 you can just also use the &apos;python&apos; command here.
python3.11 -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

**Note:** We&apos;re currently installing the `release` branch here; the `main` branch may contain experimental features that might have better results or lower memory use.

Depending on your system, you will run one of 3 commands:

```bash
# Linux
poetry install
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.


If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

There, you will possibly need to modify the following variables:

- `model_type` - Set this to `full`.
- `model_family` - Set this to `sana`.
- `pretrained_model_name_or_path` - Set this to `terminusresearch/sana-1.6b-1024px`
- `output_dir` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `train_batch_size` - for a 24G card with full gradient checkpointing, this can be as high as 6.
- `validation_resolution` - This checkpoint for Sana is a 1024px model, you should set this to `1024x1024` or one of Sana&apos;s other supported resolutions.
  - Other resolutions may be specified using commas to separate them: `1024x1024,1280x768,2048x2048`
- `validation_guidance` - Use whatever you are used to selecting at inference time for Sana.
- `validation_num_inference_steps` - Use somewhere around 50 for the best quality, though you can accept less if you&apos;re happy with the results.
- `use_ema` - setting this to `true` will greatly help obtain a more smoothed result alongside your main trained checkpoint.

- `optimizer` - You can use any optimiser you are comfortable and familiar with, but we will use `optimi-adamw` for this example.
- `mixed_precision` - It&apos;s recommended to set this to `bf16` for the most efficient training configuration, or `no` (but will consume more memory and be slower).
  - A value of `fp16` is not recommended here but may be required for certain Sana finetunes (and introduces other new issues to enable this)
- `gradient_checkpointing` - Disabling this will go the fastest, but limits your batch sizes. It is required to enable this to get the lowest VRAM usage.
- `gradient_checkpointing_interval` - If `gradient_checkpointing` feels like overkill on your GPU, you could set this to a value of 2 or higher to only checkpoint every _n_ blocks. A value of 2 would checkpoint half of the blocks, and 3 would be one-third.

Multi-GPU users can reference [this document](/OPTIONS.md#environment-configuration-variables) for information on configuring the number of GPUs to use.

#### Validation prompts

Inside `config/config.json` is the &quot;primary validation prompt&quot;, which is typically the main instance_prompt you are training on for your single subject or style. Additionally, a JSON file may be created that contains extra prompts to run through during validations.

The example config file `config/user_prompt_library.json.example` contains the following format:

```json
{
  &quot;nickname&quot;: &quot;the prompt goes here&quot;,
  &quot;another_nickname&quot;: &quot;another prompt goes here&quot;
}
```

The nicknames are the filename for the validation, so keep them short and compatible with your filesystem.

To point the trainer to this prompt library, add it to TRAINER_EXTRA_ARGS by adding a new line at the end of `config.json`:
```json
  &quot;--user_prompt_library&quot;: &quot;config/user_prompt_library.json&quot;,
```

A set of diverse prompt will help determine whether the model is collapsing as it trains. In this example, the word `&lt;token&gt;` should be replaced with your subject name (instance_prompt).

```json
{
    &quot;anime_&lt;token&gt;&quot;: &quot;a breathtaking anime-style portrait of &lt;token&gt;, capturing her essence with vibrant colors and expressive features&quot;,
    &quot;chef_&lt;token&gt;&quot;: &quot;a high-quality, detailed photograph of &lt;token&gt; as a sous-chef, immersed in the art of culinary creation&quot;,
    &quot;just_&lt;token&gt;&quot;: &quot;a lifelike and intimate portrait of &lt;token&gt;, showcasing her unique personality and charm&quot;,
    &quot;cinematic_&lt;token&gt;&quot;: &quot;a cinematic, visually stunning photo of &lt;token&gt;, emphasizing her dramatic and captivating presence&quot;,
    &quot;elegant_&lt;token&gt;&quot;: &quot;an elegant and timeless portrait of &lt;token&gt;, exuding grace and sophistication&quot;,
    &quot;adventurous_&lt;token&gt;&quot;: &quot;a dynamic and adventurous photo of &lt;token&gt;, captured in an exciting, action-filled moment&quot;,
    &quot;mysterious_&lt;token&gt;&quot;: &quot;a mysterious and enigmatic portrait of &lt;token&gt;, shrouded in shadows and intrigue&quot;,
    &quot;vintage_&lt;token&gt;&quot;: &quot;a vintage-style portrait of &lt;token&gt;, evoking the charm and nostalgia of a bygone era&quot;,
    &quot;artistic_&lt;token&gt;&quot;: &quot;an artistic and abstract representation of &lt;token&gt;, blending creativity with visual storytelling&quot;,
    &quot;futuristic_&lt;token&gt;&quot;: &quot;a futuristic and cutting-edge portrayal of &lt;token&gt;, set against a backdrop of advanced technology&quot;,
    &quot;woman&quot;: &quot;a beautifully crafted portrait of a woman, highlighting her natural beauty and unique features&quot;,
    &quot;man&quot;: &quot;a powerful and striking portrait of a man, capturing his strength and character&quot;,
    &quot;boy&quot;: &quot;a playful and spirited portrait of a boy, capturing youthful energy and innocence&quot;,
    &quot;girl&quot;: &quot;a charming and vibrant portrait of a girl, emphasizing her bright personality and joy&quot;,
    &quot;family&quot;: &quot;a heartwarming and cohesive family portrait, showcasing the bonds and connections between loved ones&quot;
}
```

&gt; ℹ️ Sana uses an odd text encoder configuration that means shorter prompts will possibly look very bad.

#### CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.

#### Sana time schedule shifting

Flow-matching models such as Sana, Sana, and SD3 have a property called &quot;shift&quot; that allows us to shift the trained portion of the timestep schedule using a simple decimal value.

##### Auto-shift
A commonly-recommended approach is to follow several recent works and enable resolution-dependent timestep shift, `--flow_schedule_auto_shift` which uses higher shift values for larger images, and lower shift values for smaller images. This results in stable but potentially mediocre training results.

##### Manual specification
_Thanks to General Awareness from Discord for the following examples_

When using a `--flow_schedule_shift` value of 0.1 (a very low value), only the finer details of the image are affected:
![image](https://github.com/user-attachments/assets/991ca0ad-e25a-4b13-a3d6-b4f2de1fe982)

When using a `--flow_schedule_shift` value of 4.0 (a very high value), the large compositional features and potentially colour space of the model becomes impacted:
![image](https://github.com/user-attachments/assets/857a1f8a-07ab-4b75-8e6a-eecff616a28d)

#### Dataset considerations

&gt; ⚠️ Image quality for training is more important for Sana than for most other models, as it will absorb the artifacts in your images *first*, and then learn the concept/subject.

It&apos;s crucial to have a substantial dataset to train your model on. There are limitations on the dataset size, and you will need to ensure that your dataset is large enough to train your model effectively. Note that the bare minimum dataset size is `train_batch_size * gradient_accumulation_steps` as well as more than `vae_batch_size`. The dataset will not be useable if it is too small.

&gt; ℹ️ With few enough images, you might see a message **no images detected in dataset** - increasing the `repeats` value will overcome this limitation.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently. In this example, we will be using [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) as the dataset.

Create a `--data_backend_config` (`config/multidatabackend.json`) document containing this:

```json
[
  {
    &quot;id&quot;: &quot;pseudo-camera-10k-sana&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;center&quot;,
    &quot;resolution&quot;: 512,
    &quot;minimum_image_size&quot;: 512,
    &quot;maximum_image_size&quot;: 512,
    &quot;target_downsample_size&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/sana/pseudo-camera-10k&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;skip_file_discovery&quot;: &quot;&quot;,
    &quot;caption_strategy&quot;: &quot;filename&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 0,
    &quot;is_regularisation_data&quot;: true
  },
  {
    &quot;id&quot;: &quot;dreambooth-subject&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 1024,
    &quot;minimum_image_size&quot;: 1024,
    &quot;maximum_image_size&quot;: 1024,
    &quot;target_downsample_size&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/sana/dreambooth-subject&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/dreambooth-subject&quot;,
    &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
    &quot;instance_prompt&quot;: &quot;the name of your subject goes here&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 1000
  },
  {
    &quot;id&quot;: &quot;dreambooth-subject-512&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 512,
    &quot;minimum_image_size&quot;: 512,
    &quot;maximum_image_size&quot;: 512,
    &quot;target_downsample_size&quot;: 512,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/sana/dreambooth-subject-512&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/dreambooth-subject&quot;,
    &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
    &quot;instance_prompt&quot;: &quot;the name of your subject goes here&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 1000
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/sana&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

&gt; ℹ️ Running 512px and 1024px datasets concurrently is supported, and could result in better convergence for Sana.

Then, create a `datasets` directory:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
    mkdir dreambooth-subject
    # place your images into dreambooth-subject/ now
popd
```

This will download about 10k photograph samples to your `datasets/pseudo-camera-10k` directory, which will be automatically created for you.

Your Dreambooth images should go into the `datasets/dreambooth-subject` directory.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `--push_to_hub` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
./train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

## Notes &amp; troubleshooting tips

### Lowest VRAM config

Currently, the lowest VRAM utilisation can be attained with:

- OS: Ubuntu Linux 24
- GPU: A single NVIDIA CUDA device (10G, 12G)
- System memory: 50G of system memory approximately
- Base model precision: `bnb-nf4`
- Optimiser: Lion 8Bit Paged, `bnb-lion8bit-paged`
- Resolution: 1024px
- Batch size: 1, zero gradient accumulation steps
- DeepSpeed: disabled / unconfigured
- PyTorch: 2.5.1
- Using `--quantize_via=cpu` to avoid outOfMemory error during startup on &lt;=16G cards.
- Enable `--gradient_checkpointing`

**NOTE**: Pre-caching of VAE embeds and text encoder outputs may use more memory and still OOM. If so, text encoder quantisation can be enabled. VAE tiling may not work for Sana at this time.

Speed was approximately 1.4 iterations per second on a 4090.

### Masked loss

If you are training a subject or style and would like to mask one or the other, see the [masked loss training](/documentation/DREAMBOOTH.md#masked-loss) section of the Dreambooth guide.

### Quantisation

Not tested thoroughly (yet).

### Learning rates

#### LoRA (--lora_type=standard)

*Not supported.*

#### LoKr (--lora_type=lycoris)
- Mild learning rates are better for LoKr (`1e-4` with AdamW, `2e-5` with Lion)
- Other algo need more exploration.
- Setting `is_regularisation_data` has unknown impact/effect with Sana (not tested)

### Image artifacts

Sana has an unknown response to image artifacts.

It&apos;s not currently known whether any common training artifacts will be produced or what the cause of these might be.

If any image quality issues arise, please open an issue on Github.

### Aspect bucketing

This model has an unknown response to aspect bucketed data. Experimentation will be helpful.</file><file path="documentation/quickstart/SD3.md">## Stable Diffusion 3

In this example, we&apos;ll be training a Stable Diffusion 3 model using the SimpleTuner toolkit and will be using the `lora` model type.

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

python -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

#### Removing DeepSpeed &amp; Bits n Bytes

These two dependencies cause numerous issues for container hosts such as RunPod and Vast.

To remove them after `poetry` has installed them, run the following command in the same terminal:

```bash
pip uninstall -y deepspeed bitsandbytes
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.

If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

There, you will need to modify the following variables:

```json
{
  &quot;model_type&quot;: &quot;lora&quot;,
  &quot;model_family&quot;: &quot;sd3&quot;,
  &quot;pretrained_model_name_or_path&quot;: &quot;stabilityai/stable-diffusion-3.5-large&quot;,
  &quot;output_dir&quot;: &quot;/home/user/outputs/models&quot;,
  &quot;validation_resolution&quot;: &quot;1024x1024,1280x768&quot;,
  &quot;validation_guidance&quot;: 3.0,
  &quot;validation_prompt&quot;: &quot;your main test prompt here&quot;,
  &quot;user_prompt_library&quot;: &quot;config/user_prompt_library.json&quot;
}
```


- `pretrained_model_name_or_path` - Set this to `stabilityai/stable-diffusion-3.5-large`. Note that you will need to log in to Huggingface and be granted access to download this model. We will go over logging in to Huggingface later in this tutorial.
  - If you prefer to train the older SD3.0 Medium (2B), use `stabilityai/stable-diffusion-3-medium-diffusers` instead.
- `MODEL_TYPE` - Set this to `lora`.
- `MODEL_FAMILY` - Set this to `sd3`.
- `OUTPUT_DIR` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `VALIDATION_RESOLUTION` - As SD3 is a 1024px model, you can set this to `1024x1024`.
  - Additionally, SD3 was fine-tuned on multi-aspect buckets, and other resolutions may be specified using commas to separate them: `1024x1024,1280x768`
- `VALIDATION_GUIDANCE` - SD3 benefits from a very-low value. Set this to `3.0`.

There are a few more if using a Mac M-series machine:

- `mixed_precision` should be set to `no`.

#### Quantised model training

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements well below the requirements of base SDXL training.

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

&gt; ⚠️ If using a JSON config file, be sure to use this format in `config.json` instead of `config.env`:

```json
{
  &quot;base_model_precision&quot;: &quot;int8-quanto&quot;,
  &quot;text_encoder_1_precision&quot;: &quot;no_change&quot;,
  &quot;text_encoder_2_precision&quot;: &quot;no_change&quot;,
  &quot;text_encoder_3_precision&quot;: &quot;no_change&quot;,
  &quot;optimizer&quot;: &quot;adamw_bf16&quot;
}
```

For `config.env` users (deprecated):

```bash
# choices: int8-quanto, int4-quanto, int2-quanto, fp8-quanto
# int8-quanto was tested with a single subject dreambooth LoRA.
# fp8-quanto does not work on Apple systems. you must use int levels.
# int2-quanto is pretty extreme and gets the whole rank-1 LoRA down to about 13.9GB VRAM.
# may the gods have mercy on your soul, should you push things Too Far.
export TRAINER_EXTRA_ARGS=&quot;--base_model_precision=int8-quanto&quot;

# Maybe you want the text encoders to remain full precision so your text embeds are cake.
# We unload the text encoders before training, so, that&apos;s not an issue during training time - only during pre-caching.
# Alternatively, you can go ham on quantisation here and run them in int4 or int8 mode, because no one can stop you.
export TRAINER_EXTRA_ARGS=&quot;${TRAINER_EXTRA_ARGS} --text_encoder_1_precision=no_change --text_encoder_2_precision=no_change&quot;

# When you&apos;re quantising the model, --base_model_default_dtype is set to bf16 by default. This setup requires adamw_bf16, but saves the most memory.
# adamw_bf16 only supports bf16 training, but any other optimiser will support both bf16 or fp32 training precision.
export OPTIMIZER=&quot;adamw_bf16&quot;
```

#### Dataset considerations

It&apos;s crucial to have a substantial dataset to train your model on. There are limitations on the dataset size, and you will need to ensure that your dataset is large enough to train your model effectively. Note that the bare minimum dataset size is `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS` as well as more than `VAE_BATCH_SIZE`. The dataset will not be useable if it is too small.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently. In this example, we will be using [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) as the dataset.

In your `/home/user/simpletuner/config` directory, create a multidatabackend.json:

```json
[
  {
    &quot;id&quot;: &quot;pseudo-camera-10k-sd3&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;center&quot;,
    &quot;resolution&quot;: 1024,
    &quot;minimum_image_size&quot;: 0,
    &quot;maximum_image_size&quot;: 1024,
    &quot;target_downsample_size&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;/home/user/simpletuner/output/cache/vae/sd3/pseudo-camera-10k&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/datasets/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;skip_file_discovery&quot;: &quot;&quot;,
    &quot;caption_strategy&quot;: &quot;filename&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/sd3/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

Then, create a `datasets` directory:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
popd
```

This will download about 10k photograph samples to your `datasets/pseudo-camera-10k` directory, which will be automatically created for you.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `push_to_hub: true` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
bash train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

## Notes &amp; troubleshooting tips

### Skip-layer guidance (SD3.5 Medium)

StabilityAI recommends enabling SLG (Skip-layer guidance) on SD 3.5 Medium inference. This doesn&apos;t impact training results, only the validation sample quality.

The following values are recommended for `config.json`:

```json
{
  &quot;--validation_guidance_skip_layers&quot;: [7, 8, 9],
  &quot;--validation_guidance_skip_layers_start&quot;: 0.01,
  &quot;--validation_guidance_skip_layers_stop&quot;: 0.2,
  &quot;--validation_guidance_skip_scale&quot;: 2.8,
  &quot;--validation_guidance&quot;: 4.0,
  &quot;--flow_use_uniform_schedule&quot;: true,
  &quot;--flow_schedule_auto_shift&quot;: true
}
```

- `..skip_scale` determines how much to scale the positive prompt prediction during skip-layer guidance. The default value of 2.8 is safe for the base model&apos;s skip value of `7, 8, 9` but will need to be increased if more layers are skipped, doubling it for each additional layer.
- `..skip_layers` tells which layers to skip during the negative prompt prediction.
- `..skip_layers_start` determine the fraction of the inference pipeline during which skip-layer guidance should begin to be applied.
- `..skip_layers_stop` will set the fraction of the total number of inference steps after which SLG will no longer be applied.

SLG can be applied for fewer steps for a weaker effect or less reduction of inference speed.

It seems that extensive training of a LoRA or LyCORIS model will require modification to these values, though it&apos;s not clear how exactly it changes.

**Lower CFG must be used during inference.**

### Model instability

The SD 3.5 Large 8B model has potential instabilities during training:

- High `--max_grad_norm` values will allow the model to explore potentially dangerous weight updates
- Learning rates can be extremely sensitive; `1e-5` works with StableAdamW but `4e-5` could explode
- Higher batch sizes help **a lot**
- The stability is not impacted by disabling quantisation or training in pure fp32

Official training code was not released alongside SD3.5, leaving developers to guess how to implement the training loop based on the [SD3.5 repository contents](https://github.com/stabilityai/sd3.5).

Some changes were made to SimpleTuner&apos;s SD3.5 support:
- Excluding more layers from quantisation
- No longer zeroing T5 padding space by default (`--t5_padding`)
- Offering a switch (`--sd3_clip_uncond_behaviour` and `--sd3_t5_uncond_behaviour`) to use empty encoded blank captions for unconditional predictions (`empty_string`, **default**) or zeros (`zero`), not a recommended setting to tweak.
- SD3.5 training loss function was updated to match that found in the upstream StabilityAI/SD3.5 repository
- Updated default `--flow_schedule_shift` value to 3 to match the static 1024px value for SD3
  - StabilityAI followed-up with documentation to use `--flow_schedule_shift=1` with `--flow_use_uniform_schedule`
  - Community members have reported that `--flow_schedule_auto_shift` works better when using mult-aspect or multi-resolution training
- Updated the hard-coded tokeniser sequence length limit to **154** with the option to revert it to **77** tokens to save disk space or compute at the cost of output quality degradation


#### Stable configuration values

These options have been known to keep SD3.5 in-tact for as long as possible:
- optimizer=adamw_bf16
- flow_schedule_shift=1
- learning_rate=1e-4
- batch_size=4 * 3 GPUs
- max_grad_norm=0.1
- base_model_precision=int8-quanto
- No loss masking or dataset regularisation, as their contribution to this instability is unknown
- `validation_guidance_skip_layers=[7,8,9]`

### Lowest VRAM config

- OS: Ubuntu Linux 24
- GPU: A single NVIDIA CUDA device (10G, 12G)
- System memory: 50G of system memory approximately
- Base model precision: `bnb-nf4`
- Optimiser: Lion 8Bit Paged, `bnb-lion8bit-paged`
- Resolution: 512px
- Batch size: 1, zero gradient accumulation steps
- DeepSpeed: disabled / unconfigured
- PyTorch: 2.5

### SageAttention

When using `--attention_mechanism=sageattention`, inference can be sped-up at validation time.

**Note**: This isn&apos;t compatible with _every_ model configuration, but it&apos;s worth trying.

### Masked loss

If you are training a subject or style and would like to mask one or the other, see the [masked loss training](/documentation/DREAMBOOTH.md#masked-loss) section of the Dreambooth guide.

### Regularisation data

For more information on regularisation datasets, see [this section](/documentation/DREAMBOOTH.md#prior-preservation-loss) and [this section](/documentation/DREAMBOOTH.md#regularisation-dataset-considerations) of the Dreambooth guide.

### Quantised training

See [this section](/documentation/DREAMBOOTH.md#quantised-model-training-loralycoris-only) of the Dreambooth guide for information on configuring quantisation for SD3 and other models.

### CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.</file><file path="documentation/quickstart/SIGMA.md">## PixArt Sigma Quickstart

In this example, we&apos;ll be training a PixArt Sigma model using the SimpleTuner toolkit and will be using the `full` model type, as it being a smaller model will likely fit in VRAM.

### Prerequisites

Make sure that you have python installed; SimpleTuner does well with 3.10 through 3.12.

You can check this by running:

```bash
python --version
```

If you don&apos;t have python 3.11 installed on Ubuntu, you can try the following:

```bash
apt -y install python3.11 python3.11-venv
```

#### Container image dependencies

For Vast, RunPod, and TensorDock (among others), the following will work on a CUDA 12.2-12.8 image:

```bash
apt -y install nvidia-cuda-toolkit libgl1-mesa-glx
```

If `libgl1-mesa-glx` is not found, you might need to use `libgl1-mesa-dri` instead. Your mileage may vary.

### Installation

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

python -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

#### Removing DeepSpeed &amp; Bits n Bytes

These two dependencies cause numerous issues for container hosts such as RunPod and Vast.

To remove them after `poetry` has installed them, run the following command in the same terminal:

```bash
pip uninstall -y deepspeed bitsandbytes
```

### Setting up the environment

To run SimpleTuner, you will need to set up a configuration file, the dataset and model directories, and a dataloader configuration file.

#### Configuration file

An experimental script, `configure.py`, may allow you to entirely skip this section through an interactive step-by-step configuration. It contains some safety features that help avoid common pitfalls.

**Note:** This doesn&apos;t configure your dataloader. You will still have to do that manually, later.

To run it:

```bash
python configure.py
```
&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.

If you prefer to manually configure:

Copy `config/config.json.example` to `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

There, you will need to modify the following variables:

```json
{
  &quot;model_type&quot;: &quot;full&quot;,
  &quot;use_bitfit&quot;: false,
  &quot;pretrained_model_name_or_path&quot;: &quot;pixart-alpha/pixart-sigma-xl-2-1024-ms&quot;,
  &quot;model_family&quot;: &quot;pixart_sigma&quot;,
  &quot;output_dir&quot;: &quot;/home/user/output/models&quot;,
  &quot;validation_resolution&quot;: &quot;1024x1024,1280x768&quot;,
  &quot;validation_guidance&quot;: 3.5
}
```

- `pretrained_model_name_or_path` - Set this to `PixArt-alpha/PixArt-Sigma-XL-2-1024-MS`.
- `MODEL_TYPE` - Set this to `full`.
- `USE_BITFIT` - Set this to `false`.
- `MODEL_FAMILY` - Set this to `pixart_sigma`.
- `OUTPUT_DIR` - Set this to the directory where you want to store your checkpoints and validation images. It&apos;s recommended to use a full path here.
- `VALIDATION_RESOLUTION` - As PixArt Sigma comes in a 1024px or 2048xp model format, you should carefully set this to `1024x1024` for this example.
  - Additionally, PixArt was fine-tuned on multi-aspect buckets, and other resolutions may be specified using commas to separate them: `1024x1024,1280x768`
- `VALIDATION_GUIDANCE` - PixArt benefits from a very-low value. Set this between `3.6` to `4.4`.

There are a few more if using a Mac M-series machine:

- `mixed_precision` should be set to `no`.

#### Dataset considerations

It&apos;s crucial to have a substantial dataset to train your model on. There are limitations on the dataset size, and you will need to ensure that your dataset is large enough to train your model effectively. Note that the bare minimum dataset size is `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`. The dataset will not be discoverable by the trainer if it is too small.

Depending on the dataset you have, you will need to set up your dataset directory and dataloader configuration file differently. In this example, we will be using [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) as the dataset.

In your `/home/user/simpletuner/config` directory, create a multidatabackend.json:

```json
[
  {
    &quot;id&quot;: &quot;pseudo-camera-10k-pixart&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;crop_style&quot;: &quot;random&quot;,
    &quot;resolution&quot;: 1.0,
    &quot;minimum_image_size&quot;: 0.25,
    &quot;maximum_image_size&quot;: 1.0,
    &quot;target_downsample_size&quot;: 1.0,
    &quot;resolution_type&quot;: &quot;area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/pixart/pseudo-camera-10k&quot;,
    &quot;instance_data_dir&quot;: &quot;/home/user/simpletuner/datasets/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;skip_file_discovery&quot;: &quot;&quot;,
    &quot;caption_strategy&quot;: &quot;filename&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/pixart/pseudo-camera-10k&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

Then, create a `datasets` directory:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
popd
```

This will download about 10k photograph samples to your `datasets/pseudo-camera-10k` directory, which will be automatically created for you.

#### Login to WandB and Huggingface Hub

You&apos;ll want to login to WandB and HF Hub before beginning training, especially if you&apos;re using `push_to_hub: true` and `--report_to=wandb`.

If you&apos;re going to be pushing items to a Git LFS repository manually, you should also run `git config --global credential.helper store`

Run the following commands:

```bash
wandb login
```

and

```bash
huggingface-cli login
```

Follow the instructions to log in to both services.

### Executing the training run

From the SimpleTuner directory, one simply has to run:

```bash
bash train.sh
```

This will begin the text embed and VAE output caching to disk.

For more information, see the [dataloader](/documentation/DATALOADER.md) and [tutorial](/TUTORIAL.md) documents.

### CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.

### SageAttention

When using `--attention_mechanism=sageattention`, inference can be sped-up at validation time.

**Note**: This isn&apos;t compatible with _every_ model configuration, but it&apos;s worth trying.</file><file path="documentation/CONTROLNET.md"># ControlNet training guide

## Background

ControlNet models are capable of many tasks, which depend on the conditioning data given at training time.

Example (taken from the Diffusers ControlNet model card):

![example](https://tripleback.net/public/controlnet-example-1.png)

On the left, you can see the &quot;canny edge map&quot; given as the conditioning input. To the right of that are the outputs the ControlNet model guided out of the base SDXL model.

When the model is used in this way, the prompt handles almost none of the composition, merely filling in the details.

## What training a ControlNet looks like

At first, when training a ControlNet, it has zero indication of control:

![example](https://tripleback.net/public/controlnet-example-2.png)
(_ControlNet trained for just 4 steps on a Stable Diffusion 2.1 model_)


The antelope prompt still has a majority of control over the composition, and the ControlNet conditioning input is ignored.

Over time, the control input should be respected:

![example](https://tripleback.net/public/controlnet-example-3.png)
(_ControlNet trained for just 100 steps on a Stable Diffusion XL model_)

At that point, a few indications of ControlNet influence began to appear, but the results were incredibly inconsistent.

A lot more than 100 steps will be needed for this to work!

## Example dataloader configuration

The dataloader configuration remains pretty close to a typical text-to-image dataset configuration:

- The main image data is the `antelope-data` set
  - The key `conditioning_data` is now set, and it should be set to the `id` value of your conditioning data that pairs with this set.
  - `dataset_type` should be `image` for the base set
- A secondary dataset is configured, called `antelope-conditioning`
  - The name isn&apos;t important - adding `-data` and `-conditioning` is only done in this example for illustrative purposes.
  - The `dataset_type` should be set to `conditioning`, indicating to the trainer that this is to be used for evaluation and conditioned input training purposes.
- Conditioning inputs are not VAE-encoded, but instead passed into the model directly during training time as pixel values. This means we don&apos;t spend any more time processing VAE embeds at the start of training!
- Though everything is explicitly labeled as `-controlnet` here, you can reuse the same text embeds that you used for normal full/LoRA tuning. ControlNet inputs do not modify the prompt embeds.
- The dataset components are labeled as being for SDXL, but they&apos;re model-agnostic other than the `resolution` values you&apos;d typically use.
- You likely want cropping enabled with `crop_style=&apos;center&apos;` or `crop_style=&apos;corner&apos;` so that the perturbations resulting from random crops don&apos;t impact your controlnet model training.
  - This is a limitation in the current version of ControlNet training, it can be improved for future releases.

```json
[
    {
        &quot;id&quot;: &quot;antelope-data&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;image&quot;,
        &quot;conditioning_data&quot;: &quot;antelope-conditioning&quot;,
        &quot;instance_data_dir&quot;: &quot;/Volumes/ml/datasets/canny-edge/animals/antelope-data&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;an antelope&quot;,
        &quot;metadata_backend&quot;: &quot;discovery&quot;,
        &quot;minimum_image_size&quot;: 1.0,
        &quot;maximum_image_size&quot;: 1.0,
        &quot;target_downsample_size&quot;: 1.0,
        &quot;cache_dir_vae&quot;: &quot;/Volumes/ml/cache/vae/sdxl/antelope-data&quot;,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;resolution&quot;: 1.0,
        &quot;resolution_type&quot;: &quot;area&quot;,
        &quot;cache_file_suffix&quot;: &quot;controlnet-sdxl&quot;
    },
    {
        &quot;id&quot;: &quot;antelope-conditioning&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;conditioning&quot;,
        &quot;instance_data_dir&quot;: &quot;/Volumes/ml/datasets/canny-edge/animals/antelope-conditioning&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;an antelope&quot;,
        &quot;metadata_backend&quot;: &quot;discovery&quot;,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;resolution&quot;: 1.0,
        &quot;resolution_type&quot;: &quot;area&quot;,
        &quot;cache_file_suffix&quot;: &quot;controlnet-sdxl&quot;
    },
    {
        &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;type&quot;: &quot;local&quot;,
        &quot;cache_dir&quot;: &quot;/Volumes/ml/cache/text/sdxl-base/controlnet&quot;
    }
]
```

## Generating conditioning image inputs

As new as ControlNet support is in SimpleTuner, we&apos;ve currently just got one option available for generating your training set:

- [create_canny_edge.py](/toolkit/datasets/controlnet/create_canny_edge.py)
  - An extremely basic example on generating a training set for Canny model training.
  - You will have to modify the `input_dir` and `output_dir` values in the script

This will take about 30 seconds for a small dataset of fewer than 100 images.

## Modifying your environment file to train ControlNet models

Just setting up the dataloader configuration won&apos;t be enough to start training ControlNet models.

Inside `config/config.json`, you will have to set the following values:

```bash
&quot;model_type&quot;: &apos;full&apos;,
&quot;controlnet&quot;: true,

# You may have to reduce TRAIN_BATCH_SIZE more than usual
&quot;train_batch_size&quot;: 1
```

## Inference on resulting ControlNet models

An SDXL example is provided here:

```py
# Update these values:
base_model = &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;         # This is the model you used as `--pretrained_model_name_or_path`
controlnet_model_path = &quot;diffusers/controlnet-canny-sdxl-1.0&quot;   # This is the path to the resulting ControlNet checkpoint
# controlnet_model_path = &quot;/path/to/controlnet/checkpoint-100&quot;

# Leave the rest alone:
from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL
from diffusers.utils import load_image
from PIL import Image
import torch
import numpy as np
import cv2

prompt = &quot;aerial view, a futuristic research complex in a bright foggy jungle, hard lighting&quot;
negative_prompt = &apos;low quality, bad quality, sketches&apos;

image = load_image(&quot;https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png&quot;)

controlnet_conditioning_scale = 0.5  # recommended for good generalization

controlnet = ControlNetModel.from_pretrained(
    controlnet_model_path,
    torch_dtype=torch.float16
)
vae = AutoencoderKL.from_pretrained(&quot;madebyollin/sdxl-vae-fp16-fix&quot;, torch_dtype=torch.float16)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    base_model,
    controlnet=controlnet,
    vae=vae,
    torch_dtype=torch.float16,
)
pipe.enable_model_cpu_offload()

image = np.array(image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
image = Image.fromarray(image)

images = pipe(
    prompt, negative_prompt=negative_prompt, image=image, controlnet_conditioning_scale=controlnet_conditioning_scale,
    ).images

images[0].save(f&quot;hug_lab.png&quot;)
```
(_Demo code lifted from the [Hugging Face SDXL ControlNet example](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0)_)</file><file path="documentation/DATALOADER.md"># Dataloader configuration file

Here is the most basic example of a dataloader configuration file, as `multidatabackend.example.json`.

```json
[
  {
    &quot;id&quot;: &quot;something-special-to-remember-by&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/path/to/data/tree&quot;,
    &quot;crop&quot;: true,
    &quot;crop_style&quot;: &quot;center&quot;,
    &quot;crop_aspect&quot;: &quot;square&quot;,
    &quot;resolution&quot;: 1024,
    &quot;minimum_image_size&quot;: 768,
    &quot;maximum_image_size&quot;: 2048,
    &quot;minimum_aspect_ratio&quot;: 0.50,
    &quot;maximum_aspect_ratio&quot;: 3.00,
    &quot;target_downsample_size&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;prepend_instance_prompt&quot;: false,
    &quot;instance_prompt&quot;: &quot;something to label every image&quot;,
    &quot;only_instance_prompt&quot;: false,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;/path/to/vaecache&quot;,
    &quot;repeats&quot;: 0
  },
  {
    &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;type&quot;: &quot;aws&quot;,
    &quot;aws_bucket_name&quot;: &quot;textembeds-something-yummy&quot;,
    &quot;aws_region_name&quot;: null,
    &quot;aws_endpoint_url&quot;: &quot;https://foo.bar/&quot;,
    &quot;aws_access_key_id&quot;: &quot;wpz-764e9734523434&quot;,
    &quot;aws_secret_access_key&quot;: &quot;xyz-sdajkhfhakhfjd&quot;,
    &quot;aws_data_prefix&quot;: &quot;&quot;,
    &quot;cache_dir&quot;: &quot;&quot;
  }
]
```

## Configuration Options

### `id`

- **Description:** Unique identifier for the dataset. It should remain constant once set, as it links the dataset to its state tracking entries.

### `dataset_type`

- **Values:** `image` | `video` | `text_embeds` | `image_embeds` | `conditioning`
- **Description:** `image` and `video` datasets contain your training data. `text_embeds` contain the outputs of the text encoder cache, and `image_embeds` contain the VAE outputs, if the model uses one. When a dataset is marked as `conditioning`, it is possible to pair it to your `image` dataset via [the conditioning_data option](#conditioning_data)
- **Note:** Text and image embed datasets are defined differently than image datasets are. A text embed dataset stores ONLY the text embed objects. An image dataset stores the training data.
- **Note:** Don&apos;t combine images and video in a **single** dataset. Split them out.

### `default`

- **Only applies to `dataset_type=text_embeds`**
- If set `true`, this text embed dataset will be where SimpleTuner stores the text embed cache for eg. validation prompt embeds. As they do not pair to image data, there needs to be a specific location for them to end up.

### `text_embeds`

- **Only applies to `dataset_type=image`**
- If unset, the `default` text_embeds dataset will be used. If set to an existing `id` of a `text_embeds` dataset, it will use that instead. Allows specific text embed datasets to be associated with a given image dataset.

### `image_embeds`

- **Only applies to `dataset_type=image`**
- If unset, the VAE outputs will be stored on the image backend. Otherwise, you may set this to the `id` of an `image_embeds` dataset, and the VAE outputs will be stored there instead. Allows associating the image_embed dataset to the image data.

### `type`

- **Values:** `aws` | `local` | `csv`
- **Description:** Determines the storage backend (local, csv or cloud) used for this dataset.

### `conditioning_type`

- **Values:** `controlnet` | `mask`
- **Description:** A dataset may contain ControlNet conditioning inputs or masks to use during loss calculations. Only one or the other may be used.

### `conditioning_data`

- **Values:** `id` value of conditioning dataset
- **Description:** As described in [the ControlNet guide](/documentation/CONTROLNET.md), an `image` dataset can be paired to its ControlNet or image mask data via this option.

### `instance_data_dir` / `aws_data_prefix`

- **Local:** Path to the data on the filesystem.
- **AWS:** S3 prefix for the data in the bucket.

### `caption_strategy`

- **textfile** requires your image.png be next to an image.txt that contains one or more captions, separated by newlines. These image+text pairs **must be in the same directory**.
- **instanceprompt** requires a value for `instance_prompt` also be provided, and will use **only** this value for the caption of every image in the set.
- **filename** will use a converted and cleaned-up version of the filename as its caption, eg. after swapping underscores for spaces.
- **parquet** will pull captions from the parquet table that contains the rest of the image metadata. use the `parquet` field to configure this. See [Parquet caption strategy](#parquet-caption-strategy--json-lines-datasets).

Both `textfile` and `parquet` support multi-captions:
- textfiles are split by newlines. Each new line will be its own separate caption.
- parquet tables can have an iterable type in the field.

### Cropping Options

- `crop`: Enables or disables image cropping.
- `crop_style`: Selects the cropping style (`random`, `center`, `corner`, `face`).
- `crop_aspect`: Chooses the cropping aspect (`closest`, `random`, `square` or `preserve`).
- `crop_aspect_buckets`: When `crop_aspect` is set to `closest` or `random`, a bucket from this list will be selected, so long as the resulting image size would not result more than 20% upscaling.

### `resolution`

- **resolution_type=area:** The final image size is determined by megapixel count - a value of 1.05 here will correspond to aspect buckets around 1024^2 (1024x1024) total pixel area, ~1_050_000 pixels.
- **resolution_type=pixel_area:** Like `area`, the final image size is by its area, but measures in pixels rather than megapixels. A value of 1024 here will generate aspect buckets around 1024^2 (1024x1024) total pixel area, ~1_050_000 pixels.
- **resolution_type=pixel:** The final image size will be determined by the smaller edge being this value.

&gt; **NOTE**: Whether images are upscaled, downscaled, or cropped, rely on the values of `minimum_image_size`, `maximum_target_size`, `target_downsample_size`, `crop`, and `crop_aspect`.

### `minimum_image_size`

- Any images whose size ends up falling underneath this value will be **excluded** from training.
- When `resolution` is measured in megapixels (`resolution_type=area`), this should be in megapixels too (eg. `1.05` megapixels to exclude images under 1024x1024 **area**)
- When `resolution` is measured in pixels, you should use the same unit here (eg. `1024` to exclude images under 1024px **shorter edge length**)
- **Recommendation**: Keep `minimum_image_size` equal to `resolution` unless you want to risk training on poorly-upsized images.

### `minimum_aspect_ratio`

- **Description:** The minimum aspect ratio of the image. If the image&apos;s aspect ratio is less than this value, it will be excluded from training.
- **Note**: If the number of images qualifying for exclusion is excessive, this might waste time at startup as the trainer will try to scan them and bucket if they are missing from the bucket lists.

&gt; **Note**: Once the aspect and metadata lists are built for your dataset, using `skip_file_discovery=&quot;vae aspect metadata&quot;` will prevent the trainer from scanning the dataset on startup, saving a lot of time.

### `maximum_aspect_ratio`

- **Description:** The maximum aspect ratio of the image. If the image&apos;s aspect ratio is greater than this value, it will be excluded from training.
- **Note**: If the number of images qualifying for exclusion is excessive, this might waste time at startup as the trainer will try to scan them and bucket if they are missing from the bucket lists.

&gt; **Note**: Once the aspect and metadata lists are built for your dataset, using `skip_file_discovery=&quot;vae aspect metadata&quot;` will prevent the trainer from scanning the dataset on startup, saving a lot of time.


#### Examples

##### Video dataset

A video dataset should be a folder of (eg. mp4) video files and the usual methods of storing captions.

```json
[
  {
    &quot;id&quot;: &quot;disney-black-and-white&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;video&quot;,
    &quot;crop&quot;: false,
    &quot;resolution&quot;: 480,
    &quot;minimum_image_size&quot;: 480,
    &quot;maximum_image_size&quot;: 480,
    &quot;target_downsample_size&quot;: 480,
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;cache_dir_vae&quot;: &quot;cache/vae/ltxvideo/disney-black-and-white&quot;,
    &quot;instance_data_dir&quot;: &quot;datasets/disney-black-and-white&quot;,
    &quot;disabled&quot;: false,
    &quot;caption_strategy&quot;: &quot;textfile&quot;,
    &quot;metadata_backend&quot;: &quot;discovery&quot;,
    &quot;repeats&quot;: 0,
    &quot;video&quot;: {
        &quot;num_frames&quot;: 125,
        &quot;min_frames&quot;: 125
    }
  },
  {
    &quot;id&quot;: &quot;text-embeds&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;cache_dir&quot;: &quot;cache/text/ltxvideo&quot;,
    &quot;disabled&quot;: false,
    &quot;write_batch_size&quot;: 128
  }
]
```

- In the `video` subsection, we have the following keys we can set:
  - `num_frames` (optional, int) is how many seconds of data we&apos;ll train on.
    - At 25 fps, 125 frames is 5 seconds of video, standard output. This should be your target.
  - `min_frames` (optional, int) determines the minimum length of a video that will be considered for training.
    - This should be at least equal to `num_frames`. Not setting it ensures it&apos;ll be equal.
  - `max_frames` (optional, int) determines the maximum length of a video that will be considered for training.
  - `is_i2v` (optional, bool) determines whether i2v training will be done on a dataset.
    - This is set to True by default for LTX. You can disable it, however.


##### Configuration
```json
    &quot;minimum_image_size&quot;: 1024,
    &quot;resolution&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel&quot;
```
##### Outcome
- Any images with a shorter edge less than **1024px** will be completely excluded from training.
- Images like `768x1024` or `1280x768` would be excluded, but `1760x1024` and `1024x1024` would not.
- No image will be upsampled, because `minimum_image_size` is equal to `resolution`

##### Configuration
```json
    &quot;minimum_image_size&quot;: 1024,
    &quot;resolution&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel_area&quot; # different from the above configuration, which is &apos;pixel&apos;
```
##### Outcome
- The image&apos;s total area (width * height) being less than the minimum area (1024 * 1024) will result in it being excluded from training.
- Images like `1280x960` would **not** be excluded because `(1280 * 960)` is greater than `(1024 * 1024)`
- No image will be upsampled, because `minimum_image_size` is equal to `resolution`

##### Configuration
```json
    &quot;minimum_image_size&quot;: 0, # or completely unset, not present in the config
    &quot;resolution&quot;: 1024,
    &quot;resolution_type&quot;: &quot;pixel&quot;,
    &quot;crop&quot;: false
```

##### Outcome
- Images will be resized so their shorter edge is 1024px while maintaining their aspect ratio
- No images will be excluded based on size
- Small images will be upscaled using naive `PIL.resize` methods that do not look good
  - Upscaling is recommended to avoid unless done by hand using an upscaler of your choice before beginning training

### `maximum_image_size` and `target_downsample_size`

Images are not resized before cropping **unless** `maximum_image_size` and `target_downsample_size` are both set. In other words, a `4096x4096` image will be directly cropped to a `1024x1024` target, which may be undesirable.

- `maximum_image_size` specifies the threshold at which the resizing will begin. It will downsample images before cropping if they are larger than this.
- `target_downsample_size` specifies how large the image will be after resample and before it is cropped.

#### Examples

##### Configuration
```json
    &quot;resolution_type&quot;: &quot;pixel_area&quot;,
    &quot;resolution&quot;: 1024,
    &quot;maximum_image_size&quot;: 1536,
    &quot;target_downsample_size&quot;: 1280,
    &quot;crop&quot;: true,
    &quot;crop_aspect&quot;: &quot;square&quot;
```

##### Outcome
- Any images with a pixel area greater than `(1536 * 1536)` will be resized so that its pixel area is roughly `(1280 * 1280)` while maintaining its original aspect ratio
- Final image size will be random-cropped to a pixel area of `(1024 * 1024)`
- Useful for training on eg. 20 megapixel datasets that need to be resized substantially before cropping to avoid massive loss of scene context in the image (like cropping a picture of a person to just a tile wall or a blurry section of the background)


---

### `prepend_instance_prompt`

- When enabled, all captions will include the `instance_prompt` value at the beginning.

### `only_instance_prompt`

- In addition to `prepend_instance_prompt`, replaces all captions in the dataset with a single phrase or trigger word.

### `repeats`

- Specifies the number of times all samples in the dataset are seen during an epoch. Useful for giving more impact to smaller datasets or maximizing the usage of VAE cache objects.
- If you have a dataset of 1000 images vs one with 100 images, you would likely want to give the lesser dataset a repeats of `9` **or greater** to bring it to 1000 total images sampled.

&gt; ℹ️ This value behaves differently to the same option in Kohya&apos;s scripts, where a value of 1 means no repeats. **For SimpleTuner, a value of 0 means no repeats**. Subtract one from your Kohya config value to obtain the equivalent for SimpleTuner, hence a value of **9** resulting from the calculation `(dataset_length + repeats * dataset_length)` .

### `is_regularisation_data`

- Also may be spelt `is_regularization_data`
- Enables parent-teacher training for LyCORIS adapters so that the prediction target prefers the base model&apos;s result for a given dataset.
  - Standard LoRA are not currently supported.

### `vae_cache_clear_each_epoch`

- When enabled, all VAE cache objects are deleted from the filesystem at the end of each dataset repeat cycle. This can be resource-intensive for large datasets, but combined with `crop_style=random` and/or `crop_aspect=random` you&apos;ll want this enabled to ensure you sample a full range of crops from each image.
- In fact, this option is **enabled by default** when using random bucketing or crops.

### `skip_file_discovery`

- You probably don&apos;t want to ever set this - it is useful only for very large datasets.
- This parameter accepts a comma or space separated list of values, eg. `vae metadata aspect text` to skip file discovery for one or more stages of the loader configuration.
- This is equivalent to the commandline option `--skip_file_discovery`
- This is helpful if you have datasets you don&apos;t need the trainer to scan on every startup, eg. their latents/embeds are already cached fully. This allows quicker startup and resumption of training.

### `preserve_data_backend_cache`

- You probably don&apos;t want to ever set this - it is useful only for very large AWS datasets.
- Like `skip_file_discovery`, this option can be set to prevent unnecessary, lengthy and costly filesystem scans at startup.
- It takes a boolean value, and if set to be `true`, the generated filesystem list cache file will not be removed at launch.
- This is helpful for very large and slow storage systems such as S3 or local SMR spinning hard drives that have extremely slow response times.
- Additionally, on S3, backend listing can add up in cost and should be avoided.

&gt; ⚠️ **Unfortunately, this cannot be set if the data is actively being changed.** The trainer will not see any new data that is added to the pool, it will have to do another full scan.

### `hash_filenames`

- When set, the VAE cache entries&apos; filenames will be hashed. This is not set by default for backwards compatibility, but it allows for datasets with very long filenames to be easily used.

## Filtering captions

### `caption_filter_list`

- **For text embed datasets only.** This may be a JSON list, a path to a txt file, or a path to a JSON document. Filter strings can be simple terms to remove from all captions, or they can be regular expressions. Additionally, sed-style `s/search/replace/` entries may be used to _replace_ strings in the caption rather than simply remove it.

#### Example filter list

A complete example list can be found [here](/config/caption_filter_list.txt.example). It contains common repetitive and negative strings that would be returned by BLIP (all common variety), LLaVA, and CogVLM.

This is a shortened example, which will be explained below:

```
arafed 
this .* has a
^this is the beginning of the string
s/this/will be found and replaced/
```

In order, the lines behave as follows:

- `arafed ` (with a space at the end) will be removed from any caption it is found in. Including a space at the end means the caption will look nicer, as double-spaces won&apos;t remain. This is unnecessary, but it looks nice.
- `this .* has a` is a regular expression that will remove anything that contains &quot;this ... has a&quot;, including any random text in between those two strings; `.*` is a regular expression that means &quot;everything we find&quot; until it finds the &quot;has a&quot; string, when it stops matching.
- `^this is the beginning of the string` will remove the phrase &quot;this is the beginning of the string&quot; from any caption, but only when it appears at the start of the caption.
- `s/this/will be found and replaced/` will result in the first instance of the term &quot;this&quot; in any caption being replaced with &quot;will be found and replaced&quot;.

&gt; ❗Use [regex 101](https://regex101.com) for help debugging and testing regular expressions.

# Advanced techniques

## Advanced Example Configuration

```json
[
  {
    &quot;id&quot;: &quot;something-special-to-remember-by&quot;,
    &quot;type&quot;: &quot;local&quot;,
    &quot;instance_data_dir&quot;: &quot;/path/to/data/tree&quot;,
    &quot;crop&quot;: false,
    &quot;crop_style&quot;: &quot;random|center|corner|face&quot;,
    &quot;crop_aspect&quot;: &quot;square|preserve|closest|random&quot;,
    &quot;crop_aspect_buckets&quot;: [0.33, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75],
    &quot;resolution&quot;: 1.0,
    &quot;resolution_type&quot;: &quot;area|pixel&quot;,
    &quot;minimum_image_size&quot;: 1.0,
    &quot;hash_filenames&quot;: true,
    &quot;prepend_instance_prompt&quot;: false,
    &quot;instance_prompt&quot;: &quot;something to label every image&quot;,
    &quot;only_instance_prompt&quot;: false,
    &quot;caption_strategy&quot;: &quot;filename|instanceprompt|parquet|textfile&quot;,
    &quot;cache_dir_vae&quot;: &quot;/path/to/vaecache&quot;,
    &quot;vae_cache_clear_each_epoch&quot;: true,
    &quot;probability&quot;: 1.0,
    &quot;repeats&quot;: 0,
    &quot;text_embeds&quot;: &quot;alt-embed-cache&quot;,
    &quot;image_embeds&quot;: &quot;vae-embeds-example&quot;
  },
  {
    &quot;id&quot;: &quot;another-special-name-for-another-backend&quot;,
    &quot;type&quot;: &quot;aws&quot;,
    &quot;aws_bucket_name&quot;: &quot;something-yummy&quot;,
    &quot;aws_region_name&quot;: null,
    &quot;aws_endpoint_url&quot;: &quot;https://foo.bar/&quot;,
    &quot;aws_access_key_id&quot;: &quot;wpz-764e9734523434&quot;,
    &quot;aws_secret_access_key&quot;: &quot;xyz-sdajkhfhakhfjd&quot;,
    &quot;aws_data_prefix&quot;: &quot;&quot;,
    &quot;cache_dir_vae&quot;: &quot;s3prefix/for/vaecache&quot;,
    &quot;vae_cache_clear_each_epoch&quot;: true,
    &quot;repeats&quot;: 0
  },
  {
      &quot;id&quot;: &quot;vae-embeds-example&quot;,
      &quot;type&quot;: &quot;local&quot;,
      &quot;dataset_type&quot;: &quot;image_embeds&quot;,
      &quot;disabled&quot;: false,
  },
  {
    &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: true,
    &quot;type&quot;: &quot;aws&quot;,
    &quot;aws_bucket_name&quot;: &quot;textembeds-something-yummy&quot;,
    &quot;aws_region_name&quot;: null,
    &quot;aws_endpoint_url&quot;: &quot;https://foo.bar/&quot;,
    &quot;aws_access_key_id&quot;: &quot;wpz-764e9734523434&quot;,
    &quot;aws_secret_access_key&quot;: &quot;xyz-sdajkhfhakhfjd&quot;,
    &quot;aws_data_prefix&quot;: &quot;&quot;,
    &quot;cache_dir&quot;: &quot;&quot;
  },
  {
    &quot;id&quot;: &quot;alt-embed-cache&quot;,
    &quot;dataset_type&quot;: &quot;text_embeds&quot;,
    &quot;default&quot;: false,
    &quot;type&quot;: &quot;local&quot;,
    &quot;cache_dir&quot;: &quot;/path/to/textembed_cache&quot;
  }
]
```

## Train directly from CSV URL list

**Note: Your CSV must contain the captions for your images.**

&gt; ⚠️ This is an advanced **and** experimental feature, and you may run into problems. If you do, please open an [issue](https://github.com/bghira/simpletuner/issues)!

Instead of manually downloading your data from a URL list, you might wish to plug them in directly to the trainer.

**Note:** It&apos;s always better to manually download the image data. Another strategy to save local disk space might be to try [using cloud storage with local encoder caches](#local-cache-with-cloud-dataset) instead.

### Advantages

- No need to directly download the data
- Can make use of SimpleTuner&apos;s caption toolkit to directly caption the URL list
- Saves on disk space, since only the image embeds (if applicable) and text embeds are stored

### Disadvantages

- Requires a costly and potentially slow aspect bucket scan where each image is downloaded and its metadata collected
- The downloaded images are cached on-disk, which can grow to be very large. This is an area of improvement to work on, as the cache management in this version is very basic, write-only/delete-never
- If your dataset has a large number of invalid URLs, these might waste time on resumption as, currently, bad samples are **never** removed from the URL list
  - **Suggestion:** Run a URL validation task beforehand and remove any bad samples.

### Configuration

Required keys:

- `type: &quot;csv&quot;`
- `csv_caption_column`
- `csv_cache_dir`
- `caption_strategy: &quot;csv&quot;`

```json
[
    {
        &quot;id&quot;: &quot;csvtest&quot;,
        &quot;type&quot;: &quot;csv&quot;,
        &quot;csv_caption_column&quot;: &quot;caption&quot;,
        &quot;csv_file&quot;: &quot;/Volumes/ml/dataset/test_list.csv&quot;,
        &quot;csv_cache_dir&quot;: &quot;/Volumes/ml/cache/csv/test&quot;,
        &quot;cache_dir_vae&quot;: &quot;/Volumes/ml/cache/vae/sdxl&quot;,
        &quot;caption_strategy&quot;: &quot;csv&quot;,
        &quot;image_embeds&quot;: &quot;image-embeds&quot;,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;resolution&quot;: 1024,
        &quot;maximum_image_size&quot;: 1024,
        &quot;target_downsample_size&quot;: 1024,
        &quot;resolution_type&quot;: &quot;pixel&quot;,
        &quot;minimum_image_size&quot;: 0,
        &quot;disabled&quot;: false,
        &quot;skip_file_discovery&quot;: &quot;&quot;,
        &quot;preserve_data_backend_cache&quot;: false,
        &quot;hash_filenames&quot;: true
    },
    {
      &quot;id&quot;: &quot;image-embeds&quot;,
      &quot;type&quot;: &quot;local&quot;
    },
    {
        &quot;id&quot;: &quot;text-embeds&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;cache_dir&quot;: &quot;/Volumes/ml/cache/text/sdxl&quot;,
        &quot;disabled&quot;: false,
        &quot;preserve_data_backend_cache&quot;: false,
        &quot;skip_file_discovery&quot;: &quot;&quot;,
        &quot;write_batch_size&quot;: 128
    }
]
```

## Parquet caption strategy / JSON Lines datasets

&gt; ⚠️ This is an advanced feature, and will not be necessary for most users.

When training a model with a very-large dataset numbering in the hundreds of thousands or millions of images, it&apos;s fastest to store your metadata inside a parquet database instead of txt files - especially when your training data is stored on an S3 bucket.

Using the parquet caption strategy allows you to name all of your files by their `id` value, and change their caption column via a config value rather than updating many text files, or having to rename the files to update their captions.

Here is an example dataloader configuration that makes use of the captions and data in the [photo-concept-bucket](https://huggingface.co/datasets/ptx0/photo-concept-bucket) dataset:

```json
{
  &quot;id&quot;: &quot;photo-concept-bucket&quot;,
  &quot;type&quot;: &quot;local&quot;,
  &quot;instance_data_dir&quot;: &quot;/models/training/datasets/photo-concept-bucket-downloads&quot;,
  &quot;caption_strategy&quot;: &quot;parquet&quot;,
  &quot;metadata_backend&quot;: &quot;parquet&quot;,
  &quot;parquet&quot;: {
    &quot;path&quot;: &quot;photo-concept-bucket.parquet&quot;,
    &quot;filename_column&quot;: &quot;id&quot;,
    &quot;caption_column&quot;: &quot;cogvlm_caption&quot;,
    &quot;fallback_caption_column&quot;: &quot;tags&quot;,
    &quot;width_column&quot;: &quot;width&quot;,
    &quot;height_column&quot;: &quot;height&quot;,
    &quot;identifier_includes_extension&quot;: false
  },
  &quot;resolution&quot;: 1.0,
  &quot;minimum_image_size&quot;: 0.75,
  &quot;maximum_image_size&quot;: 2.0,
  &quot;target_downsample_size&quot;: 1.5,
  &quot;prepend_instance_prompt&quot;: false,
  &quot;instance_prompt&quot;: null,
  &quot;only_instance_prompt&quot;: false,
  &quot;disable&quot;: false,
  &quot;cache_dir_vae&quot;: &quot;/models/training/vae_cache/photo-concept-bucket&quot;,
  &quot;probability&quot;: 1.0,
  &quot;skip_file_discovery&quot;: &quot;&quot;,
  &quot;preserve_data_backend_cache&quot;: false,
  &quot;vae_cache_clear_each_epoch&quot;: true,
  &quot;repeats&quot;: 1,
  &quot;crop&quot;: true,
  &quot;crop_aspect&quot;: &quot;closest&quot;,
  &quot;crop_style&quot;: &quot;random&quot;,
  &quot;crop_aspect_buckets&quot;: [1.0, 0.75, 1.23],
  &quot;resolution_type&quot;: &quot;area&quot;
}
```

In this configuration:

- `caption_strategy` is set to `parquet`.
- `metadata_backend` is set to `parquet`.
- A new section, `parquet` must be defined:
  - `path` is the path to the parquet or JSONL file.
  - `filename_column` is the name of the column in the table that contains the filenames. For this case, we are using the numeric `id` column (recommended).
  - `caption_column` is the name of the column in the table that contains the captions. For this case, we are using the `cogvlm_caption` column. For LAION datasets, this would be the TEXT field.
  - `width_column` and `height_column` can be a column containing strings, int, or even a single-entry Series data type, measuring the actual image&apos;s dimensions. This notably improves the dataset preparation time, as we don&apos;t need to access the real images to discover this information.
  - `fallback_caption_column` is an optional name of a column in the table that contains fallback captions. These are used if the primary caption field is empty. For this case, we are using the `tags` column.
  - `identifier_includes_extension` should be set to `true` when your filename column contains the image extension. Otherwise, the extension will be assumed as `.png`. It is recommended to include filename extensions in your table filename column.

&gt; ⚠️ Parquet support capability is limited to reading captions. You must separately populate a data source with your image samples using &quot;{id}.png&quot; as their filename. See scripts in the [toolkit/datasets](toolkit/datasets) directory for ideas.

As with other dataloader configurations:

- `prepend_instance_prompt` and `instance_prompt` behave as normal.
- Updating a sample&apos;s caption in between training runs will cache the new embed, but not remove the old (orphaned) unit.
- When an image doesn&apos;t exist in a dataset, its filename will be used as its caption and an error will be emitted.

## Local cache with cloud dataset

In order to maximise the use of costly local NVMe storage, you may wish to store just the image files (png, jpg) on an S3 bucket, and use the local storage to cache your extracted feature maps from the text encoder(s) and VAE (if applicable).

In this example configuration:

- Image data is stored on an S3-compatible bucket
- VAE data is stored in /local/path/to/cache/vae
- Text embeds are stored in /local/path/to/cache/textencoder

&gt; ⚠️ Remember to configure the other dataset options, such as `resolution` and `crop`

```json
[
    {
        &quot;id&quot;: &quot;data&quot;,
        &quot;type&quot;: &quot;aws&quot;,
        &quot;aws_bucket_name&quot;: &quot;text-vae-embeds&quot;,
        &quot;aws_endpoint_url&quot;: &quot;https://storage.provider.example&quot;,
        &quot;aws_access_key_id&quot;: &quot;exampleAccessKey&quot;,
        &quot;aws_secret_access_key&quot;: &quot;exampleSecretKey&quot;,
        &quot;aws_region_name&quot;: null,
        &quot;cache_dir_vae&quot;: &quot;/local/path/to/cache/vae/&quot;,
        &quot;caption_strategy&quot;: &quot;parquet&quot;,
        &quot;metadata_backend&quot;: &quot;parquet&quot;,
        &quot;parquet&quot;: {
            &quot;path&quot;: &quot;train.parquet&quot;,
            &quot;caption_column&quot;: &quot;caption&quot;,
            &quot;filename_column&quot;: &quot;filename&quot;,
            &quot;width_column&quot;: &quot;width&quot;,
            &quot;height_column&quot;: &quot;height&quot;,
            &quot;identifier_includes_extension&quot;: true
        },
        &quot;preserve_data_backend_cache&quot;: false,
        &quot;image_embeds&quot;: &quot;vae-embed-storage&quot;
    },
    {
        &quot;id&quot;: &quot;vae-embed-storage&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;image_embeds&quot;
    },
    {
        &quot;id&quot;: &quot;text-embed-storage&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;cache_dir&quot;: &quot;/local/path/to/cache/textencoder/&quot;,
        &quot;write_batch_size&quot;: 128
    }
]
```

**Note:** The `image_embeds` dataset does not have any options to set for data paths. Those are configured via `cache_dir_vae` on the image backend.

## Custom aspect ratio-to-resolution mapping

When SimpleTuner first launches, it generates resolution-specific aspect mapping lists that link a decimal aspect-ratio value to its target pixel size.

It&apos;s possible to create a custom mapping that forces the trainer to adjust to your chosen target resolution instead of its own calculations. This functionality is provided at your own risk, as it can obviously cause great harm if configured incorrectly.

To create the custom mapping:

- Create a file that follows the example (below)
- Name the file using the format `aspect_ratio_map-{resolution}.json`
  - For a configuration value of `resolution=1.0` / `resolution_type=area`, the mapping filename will be `aspect_resolution_map-1.0.json`
- Place this file in the location specified as `--output_dir`
  - This is the same location where your checkpoints and validation images will be found.
- No additional configuration flags or options are required. It will be automatically discovered and used, as long as the name and location are correct.

### Example mapping configuration

This is an example aspect ratio mapping generated by SimpleTuner. You don&apos;t need to manually configure this, as the trainer will automatically create one. However, for full control over the resulting resolutions, these mappings are supplied as a starting point for modification.

- The dataset had more than 1 million images
- The dataloader `resolution` was set to `1.0`
- The dataloader `resolution_type` was set to `area`

This is the most common configuration, and list of aspect buckets trainable for a 1 megapixel model.

```json
{
    &quot;0.07&quot;: [320, 4544],    &quot;0.38&quot;: [640, 1664],    &quot;0.88&quot;: [960, 1088],    &quot;1.92&quot;: [1472, 768],    &quot;3.11&quot;: [1792, 576],    &quot;5.71&quot;: [2560, 448],
    &quot;0.08&quot;: [320, 3968],    &quot;0.4&quot;: [640, 1600],     &quot;0.89&quot;: [1024, 1152],   &quot;2.09&quot;: [1472, 704],    &quot;3.22&quot;: [1856, 576],    &quot;6.83&quot;: [2624, 384],
    &quot;0.1&quot;: [320, 3328],     &quot;0.41&quot;: [704, 1728],    &quot;0.94&quot;: [1024, 1088],   &quot;2.18&quot;: [1536, 704],    &quot;3.33&quot;: [1920, 576],    &quot;7.0&quot;: [2688, 384],
    &quot;0.11&quot;: [384, 3520],    &quot;0.42&quot;: [704, 1664],    &quot;1.06&quot;: [1088, 1024],   &quot;2.27&quot;: [1600, 704],    &quot;3.44&quot;: [1984, 576],    &quot;8.0&quot;: [3072, 384],
    &quot;0.12&quot;: [384, 3200],    &quot;0.44&quot;: [704, 1600],    &quot;1.12&quot;: [1152, 1024],   &quot;2.5&quot;: [1600, 640],     &quot;3.88&quot;: [1984, 512],
    &quot;0.14&quot;: [384, 2688],    &quot;0.46&quot;: [704, 1536],    &quot;1.13&quot;: [1088, 960],    &quot;2.6&quot;: [1664, 640],     &quot;4.0&quot;: [2048, 512],
    &quot;0.15&quot;: [448, 3008],    &quot;0.48&quot;: [704, 1472],    &quot;1.2&quot;: [1152, 960],     &quot;2.7&quot;: [1728, 640],     &quot;4.12&quot;: [2112, 512],
    &quot;0.16&quot;: [448, 2816],    &quot;0.5&quot;: [768, 1536],     &quot;1.36&quot;: [1216, 896],    &quot;2.8&quot;: [1792, 640],     &quot;4.25&quot;: [2176, 512],
    &quot;0.19&quot;: [448, 2304],    &quot;0.52&quot;: [768, 1472],    &quot;1.46&quot;: [1216, 832],    &quot;3.11&quot;: [1792, 576],    &quot;4.38&quot;: [2240, 512],
    &quot;0.24&quot;: [512, 2112],    &quot;0.55&quot;: [768, 1408],    &quot;1.54&quot;: [1280, 832],    &quot;3.22&quot;: [1856, 576],    &quot;5.0&quot;: [2240, 448],
    &quot;0.26&quot;: [512, 1984],    &quot;0.59&quot;: [832, 1408],    &quot;1.83&quot;: [1408, 768],    &quot;3.33&quot;: [1920, 576],    &quot;5.14&quot;: [2304, 448],
    &quot;0.29&quot;: [576, 1984],    &quot;0.62&quot;: [832, 1344],    &quot;1.92&quot;: [1472, 768],    &quot;3.44&quot;: [1984, 576],    &quot;5.71&quot;: [2560, 448],
    &quot;0.31&quot;: [576, 1856],    &quot;0.65&quot;: [832, 1280],    &quot;2.09&quot;: [1472, 704],    &quot;3.88&quot;: [1984, 512],    &quot;6.83&quot;: [2624, 384],
    &quot;0.34&quot;: [640, 1856],    &quot;0.68&quot;: [832, 1216],    &quot;2.18&quot;: [1536, 704],    &quot;4.0&quot;: [2048, 512],     &quot;7.0&quot;: [2688, 384],
    &quot;0.38&quot;: [640, 1664],    &quot;0.74&quot;: [896, 1216],    &quot;2.27&quot;: [1600, 704],    &quot;4.12&quot;: [2112, 512],    &quot;8.0&quot;: [3072, 384],
    &quot;0.4&quot;: [640, 1600],     &quot;0.83&quot;: [960, 1152],    &quot;2.5&quot;: [1600, 640],     &quot;4.25&quot;: [2176, 512],
    &quot;0.41&quot;: [704, 1728],    &quot;0.88&quot;: [960, 1088],    &quot;2.6&quot;: [1664, 640],     &quot;4.38&quot;: [2240, 512],
    &quot;0.42&quot;: [704, 1664],    &quot;0.89&quot;: [1024, 1152],   &quot;2.7&quot;: [1728, 640],     &quot;5.0&quot;: [2240, 448],
    &quot;0.44&quot;: [704, 1600],    &quot;0.94&quot;: [1024, 1088],   &quot;2.8&quot;: [1792, 640],     &quot;5.14&quot;: [2304, 448]
}
```

For Stable Diffusion 1.5 / 2.0-base (512px) models, the following mapping will work:

```json
{
    &quot;1.3&quot;: [832, 640], &quot;1.0&quot;: [768, 768], &quot;2.0&quot;: [1024, 512],
    &quot;0.64&quot;: [576, 896], &quot;0.77&quot;: [640, 832], &quot;0.79&quot;: [704, 896],
    &quot;0.53&quot;: [576, 1088], &quot;1.18&quot;: [832, 704], &quot;0.85&quot;: [704, 832],
    &quot;0.56&quot;: [576, 1024], &quot;0.92&quot;: [704, 768], &quot;1.78&quot;: [1024, 576],
    &quot;1.56&quot;: [896, 576], &quot;0.67&quot;: [640, 960], &quot;1.67&quot;: [960, 576],
    &quot;0.5&quot;: [512, 1024], &quot;1.09&quot;: [768, 704], &quot;1.08&quot;: [832, 768],
    &quot;0.44&quot;: [512, 1152], &quot;0.71&quot;: [640, 896], &quot;1.4&quot;: [896, 640],
    &quot;0.39&quot;: [448, 1152], &quot;2.25&quot;: [1152, 512], &quot;2.57&quot;: [1152, 448],
    &quot;0.4&quot;: [512, 1280], &quot;3.5&quot;: [1344, 384], &quot;2.12&quot;: [1088, 512],
    &quot;0.3&quot;: [448, 1472], &quot;2.71&quot;: [1216, 448], &quot;8.25&quot;: [2112, 256],
    &quot;0.29&quot;: [384, 1344], &quot;2.86&quot;: [1280, 448], &quot;6.2&quot;: [1984, 320],
    &quot;0.6&quot;: [576, 960]
}
```</file><file path="documentation/DEEPFLOYD.md"># DeepFloyd IF

&gt; ⚠️ Support for tuning DeepFloyd-IF is deprecated, and may not work. Open an issue report if any problems are encountered.

&gt; 🤷🏽‍♂️ Training DeepFloyd requires at least 24G VRAM for a LoRA. This guide focuses on the 400M parameter base model, though the 4.3B XL flavour can be trained using the same guidelines.

## Background

In spring of 2023, StabilityAI released a cascaded pixel diffusion model called DeepFloyd.
![](https://tripleback.net/public/deepfloyd.png)

Comparing briefly to Stable Diffusion XL:
- Text encoder
  - SDXL uses two CLIP encoders, &quot;OpenCLIP G/14&quot; and &quot;OpenAI CLIP-L/14&quot;
  - DeepFloyd uses a single self-supervised transformer model, Google&apos;s T5 XXL
- Parameter count
  - DeepFloyd comes in multiple flavours of density: 400M, 900M, and 4.3B parameters. Each larger unit is successively more expensive to train.
  - SDXL has just one, ~3B parameters.
  - DeepFloyd&apos;s text encoder has 11B parameters in it alone, making the fattest configuration roughly 15.3B parameters.
- Model count
  - DeepFloyd runs in **three** stages: 64px -&gt; 256px -&gt; 1024px
    - Each stage fully completes its denoising objective
  - SDXL runs in **two** stages, including its refiner, from 1024px -&gt; 1024px
    - Each stage only partly completes its denoising objective
- Design
  - DeepFloyd&apos;s three models increase resolution and fine details
  - SDXL&apos;s two models manage fine details and composition

For both models, the first stage defines most of the image&apos;s composition (where large items / shadows appear).

## Model assessment

Here&apos;s what you can expect when using DeepFloyd for training or inference.

### Aesthetics

When compared to SDXL or Stable Diffusion 1.x/2.x, DeepFloyd&apos;s aesthetics lie somewhere between Stable Diffusion 2.x and SDXL.


### Disadvantages

This is not a popular model, for various reasons:

- Inference-time compute VRAM requirement is heavier than other models
- Training-time compute VRAM requirements dwarf other models
  - A full u-net tune needing more than 48G VRAM
  - LoRA at rank-32, batch-4 needs ~24G VRAM
  - The text embed cache objects are ENORMOUS (multiple Megabytes each, vs hundreds of Kilobytes for SDXL&apos;s dual CLIP embeds)
  - The text embed cache objects are SLOW TO CREATE, about 9-10 per second currently on an A6000 non-Ada.
- The default aesthetic is worse than other models (like trying to train vanilla SD 1.5)
- There&apos;s **three** models to finetune or load onto your system during inference (four if you count the text encoder)
- The promises from StabilityAI did not meet the reality of what it felt like to use the model (over-hyped)
- The DeepFloyd-IF license is restrictive against commercial use.
  - This didn&apos;t impact the NovelAI weights, which were in fact leaked illicitly. The commercial license nature seems like a convenient excuse, considering the other, bigger issues.

### Advantages

However, DeepFloyd really has its upsides that often go overlooked:

- At inference time, the T5 text encoder demonstrates a strong understanding of the world
- Can be natively trained on very-long captions
- The first stage is ~64x64 pixel area, and can be trained on multi-aspect resolutions
  - The low-resolution nature of the training data means DeepFloyd was _the only model_ capable of training on _ALL_ of LAION-A (few images are under 64x64 in LAION)
- Each stage can be tuned independently, focusing on different objectives
  - The first stage can be tuned focusing on compositional qualities, and the later stages are tuned for better upscaled details
- It trains very quickly despite its larger training memory footprint
  - Trains quicker in terms of throughput - a high samples per hour rate is observed on stage 1 tuning
  - Learns more quickly than a CLIP equivalent model, perhaps to the detriment of people used to training CLIP models
    - In other words, you will have to adjust your expectations of learning rates and training schedules
- There is no VAE, the training samples are directly downscaled into their target size and the pixels are consumed by the U-net
- It supports ControlNet LoRAs and many other tricks that work on typical linear CLIP u-nets.

## Fine-tuning a LoRA

&gt; ⚠️ Due to the compute requirements of full u-net backpropagation in even DeepFloyd&apos;s smallest 400M model, it has not been tested. LoRA will be used for this document, though full u-net tuning should also work.

Training DeepFloyd makes use of the &quot;legacy&quot; SD 1.x/2.x trainer in SimpleTuner to reduce code duplication by keeping similar models together.

As such, we&apos;ll be making use of the `sd2x-env.sh` configuration file for tuning DeepFloyd:

### sd2x-env.sh

```bash
# Possible values:
# - deepfloyd-full
# - deepfloyd-lora
# - deepfloyd-stage2
# - deepfloyd-stage2-lora
export MODEL_TYPE=&quot;deepfloyd-lora&quot;

# DoRA isn&apos;t tested a whole lot yet. It&apos;s still new and experimental.
export USE_DORA=false
# Bitfit hasn&apos;t been tested for efficacy on DeepFloyd.
# It will probably work, but no idea what the outcome is.
export USE_BITFIT=false

# Highest learning rate to use.
export LEARNING_RATE=4e-5 #@param {type:&quot;number&quot;}
# For schedules that decay or oscillate, this will be the end LR or the bottom of the valley.
export LEARNING_RATE_END=4e-6 #@param {type:&quot;number&quot;}

## Using a Huggingface Hub model for Stage 1 tuning:
#export MODEL_NAME=&quot;DeepFloyd/IF-I-M-v1.0&quot;
## Using a Huggingface Hub model for Stage 2 tuning:
#export MODEL_NAME=&quot;DeepFloyd/IF-II-M-v1.0&quot;
# Using a local path to a huggingface hub model or saved checkpoint:
#export MODEL_NAME=&quot;/notebooks/datasets/models/pipeline&quot;

# Where to store your results.
export BASE_DIR=&quot;/training&quot;
export OUTPUT_DIR=&quot;${BASE_DIR}/models/deepfloyd&quot;
export DATALOADER_CONFIG=&quot;multidatabackend_deepfloyd.json&quot;

# Max number of steps OR epochs can be used. But we default to Epochs.
export MAX_NUM_STEPS=50000
export NUM_EPOCHS=0

# Adjust this for your GPU memory size.
export TRAIN_BATCH_SIZE=1

# &quot;pixel&quot; is using pixel edge length on the smaller or square side of the image.
# this is how DeepFloyd was originally trained.
export RESOLUTION_TYPE=&quot;pixel&quot;
export RESOLUTION=64          # 1.0 Megapixel training sizes

# Validation is when the model is used during training to make test outputs.
export VALIDATION_RESOLUTION=96x64                                                  # The resolution of the validation images. Default: 64x64
export VALIDATION_STEPS=250                                                         # How long between each validation run. Default: 250
export VALIDATION_NUM_INFERENCE_STEPS=25                                            # How many inference steps to do. Default: 25
export VALIDATION_PROMPT=&quot;an ethnographic photograph of a teddy bear at a picnic&quot;   # What to make for the first/only test image.
export VALIDATION_NEGATIVE_PROMPT=&quot;blurry, ugly, cropped, amputated&quot;                # What to avoid in the first/only test image.

# These can be left alone.
export VALIDATION_GUIDANCE=7.5
export VALIDATION_GUIDANCE_RESCALE=0.0
export VALIDATION_SEED=42

export GRADIENT_ACCUMULATION_STEPS=1         # Accumulate over many steps. Default: 1
export MIXED_PRECISION=&quot;bf16&quot;                # SimpleTuner requires bf16.
export PURE_BF16=true                        # Will not use mixed precision, but rather pure bf16 (bf16 requires pytorch 2.3 on MPS.)
export OPTIMIZER=&quot;adamw_bf16&quot;
export USE_XFORMERS=true
```

A keen eye will have observed the following:

- The `MODEL_TYPE` is specified as deepfloyd-compatible
- The `MODEL_NAME` is pointing to Stage I or II
- `RESOLUTION` is now `64` and `RESOLUTION_TYPE` is `pixel`
- `USE_XFORMERS` is set to `true`, but AMD and Apple users won&apos;t be able to set this, requiring more VRAM.
  - **Note** Apple MPS currently has a bug preventing DeepFloyd tuning from working at all.

For more thorough validations, the value for `VALIDATION_RESOLUTION` can be set as:

- `VALIDATION_RESOLUTION=64` will result in a 64x64 square image.
- `VALIDATION_RESOLUTION=96x64` will result in a 3:2 widescreen image.
- `VALIDATION_RESOLUTION=64,96,64x96,96x64` will result in four images being generated for each validation:
  - 64x64
  - 96x96
  - 64x96
  - 96x64

### multidatabackend_deepfloyd.json

Now let&apos;s move onto configuring the dataloader for DeepFloyd training. This will be nearly identical to configuration of SDXL or legacy model datasets, with a focus on resolution parameters.

```json
[
    {
        &quot;id&quot;: &quot;primary-dataset&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/data/primary-dataset&quot;,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;random&quot;,
        &quot;resolution&quot;: 64,
        &quot;resolution_type&quot;: &quot;pixel&quot;,
        &quot;minimum_image_size&quot;: 64,
        &quot;maximum_image_size&quot;: 256,
        &quot;target_downsample_size&quot;: 128,
        &quot;prepend_instance_prompt&quot;: false,
        &quot;instance_prompt&quot;: &quot;Your Subject Trigger Phrase or Word&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;repeats&quot;: 1
    },
    {
        &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;disable&quot;: false,
        &quot;type&quot;: &quot;local&quot;,
        &quot;cache_dir&quot;: &quot;/training/cache/deepfloyd/text/dreambooth&quot;
    }
]
```

Provided above is a basic Dreambooth configuration for DeepFloyd:

- The values for `resolution` and `resolution_type` are set to `64` and `pixel`, respectively
- The value for `minimum_image_size` is reduced to 64 pixels to ensure we don&apos;t accidentally upsample any smaller images
- The value for `maximum_image_size` is set to 256 pixels to ensure that any large images do not become cropped at a ratio of more than 4:1, which may result in catastrophic scene context loss
- The value for `target_downsample_size` is set to 128 pixels so that any images larger than `maximum_image_size` of 256 pixels are first resized to 128 pixels before cropping

Note: images are downsampled 25% at a time so to avoid extreme leaps in image size causing an incorrect averaging of the scene&apos;s details.

## Running inference

Currently, DeepFloyd does not have any dedicated inference scripts in the SimpleTuner toolkit.

Other than the built-in validations process, you may want to reference [this document from Hugging Face](https://huggingface.co/docs/diffusers/v0.23.1/en/training/dreambooth#if) which contains a small example for running inference afterward:

```py
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(&quot;DeepFloyd/IF-I-M-v1.0&quot;, use_safetensors=True)
pipe.load_lora_weights(&quot;&lt;lora weights path&gt;&quot;)
pipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type=&quot;fixed_small&quot;)
```

&gt; ⚠️ Note that the first value for `DiffusionPipeline.from_pretrained(...)` is set to `IF-I-M-v1.0`, but you must update this to use the base model path that you trained your LoRA on.

&gt; ⚠️ Note that not all of the recommendations from Hugging Face apply to SimpleTuner. For example, we can tune DeepFloyd stage I LoRA in just 22G of VRAM vs 28G for Diffusers&apos; example dreambooth scripts thanks to efficient pre-caching and pure-bf16 optimiser states. 8Bit AdamW isn&apos;t currently supported by SimpleTuner.

## Fine-tuning the super-resolution stage II model

DeepFloyd&apos;s stage II model takes inputs around 64x64 (or 96x64) images, and returns the resulting upscaled image using the `VALIDATION_RESOLUTION` setting.

The eval images are automatically collected from your datasets, such that `--num_eval_images` will specify how many upscale images to select from each dataset. The images are currently selected at random - but they&apos;ll remain the same on each session.

A few more checks are in place to ensure you don&apos;t accidentally run with the incorrect sizes set.

To train stage II, you just need to follow the steps above, using `deepfloyd-stage2-lora` in place of `deepfloyd-lora` for `MODEL_TYPE`:

```bash
export MODEL_TYPE=&quot;deepfloyd-stage2-lora&quot;
```</file><file path="documentation/DEEPSPEED.md"># DeepSpeed offload / multi-GPU training

SimpleTuner v0.7 includes preliminary support for training SDXL using DeepSpeed ZeRO stages 1 through 3.

&gt; ⚠️ Stable Diffusion 3 support for DeepSpeed is not tested, and will be unlikely to work without modification.

**Training SDXL 1.0 on 9237MiB of VRAM**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:08:00.0 Off |                  Off |
|  0%   43C    P2   100W / 450W |   9237MiB / 24564MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     11500      C   ...uner/.venv/bin/python3.11     9232MiB |
+-----------------------------------------------------------------------------+
```

These memory savings have been achieved through the use of DeepSpeed ZeRO Stage 2 offload. Without that, the SDXL U-net will consume more than 24G of VRAM, causing the dreaded CUDA Out of Memory exception.

## What is DeepSpeed?

ZeRO stands for **Zero Redundancy Optimizer**. This technique reduces the memory consumption of each GPU by partitioning the various model training states (weights, gradients, and optimizer states) across the available devices (GPUs and CPUs).

ZeRO is being implemented as incremental stages of optimizations, where optimizations in earlier stages are available in the later stages. To deep dive into ZeRO, please see the original [paper](https://arxiv.org/abs/1910.02054v3) (1910.02054v3).

## Known issues

### LoRA support

Due to how DeepSpeed changes the model saving routines, it&apos;s not currently supported to train LoRA with DeepSpeed.

This may change in a future release.

### Enabling / disabling DeepSpeed on existing checkpoints

Currently in SimpleTuner, DeepSpeed cannot be **enabled** when resuming from a checkpoint that did **not** previously use DeepSpeed.

Conversely, DeepSpeed cannot be **disabled** when attempting to resume training from a checkpoint that was trained using DeepSpeed.

To workaround this issue, export the training pipeline to a complete set of model weights before attempting to enable/disable DeepSpeed on an in-progress training session.

It&apos;s unlikely this support will ever come to fruition, as DeepSpeed&apos;s optimiser is very different from any of the usual choices.

## DeepSpeed Stages

DeepSpeed offers three levels of optimisation for training a model, with each increase having more and more overhead.

Especially for multi-GPU training, the CPU transfers are currently not highly optimised within DeepSpeed.

Because of this overhead, it is recommended that the **lowest** level of DeepSpeed that works, be the one you select.

### Stage 1

The optimizer states (e.g., for Adam optimizer, 32-bit weights, and the first, and second moment estimates) are partitioned across the processes, so that each process updates only its partition.

### Stage 2

The reduced 32-bit gradients for updating the model weights are also partitioned such that each process retains only the gradients corresponding to its portion of the optimizer states.

### Stage 3

The 16-bit model parameters are partitioned across the processes. ZeRO-3 will automatically collect and partition them during the forward and backward passes.

## Enabling DeepSpeed

The [official tutorial](https://www.deepspeed.ai/tutorials/zero/) is very well-structured and includes many scenarios not outlined here.

DeepSpeed is supported by 🤗Accelerate, and can be easily enabled through `accelerate config`:

```
----------------------------------------------------------------------------------------------------------------------------
In which compute environment are you running?
This machine
----------------------------------------------------------------------------------------------------------------------------
Which type of machine are you using?
No distributed training
Do you want to run your training on CPU only (even if a GPU / Apple Silicon / Ascend NPU device is available)? [yes/NO]:NO  
Do you wish to optimize your script with torch dynamo?[yes/NO]:NO
Do you want to use DeepSpeed? [yes/NO]: yes
Do you want to specify a json file to a DeepSpeed config? [yes/NO]: NO
----------------------------------------------------------------------------------------------------------------------------
What should be your DeepSpeed&apos;s ZeRO optimization stage?
1
How many gradient accumulation steps you&apos;re passing in your script? [1]: 4                                                  
Do you want to use gradient clipping? [yes/NO]:
Do you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]:
How many GPU(s) should be used for distributed training? [1]:
----------------------------------------------------------------------------------------------------------------------------
Do you wish to use FP16 or BF16 (mixed precision)?bf16                                                                                                                        
accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml                              
```

This results in the following yaml file:

```yaml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  gradient_accumulation_steps: 4
  zero3_init_flag: false
  zero_stage: 1
distributed_type: DEEPSPEED
downcast_bf16: &apos;no&apos;
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

## Configuring SimpleTuner

SimpleTuner requires no special configuration for its use of DeepSpeed.

If using ZeRO stage 2 or 3 with NVMe offload, `--offload_param_path=/path/to/offload` can be supplied, to store the parameter/optimiser offload files on a dedicated partition. This storage should ideally be an NVMe device, but any storage will do.

### DeepSpeed Optimizer / Learning rate scheduler

DeepSpeed uses its own learning rate scheduler and, by default, a heavily-optimised version of AdamW - though, not 8bit. That seems less important for DeepSpeed, as things will tend to stay closer to the CPU.

If a `scheduler` or `optimizer` are configured in your `default_config.yaml`, those will be used. If no `scheduler` or `optimizer` are defined, the default `AdamW` and `WarmUp` options will be used as optimiser and scheduler, respectively.

## Some quick test results

Using a 4090 24G GPU:

* We can now train the full U-net at 1 megapixel (1024^2 pixel area) using just **13102MiB of VRAM for batch size 8**
  * This operated at 8 seconds per iteration. This means 1000 steps of training can be done in a little under 2 and 1/2 hours.
  * As indicated in the DeepSpeed tutorial, it may be advantageous to attempt to tune the batch size to a lower value, so that the available VRAM is used for parameters and optimiser states.
    * However, SDXL is a relatively small model, and we can potentially avoid some of the recommendations if the performance impact is acceptable.
* At **128x128** image size on a batch size of 8, training consumes as little as **9237MiB of VRAM**. This is a potentially niche use case for pixel art training, which requires a 1:1 mapping with the latent space.

Within these parameters, you will find varying degrees of success and can quite possibly even fit the full u-net training into as little as 8GiB of VRAM at 1024x1024 for a batch size of 1 (untested).

Since SDXL was trained for many steps on a large distribution of image resolutions and aspect ratios, you can even reduce the pixel area down to .75 megapixels, roughly 768x768 and further optimise memory use.

# AMD device support

I do not have any consumer or workstation-grade AMD GPUs, however, there are some reports that the MI50 (now going out of support) and other higher grade Instinct cards **do** work with DeepSpeed. AMD maintains a repository for their implementation.

# EMA training (Exponential moving average)

While EMA is a great way to smooth out gradients and improve generalisation abilities of the resulting weights, it is a very memory heavy affair.

EMA holds a shadow copy of the model parameters in memory, essentially doubling the footprint of the model. For SimpleTuner, EMA is not passed through the Accelerator module, which means it is not impacted by DeepSpeed. This means the memory savings that we saw with the base U-net, are not realised with the EMA model.

However, by default, the EMA model is kept on CPU.</file><file path="documentation/DISTRIBUTED.md"># Distributed Training (Multi-node)

This document contains notes* on configuring a 4-way 8xH100 cluster for use with SimpleTuner.

&gt; *This guide does not contain full end-to-end installation instructions. Instead, these serve as considerations to take when following the [INSTALL](/INSTALL.md) document or one of the [quickstart guides](/documentation/QUICKSTART.md).

## Storage backend

Multi-node training requires by default the use of shared storage between nodes for the `output_dir`


### Ubuntu NFS example

Just a basic storage example that will get you started.

#### On the &apos;master&apos; node that will write the checkpoints

**1. Install NFS Server Packages**

```bash
sudo apt update
sudo apt install nfs-kernel-server
```

**2. Configure the NFS Export**

Edit the NFS exports file to share the directory:

```bash
sudo nano /etc/exports
```

Add the following line at the end of the file (replace `slave_ip` with the actual IP address of your slave machine):

```
/home/ubuntu/simpletuner/output slave_ip(rw,sync,no_subtree_check)
```

*If you want to allow multiple slaves or an entire subnet, you can use:*

```
/home/ubuntu/simpletuner/output subnet_ip/24(rw,sync,no_subtree_check)
```

**3. Export the Shared Directory**

```bash
sudo exportfs -a
```

**4. Restart the NFS Server**

```bash
sudo systemctl restart nfs-kernel-server
```

**5. Verify NFS Server Status**

```bash
sudo systemctl status nfs-kernel-server
```

---

#### On the slave nodes that send optimiser and other states

**1. Install NFS Client Packages**

```bash
sudo apt update
sudo apt install nfs-common
```

**2. Create the Mount Point Directory**

Ensure the directory exists (it should already exist based on your setup):

```bash
sudo mkdir -p /home/ubuntu/simpletuner/output
```

*Note:* If the directory contains data, back it up, as mounting will hide existing contents.

**3. Mount the NFS Share**

Mount the master&apos;s shared directory to the slave&apos;s local directory (replace `master_ip` with the master&apos;s IP address):

```bash
sudo mount master_ip:/home/ubuntu/simpletuner/output /home/ubuntu/simpletuner/output
```

**4. Verify the Mount**

Check that the mount is successful:

```bash
mount | grep /home/ubuntu/simpletuner/output
```

**5. Test Write Access**

Create a test file to ensure you have write permissions:

```bash
touch /home/ubuntu/simpletuner/output/test_file_from_slave.txt
```

Then, check on the master machine if the file appears in `/home/ubuntu/simpletuner/output`.

**6. Make the Mount Persistent**

To ensure the mount persists across reboots, add it to the `/etc/fstab` file:

```bash
sudo nano /etc/fstab
```

Add the following line at the end:

```
master_ip:/home/ubuntu/simpletuner/output /home/ubuntu/simpletuner/output nfs defaults 0 0
```

---

#### **Additional Considerations:**

- **User Permissions:** Ensure that the `ubuntu` user has the same UID and GID on both machines so that file permissions are consistent. You can check UIDs with `id ubuntu`.

- **Firewall Settings:** If you have a firewall enabled, make sure to allow NFS traffic. On the master machine:

  ```bash
  sudo ufw allow from slave_ip to any port nfs
  ```

- **Synchronize Clocks:** It&apos;s good practice to have both systems&apos; clocks synchronized, especially in distributed setups. Use `ntp` or `systemd-timesyncd`.

- **Testing DeepSpeed Checkpoints:** Run a small DeepSpeed job to confirm that checkpoints are correctly written to the master&apos;s directory.


## Dataloader configuration

Very-large datasets can be a challenge to efficiently manage. SimpleTuner will automatically shard datasets over each node and distribute pre-processing across every available GPU in the cluster, while using asynchronous queues and threads to maintain throughput.

### Slow image scan / discovery

The **discovery** backend currently restricts aspect bucket data collection to a single node. This can take an **extremely** long time with very-large datasets as each image has to be read from storage to retrieve its geometry.

To work-around this problem, the [parquet metadata_backend](/documentation/DATALOADER.md#parquet-caption-strategy--json-lines-datasets) should be used, allowing you to preprocess your data in any manner accessible to you. As outlined in the linked document section, the parquet table contains the `filename`, `width`, `height`, and `caption` columns to help quickly and efficiently sort the data into its respective buckets.


### Storage space

Huge datasets, especially when using the T5-XXL text encoder, will consume enormous quantities of space for the original data, the image embeds, and the text embeds.

#### Cloud storage

Using a provider such as Cloudflare R2, one can generate extremely large datasets with very little storage fees.

See the [dataloader configuration guide](/documentation/DATALOADER.md#local-cache-with-cloud-dataset) for an example of how to configure the `aws` type in `multidatabackend.json`

- Image data can be stored locally or via S3
  - If images are in S3, the preprocessing speed reduces according to network bandwidth
  - If images are stored locally, this does not take advantage of NVMe throughput during **training**
- Image embeds and text embeds can be separately stored on local or cloud storage
  - Placing embeds on cloud storage reduce the training rate very little, as they are fetched in parallel

Ideally, all images and all embeds are simply maintained in a cloud storage bucket. This greatly simplifies the risk of issues during pre-processing and resuming training.

#### Preserving filesystem scan caches

If your datasets so large that scanning for new images becomes a bottleneck, adding `preserve_data_backend_cache=true` to each dataloader config entry will prevent the backend from being scanned for new images.

**Note** that you should then use the `image_embeds` data backend type ([more information here](/documentation/DATALOADER.md#local-cache-with-cloud-dataset)) to allow these cache lists to live separately in case your pre-processing job is interrupted. This will prevent the **image list** from being re-scanned at startup.

#### Data compression

Data compression should be enabled by adding the following to `config.json`:

```json
{
    ...
    &quot;--compress_disk_cache&quot;: true,
    ...
}
```

This will use inline gzip to reduce the amount of redundant disk space consumed by the rather-large text and image embeds.

## Configuring via 🤗 Accelerate

When using `accelerate config` (`/home/user/.cache/huggingface/accelerate/default_config.yaml`) to deploy SimpleTuner, these options will take priority over the contents of `config/config.env`

An example default_config for Accelerate that does not include DeepSpeed:

```yaml
# this should be updated on EACH node.
machine_rank: 0
# Everything below here is the same on EACH node.
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: MULTI_GPU
downcast_bf16: &apos;no&apos;
dynamo_config:
  dynamo_backend: NO
enable_cpu_affinity: false
main_process_ip: 10.0.0.100
main_process_port: 8080
main_training_function: main
mixed_precision: bf16
num_machines: 4
num_processes: 32
rdzv_backend: static
same_network: false
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

### DeepSpeed

This document doesn&apos;t go into as much detail as the [dedicated page](/documentation/DEEPSPEED.md).

When optimising training on DeepSpeed for multi-node, using the lowest-possible ZeRO level is **essential**.

For example, an 80G NVIDIA GPU can successfully train Flux with ZeRO level 1 offload, minimising overhead substantially.

Adding the following lines 

```yaml
# Update this from MULTI_GPU to DEEPSPEED
distributed_type: DEEPSPEED
deepspeed_config:
  deepspeed_multinode_launcher: standard
  gradient_accumulation_steps: 1
  gradient_clipping: 0.01
  zero3_init_flag: false
  zero_stage: 1
```

### torch compile optimisation

For extra performance (with a drawback of compatibility issues) you can enable torch compile by adding the following lines into each node&apos;s yaml config:

```yaml
dynamo_config:
  # Update this from NO to INDUCTOR
  dynamo_backend: INDUCTOR
  dynamo_mode: max-autotune
  dynamo_use_dynamic: false
  dynamo_use_fullgraph: false
```

## Expected performance

- 4x H100 SXM5 nodes connected via local network
- 1TB of memory per node
- Training cache streaming from shared S3-compatible data backend (Cloudflare R2) in same region
- Batch size of **8** per accelerator, and **no** gradient accumulation steps
  - Total effective batch size is **256**
- Resolution is at 1024px with data bucketing enabled
- **Speed**: 15 seconds per step with 1024x1024 data when full-rank training Flux.1-dev (12B)

Lower batch sizes, lower resolution, and enabling torch compile can bring the speed into **iterations per second**:

- Reduce resolution to 512px and disable data bucketing (square crops only)
- Swap DeepSpeed from AdamW to Lion fused optimiser
- Enable torch compile with max-autotune
- **Speed**: 2 iterations per second

## Distributed training caveats

- Every node must have the same number of accelerators available
- Only LoRA/LyCORIS can be quantised, so full distributed model training requires DeepSpeed instead
- This is a very high-cost operation, and high batch sizes might slow you down more than you want, requiring scaling up the count of GPUs in the cluster. A careful balance of budgeting should be considered.
- (DeepSpeed) Validations might need to be disabled when training with DeepSpeed ZeRO 3
- (DeepSpeed) Model saving ends up creating weird sharded copies when saving with ZeRO level 3, but levels 1 and 2 function as expected
- (DeepSpeed) The use of DeepSpeed&apos;s CPU-based optimisers becomes required as it handles sharding and offload of the optim states.</file><file path="documentation/DOCKER.md"># Docker for SimpleTuner

This Docker configuration provides a comprehensive environment for running the SimpleTuner application on various platforms including Runpod, Vast.ai, and other Docker-compatible hosts. It is optimized for ease of use and robustness, integrating tools and libraries essential for machine learning projects.

## Container Features

- **CUDA-enabled Base Image**: Built from `nvidia/cuda:11.8.0-runtime-ubuntu22.04` to support GPU-accelerated applications.
- **Development Tools**: Includes Git, SSH, and various utilities like `tmux`, `vim`, `htop`.
- **Python and Libraries**: Comes with Python 3.10 and essential libraries like `poetry` for Python package management.
- **Huggingface and WandB Integration**: Pre-configured for seamless integration with Huggingface Hub and WandB, facilitating model sharing and experiment tracking.

## Getting Started

### Windows OS support via WSL (Experimental)

The following guide was tested in a WSL2 Distro that has Dockerengine installed.


### 1. Building the Container

Clone the repository and navigate to the directory containing the Dockerfile. Build the Docker image using:

```bash
docker build -t simpletuner .
```

### 2. Running the Container

To run the container with GPU support, execute:

```bash
docker run --gpus all -it -p 22:22 simpletuner
```

This command sets up the container with GPU access and maps the SSH port for external connectivity.

### 3. Environment Variables

To facilitate integration with external tools, the container supports environment variables for Huggingface and WandB tokens. Pass these at runtime as follows:

```bash
docker run --gpus all -e HUGGING_FACE_HUB_TOKEN=&apos;your_token&apos; -e WANDB_TOKEN=&apos;your_token&apos; -it -p 22:22 simpletuner
```

### 4. Data Volumes

For persistent storage and data sharing between the host and the container, mount a data volume:

```bash
docker run --gpus all -v /path/on/host:/workspace -it -p 22:22 simpletuner
```

### 5. SSH Access

SSH into the container is configured by default. Ensure you provide your SSH public key through the appropriate environment variable (`SSH_PUBLIC_KEY` for Vast.ai or `PUBLIC_KEY` for Runpod).

### 6. Using SimpleTuner

Navigate to the SimpleTuner directory, activate the Python virtual environment, and start using or developing the application:

```bash
cd SimpleTuner
source .venv/bin/activate
```

Run training scripts or other provided utilities directly within this environment.

## Additional Configuration

### Custom Scripts and Configurations

If you want to add custom startup scripts or modify configurations, extend the entry script (`docker-start.sh`) to fit your specific needs.

If any capabilities cannot be achieved through this setup, please open a new issue.

### Docker Compose

For users who prefer `docker-compose.yaml`, this template is provided for you to extend and customise for your needs.

Once the stack is deployed you can connect to the container and start operating in it as mentioned in the steps above.

```bash
docker compose up -d

docker exec -it simpletuner /bin/bash
```

```docker-compose.yaml
services:
  simpletuner:
    container_name: simpletuner
    build:
      context: [Path to the repository]/SimpleTuner
      dockerfile: Dockerfile
    ports:
      - &quot;[port to connect to the container]:22&quot;
    volumes:
      - &quot;[path to your datasets]:/datasets&quot;
      - &quot;[path to your configs]:/workspace/SimpleTuner/config&quot;
    environment:
      HUGGING_FACE_HUB_TOKEN: [your hugging face token]
      WANDB_TOKEN: [your wanddb token]
    command: [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

&gt; ⚠️ Please be cautious of handling your WandB and Hugging Face tokens! It&apos;s advised not to commit them even to a private version-control repository to ensure they are not leaked. For production use-cases, key management storage is recommended, but out of scope for this guide.
---

## Troubleshooting

### CUDA Version Mismatch

**Symptom**: The application fails to utilize the GPU, or errors related to CUDA libraries appear when attempting to run GPU-accelerated tasks.

**Cause**: This issue may occur if the CUDA version installed within the Docker container does not match the CUDA driver version available on the host machine.

**Solution**:
1. **Check CUDA Driver Version on Host**: Determine the version of the CUDA driver installed on the host machine by running:
   ```bash
   nvidia-smi
   ```
   This command will display the CUDA version at the top right of the output.

2. **Match Container CUDA Version**: Ensure that the version of the CUDA toolkit in your Docker image is compatible with the host&apos;s CUDA driver. NVIDIA generally allows forward compatibility but check the specific compatibility matrix on the NVIDIA website.

3. **Rebuild the Image**: If necessary, modify the base image in the Dockerfile to match the host’s CUDA driver. For example, if your host runs CUDA 11.2 and the container is set up for CUDA 11.8, you might need to switch to an appropriate base image:
   ```Dockerfile
   FROM nvidia/cuda:11.2.0-runtime-ubuntu22.04
   ```
   After modifying the Dockerfile, rebuild the Docker image.

### SSH Connection Issues

**Symptom**: Unable to connect to the container via SSH.

**Cause**: Misconfiguration of SSH keys or the SSH service not starting correctly.

**Solution**:
1. **Check SSH Configuration**: Ensure that the public SSH key is correctly added to `~/.ssh/authorized_keys` in the container. Also, verify that the SSH service is up and running by entering the container and executing:
   ```bash
   service ssh status
   ```
2. **Exposed Ports**: Confirm that the SSH port (22) is properly exposed and mapped when starting the container, as shown in the running instructions:
   ```bash
   docker run --gpus all -it -p 22:22 simpletuner
   ```

### General Advice

- **Logs and Output**: Review the container logs and output for any error messages or warnings that can provide more context on the issue.
- **Documentation and Forums**: Consult the Docker and NVIDIA CUDA documentation for more detailed troubleshooting advice. Community forums and issue trackers related to the specific software or dependencies you are using can also be valuable resources.</file><file path="documentation/DREAMBOOTH.md"># Dreambooth (single-subject training)

## Background

The term Dreambooth refers to a technique developed by Google to inject subjects by finetuning them into a model using a small set of high quality images ([paper](https://dreambooth.github.io))

In the context of fine-tuning, Dreambooth adds new techniques to help prevent model collapse due to eg. overfitting or artifacts.

### Regularisation images

Regularisation images are typically generated by the model you are training, using a token that resembles your class.

They do not **have** to be synthetic images generated by the model, but this possibly has better performance than using real data (eg. photographs of real persons).

Example: If you are training in images of a male subject, your regularisation data would be photographs or synthetic generated samples of random male subjects.

&gt; 🟢 Regularisation images can be configured as a separate dataset, allowing them to mix evenly with your training data.

### Rare token training

A concept of dubious value from the original paper was to do a reverse search through the model&apos;s tokenizer vocabulary to find a &quot;rare&quot; string that had very little training associated to it.

Since that time, the idea has evolved and debated, with an opposing camp deciding to train against a celebrity&apos;s name that looks similar enough, as this requires less compute.

&gt; 🟡 Rare token training is supported in SimpleTuner, but there&apos;s no tool available to help you find one.

### Prior preservation loss

The model contains something called a &quot;prior&quot; which could, in theory, be preserved during Dreambooth training. In experiments with Stable Diffusion however, it didn&apos;t seem to help - the model just overfits on its own knowledge.

&gt; 🟢 ([#1031](https://github.com/bghira/SimpleTuner/issues/1031)) Prior preservation loss is supported in SimpleTuner when training LyCORIS adapters by setting `is_regularisation_data` on that dataset.

### Masked loss

Image masks may be defined in pairs with image data. The dark portions of the mask will cause the loss calculations to ignore these parts of the image.

An example [script](/toolkit/datasets/masked_loss/generate_dataset_masks.py) exists to generate these masks, given an input_dir and output_dir:

```bash
python generate_dataset_masks.py --input_dir /images/input \
                      --output_dir /images/output \
                      --text_input &quot;person&quot;
```

However, this does not have any advanced functionality such as mask padding blurring.

When defining your image mask dataset:

- Every image must have a mask. Use an all-white image if you do not want to mask.
- Set `dataset_type=conditioning` on your conditioning (mask) data folder
- Set `conditioning_type=mask` on your mask dataset
- Set `conditioning_data=` to your conditioning dataset `id` on your image dataset

```json
[
    {
        &quot;id&quot;: &quot;dreambooth-data&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;image&quot;,
        &quot;conditioning_data&quot;: &quot;dreambooth-conditioning&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/test_datasets/dreambooth&quot;,
        &quot;cache_dir_vae&quot;: &quot;/training/cache/vae/sdxl/dreambooth-data&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;an dreambooth&quot;,
        &quot;metadata_backend&quot;: &quot;discovery&quot;,
        &quot;resolution&quot;: 1024,
        &quot;minimum_image_size&quot;: 1024,
        &quot;maximum_image_size&quot;: 1024,
        &quot;target_downsample_size&quot;: 1024,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;
    },
    {
        &quot;id&quot;: &quot;dreambooth-conditioning&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;conditioning&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/test_datasets/dreambooth_mask&quot;,
        &quot;resolution&quot;: 1024,
        &quot;minimum_image_size&quot;: 1024,
        &quot;maximum_image_size&quot;: 1024,
        &quot;target_downsample_size&quot;: 1024,
        &quot;crop&quot;: true,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;conditioning_type&quot;: &quot;mask&quot;
    },
    {
        &quot;id&quot;: &quot;an example backend for text embeds.&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;type&quot;: &quot;local&quot;,
        &quot;cache_dir&quot;: &quot;/training/cache/text/sdxl-base/masked_loss&quot;
    }
]
```

## Setup

Following the [tutorial](/TUTORIAL.md) is required before you can continue into Dreambooth-specific configuration.

For DeepFloyd tuning, it&apos;s recommended to visit [this page](/documentation/DEEPFLOYD.md) for specific tips related to that model&apos;s setup.

### Quantised model training (LoRA/LyCORIS only)

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements.

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

Available precision levels depend on your hardware and its capabilities.

- int2-quanto, int4-quanto, **int8-quanto** (recommended)
- fp8-quanto, fp8-torchao (only for CUDA &gt;= 8.9, eg. 4090 or H100)
- nf4-bnb (required for low-VRAM users)

Inside your config.json, the following values should be modified or added:
```json
{
    &quot;base_model_precision&quot;: &quot;int8-quanto&quot;,
    &quot;text_encoder_1_precision&quot;: &quot;no_change&quot;,
    &quot;text_encoder_2_precision&quot;: &quot;no_change&quot;,
    &quot;text_encoder_3_precision&quot;: &quot;no_change&quot;
}
```

Inside our dataloader config `multidatabackend-dreambooth.json`, it will look something like this:

```json
[
    {
        &quot;id&quot;: &quot;subjectname-data-512px&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/subjectname&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;subjectname&quot;,
        &quot;cache_dir_vae&quot;: &quot;/training/vae_cache/subjectname&quot;,
        &quot;repeats&quot;: 100,
        &quot;crop&quot;: false,
        &quot;resolution&quot;: 512,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;minimum_image_size&quot;: 192
    },
    {
        &quot;id&quot;: &quot;subjectname-data-1024px&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/subjectname&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;subjectname&quot;,
        &quot;cache_dir_vae&quot;: &quot;/training/vae_cache/subjectname-1024px&quot;,
        &quot;repeats&quot;: 100,
        &quot;crop&quot;: false,
        &quot;resolution&quot;: 1024,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;minimum_image_size&quot;: 768
    },
    {
        &quot;id&quot;: &quot;regularisation-data&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/regularisation&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;a picture of a man&quot;,
        &quot;cache_dir_vae&quot;: &quot;/training/vae_cache/regularisation&quot;,
        &quot;repeats&quot;: 0,
        &quot;resolution&quot;: 512,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;minimum_image_size&quot;: 192,
        &quot;is_regularisation_data&quot;: true
    },
    {
        &quot;id&quot;: &quot;regularisation-data-1024px&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: &quot;/training/datasets/regularisation&quot;,
        &quot;caption_strategy&quot;: &quot;instanceprompt&quot;,
        &quot;instance_prompt&quot;: &quot;a picture of a man&quot;,
        &quot;cache_dir_vae&quot;: &quot;/training/vae_cache/regularisation-1024px&quot;,
        &quot;repeats&quot;: 0,
        &quot;resolution&quot;: 1024,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;minimum_image_size&quot;: 768,
        &quot;is_regularisation_data&quot;: true
    },
    {
        &quot;id&quot;: &quot;textembeds&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;cache_dir&quot;: &quot;/training/text_cache/sdxl_base&quot;
    }
]
```

Some key values have been tweaked to make training a single subject easier:

- We now have two datasets configured twice, for a total of four datasets. Regularisation data is optional, and training may work better without it. You can remove that dataset from the list if desired.
- Resolution is set to 512px and 1024px mixed bucketing which can help improve training speed and convergence
- Minimum image size is set to 192px or 768px which will allow us to upsample some smaller images, which might be needed for datasets with a few important but low resolution images.
- `caption_strategy` is now `instanceprompt`, which means we will use `instance_prompt` value for every image in the dataset as its caption.
  - **Note:** Using the instance prompt is the traditional method of Dreambooth training, but short captions may work better. If you find the model fails to generalise, it may be worth attempting to use captions.

### Regularisation dataset considerations

For a regularisation dataset:

- Set `repeats` very high on your Dreambooth subject so that your image count in the Dreambooth data is multiplied `repeats` times to surpass the image count of your regularisation set
  - If your Regularisation set has 1000 images, and you have 10 images in your training set, you&apos;d want a repeats value of at least 100 to get fast results
- `minimum_image_size` has been increased to ensure we don&apos;t introduce too many low-quality artifacts
- Similarly, using more descriptive captions may help avoid forgetting. Switching from `instanceprompt` to `textfile` or other strategies will require creating `.txt` files for each image.
- When `is_regularisation_data` (or 🇺🇸 `is_regularization_data` with a z, for the American users) is set, the data from this set will be fed into the base model to obtain a prediction that can be used as a loss target for the student LyCORIS model.
  - Note, currently this only functions on a LyCORIS adapter.

## Selecting an instance prompt

As mentioned earlier, the original focus of Dreambooth was the selection of rare tokens to train on.

Alternatively, one might use the real name of their subject, or a &apos;similar enough&apos; celebrity.

After a number of training experiments, it seems as though a &apos;similar enough&apos; celebrity is the best choice, especially if prompting the model for the person&apos;s real name ends up looking dissimilar.

# Exponential moving average (EMA)

A second model can be trained in parallel to your checkpoint, nearly for free - only the resulting system memory (by default) is consumed, rather than more VRAM.

Applying `use_ema=true` in your config file will enable this feature.

# CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.

# Refiner tuning

If you&apos;re a fan of the SDXL refiner, you may find that it causes your generations to &quot;ruin&quot; the results of your Dreamboothed model.

SimpleTuner supports training the SDXL refiner using LoRA and full rank.

This requires a couple considerations:
- The images should be purely high-quality
- The text embeds cannot be shared with the base model&apos;s
- The VAE embeds **can** be shared with the base model

You&apos;ll need to update `cache_dir` in your dataloader configuration, `multidatabackend.json`:

```json
[
    {
        &quot;id&quot;: &quot;textembeds&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;dataset_type&quot;: &quot;text_embeds&quot;,
        &quot;default&quot;: true,
        &quot;cache_dir&quot;: &quot;/training/text_cache/sdxl_refiner&quot;
    }
]
```

If you wish to target a specific aesthetic score with your data, you can add this to `config/config.json`:

```bash
&quot;--data_aesthetic_score&quot;: 5.6,
```

Update **5.6** to the score you would like to target. The default is **7.0**.

&gt; ⚠️ When training the SDXL refiner, your validation prompts will be ignored. Instead, random images from your datasets will be refined.</file><file path="documentation/LYCORIS.md"># LyCORIS

## Background

[LyCORIS](https://github.com/KohakuBlueleaf/LyCORIS) is an extensive suite of parameter-efficient fine-tuning (PEFT) methods that allow you to finetune models while using less VRAM and produces smaller distributable weights.

## Using LyCORIS

To use LyCORIS, set `--lora_type=lycoris` and then set `--lycoris_config=config/lycoris_config.json`, where `config/lycoris_config.json` is the location of your LyCORIS configuration file.

The following will go into your `config.json`:
```json
{
    &quot;model_type&quot;: &quot;lora&quot;,
    &quot;lora_type&quot;: &quot;lycoris&quot;,
    &quot;lycoris_config&quot;: &quot;config/lycoris_config.json&quot;,
    &quot;validation_lycoris_strength&quot;: 1.0,
    ...the rest of your settings...
}
```


The LyCORIS configuration file is in the format:

```json
{
    &quot;algo&quot;: &quot;lokr&quot;,
    &quot;multiplier&quot;: 1.0,
    &quot;linear_dim&quot;: 10000,
    &quot;linear_alpha&quot;: 1,
    &quot;factor&quot;: 10,
    &quot;apply_preset&quot;: {
        &quot;target_module&quot;: [
            &quot;Attention&quot;,
            &quot;FeedForward&quot;
        ],
        &quot;module_algo_map&quot;: {
            &quot;Attention&quot;: {
                &quot;factor&quot;: 10
            },
            &quot;FeedForward&quot;: {
                &quot;factor&quot;: 4
            }
        }
    }
}
```

### Fields

Optional fields:
- apply_preset for LycorisNetwork.apply_preset
- any keyword arguments specific to the selected algorithm, at the end.

Mandatory fields:
- multiplier, which should be set to 1.0 only unless you know what to expect
- linear_dim
- linear_alpha

For more information on LyCORIS, please refer to the [documentation in the library](https://github.com/KohakuBlueleaf/LyCORIS/tree/main/docs).

## Potential problems

When using Lycoris on SDXL, it&apos;s noted that training the FeedForward modules may break the model and send loss into `NaN` (Not-a-Number) territory.

This seems to be potentially exacerbated when using SageAttention (with `--sageattention_usage=training`), making it all but guaranteed that the model will immediately fail.

The solution is to remove the `FeedForward` modules from the lycoris config and train only the `Attention` blocks.

## LyCORIS Inference Example

Here is a simple FLUX.1-dev inference script showing how to wrap your unet or transformer with create_lycoris_from_weights and then use it for inference.

```py
import torch

from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL
from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel
from diffusers.pipelines.flux.pipeline_flux import FluxPipeline
from transformers import AutoModelForCausalLM, CLIPTextModel, CLIPTokenizer,T5EncoderModel, T5TokenizerFast

from lycoris import create_lycoris_from_weights

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
dtype = torch.bfloat16
bfl_repo = &quot;black-forest-labs/FLUX.1-dev&quot;

scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder=&quot;scheduler&quot;)
text_encoder = CLIPTextModel.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;, torch_dtype=dtype)
tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;, torch_dtype=dtype)
text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder=&quot;text_encoder_2&quot;, torch_dtype=dtype)
tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder=&quot;tokenizer_2&quot;, torch_dtype=dtype)
vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder=&quot;vae&quot;, torch_dtype=dtype)
transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder=&quot;transformer&quot;)

lycoris_safetensors_path = &apos;pytorch_lora_weights.safetensors&apos;
lycoris_strength = 1.0
wrapper, _ = create_lycoris_from_weights(lycoris_strength, lycoris_safetensors_path, transformer)
wrapper.merge_to() # using apply_to() will be slower.

transformer.to(device, dtype=dtype)

pipe = FluxPipeline(
    scheduler=scheduler,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    text_encoder_2=text_encoder_2,
    tokenizer_2=tokenizer_2,
    vae=vae,
    transformer=transformer,
)

pipe.enable_sequential_cpu_offload()

with torch.inference_mode():
    image = pipe(
        prompt=&quot;a pokemon that looks like a pizza is eating a popsicle&quot;,
        width=1280,
        height=768,
        num_inference_steps=15,
        generator=generator,
        guidance_scale=3.5,
    ).images[0]
image.save(&apos;image.png&apos;)

# optionally, save a merged pipeline containing the LyCORIS baked-in:
pipe.save_pretrained(&apos;/path/to/output/pipeline&apos;)
```</file><file path="documentation/MIXTURE_OF_EXPERTS.md"># Mixture-of-Experts

SimpleTuner allows splitting the task of training in two, such that the self-attention and cross-attention stages of inference can effectively be split between two entirely different sets of weights.

In this example, we will use SegMind&apos;s collaborative effort with Hugging Face, [SSD-1B](https://huggingface.co/segmind/ssd-1b) to create two new models that train more reliably and have better resulting fine details than a single model.

Thanks to the small size of the SSD-1B model, training on even lighter-weight hardware is possible. Since we&apos;re starting our model from their pretrained weights, we have to abide by their Apache 2.0 license - but this is relatively straightforward. You can even use the resulting weights in a commercial setting!

When SDXL 0.9 and 1.0 were introduced, they both contained a full base model with a split-schedule refiner.

- The base model was trained on steps 999 to 0
  - The base model is more than 3B parameters, and functions entirely standalone.
- The refiner model was trained on steps 199 to 0
  - The refiner model is also more than 3B parameters, a seemingly unnecessary waste of resources. It does not function on its own without substantial deformations and a bias toward cartoonish outputs.

Let&apos;s see how we can improve this situation.


## The Base model, &quot;Stage One&quot;

The first portion of a mixture-of-experts is known as the base model. As mentioned in SDXL&apos;s case, it is trained on all 1000 timesteps - but it doesn&apos;t need to be. The following configuration will train the base model on just 650 steps out of the total 1000, saving time and training more reliably.

### Environment configuration

Setting the following values in your `config/config.env` will get us started:

```bash
# Ensure these aren&apos;t incorrectly set.
export USE_BITFIT=false
export USE_DORA=false
# lora could be used here instead, but the concept hasn&apos;t been explored.
export MODEL_TYPE=&quot;full&quot;
export MODEL_FAMILY=&quot;sdxl&quot;
export MODEL_NAME=&quot;segmind/SSD-1B&quot;
# The original Segmind model used a learning rate of 1e-5, which is
# probably too high for whatever batch size most users can pull off.
export LEARNING_RATE=4e-7

# We really want this as high as you can tolerate.
# - If training is very slow, ensure your CHECKPOINT_STEPS and VALIDATION_STEPS
#   are set low enough that you&apos;ll get a checkpoint every couple hours.
# - The original Segmind models used a batch size of 32 with 4 accumulations.
export TRAIN_BATCH_SIZE=8
export GRADIENT_ACCUMULATION_STEPS=1

# If you are running on a beefy machine that doesn&apos;t fully utilise its VRAM during training, set this to &quot;false&quot; and your training will go faster.
export USE_GRADIENT_CHECKPOINTING=true

# Enable first stage model training
export TRAINER_EXTRA_ARGS=&quot;--refiner_training --refiner_training_strength=0.35 --refiner_training_invert_schedule&quot;

# Optionally reparameterise it to v-prediction/zero-terminal SNR. &apos;sample&apos; prediction_type can be used instead for x-prediction.
# This will start out looking pretty terrible until about 1500-2500 steps have passed, but it could be very worthwhile.
export TRAINER_EXTRA_ARGS=&quot;${TRAINER_EXTRA_ARGS} --prediction_type=v_prediction --rescale_betas_zero_snr --training_scheduler_timestep_spacing=trailing&quot;
```

### Dataloader configuration

No special considerations for dataloader configuration are necessary. See the [dataloader config guide](/documentation/DATALOADER.md) for more information on this step.

### Validation

Currently, SimpleTuner does not engage the second stage model during stage one evaluations.

Future work will support this as an option, in case a stage two model already exists, or is being trained concurrently.

---

## The Refiner model, &quot;Stage Two&quot;

### Comparison to training the SDXL refiner

- Unlike the SDXL refiner, when using Segmind SSD-1B for both stages the text embeds **can** be shared between the two training jobs
  - The SDXL refiner uses a different text embed layout versus the SDXL base model.
- The VAE embeds **can** be shared between the training jobs, just like the SDXL refiner. Both models use the same input layout.
- No aesthetic score is used for the Segmind models, instead they use the same microconditioning inputs as SDXL, eg. crop coordinates
- Training goes much faster, as the model is much smaller, and text embeds can be reused from stage one training

### Environment Configuration

Update the following values in your `config/config.env` to swap training over to your stage two model. It might be convenient to keep a copy of the base model configuration.

```bash
# Update your OUTPUT_DIR value, so that we don&apos;t overwrite the stage one model checkpoints.
export OUTPUT_DIR=&quot;/some/new/path&quot;

# We&apos;ll swap --refiner_training_invert_schedule for --validation_using_datasets
# - Train the end of the model instead of the beginning
# - Validate using images as input for partial denoising to evaluate fine detail improvements
export TRAINER_EXTRA_ARGS=&quot;--refiner_training --refiner_training_strength=0.35 --validation_using_datasets&quot;

# Don&apos;t update these values if you&apos;ve set them on the stage one. Be sure to use the same parameterisation for both models!
export TRAINER_EXTRA_ARGS=&quot;${TRAINER_EXTRA_ARGS} --prediction_type=v_prediction --rescale_betas_zero_snr --training_scheduler_timestep_spacing=trailing&quot;
```

### Dataset format

The images should be purely high-quality - remove any datasets you find questionable in terms of compression artifacts or other errors.

Other than that, the same exact dataloader configuration can be used between the two training jobs.

If you&apos;d like a demonstration dataset, [pseudo-camera-10k](https://huggingface.co/datasets/ptx0/pseudo-camera-10k) is a solid choice with permissive licensing.

### Validation

Stage two refiner training will automatically select images from each of your training sets, and use those as inputs for partial denoising at validation time.

## CLIP score tracking

If you wish to enable evaluations to score the model&apos;s performance, see [this document](/documentation/evaluation/CLIP_SCORES.md) for information on configuring and interpreting CLIP scores.

# Stable evaluation loss

If you wish to use stable MSE loss to score the model&apos;s performance, see [this document](/documentation/evaluation/EVAL_LOSS.md) for information on configuring and interpreting evaluation loss.

## Putting it all together at inference time

If you&apos;d like to plug both of the models together to experiment with in a simple script, this will get you started:

```py
from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline, UniPCMultistepScheduler
from torch import float16, cuda
from torch.backends import mps

# For a training_refiner_strength of .35, you&apos;ll set the base model strength to 0.65.
# Formula: 1 - training_refiner_strength
training_refiner_strength = 0.35
base_model_power = 1 - training_refiner_strength
# Reduce this for lower quality but speed-up.
num_inference_steps = 40
# Update these to your local or hugging face hub paths.
stage_1_model_id = &apos;ptx0/terminus-xl-velocity-v2&apos;
stage_2_model_id = &apos;ptx0/terminus-xl-refiner&apos;
torch_device = &apos;cuda&apos; if cuda.is_available() else &apos;mps&apos; if mps.is_available() else &apos;cpu&apos;

pipe = StableDiffusionXLPipeline.from_pretrained(stage_1_model_id, add_watermarker=False, torch_dtype=float16).to(torch_device)
pipe.scheduler = UniPCMultistepScheduler.from_pretrained(stage_1_model_id, subfolder=&quot;scheduler&quot;)
img2img_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(stage_2_model_id).to(device=torch_device, dtype=float16)
img2img_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(stage_1_model_id, subfolder=&quot;scheduler&quot;)

prompt = &quot;An astronaut riding a green horse&quot;

# Important: update this to True if you reparameterised the models.
use_zsnr = True

image = pipe(
    prompt=prompt,
    num_inference_steps=num_inference_steps,
    denoising_end=base_model_power,
    guidance_scale=9.2,
    guidance_rescale=0.7 if use_zsnr else 0.0,
    output_type=&quot;latent&quot;,
).images
image = img2img_pipe(
    prompt=prompt,
    num_inference_steps=num_inference_steps,
    denoising_start=base_model_power,
    guidance_scale=9.2,
    guidance_rescale=0.7 if use_zsnr else 0.0,
    image=image,
).images[0]
image.save(&apos;demo.png&apos;, format=&quot;PNG&quot;)
```

Some experimentations you can run:
- Play with some values here such as `base_model_power` or `num_inference_steps`, which must be the same for both pipelines.
- You can also play with `guidance_scale`, `guidance_rescale` which can be set differently for each stage. These impact contrast and realism.
- Using separate prompts between the base and refiner models to shift the guidance focus for fine details.</file><file path="documentation/QUICKSTART.md"># Quickstart Guide

&gt; ⚠️ These tutorials are a work-in-progress. They contain full end-to-end instructions for a basic training session.

**Note**: For more advanced configurations, see the [tutorial](/TUTORIAL.md), [dataloader configuration guide](/documentation/DATALOADER.md), and the [options breakdown](/OPTIONS.md) pages.

## PixArt Sigma (1K, 2K &amp; 4K)

For a fun and lightweight model, see [this quickstart guide](/documentation/quickstart/SIGMA.md)

## NVLabs Sana (1024px, currently)

Probably the fastest model currently; see [this quickstart guide](/documentation/quickstart/SANA.md)

## Kwai Kolors

An SDXL-like U-net based architecture that uses a language model called ChatGLM for its text parsing can be found [here](/documentation/quickstart/KOLORS.md)

## Stable Diffusion 3

For personalisation of the Stable Diffusion 3 model family, see [this quickstart guide](/documentation/quickstart/SD3.md)

## Flux.1

For training of the enormous monster known as Flux, see [its specific quickstart guide](/documentation/quickstart/FLUX.md)</file><file path="helpers/caching/memory.py">def reclaim_memory():
    import gc
    import torch
    if torch.cuda.is_available():
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
        torch.mps.synchronize()
        gc.collect()</file><file path="helpers/caching/text_embeds.py">import os
import torch
import hashlib
import logging
import time
import gc
from tqdm import tqdm
from helpers.data_backend.base import BaseDataBackend
from helpers.training.state_tracker import StateTracker
from helpers.prompts import PromptHandler
from helpers.training.multi_process import rank_info
from queue import Queue
import queue
from threading import Thread
from concurrent.futures import ThreadPoolExecutor
from helpers.training.multi_process import _get_rank as get_rank, should_log
from helpers.webhooks.mixin import WebhookMixin
logger = logging.getLogger(&quot;TextEmbeddingCache&quot;)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(&quot;ERROR&quot;)
def _encode_sd3_prompt_with_t5(
    text_encoder,
    tokenizer,
    prompt=None,
    num_images_per_prompt=1,
    device=None,
    zero_padding_tokens: bool = True,
    max_sequence_length: int = 77,
):
    prompt = [prompt] if isinstance(prompt, str) else prompt
    batch_size = len(prompt)
    text_inputs = tokenizer(
        prompt,
        padding=&quot;max_length&quot;,
        max_length=max_sequence_length,
        truncation=True,
        add_special_tokens=True,
        return_tensors=&quot;pt&quot;,
    )
    text_input_ids = text_inputs.input_ids
    prompt_embeds = text_encoder(text_input_ids.to(device))[0]
    dtype = text_encoder.dtype
    prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
    _, seq_len, _ = prompt_embeds.shape
    # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
    prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)
    attention_mask = text_inputs.attention_mask.to(device)
    if zero_padding_tokens:
        # for some reason, SAI&apos;s reference code doesn&apos;t bother to mask the prompt embeddings.
        # this can lead to a problem where the model fails to represent short and long prompts equally well.
        # additionally, the model learns the bias of the prompt embeds&apos; noise.
        return prompt_embeds * attention_mask.unsqueeze(-1).expand(prompt_embeds.shape)
    else:
        return prompt_embeds
def _encode_sd3_prompt_with_clip(
    text_encoder,
    tokenizer,
    prompt: str,
    device=None,
    num_images_per_prompt: int = 1,
    max_token_length: int = 77,
):
    prompt = [prompt] if isinstance(prompt, str) else prompt
    batch_size = len(prompt)
    text_inputs = tokenizer(
        prompt,
        padding=&quot;max_length&quot;,
        max_length=max_token_length,
        truncation=True,
        return_tensors=&quot;pt&quot;,
    )
    text_input_ids = text_inputs.input_ids
    prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)
    pooled_prompt_embeds = prompt_embeds[0]
    prompt_embeds = prompt_embeds.hidden_states[-2]
    prompt_embeds = prompt_embeds.to(dtype=text_encoder.dtype, device=device)
    _, seq_len, _ = prompt_embeds.shape
    # duplicate text embeddings for each generation per prompt, using mps friendly method
    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
    prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)
    return prompt_embeds, pooled_prompt_embeds
class TextEmbeddingCache(WebhookMixin):
    prompts = {}
    def __init__(
        self,
        id: str,
        data_backend: BaseDataBackend,
        text_encoders,
        tokenizers,
        accelerator,
        webhook_progress_interval: int = 100,
        cache_dir: str = &quot;cache&quot;,
        model_type: str = &quot;sdxl&quot;,
        prompt_handler: PromptHandler = None,
        write_batch_size: int = 128,
        read_batch_size: int = 25,
        process_queue_size: int = 16,
        text_encoder_batch_size: int = 4,
        max_workers: int = 32,
    ):
        self.id = id
        if data_backend.id != id:
            raise ValueError(
                f&quot;TextEmbeddingCache received incorrect data_backend: {data_backend}&quot;
            )
        self.should_abort = False
        self.data_backend = data_backend
        self.text_encoders = text_encoders
        self.tokenizers = tokenizers
        self.accelerator = accelerator
        self.cache_dir = cache_dir
        self.model_type = model_type
        self.pipeline = None
        if self.model_type == &quot;sana&quot;:
            from diffusers.pipelines.sana import SanaPipeline
            self.pipeline = SanaPipeline.from_pretrained(
                pretrained_model_name_or_path=StateTracker.get_args().pretrained_model_name_or_path,
                text_encoder=text_encoders[0],
                tokenizer=tokenizers[0],
                vae=None,
                transformer=None,
            )
        if self.model_type == &quot;flux&quot;:
            from diffusers.pipelines.flux import FluxPipeline
            self.pipeline = FluxPipeline.from_pretrained(
                pretrained_model_name_or_path=StateTracker.get_args().pretrained_model_name_or_path,
                text_encoder=text_encoders[0],
                text_encoder_2=text_encoders[1],
                tokenizer=tokenizers[0],
                tokenizer_2=tokenizers[1],
                transformer=None,
                vae=None,
            )
        self.prompt_handler = prompt_handler
        self.write_batch_size = write_batch_size
        self.read_batch_size = read_batch_size
        self.process_queue_size = process_queue_size
        self.write_thread_bar = None
        self.text_encoder_batch_size = text_encoder_batch_size
        self.max_workers = max_workers
        self.rank_info = rank_info()
        if self.data_backend.type == &quot;local&quot;:
            self.cache_dir = os.path.abspath(self.cache_dir)
        self.data_backend.create_directory(self.cache_dir)
        self.write_queue = Queue()
        self.process_write_batches = True
        self.batch_write_thread = Thread(
            target=self.batch_write_embeddings,
            name=f&quot;batch_write_thread_{self.id}&quot;,
            daemon=True,
        )
        self.batch_write_thread.start()
        self.webhook_progress_interval = webhook_progress_interval
    def debug_log(self, msg: str):
        logger.debug(f&quot;{self.rank_info}(id={self.id}) {msg}&quot;)
    def create_hash(self, caption):
        if caption is None:
            # It&apos;s gross, but some images do not have captions.
            caption = &quot;&quot;
        # Precomputed part of the format string
        hash_format = f&quot;-{self.model_type}&quot;
        # Reuse the hash object
        md5_hash = hashlib.md5()
        md5_hash.update(str(caption).encode())
        # logger.debug(f&quot;Hashing caption: {caption}&quot;)
        result = md5_hash.hexdigest() + hash_format
        # logger.debug(f&quot;-&gt; {result}&quot;)
        return result
    def hash_prompt_with_path(self, caption):
        return os.path.join(self.cache_dir, self.create_hash(caption) + &quot;.pt&quot;)
    def hash_prompt(self, caption):
        return self.create_hash(caption) + &quot;.pt&quot;
    def discover_all_files(self):
        &quot;&quot;&quot;Identify all files in the data backend.&quot;&quot;&quot;
        logger.info(
            f&quot;{self.rank_info}(id={self.id}) Listing all text embed cache entries&quot;
        )
        # This isn&apos;t returned, because we merely check if it&apos;s stored, or, store it.
        (
            StateTracker.get_text_cache_files(data_backend_id=self.id)
            or StateTracker.set_text_cache_files(
                self.data_backend.list_files(
                    instance_data_dir=self.cache_dir,
                    file_extensions=[&quot;pt&quot;],
                ),
                data_backend_id=self.id,
            )
        )
        self.debug_log(&quot; -&gt; done listing all text embed cache entries&quot;)
    def save_to_cache(self, filename, embeddings):
        &quot;&quot;&quot;Add write requests to the queue instead of writing directly.&quot;&quot;&quot;
        if not self.batch_write_thread.is_alive():
            logger.debug(&quot;Restarting background write thread.&quot;)
            # Start the thread again.
            self.process_write_batches = True
            self.batch_write_thread = Thread(target=self.batch_write_embeddings)
            self.batch_write_thread.start()
        self.write_queue.put((embeddings, filename))
        logger.debug(
            f&quot;save_to_cache called for {filename}, write queue has {self.write_queue.qsize()} items, and the write thread&apos;s status: {self.batch_write_thread.is_alive()}&quot;
        )
    def batch_write_embeddings(self):
        &quot;&quot;&quot;Process write requests in batches.&quot;&quot;&quot;
        batch = []
        written_elements = 0
        while True:
            try:
                # Block until an item is available or timeout occurs
                first_item = self.write_queue.get(timeout=1)
                batch = [first_item]
                # Try to get more items without blocking
                while (
                    not self.write_queue.empty() and len(batch) &lt; self.write_batch_size
                ):
                    logger.debug(&quot;Retrieving more items from the queue.&quot;)
                    items = self.write_queue.get_nowait()
                    batch.append(items)
                    logger.debug(f&quot;Batch now contains {len(batch)} items.&quot;)
                self.process_write_batch(batch)
                self.write_thread_bar.update(len(batch))
                logger.debug(&quot;Processed batch write.&quot;)
                written_elements += len(batch)
            except queue.Empty:
                # Timeout occurred, no items were ready
                if not self.process_write_batches:
                    if len(batch) &gt; 0:
                        self.process_write_batch(batch)
                        self.write_thread_bar.update(len(batch))
                    logger.debug(
                        f&quot;Exiting batch write thread, no more work to do after writing {written_elements} elements&quot;
                    )
                    break
                logger.debug(
                    f&quot;Queue is empty. Retrieving new entries. Should retrieve? {self.process_write_batches}&quot;
                )
                pass
            except Exception:
                logger.exception(&quot;An error occurred while writing embeddings to disk.&quot;)
        logger.debug(&quot;Exiting background batch write thread.&quot;)
    def process_write_batch(self, batch):
        &quot;&quot;&quot;Write a batch of embeddings to the cache.&quot;&quot;&quot;
        logger.debug(f&quot;Writing {len(batch)} items to disk&quot;)
        logger.debug(f&quot;Batch: {batch}&quot;)
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self.data_backend.torch_save, *args) for args in batch
            ]
            for future in futures:
                future.result()  # Wait for all writes to complete
        logger.debug(f&quot;Completed write batch of {len(batch)} items&quot;)
    def load_from_cache(self, filename):
        result = self.data_backend.torch_load(filename)
        return result
    def encode_flux_prompt(
        self,
        text_encoders,
        tokenizers,
        prompt: str,
        is_validation: bool = False,
        zero_padding_tokens: bool = True,
    ):
        &quot;&quot;&quot;
        Encode a prompt for a Flux model.
        Args:
            text_encoders: List of text encoders.
            tokenizers: List of tokenizers.
            prompt: The prompt to encode.
            num_images_per_prompt: The number of images to generate per prompt.
            is_validation: Whether the prompt is for validation. No-op for SD3.
        Returns:
            Tuple of (prompt_embeds, pooled_prompt_embeds).
        &quot;&quot;&quot;
        from helpers.models.flux import FluxPipeline
        pipe = FluxPipeline(
            self.pipeline.scheduler,
            self.pipeline.vae,
            self.pipeline.text_encoder,
            self.pipeline.tokenizer,
            self.pipeline.text_encoder_2,
            self.pipeline.tokenizer_2,
            self.pipeline.transformer,
        )
        prompt_embeds, pooled_prompt_embeds, time_ids, masks = pipe.encode_prompt(
            prompt=prompt,
            prompt_2=prompt,
            device=self.accelerator.device,
            max_sequence_length=StateTracker.get_args().tokenizer_max_length,
        )
        if zero_padding_tokens:
            # we can zero the padding tokens if we&apos;re just going to mask them later anyway.
            prompt_embeds = prompt_embeds * masks.to(
                device=prompt_embeds.device
            ).unsqueeze(-1).expand(prompt_embeds.shape)
        return prompt_embeds, pooled_prompt_embeds, time_ids, masks
    # Adapted from pipelines.StableDiffusion3Pipeline.encode_prompt
    def encode_sd3_prompt(
        self,
        text_encoders,
        tokenizers,
        prompt: str,
        is_validation: bool = False,
        zero_padding_tokens: bool = False,
    ):
        &quot;&quot;&quot;
        Encode a prompt for an SD3 model.
        Args:
            text_encoders: List of text encoders.
            tokenizers: List of tokenizers.
            prompt: The prompt to encode.
            num_images_per_prompt: The number of images to generate per prompt.
            is_validation: Whether the prompt is for validation. No-op for SD3.
        Returns:
            Tuple of (prompt_embeds, pooled_prompt_embeds).
        &quot;&quot;&quot;
        prompt = [prompt] if isinstance(prompt, str) else prompt
        num_images_per_prompt = 1
        clip_tokenizers = tokenizers[:2]
        clip_text_encoders = text_encoders[:2]
        clip_prompt_embeds_list = []
        clip_pooled_prompt_embeds_list = []
        for tokenizer, text_encoder in zip(clip_tokenizers, clip_text_encoders):
            prompt_embeds, pooled_prompt_embeds = _encode_sd3_prompt_with_clip(
                text_encoder=text_encoder,
                tokenizer=tokenizer,
                prompt=prompt,
                device=self.accelerator.device,
                num_images_per_prompt=num_images_per_prompt,
            )
            clip_prompt_embeds_list.append(prompt_embeds)
            clip_pooled_prompt_embeds_list.append(pooled_prompt_embeds)
        clip_prompt_embeds = torch.cat(clip_prompt_embeds_list, dim=-1)
        pooled_prompt_embeds = torch.cat(clip_pooled_prompt_embeds_list, dim=-1)
        t5_prompt_embed = _encode_sd3_prompt_with_t5(
            text_encoders[-1],
            tokenizers[-1],
            prompt=prompt,
            num_images_per_prompt=num_images_per_prompt,
            device=self.accelerator.device,
            zero_padding_tokens=zero_padding_tokens,
            max_sequence_length=StateTracker.get_args().tokenizer_max_length,
        )
        clip_prompt_embeds = torch.nn.functional.pad(
            clip_prompt_embeds,
            (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1]),
        )
        prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)
        return prompt_embeds, pooled_prompt_embeds
    def encode_legacy_prompt(self, text_encoder, tokenizer, prompt):
        input_tokens = tokenizer(
            PromptHandler.filter_caption(self.data_backend, prompt),
            truncation=True,
            padding=&quot;max_length&quot;,
            max_length=tokenizer.model_max_length,
            return_tensors=&quot;pt&quot;,
        ).input_ids.to(self.accelerator.device)
        output = text_encoder(input_tokens)[0]
        # self.debug_log(f&quot;Legacy prompt shape: {output.shape}&quot;)
        # self.debug_log(f&quot;Legacy prompt encoded: {output}&quot;)
        return output
    # Adapted from pipelines.StableDiffusionXLPipeline.encode_sdxl_prompt
    def encode_sdxl_prompt(
        self,
        text_encoders,
        tokenizers,
        prompt,
        is_validation: bool = False,
    ):
        prompt_embeds_list = []
        emitted_warning = False
        try:
            for tokenizer, text_encoder in zip(tokenizers, text_encoders):
                if tokenizer is None or text_encoder is None:
                    # SDXL Refiner only has one text encoder and tokenizer
                    continue
                if type(prompt) is not str and type(prompt) is not list:
                    prompt = str(prompt)
                max_seq_len = 256 if self.model_type == &quot;kolors&quot; else 77
                text_inputs = tokenizer(
                    prompt,
                    padding=&quot;max_length&quot;,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                    max_length=max_seq_len,
                )
                untruncated_ids = tokenizer(
                    prompt,
                    padding=&quot;longest&quot;,
                    return_tensors=&quot;pt&quot;,
                    max_length=max_seq_len,
                ).input_ids
                if untruncated_ids.shape[
                    -1
                ] &gt; tokenizer.model_max_length and not torch.equal(
                    text_inputs.input_ids, untruncated_ids
                ):
                    removed_text = tokenizer.batch_decode(
                        untruncated_ids[:, tokenizer.model_max_length - 1 : -1]
                    )
                    if not emitted_warning:
                        # Only print this once. It&apos;s a bit spammy otherwise.
                        emitted_warning = True
                        logger.warning(
                            f&quot;The following part of your input was truncated because CLIP can only handle sequences up to {tokenizer.model_max_length} tokens: {removed_text}&quot;
                        )
                if self.model_type == &quot;sdxl&quot;:
                    prompt_embeds_output = text_encoder(
                        text_inputs.input_ids.to(self.accelerator.device),
                        output_hidden_states=True,
                    )
                    # We are always interested in the pooled output of the final text encoder
                    pooled_prompt_embeds = prompt_embeds_output[0]
                    prompt_embeds = prompt_embeds_output.hidden_states[-2]
                elif self.model_type == &quot;kolors&quot;:
                    # we pass the attention mask into the text encoder. it transforms the embeds but does not attend to them.
                    # unfortunately, kolors does not return the attention mask for later use by the U-net to avoid attending to the padding tokens.
                    prompt_embeds_output = text_encoder(
                        input_ids=text_inputs[&quot;input_ids&quot;].to(self.accelerator.device),
                        attention_mask=text_inputs[&quot;attention_mask&quot;].to(
                            self.accelerator.device
                        ),
                        position_ids=text_inputs[&quot;position_ids&quot;],
                        output_hidden_states=True,
                    )
                    # the ChatGLM encoder output is hereby mangled in fancy ways for Kolors to be useful.
                    prompt_embeds = (
                        prompt_embeds_output.hidden_states[-2].permute(1, 0, 2).clone()
                    )
                    # [max_sequence_length, batch, hidden_size] -&gt; [batch, hidden_size]
                    pooled_prompt_embeds = prompt_embeds_output.hidden_states[-1][
                        -1, :, :
                    ].clone()
                else:
                    raise ValueError(f&quot;Unknown model type: {self.model_type}&quot;)
                bs_embed, seq_len, _ = prompt_embeds.shape
                prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)
                # Clear out anything we moved to the text encoder device
                text_inputs.input_ids.to(&quot;cpu&quot;)
                del prompt_embeds_output
                del text_inputs
                prompt_embeds_list.append(prompt_embeds)
        except Exception as e:
            import traceback
            logger.error(
                f&quot;Failed to encode prompt: {prompt}\n-&gt; error: {e}\n-&gt; traceback: {traceback.format_exc()}&quot;
            )
            raise e
        prompt_embeds = torch.cat(prompt_embeds_list, dim=-1)
        return prompt_embeds, pooled_prompt_embeds
    # Adapted from pipelines.StableDiffusionXLPipeline.encode_prompt
    def encode_sdxl_prompts(
        self,
        text_encoders,
        tokenizers,
        prompts,
        is_validation: bool = False,
    ):
        prompt_embeds_all = []
        pooled_prompt_embeds_all = []
        for prompt in prompts:
            prompt_embeds, pooled_prompt_embeds = self.encode_sdxl_prompt(
                text_encoders, tokenizers, prompt, is_validation
            )
            prompt_embeds_all.append(prompt_embeds)
            pooled_prompt_embeds_all.append(pooled_prompt_embeds)
        return torch.stack(prompt_embeds_all).squeeze(dim=1), torch.stack(
            pooled_prompt_embeds_all
        ).squeeze(dim=1)
    def encode_prompt(self, prompt: str, is_validation: bool = False):
        if self.model_type == &quot;sdxl&quot; or self.model_type == &quot;kolors&quot;:
            return self.encode_sdxl_prompt(
                self.text_encoders, self.tokenizers, prompt, is_validation
            )
        elif self.model_type == &quot;sd3&quot;:
            return self.encode_sd3_prompt(
                self.text_encoders,
                self.tokenizers,
                prompt,
                is_validation,
                zero_padding_tokens=(
                    True if StateTracker.get_args().t5_padding == &quot;zero&quot; else False
                ),
            )
        else:
            return self.encode_legacy_prompt(
                self.text_encoders[0], self.tokenizers[0], prompt
            )
    def tokenize_t5_prompt(self, prompt, tokenizer_max_length=None):
        if tokenizer_max_length is not None:
            max_length = tokenizer_max_length
        else:
            # prevent runaway token length sizes.
            # huge captions aren&apos;t very helpful, and if you want them, use --tokenizer_max_length
            max_length = 144
        text_inputs = self.tokenizers[0](
            prompt,
            truncation=True,
            padding=&quot;max_length&quot;,
            max_length=max_length,
            return_tensors=&quot;pt&quot;,
        )
        return text_inputs
    def encode_t5_prompt(self, input_ids, attention_mask):
        text_input_ids = input_ids.to(self.text_encoders[0].device)
        attention_mask = attention_mask.to(self.text_encoders[0].device)
        prompt_embeds = self.text_encoders[0](
            text_input_ids,
            attention_mask=attention_mask,
            return_dict=False,
        )[0]
        prompt_embeds = prompt_embeds.to(&quot;cpu&quot;)
        return prompt_embeds
    def compute_t5_prompt(self, prompt: str):
        &quot;&quot;&quot;
        Tokenise, encode, optionally mask, and then return a prompt_embed for a T5 model.
        Args:
            prompt: The prompt to encode.
        Returns:
            Tuple of (prompt_embeds, attention_mask)
        &quot;&quot;&quot;
        logger.debug(f&quot;Computing T5 caption for: {prompt}&quot;)
        text_inputs = self.tokenize_t5_prompt(
            prompt, tokenizer_max_length=StateTracker.get_args().tokenizer_max_length
        )
        result = self.encode_t5_prompt(
            text_inputs.input_ids,
            text_inputs.attention_mask,
        )
        attn_mask = text_inputs.attention_mask
        del text_inputs
        return result, attn_mask
    def compute_gemma_prompt(self, prompt: str, is_negative_prompt: bool):
        prompt_embeds, prompt_attention_mask, _, _ = self.pipeline.encode_prompt(
            prompt=prompt,
            do_classifier_free_guidance=False,
            device=self.accelerator.device,
            clean_caption=False,
            max_sequence_length=300,
            complex_human_instruction=(
                StateTracker.get_args().sana_complex_human_instruction
                if not is_negative_prompt
                else None
            ),
        )
        return prompt_embeds, prompt_attention_mask
    def compute_embeddings_for_prompts(
        self,
        all_prompts,
        return_concat: bool = True,
        is_validation: bool = False,
        load_from_cache: bool = True,
        is_negative_prompt: bool = False,
    ):
        logger.debug(&quot;Initialising text embed calculator...&quot;)
        if not self.batch_write_thread.is_alive():
            logger.debug(&quot;Restarting background write thread.&quot;)
            # Start the thread again.
            self.process_write_batches = True
            self.batch_write_thread = Thread(target=self.batch_write_embeddings)
            self.batch_write_thread.start()
        existing_cache_filenames = list(
            StateTracker.get_text_cache_files(data_backend_id=self.id).keys()
        )
        # Parallel processing for hashing
        with ThreadPoolExecutor() as executor:
            all_cache_filenames = list(
                executor.map(self.hash_prompt_with_path, all_prompts)
            )
        # Create a set for faster lookups
        existing_cache_filenames_set = set(existing_cache_filenames)
        # Determine which prompts are not cached
        uncached_prompts = [
            prompt
            for prompt, filename in zip(all_prompts, all_cache_filenames)
            if filename not in existing_cache_filenames_set
        ]
        # If all prompts are cached and certain conditions are met, return None
        if not uncached_prompts and not return_concat:
            self.debug_log(
                f&quot;All prompts are cached, ignoring (uncached_prompts={uncached_prompts}, is_validation={is_validation}, return_concat={return_concat})&quot;
            )
            return None
        else:
            self.debug_log(
                f&quot;(uncached_prompts={uncached_prompts}, is_validation={is_validation}, return_concat={return_concat})&quot;
            )
        # Proceed with uncached prompts
        raw_prompts = uncached_prompts if uncached_prompts else all_prompts
        output = None
        if self.model_type == &quot;sdxl&quot; or self.model_type == &quot;kolors&quot;:
            output = self.compute_embeddings_for_sdxl_prompts(
                raw_prompts,
                return_concat=return_concat,
                is_validation=is_validation,
                load_from_cache=load_from_cache,
            )
        elif (
            self.model_type == &quot;legacy&quot;
            or self.model_type == &quot;pixart_sigma&quot;
            or self.model_type == &quot;smoldit&quot;
        ):
            # both sd1.x/2.x and t5 style models like pixart use this flow.
            output = self.compute_embeddings_for_legacy_prompts(
                raw_prompts,
                return_concat=return_concat,
                load_from_cache=load_from_cache,
            )
        elif self.model_type == &quot;sd3&quot;:
            output = self.compute_embeddings_for_sd3_prompts(
                raw_prompts,
                return_concat=return_concat,
                load_from_cache=load_from_cache,
            )
        elif self.model_type == &quot;flux&quot;:
            output = self.compute_embeddings_for_flux_prompts(
                raw_prompts,
                return_concat=return_concat,
                load_from_cache=load_from_cache,
            )
        elif self.model_type == &quot;sana&quot;:
            output = self.compute_embeddings_for_sana_prompts(
                raw_prompts,
                return_concat=return_concat,
                load_from_cache=load_from_cache,
                is_negative_prompt=is_negative_prompt,
            )
        elif self.model_type == &quot;ltxvideo&quot;:
            output = self.compute_embeddings_for_ltxvideo_prompts(
                raw_prompts,
                return_concat=return_concat,
                load_from_cache=load_from_cache,
                is_negative_prompt=is_negative_prompt,
            )
        else:
            raise ValueError(
                f&quot;No such text encoding backend for model type &apos;{self.model_type}&apos;&quot;
            )
        # logger.debug(f&quot;Returning output: {output}&quot;)
        return output
    def split_captions_between_processes(self, all_captions: list):
        with self.accelerator.split_between_processes(all_captions) as split:
            split_captions = split
        self.debug_log(
            f&quot;Before splitting, we had {len(all_captions)} captions. After splitting, we have {len(split_captions)} unprocessed files.&quot;
        )
        # # Print the first 5 as a debug log:
        self.debug_log(f&quot;Local unprocessed captions: {split_captions[:5]} (truncated)&quot;)
        return split_captions
    def compute_embeddings_for_sdxl_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        is_validation: bool = False,
        load_from_cache: bool = True,
    ):
        prompt_embeds_all = []
        add_text_embeds_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and is_validation
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            load_from_cache = False
            should_encode = True
        # self.debug_log(
        #     f&quot;compute_embeddings_for_sdxl_prompts received list of prompts: {list(prompts)[:5]}&quot;
        # )
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                disable=return_concat,
                miniters=50,
                leave=False,
                ncols=125,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                debug_msg = f&quot;Processing file: {filename}, prompt: {prompt}&quot;
                prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                debug_msg = f&quot;{debug_msg}\n -&gt; filtered prompt: {prompt}&quot;
                logger.debug(debug_msg)
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        prompt_embeds, add_text_embeds = self.load_from_cache(filename)
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                            f&quot;\n-&gt; id: {self.id}, data_backend id: {self.data_backend.id}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # If load_from_cache is True, should_encode would be False unless we failed to load.
                    # self.debug_log(f&quot;Encoding prompt: {prompt}&quot;)
                    prompt_embeds, pooled_prompt_embeds = self.encode_sdxl_prompts(
                        self.text_encoders,
                        self.tokenizers,
                        [prompt],
                        is_validation,
                    )
                    add_text_embeds = pooled_prompt_embeds
                    # If the prompt is empty, zero out the embeddings
                    if prompt == &quot;&quot;:
                        prompt_embeds = torch.zeros_like(prompt_embeds)
                        add_text_embeds = torch.zeros_like(add_text_embeds)
                    # Get the current size of the queue.
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            time.sleep(0.1)
                    self.debug_log(f&quot;Adding embed to write queue: {filename}&quot;)
                    self.save_to_cache(filename, (prompt_embeds, add_text_embeds))
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                    if return_concat:
                        prompt_embeds = prompt_embeds.to(self.accelerator.device)
                        add_text_embeds = add_text_embeds.to(self.accelerator.device)
                    else:
                        del prompt_embeds
                        del add_text_embeds
                        del pooled_prompt_embeds
                        continue
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    add_text_embeds_all.append(add_text_embeds)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            logger.debug(
                f&quot;Exiting text cache write busy-loop, {self.write_queue.qsize()} items remaining.&quot;
            )
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                del add_text_embeds_all
                return
            prompt_embeds_all = torch.cat(prompt_embeds_all, dim=0)
            add_text_embeds_all = torch.cat(add_text_embeds_all, dim=0)
        return prompt_embeds_all, add_text_embeds_all
    def compute_embeddings_for_legacy_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        load_from_cache: bool = True,
    ):
        logger.debug(
            f&quot;compute_embeddings_for_legacy_prompts arguments: prompts={prompts}, return_concat={return_concat}, load_from_cache={load_from_cache}&quot;
        )
        prompt_embeds_all = []
        prompt_embeds_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and not load_from_cache
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            should_encode = True
            logger.debug(&quot;Setting should_encode = True&quot;)
        # self.debug_log(
        #     f&quot;compute_embeddings_for_legacy_prompts received list of prompts: {list(prompts)[:5]}&quot;
        # )
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            attention_mask = None
            attention_masks_all = []
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                leave=False,
                ncols=125,
                disable=return_concat,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                if prompt != &quot;&quot;:
                    prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                if prompt is None:
                    continue
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        logging.debug(&quot;Loading embed from cache.&quot;)
                        prompt_embeds = self.load_from_cache(filename)
                        if type(prompt_embeds) is tuple and len(prompt_embeds) == 2:
                            # we have an attention mask stored with the embed.
                            prompt_embeds, attention_mask = prompt_embeds
                        logging.debug(f&quot;Loaded embeds: {prompt_embeds.shape}&quot;)
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # self.debug_log(f&quot;Encoding prompt: {prompt}&quot;)
                    # Get the current size of the queue.
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            logger.debug(&quot;Waiting for write thread to catch up.&quot;)
                            time.sleep(5)
                    if (
                        &quot;deepfloyd&quot; in StateTracker.get_args().model_type
                        or self.model_type == &quot;pixart_sigma&quot;
                        or self.model_type == &quot;smoldit&quot;
                    ):
                        # TODO: Batch this
                        prompt_embeds, attention_mask = self.compute_t5_prompt(
                            prompt=prompt,
                        )
                        if &quot;deepfloyd&quot; not in StateTracker.get_args().model_type:
                            # we have to store the attn mask with the embed for pixart.
                            # smoldit requires the attn mask at inference time 💪🏽
                            prompt_embeds = (prompt_embeds, attention_mask)
                    else:
                        prompt_embeds = self.encode_legacy_prompt(
                            self.text_encoders[0], self.tokenizers[0], [prompt]
                        )
                    if return_concat:
                        if type(prompt_embeds) is tuple:
                            prompt_embeds = (
                                prompt_embeds[0].to(self.accelerator.device),
                                prompt_embeds[1].to(self.accelerator.device),
                            )
                        else:
                            prompt_embeds = prompt_embeds.to(self.accelerator.device)
                    self.save_to_cache(filename, prompt_embeds)
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                if not return_concat:
                    del prompt_embeds
                    prompt_embeds = None
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    if attention_mask is not None:
                        attention_masks_all.append(attention_mask)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            logger.debug(
                f&quot;Exiting text cache write busy-loop, {self.write_queue.qsize()} items remaining.&quot;
            )
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                gc.collect()
                return
        # logger.debug(f&quot;Returning all prompt embeds: {prompt_embeds_all}&quot;)
        if len(attention_masks_all) &gt; 0:
            return prompt_embeds_all, attention_masks_all
        return prompt_embeds_all
    def compute_embeddings_for_sana_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        load_from_cache: bool = True,
        is_negative_prompt: bool = False,
    ):
        logger.debug(
            f&quot;compute_embeddings_for_sana_prompts arguments: prompts={prompts}, return_concat={return_concat}, load_from_cache={load_from_cache}&quot;
        )
        prompt_embeds_all = []
        prompt_embeds_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and not load_from_cache
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            should_encode = True
            logger.debug(&quot;Setting should_encode = True&quot;)
        # self.debug_log(
        #     f&quot;compute_embeddings_for_sana_prompts received list of prompts: {list(prompts)[:5]}&quot;
        # )
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            attention_mask = None
            attention_masks_all = []
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                leave=False,
                ncols=125,
                disable=return_concat,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                if prompt != &quot;&quot;:
                    prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                if prompt is None:
                    continue
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        logging.debug(&quot;Loading embed from cache.&quot;)
                        prompt_embeds = self.load_from_cache(filename)
                        if type(prompt_embeds) is tuple and len(prompt_embeds) == 2:
                            # we have an attention mask stored with the embed.
                            prompt_embeds, attention_mask = prompt_embeds
                        logging.debug(f&quot;Loaded embeds: {prompt_embeds.shape}&quot;)
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # self.debug_log(f&quot;Encoding prompt: {prompt}&quot;)
                    # Get the current size of the queue.
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            logger.debug(&quot;Waiting for write thread to catch up.&quot;)
                            time.sleep(5)
                    # TODO: Batch this
                    prompt_embeds, attention_mask = self.compute_gemma_prompt(
                        prompt=prompt, is_negative_prompt=is_negative_prompt
                    )
                    if &quot;deepfloyd&quot; not in StateTracker.get_args().model_type:
                        # we have to store the attn mask with the embed for pixart.
                        # smoldit requires the attn mask at inference time 💪🏽
                        prompt_embeds = (prompt_embeds, attention_mask)
                    if return_concat:
                        if type(prompt_embeds) is tuple:
                            prompt_embeds = (
                                prompt_embeds[0].to(self.accelerator.device),
                                prompt_embeds[1].to(self.accelerator.device),
                            )
                        else:
                            prompt_embeds = prompt_embeds.to(self.accelerator.device)
                    self.save_to_cache(filename, prompt_embeds)
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                if not return_concat:
                    del prompt_embeds
                    prompt_embeds = None
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    if attention_mask is not None:
                        attention_masks_all.append(attention_mask)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            logger.debug(
                f&quot;Exiting text cache write busy-loop, {self.write_queue.qsize()} items remaining.&quot;
            )
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                gc.collect()
                return
        # logger.debug(f&quot;Returning all prompt embeds: {prompt_embeds_all}&quot;)
        if len(attention_masks_all) &gt; 0:
            return prompt_embeds_all, attention_masks_all
        return prompt_embeds_all
    def compute_embeddings_for_ltxvideo_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        load_from_cache: bool = True,
        is_negative_prompt: bool = False,
    ):
        logger.debug(
            f&quot;compute_embeddings_for_ltxvideo_prompts arguments: prompts={prompts}, return_concat={return_concat}, load_from_cache={load_from_cache}&quot;
        )
        prompt_embeds_all = []
        prompt_embeds_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and not load_from_cache
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            should_encode = True
            logger.debug(&quot;Setting should_encode = True&quot;)
        # self.debug_log(
        #     f&quot;compute_embeddings_for_ltxvideo_prompts received list of prompts: {list(prompts)[:5]}&quot;
        # )
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            attention_mask = None
            attention_masks_all = []
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                leave=False,
                ncols=125,
                disable=return_concat,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                if prompt != &quot;&quot;:
                    prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                if prompt is None:
                    continue
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        logging.debug(&quot;Loading embed from cache.&quot;)
                        prompt_embeds = self.load_from_cache(filename)
                        if type(prompt_embeds) is tuple and len(prompt_embeds) == 2:
                            # we have an attention mask stored with the embed.
                            prompt_embeds, attention_mask = prompt_embeds
                        logging.debug(f&quot;Loaded embeds: {prompt_embeds.shape}&quot;)
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # self.debug_log(f&quot;Encoding prompt: {prompt}&quot;)
                    # Get the current size of the queue.
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            logger.debug(&quot;Waiting for write thread to catch up.&quot;)
                            time.sleep(5)
                    # TODO: Batch this
                    prompt_embeds, attention_mask = self.compute_t5_prompt(
                        prompt=prompt
                    )
                    prompt_embeds = (prompt_embeds, attention_mask)
                    if return_concat:
                        if type(prompt_embeds) is tuple:
                            prompt_embeds = (
                                prompt_embeds[0].to(self.accelerator.device),
                                prompt_embeds[1].to(self.accelerator.device),
                            )
                        else:
                            prompt_embeds = prompt_embeds.to(self.accelerator.device)
                    self.save_to_cache(filename, prompt_embeds)
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                if not return_concat:
                    del prompt_embeds
                    prompt_embeds = None
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    if attention_mask is not None:
                        attention_masks_all.append(attention_mask)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            logger.debug(
                f&quot;Exiting text cache write busy-loop, {self.write_queue.qsize()} items remaining.&quot;
            )
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                gc.collect()
                return
        # logger.debug(f&quot;Returning all prompt embeds: {prompt_embeds_all}&quot;)
        if len(attention_masks_all) &gt; 0:
            return prompt_embeds_all, attention_masks_all
        return prompt_embeds_all
    def compute_embeddings_for_flux_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        is_validation: bool = False,
        load_from_cache: bool = True,
    ):
        prompt_embeds_all = []
        add_text_embeds_all = []
        time_ids_all = []
        masks_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and is_validation
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            load_from_cache = False
            should_encode = True
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                disable=return_concat,
                miniters=50,
                leave=False,
                ncols=125,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                debug_msg = f&quot;Processing file: {filename}, prompt: {prompt}&quot;
                prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                debug_msg = f&quot;{debug_msg}\n -&gt; filtered prompt: {prompt}&quot;
                if prompt is None:
                    logger.error(f&quot;Filename {filename} does not have a caption.&quot;)
                    continue
                logger.debug(debug_msg)
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        _flux_embed = self.load_from_cache(filename)
                        if len(_flux_embed) == 3:
                            # legacy flux embed w/o attn mask
                            prompt_embeds, add_text_embeds, time_ids = _flux_embed
                            masks = None
                        elif len(_flux_embed) == 4:
                            # flux embed with attn mask
                            prompt_embeds, add_text_embeds, time_ids, masks = (
                                _flux_embed
                            )
                        del _flux_embed
                        logger.debug(
                            f&quot;Cached Flux text embeds: {prompt_embeds.shape}, {add_text_embeds.shape}, {time_ids.shape}, {masks.shape if masks is not None else None}&quot;
                        )
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                            f&quot;\n-&gt; id: {self.id}, data_backend id: {self.data_backend.id}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # If load_from_cache is True, should_encode would be False unless we failed to load.
                    self.debug_log(f&quot;Encoding prompt: {prompt}&quot;)
                    prompt_embeds, pooled_prompt_embeds, time_ids, masks = (
                        self.encode_flux_prompt(
                            self.text_encoders,
                            self.tokenizers,
                            [prompt],
                            is_validation,
                            zero_padding_tokens=StateTracker.get_args().t5_padding
                            == &quot;zero&quot;,
                        )
                    )
                    logger.debug(
                        f&quot;Flux prompt embeds: {prompt_embeds.shape}, {pooled_prompt_embeds.shape}, {time_ids.shape}, {masks.shape}&quot;
                    )
                    add_text_embeds = pooled_prompt_embeds
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            time.sleep(0.1)
                    self.debug_log(f&quot;Adding embed to write queue: {filename}&quot;)
                    self.save_to_cache(
                        filename, (prompt_embeds, add_text_embeds, time_ids, masks)
                    )
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                    if return_concat:
                        prompt_embeds = prompt_embeds.to(self.accelerator.device)
                        add_text_embeds = add_text_embeds.to(self.accelerator.device)
                        time_ids = time_ids.to(self.accelerator.device)
                        masks = masks.to(self.accelerator.device)
                    else:
                        del prompt_embeds
                        del add_text_embeds
                        del pooled_prompt_embeds
                        del masks
                        continue
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    add_text_embeds_all.append(add_text_embeds)
                    time_ids_all.append(time_ids)
                    masks_all.append(masks)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                del add_text_embeds_all
                del time_ids_all
                del masks_all
                return
            logger.debug(f&quot;Returning all prompt embeds: {prompt_embeds_all}&quot;)
            prompt_embeds_all = torch.cat(prompt_embeds_all, dim=0)
            add_text_embeds_all = torch.cat(add_text_embeds_all, dim=0)
            time_ids_all = torch.cat(time_ids_all, dim=0)
            # if any masks_all are None, we can&apos;t cat
            masks_all = torch.cat(masks_all, dim=0) if None not in masks_all else None
        return prompt_embeds_all, add_text_embeds_all, time_ids_all, masks_all
    def compute_embeddings_for_sd3_prompts(
        self,
        prompts: list = None,
        return_concat: bool = True,
        is_validation: bool = False,
        load_from_cache: bool = True,
    ):
        prompt_embeds_all = []
        add_text_embeds_all = []
        should_encode = not load_from_cache
        args = StateTracker.get_args()
        if should_encode:
            local_caption_split = self.split_captions_between_processes(
                prompts or self.prompts
            )
        else:
            local_caption_split = prompts or self.prompts
        if (
            hasattr(args, &quot;cache_clear_validation_prompts&quot;)
            and args.cache_clear_validation_prompts
            and is_validation
        ):
            # If --cache_clear_validation_prompts was provided, we will forcibly overwrite them.
            load_from_cache = False
            should_encode = True
        # self.debug_log(
        #     f&quot;compute_embeddings_for_sdxl_prompts received list of prompts: {list(prompts)[:5]}&quot;
        # )
        if self.webhook_handler is not None:
            last_reported_index = 0
            self.send_progress_update(
                type=&quot;init_cache_text_embeds_started&quot;,
                progress=int(0 // len(local_caption_split)),
                total=len(local_caption_split),
                current=0,
            )
        self.write_thread_bar = tqdm(
            desc=&quot;Write embeds to disk&quot;,
            leave=False,
            ncols=125,
            disable=return_concat,
            total=len(local_caption_split),
            position=get_rank(),
        )
        with torch.no_grad():
            last_reported_index = 0
            for prompt in tqdm(
                local_caption_split,
                desc=&quot;Processing prompts&quot;,
                disable=return_concat,
                miniters=50,
                leave=False,
                ncols=125,
                position=get_rank() + self.accelerator.num_processes + 1,
            ):
                filename = os.path.join(self.cache_dir, self.hash_prompt(prompt))
                debug_msg = f&quot;Processing file: {filename}, prompt: {prompt}&quot;
                prompt = PromptHandler.filter_caption(self.data_backend, prompt)
                debug_msg = f&quot;{debug_msg}\n -&gt; filtered prompt: {prompt}&quot;
                if prompt is None:
                    logger.error(f&quot;Filename {filename} does not have a caption.&quot;)
                    continue
                logger.debug(debug_msg)
                if return_concat and load_from_cache:
                    try:
                        # We attempt to load.
                        prompt_embeds, add_text_embeds = self.load_from_cache(filename)
                        logger.debug(
                            f&quot;Cached SD3 embeds: {prompt_embeds.shape}, {add_text_embeds.shape}&quot;
                        )
                    except Exception as e:
                        # We failed to load. Now encode the prompt.
                        logger.error(
                            f&quot;Failed retrieving prompt from cache:&quot;
                            f&quot;\n-&gt; prompt: {prompt}&quot;
                            f&quot;\n-&gt; filename: {filename}&quot;
                            f&quot;\n-&gt; error: {e}&quot;
                            f&quot;\n-&gt; id: {self.id}, data_backend id: {self.data_backend.id}&quot;
                        )
                        should_encode = True
                        raise Exception(
                            &quot;Cache retrieval for text embed file failed. Ensure your dataloader config value for skip_file_discovery does not contain &apos;text&apos;, and that preserve_data_backend_cache is disabled or unset.&quot;
                        )
                if should_encode:
                    # If load_from_cache is True, should_encode would be False unless we failed to load.
                    self.debug_log(
                        f&quot;Encoding filename {filename} :: device {self.text_encoders[0].device} :: prompt {prompt}&quot;
                    )
                    prompt_embeds, pooled_prompt_embeds = self.encode_sd3_prompt(
                        self.text_encoders,
                        self.tokenizers,
                        [prompt],
                        is_validation,
                        zero_padding_tokens=(
                            True
                            if StateTracker.get_args().t5_padding == &quot;zero&quot;
                            else False
                        ),
                    )
                    logger.debug(
                        f&quot;Filename {filename} SD3 prompt embeds: {prompt_embeds.shape}, {pooled_prompt_embeds.shape}&quot;
                    )
                    add_text_embeds = pooled_prompt_embeds
                    # StabilityAI say not to zero them out.
                    if prompt == &quot;&quot;:
                        if StateTracker.get_args().sd3_clip_uncond_behaviour == &quot;zero&quot;:
                            prompt_embeds = torch.zeros_like(prompt_embeds)
                        if StateTracker.get_args().sd3_t5_uncond_behaviour == &quot;zero&quot;:
                            add_text_embeds = torch.zeros_like(add_text_embeds)
                    # Get the current size of the queue.
                    current_size = self.write_queue.qsize()
                    if current_size &gt;= 2048:
                        log_msg = str(
                            f&quot;[WARNING] Write queue size is {current_size}. This is quite large.&quot;
                            &quot; Consider increasing the write batch size. Delaying encode so that writes can catch up.&quot;
                        )
                        self.write_thread_bar.write(log_msg)
                        while self.write_queue.qsize() &gt; 100:
                            time.sleep(0.1)
                    self.debug_log(f&quot;Adding embed to write queue: {filename}&quot;)
                    self.save_to_cache(filename, (prompt_embeds, add_text_embeds))
                    if (
                        self.webhook_handler is not None
                        and int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        &lt; 10
                    ):
                        last_reported_index = int(
                            self.write_thread_bar.n % self.webhook_progress_interval
                        )
                        self.send_progress_update(
                            type=&quot;init_cache_text_embeds_status_update&quot;,
                            progress=int(
                                self.write_thread_bar.n
                                // len(local_caption_split)
                                * 100
                            ),
                            total=len(local_caption_split),
                            current=0,
                        )
                    if return_concat:
                        prompt_embeds = prompt_embeds.to(self.accelerator.device)
                        add_text_embeds = add_text_embeds.to(self.accelerator.device)
                    else:
                        del prompt_embeds
                        del add_text_embeds
                        del pooled_prompt_embeds
                        continue
                if return_concat:
                    prompt_embeds_all.append(prompt_embeds)
                    add_text_embeds_all.append(add_text_embeds)
            while self.write_queue.qsize() &gt; 0:
                time.sleep(0.1)  # Sleep briefly to avoid busy-waiting
            if self.webhook_handler is not None:
                self.send_progress_update(
                    type=&quot;init_cache_text_embeds_status_complete&quot;,
                    progress=100,
                    total=len(local_caption_split),
                    current=len(local_caption_split),
                )
            # Close the tqdm progress bar after the loop
            self.write_thread_bar.close()
            self.process_write_batches = False
            if not return_concat:
                del prompt_embeds_all
                del add_text_embeds_all
                return
            logger.debug(f&quot;Returning all prompt embeds: {prompt_embeds_all}&quot;)
            prompt_embeds_all = torch.cat(prompt_embeds_all, dim=0)
            add_text_embeds_all = torch.cat(add_text_embeds_all, dim=0)
        return prompt_embeds_all, add_text_embeds_all
    def __del__(self):
        &quot;&quot;&quot;Ensure that the batch write thread is properly closed.&quot;&quot;&quot;
        if self.batch_write_thread.is_alive():
            self.batch_write_thread.join()</file><file path="helpers/caching/vae.py">import os
import torch
import logging
import traceback
from concurrent.futures import ThreadPoolExecutor
from random import shuffle
from tqdm import tqdm
from pathlib import Path
from PIL import Image
from numpy import str_ as numpy_str
from helpers.multiaspect.image import MultiaspectImage
from helpers.image_manipulation.training_sample import TrainingSample, PreparedSample
from helpers.data_backend.base import BaseDataBackend
from helpers.metadata.backends.base import MetadataBackend
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import _get_rank as get_rank
from helpers.training.multi_process import rank_info
from queue import Queue
from concurrent.futures import as_completed
from hashlib import sha256
from helpers.training import image_file_extensions
from helpers.webhooks.mixin import WebhookMixin
logger = logging.getLogger(&quot;VAECache&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def prepare_sample(
    image: Image.Image = None, data_backend_id: str = None, filepath: str = None
):
    metadata = StateTracker.get_metadata_by_filepath(
        filepath, data_backend_id=data_backend_id
    )
    data_backend = StateTracker.get_data_backend(data_backend_id)
    data_sampler = data_backend.get(&quot;sampler&quot;)
    image_data = image
    if image_data is None:
        image_data = data_sampler.yield_single_image(filepath)
    training_sample = TrainingSample(
        image=image_data,
        data_backend_id=data_backend_id,
        image_metadata=metadata,
        image_path=filepath,
    )
    prepared_sample = training_sample.prepare()
    return (
        prepared_sample.image,
        prepared_sample.crop_coordinates,
        prepared_sample.aspect_ratio,
    )
class VAECache(WebhookMixin):
    def __init__(
        self,
        id: str,
        vae,
        accelerator,
        metadata_backend: MetadataBackend,
        instance_data_dir: str,
        image_data_backend: BaseDataBackend,
        webhook_progress_interval: int = 100,
        cache_data_backend: BaseDataBackend = None,
        cache_dir=&quot;vae_cache&quot;,
        resolution: float = 1024,
        maximum_image_size: float = None,
        target_downsample_size: float = None,
        num_video_frames: int = 125,
        delete_problematic_images: bool = False,
        write_batch_size: int = 25,
        read_batch_size: int = 25,
        process_queue_size: int = 16,
        vae_batch_size: int = 4,
        resolution_type: str = &quot;pixel&quot;,
        minimum_image_size: int = None,
        max_workers: int = 32,
        vae_cache_ondemand: bool = False,
        hash_filenames: bool = False,
        dataset_type: str = None,
    ):
        self.id = id
        self.dataset_type = dataset_type
        if image_data_backend and image_data_backend.id != id:
            raise ValueError(
                f&quot;VAECache received incorrect image_data_backend: {image_data_backend}&quot;
            )
        self.image_data_backend = image_data_backend
        self.cache_data_backend = (
            cache_data_backend if cache_data_backend is not None else image_data_backend
        )
        self.hash_filenames = hash_filenames
        self.vae = vae
        self.accelerator = accelerator
        self.cache_dir = cache_dir
        if len(self.cache_dir) &gt; 0 and self.cache_dir[-1] == &quot;/&quot;:
            # Remove trailing slash
            self.cache_dir = self.cache_dir[:-1]
        if self.cache_data_backend and self.cache_data_backend.type == &quot;local&quot;:
            self.cache_dir = os.path.abspath(self.cache_dir)
            self.cache_data_backend.create_directory(self.cache_dir)
        self.resolution = resolution
        self.resolution_type = resolution_type
        self.minimum_image_size = minimum_image_size
        self.webhook_progress_interval = webhook_progress_interval
        self.delete_problematic_images = delete_problematic_images
        self.write_batch_size = write_batch_size
        self.read_batch_size = read_batch_size
        self.process_queue_size = process_queue_size
        self.vae_batch_size = vae_batch_size
        self.instance_data_dir = instance_data_dir
        self.transform_image = MultiaspectImage.get_image_transforms()
        self.transform_video = None
        self.num_video_frames = None
        if self.dataset_type == &quot;video&quot;:
            self.num_video_frames = num_video_frames
            self.transform_video = MultiaspectImage.get_video_transforms()
        self.rank_info = rank_info()
        self.metadata_backend = metadata_backend
        if self.metadata_backend and not self.metadata_backend.image_metadata_loaded:
            self.metadata_backend.load_image_metadata()
        self.vae_cache_ondemand = vae_cache_ondemand
        self.max_workers = max_workers
        if (maximum_image_size and not target_downsample_size) or (
            target_downsample_size and not maximum_image_size
        ):
            raise ValueError(
                &quot;Both maximum_image_size and target_downsample_size must be specified.&quot;
                f&quot;Only {&apos;maximum_image_size&apos; if maximum_image_size else &apos;target_downsample_size&apos;} was specified.&quot;
            )
        self.maximum_image_size = maximum_image_size
        self.target_downsample_size = target_downsample_size
        self.read_queue = Queue()
        self.process_queue = Queue()
        self.write_queue = Queue()
        self.vae_input_queue = Queue()
    def debug_log(self, msg: str):
        logger.debug(f&quot;{self.rank_info}{msg}&quot;)
    def generate_vae_cache_filename(self, filepath: str) -&gt; tuple:
        &quot;&quot;&quot;Get the cache filename for a given image filepath and its base name.&quot;&quot;&quot;
        if filepath.endswith(&quot;.pt&quot;):
            return filepath, os.path.basename(filepath)
        # Extract the base name from the filepath and replace the image extension with .pt
        base_filename = os.path.splitext(os.path.basename(filepath))[0]
        if self.hash_filenames:
            base_filename = str(sha256(str(base_filename).encode()).hexdigest())
        base_filename = str(base_filename) + &quot;.pt&quot;
        # Find the subfolders the sample was in, and replace the instance_data_dir with the cache_dir
        subfolders = &quot;&quot;
        if self.instance_data_dir is not None:
            subfolders = os.path.dirname(filepath).replace(self.instance_data_dir, &quot;&quot;)
            subfolders = subfolders.lstrip(os.sep)
        if len(subfolders) &gt; 0:
            full_filename = os.path.join(self.cache_dir, subfolders, base_filename)
            # logger.debug(
            #     f&quot;full_filename: {full_filename} = os.path.join({self.cache_dir}, {subfolders}, {base_filename})&quot;
            # )
        else:
            full_filename = os.path.join(self.cache_dir, base_filename)
            # logger.debug(
            #     f&quot;full_filename: {full_filename} = os.path.join({self.cache_dir}, {base_filename})&quot;
            # )
        return full_filename, base_filename
    def _image_filename_from_vaecache_filename(self, filepath: str) -&gt; tuple[str, str]:
        test_filepath, _ = self.generate_vae_cache_filename(filepath)
        result = self.vae_path_to_image_path.get(test_filepath, None)
        return result
    def build_vae_cache_filename_map(self, all_image_files: list):
        &quot;&quot;&quot;Build a map of image filepaths to their corresponding cache filenames.&quot;&quot;&quot;
        self.image_path_to_vae_path = {}
        self.vae_path_to_image_path = {}
        for image_file in all_image_files:
            cache_filename, _ = self.generate_vae_cache_filename(image_file)
            if self.cache_data_backend.type == &quot;local&quot;:
                cache_filename = os.path.abspath(cache_filename)
            self.image_path_to_vae_path[image_file] = cache_filename
            self.vae_path_to_image_path[cache_filename] = image_file
    def already_cached(self, filepath: str) -&gt; bool:
        test_path = self.image_path_to_vae_path.get(filepath, None)
        if self.cache_data_backend.exists(test_path):
            return True
        return False
    def _read_from_storage(
        self, filename: str, hide_errors: bool = False
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;Read an image or cache object from the storage backend.
        Args:
            filename (str): The path to the cache item, eg. `vae_cache/foo.pt` or `instance_data_dir/foo.png`
        Returns:
            Image or cache object
        &quot;&quot;&quot;
        if os.path.splitext(filename)[1] != &quot;.pt&quot;:
            try:
                return self.image_data_backend.read_image(filename)
            except Exception as e:
                if self.delete_problematic_images:
                    self.metadata_backend.remove_image(filename)
                    self.image_data_backend.delete(filename)
                    self.debug_log(
                        f&quot;Deleted {filename} because it was problematic: {e}&quot;
                    )
                raise e
        try:
            torch_data = self.cache_data_backend.torch_load(filename)
            if isinstance(torch_data, torch.Tensor):
                torch_data = torch_data.to(&quot;cpu&quot;)
            elif isinstance(torch_data, dict):
                torch_data[&quot;latents&quot;] = torch_data[&quot;latents&quot;].to(&quot;cpu&quot;)
            return torch_data
        except Exception as e:
            if hide_errors:
                self.debug_log(
                    f&quot;Filename: {filename}, returning None even though read_from_storage found no object, since hide_errors is True: {e}&quot;
                )
                return None
            raise e
    def retrieve_from_cache(self, filepath: str):
        &quot;&quot;&quot;
        Use the encode_images method to emulate a single image encoding.
        &quot;&quot;&quot;
        return self.encode_images([None], [filepath])[0]
    def retreve_batch_from_cache(self, filepaths: list):
        &quot;&quot;&quot;
        Use the encode_images method to emulate a batch of image encodings.
        &quot;&quot;&quot;
        return self.encode_images([None] * len(filepaths), filepaths)
    def discover_all_files(self):
        &quot;&quot;&quot;Identify all files in the data backend.&quot;&quot;&quot;
        all_image_files = StateTracker.get_image_files(
            data_backend_id=self.id
        ) or StateTracker.set_image_files(
            self.image_data_backend.list_files(
                instance_data_dir=self.instance_data_dir,
                file_extensions=image_file_extensions,
            ),
            data_backend_id=self.id,
        )
        # This isn&apos;t returned, because we merely check if it&apos;s stored, or, store it.
        (
            StateTracker.get_vae_cache_files(data_backend_id=self.id)
            or StateTracker.set_vae_cache_files(
                self.cache_data_backend.list_files(
                    instance_data_dir=self.cache_dir,
                    file_extensions=[&quot;pt&quot;],
                ),
                data_backend_id=self.id,
            )
        )
        self.debug_log(
            f&quot;VAECache discover_all_files found {len(all_image_files)} images&quot;
        )
        return all_image_files
    def init_vae(self):
        if StateTracker.get_args().model_family == &quot;sana&quot;:
            from diffusers import AutoencoderDC as AutoencoderClass
        else:
            from diffusers import AutoencoderKL as AutoencoderClass
        args = StateTracker.get_args()
        vae_path = (
            args.pretrained_model_name_or_path
            if args.pretrained_vae_model_name_or_path is None
            else args.pretrained_vae_model_name_or_path
        )
        precached_vae = StateTracker.get_vae()
        self.vae = precached_vae or AutoencoderClass.from_pretrained(
            vae_path,
            subfolder=&quot;vae&quot; if args.pretrained_vae_model_name_or_path is None else None,
            revision=args.revision,
            force_upcast=False,
        ).to(self.accelerator.device)
        if self.vae.device != self.accelerator.device:
            self.vae = self.vae.to(self.accelerator.device)
        StateTracker.set_vae(self.vae)
    def rebuild_cache(self):
        &quot;&quot;&quot;
        First, we&apos;ll clear the cache before rebuilding it.
        &quot;&quot;&quot;
        self.debug_log(&quot;Rebuilding cache.&quot;)
        if self.accelerator.is_local_main_process:
            self.debug_log(&quot;Updating StateTracker with new VAE cache entry list.&quot;)
            StateTracker.set_vae_cache_files(
                self.cache_data_backend.list_files(
                    instance_data_dir=self.cache_dir,
                    file_extensions=[&quot;pt&quot;],
                ),
                data_backend_id=self.id,
            )
        self.accelerator.wait_for_everyone()
        self.debug_log(&quot;-&gt; Clearing cache objects&quot;)
        self.clear_cache()
        self.debug_log(&quot;-&gt; Split tasks between GPU(s)&quot;)
        self.discover_unprocessed_files()
        self.debug_log(&quot;-&gt; Load VAE&quot;)
        self.init_vae()
        if not StateTracker.get_args().vae_cache_ondemand:
            self.debug_log(&quot;-&gt; Process VAE cache&quot;)
            self.process_buckets()
            if self.accelerator.is_local_main_process:
                self.debug_log(&quot;Updating StateTracker with new VAE cache entry list.&quot;)
                StateTracker.set_vae_cache_files(
                    self.cache_data_backend.list_files(
                        instance_data_dir=self.cache_dir,
                        file_extensions=[&quot;pt&quot;],
                    ),
                    data_backend_id=self.id,
                )
            self.accelerator.wait_for_everyone()
        self.debug_log(&quot;-&gt; Completed cache rebuild&quot;)
    def clear_cache(self):
        &quot;&quot;&quot;
        Clear all .pt files in our data backend&apos;s cache prefix, as obtained from self.discover_all_files().
        We can&apos;t simply clear the directory, because it might be mixed with the image samples (in the case of S3)
        We want to thread this, using the data_backend.delete function as the worker function.
        &quot;&quot;&quot;
        futures = []
        all_cache_files = StateTracker.get_vae_cache_files(data_backend_id=self.id)
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for filename in all_cache_files:
                full_path = os.path.join(self.cache_dir, filename)
                self.debug_log(f&quot;Would delete: {full_path}&quot;)
                futures.append(
                    executor.submit(self.cache_data_backend.delete, full_path)
                )
            for future in tqdm(
                as_completed(futures),
                total=len(futures),
                desc=f&quot;Deleting files for backend {self.id}&quot;,
                position=get_rank(),
                ncols=125,
                leave=False,
            ):
                try:
                    future.result()
                except Exception as e:
                    logger.debug(f&quot;Error deleting file {filename}&quot;, e)
        # Clear the StateTracker list of VAE objects:
        StateTracker.set_vae_cache_files([], data_backend_id=self.id)
    def _list_cached_images(self):
        &quot;&quot;&quot;
        Return a set of filenames (without the .pt extension) that have been processed.
        &quot;&quot;&quot;
        # Extract array of tuple into just, an array of files:
        pt_files = StateTracker.get_vae_cache_files(data_backend_id=self.id)
        # Extract just the base filename without the extension
        results = {os.path.splitext(f)[0] for f in pt_files}
        # self.debug_log(
        #     f&quot;Found {len(pt_files)} cached files in {self.cache_dir} (truncated): {list(results)[:5]}&quot;
        # )
        return results
    def discover_unprocessed_files(self, directory: str = None):
        &quot;&quot;&quot;Identify files that haven&apos;t been processed yet.&quot;&quot;&quot;
        all_image_files = set(StateTracker.get_image_files(data_backend_id=self.id))
        existing_cache_files = set(
            StateTracker.get_vae_cache_files(data_backend_id=self.id)
        )
        # Convert cache filenames to their corresponding image filenames
        already_cached_images = []
        for cache_file in existing_cache_files:
            try:
                n = self._image_filename_from_vaecache_filename(cache_file)
                if n is None:
                    continue
                already_cached_images.append(n)
            except Exception as e:
                logger.error(
                    f&quot;Could not find image path for cache file {cache_file}: {e}&quot;
                )
                continue
        # Identify unprocessed files
        self.local_unprocessed_files = list(
            set(all_image_files) - set(already_cached_images)
        )
        return self.local_unprocessed_files
    def _reduce_bucket(
        self,
        bucket: str,
        aspect_bucket_cache: dict,
        processed_images: dict,
        do_shuffle: bool = True,
    ):
        &quot;&quot;&quot;
        Given a bucket, return the relevant files for that bucket.
        &quot;&quot;&quot;
        relevant_files = []
        total_files = 0
        skipped_files = 0
        for full_image_path in aspect_bucket_cache[bucket]:
            total_files += 1
            comparison_path = self.generate_vae_cache_filename(full_image_path)[0]
            if os.path.splitext(comparison_path)[0] in processed_images:
                # processed_images contains basename *cache* paths:
                skipped_files += 1
                # self.debug_log(
                #     f&quot;Reduce bucket {bucket}, skipping ({skipped_files}/{total_files}) {full_image_path} because it is in processed_images&quot;
                # )
                continue
            if full_image_path not in self.local_unprocessed_files:
                # full_image_path is the full *image* path:
                skipped_files += 1
                # self.debug_log(
                #     f&quot;Reduce bucket {bucket}, skipping ({skipped_files}/{total_files}) {full_image_path} because it is not in local_unprocessed_files&quot;
                # )
                continue
            # self.debug_log(
            #     f&quot;Reduce bucket {bucket}, adding ({len(relevant_files)}/{total_files}) {full_image_path}&quot;
            # )
            relevant_files.append(full_image_path)
        if do_shuffle:
            shuffle(relevant_files)
        # self.debug_log(
        #     f&quot;Reduced bucket {bucket} down from {len(aspect_bucket_cache[bucket])} to {len(relevant_files)} relevant files.&quot;
        #     f&quot; Our system has {len(self.local_unprocessed_files)} total images in its assigned slice for processing across all buckets.&quot;
        # )
        return relevant_files
    def process_video_latents(self, latents_uncached):
        output_cache_entry = latents_uncached
        if StateTracker.get_model_family() in [&quot;ltxvideo&quot;]:
            from helpers.models.ltxvideo import (
                normalize_ltx_latents,
                pack_ltx_latents,
                unpack_ltx_latents,
            )
            # hardcode patch size to 1 for LTX Video.
            # patch_size, patch_size_t = self.transformer.config.patch_size, self.transformer.config.patch_size_t
            patch_size, patch_size_t = 1, 1
            _, _, num_frames, height, width = latents_uncached.shape
            logger.debug(f&quot;Latents shape: {latents_uncached.shape}&quot;)
            latents_uncached = normalize_ltx_latents(
                latents_uncached, self.vae.latents_mean, self.vae.latents_std
            )
            output_cache_entry = {
                &quot;latents&quot;: latents_uncached.shape,  # we&apos;ll log the shape first
                &quot;num_frames&quot;: self.num_video_frames,
                &quot;height&quot;: height,
                &quot;width&quot;: width,
            }
            logger.debug(f&quot;Video latent processing results: {output_cache_entry}&quot;)
            # we&apos;ll now overwrite the latents after logging.
            output_cache_entry[&quot;latents&quot;] = latents_uncached
        return output_cache_entry
    def prepare_video_latents(self, samples):
        if StateTracker.get_model_family() == &quot;ltxvideo&quot;:
            if samples.ndim == 4:
                logger.debug(&quot;PROCESSING IMAGE to VIDEO LATENTS CONVERSION&quot;)
                logger.debug(f&quot;Unsqueeze from dim {samples.shape}&quot;)
                samples = samples.unsqueeze(2)
                logger.debug(f&quot;New dim: {samples.shape}&quot;)
            assert samples.ndim == 5, f&quot;Expected 5D tensor, got {samples.ndim}D tensor&quot;
            logger.debug(
                f&quot;PROCESSING VIDEO to VIDEO LATENTS CONVERSION ({samples.shape})&quot;
            )
            # images are torch.Size([1, 3, 1, 640, 448]) (B, C, F, H, W) but videos are torch.Size([2, 600, 3, 384, 395])
            # we have to permute the video latent samples to match the image latent samples
            if samples.shape[2] == 3:
                samples = samples.permute(0, 2, 1, 3, 4)
            num_frames = samples.shape[1]
            if (
                self.num_video_frames is not None
                and self.num_video_frames != num_frames
            ):
                # we&apos;ll discard along dim2 after num_video_frames
                samples = samples[:, :, :125, :, :]
                logger.info(f&quot;Sliced to {samples.shape}&quot;)
        logger.debug(f&quot;Permute to: {samples.shape}&quot;)
        logger.debug(f&quot;Final samples shape: {samples.shape}&quot;)
        return samples
    def encode_images(self, images, filepaths, load_from_cache=True):
        &quot;&quot;&quot;
        Encode a batch of input images. Images must be the same dimension.
        If load_from_cache=True, we read from the VAE cache rather than encode.
        If load_from_cache=True, we will throw an exception if the entry is not found.
        &quot;&quot;&quot;
        batch_size = len(images)
        if batch_size != len(filepaths):
            raise ValueError(&quot;Mismatch between number of images and filepaths.&quot;)
        full_filenames = [
            self.generate_vae_cache_filename(filepath)[0] for filepath in filepaths
        ]
        # Check cache for each image and filter out already cached ones
        uncached_images = []
        uncached_image_indices = [
            i
            for i, filename in enumerate(full_filenames)
            if not self.cache_data_backend.exists(filename)
        ]
        uncached_image_paths = [
            filepaths[i]
            for i, filename in enumerate(full_filenames)
            if i in uncached_image_indices
        ]
        # We need to populate any uncached images with the actual image data if they are None.
        missing_images = [
            i
            for i, image in enumerate(images)
            if i in uncached_image_indices and image is None
        ]
        missing_image_pixel_values = []
        written_latents = []
        if len(missing_images) &gt; 0 and self.vae_cache_ondemand:
            missing_image_paths = [filepaths[i] for i in missing_images]
            missing_image_data_generator = self._read_from_storage_concurrently(
                missing_image_paths, hide_errors=True
            )
            # extract images from generator:
            missing_image_data = [
                retrieved_image_data[1]
                for retrieved_image_data in missing_image_data_generator
            ]
            missing_image_pixel_values = self._process_images_in_batch(
                missing_image_paths, missing_image_data, disable_queue=True
            )
            missing_image_vae_outputs = self._encode_images_in_batch(
                image_pixel_values=missing_image_pixel_values, disable_queue=True
            )
            written_latents = self._write_latents_in_batch(missing_image_vae_outputs)
            if len(written_latents) == len(images):
                return written_latents
        if len(uncached_image_indices) &gt; 0:
            uncached_images = [images[i] for i in uncached_image_indices]
        elif len(missing_images) &gt; 0 and len(missing_image_pixel_values) &gt; 0:
            uncached_images = []
            for i in uncached_image_indices:
                if images[i] is not None:
                    uncached_images.append(images[i])
                elif i in missing_image_pixel_values:
                    uncached_images.append(missing_image_pixel_values[i])
        if (
            len(uncached_image_indices) &gt; 0
            and load_from_cache
            and not self.vae_cache_ondemand
        ):
            # We wanted only uncached images. Something went wrong.
            raise Exception(
                f&quot;(id={self.id}) Some images were not correctly cached during the VAE Cache operations. Ensure --skip_file_discovery=vae is not set.\nProblematic images: {uncached_image_paths}&quot;
            )
        latents = []
        if load_from_cache:
            # If all images are cached, simply load them
            latents = [
                self._read_from_storage(filename, hide_errors=self.vae_cache_ondemand)
                for filename in full_filenames
                if filename not in uncached_images
            ]
        if len(uncached_images) &gt; 0 and (
            len(images) != len(latents) or len(filepaths) != len(latents)
        ):
            # Process images not found in cache
            with torch.no_grad():
                processed_images = torch.stack(uncached_images).to(
                    self.accelerator.device, dtype=StateTracker.get_vae_dtype()
                )
                processed_images = self.prepare_video_latents(processed_images)
                logger.info(f&quot;Encoding: {processed_images.shape}&quot;)
                latents_uncached = self.vae.encode(processed_images)
                if hasattr(latents_uncached, &quot;latent_dist&quot;):
                    latents_uncached = latents_uncached.latent_dist.sample()
                latents_uncached = self.process_video_latents(latents_uncached)
                if (
                    hasattr(self.vae, &quot;config&quot;)
                    and hasattr(self.vae.config, &quot;shift_factor&quot;)
                    and self.vae.config.shift_factor is not None
                ):
                    latents_uncached = (
                        latents_uncached - self.vae.config.shift_factor
                    ) * self.vae.config.scaling_factor
                elif isinstance(latents_uncached, torch.Tensor):
                    latents_uncached = (
                        getattr(latents_uncached, &quot;latent&quot;, latents_uncached)
                        * self.vae.config.scaling_factor
                    )
                    logger.debug(f&quot;Latents shape: {latents_uncached.shape}&quot;)
            # Prepare final latents list by combining cached and newly computed latents
            if isinstance(latents_uncached, dict):
                raw_latents = latents_uncached[&quot;latents&quot;]
                num_samples = raw_latents.shape[0]
                for i in range(num_samples):
                    # Each sub-dict is shape [1, 128, F, H, W]
                    single_latent = raw_latents[i : i + 1].squeeze(0)
                    chunk = {
                        &quot;latents&quot;: single_latent,
                        &quot;num_frames&quot;: latents_uncached[&quot;num_frames&quot;],
                        &quot;height&quot;: latents_uncached[&quot;height&quot;],
                        &quot;width&quot;: latents_uncached[&quot;width&quot;],
                    }
                    latents.append(chunk)
            else:
                cached_idx, uncached_idx = 0, 0
                for i in range(batch_size):
                    if i in uncached_image_indices:
                        latents.append(latents_uncached[uncached_idx])
                        uncached_idx += 1
                    else:
                        latents.append(self._read_from_storage(full_filenames[i]))
                        cached_idx += 1
        return latents
    def _write_latents_in_batch(self, input_latents: list = None):
        # Pull the &apos;filepaths&apos; and &apos;latents&apos; from self.write_queue
        filepaths, latents = [], []
        if input_latents is not None:
            qlen = len(input_latents)
        else:
            qlen = self.write_queue.qsize()
        for idx in range(0, qlen):
            if input_latents:
                output_file, filepath, latent_vector = input_latents.pop()
            else:
                output_file, filepath, latent_vector = self.write_queue.get()
            file_extension = os.path.splitext(output_file)[1]
            if file_extension != &quot;.pt&quot;:
                raise ValueError(
                    f&quot;Cannot write a latent embedding to an image path, {output_file}&quot;
                )
            filepaths.append(output_file)
            # pytorch will hold onto all of the tensors in the list if we do not use clone()
            if isinstance(latent_vector, dict):
                latent_vector[&quot;latents&quot;] = latent_vector[&quot;latents&quot;].clone()
                latents.append(latent_vector)
            else:
                latents.append(latent_vector.clone())
        self.cache_data_backend.write_batch(filepaths, latents)
        return latents
    def _process_images_in_batch(
        self,
        image_paths: list = None,
        image_data: list = None,
        disable_queue: bool = False,
    ) -&gt; None:
        &quot;&quot;&quot;Process a queue of images. This method assumes our batch size has been reached.
        Args:
            image_paths: list If given, image_data must also be supplied. This will avoid the use of the Queues.
            image_data: list Provided Image objects for corresponding image_paths.
        Returns:
            None
        &quot;&quot;&quot;
        try:
            # self.debug_log(
            #     f&quot;Processing batch of images into VAE embeds. image_paths: {type(image_paths)}, image_data: {type(image_data)}&quot;
            # )
            initial_data = []
            filepaths = []
            if image_paths is not None and image_data is not None:
                qlen = len(image_paths)
            else:
                qlen = self.process_queue.qsize()
            # First Loop: Preparation and Filtering
            for _ in range(qlen):
                if image_paths:
                    # retrieve image data from Generator, image_data:
                    filepath = image_paths.pop()
                    image = image_data.pop()
                    aspect_bucket = (
                        self.metadata_backend.get_metadata_attribute_by_filepath(
                            filepath=filepath, attribute=&quot;aspect_bucket&quot;
                        )
                    )
                else:
                    filepath, image, aspect_bucket = self.process_queue.get()
                if self.minimum_image_size is not None:
                    if not self.metadata_backend.meets_resolution_requirements(
                        image_path=filepath
                    ):
                        self.debug_log(
                            f&quot;Skipping {filepath} because it does not meet the minimum image size requirement of {self.minimum_image_size}&quot;
                        )
                        continue
                # image.save(f&quot;test_{os.path.basename(filepath)}.png&quot;)
                initial_data.append((filepath, image, aspect_bucket))
            # Process Pool Execution
            processed_images = []
            with ThreadPoolExecutor(self.max_workers) as executor:
                futures = [
                    executor.submit(
                        prepare_sample,
                        data_backend_id=self.id,
                        filepath=data[0],
                    )
                    for data in initial_data
                ]
                first_aspect_ratio = None
                for future in futures:
                    try:
                        result = (
                            future.result()
                        )  # Returns PreparedSample or tuple(image, crop_coordinates, aspect_ratio)
                        if result:  # Ensure result is not None or invalid
                            processed_images.append(result)
                            if first_aspect_ratio is None:
                                first_aspect_ratio = result[2]
                            elif (
                                type(result) is PreparedSample
                                and result.aspect_ratio is not None
                                and first_aspect_ratio is not None
                                and result.aspect_ratio != first_aspect_ratio
                            ):
                                raise ValueError(
                                    f&quot;({type(result)}) Image {filepath} has a different aspect ratio ({result.aspect_ratio}) than the first image in the batch ({first_aspect_ratio}).&quot;
                                )
                            elif (
                                type(result) is tuple
                                and result[2]
                                and first_aspect_ratio is not None
                                and result[2] != first_aspect_ratio
                            ):
                                raise ValueError(
                                    f&quot;({type(result)}) Image {filepath} has a different aspect ratio ({result[2]}) than the first image in the batch ({first_aspect_ratio}).&quot;
                                )
                    except Exception as e:
                        logger.error(
                            f&quot;Error processing image in pool: {e}, traceback: {traceback.format_exc()}&quot;
                        )
            # Second Loop: Final Processing
            is_final_sample = False
            output_values = []
            first_aspect_ratio = None
            for idx, (image, crop_coordinates, new_aspect_ratio) in enumerate(
                processed_images
            ):
                if idx == len(processed_images) - 1:
                    is_final_sample = True
                if first_aspect_ratio is None:
                    first_aspect_ratio = new_aspect_ratio
                elif new_aspect_ratio != first_aspect_ratio:
                    is_final_sample = True
                    first_aspect_ratio = new_aspect_ratio
                filepath, _, aspect_bucket = initial_data[idx]
                filepaths.append(filepath)
                if self.transform_video is not None:
                    logger.info(f&quot;Running video transformations on {image.shape}&quot;)
                    pixel_values = self.transform_video(image).to(
                        self.accelerator.device, dtype=self.vae.dtype
                    )
                else:
                    pixel_values = self.transform_image(image).to(
                        self.accelerator.device, dtype=self.vae.dtype
                    )
                output_value = (pixel_values, filepath, aspect_bucket, is_final_sample)
                output_values.append(output_value)
                if not disable_queue:
                    self.vae_input_queue.put(
                        (pixel_values, filepath, aspect_bucket, is_final_sample)
                    )
                # Update the crop_coordinates in the metadata document
                # NOTE: This is currently a no-op because the metadata is now considered &apos;trustworthy&apos;.
                #       The VAE encode uses the preexisting metadata, and the TrainingSample class will not update.
                #       However, we&apos;ll check that the values didn&apos;t change anyway, just in case.
                if crop_coordinates:
                    current_crop_coordinates = (
                        self.metadata_backend.get_metadata_attribute_by_filepath(
                            filepath=filepath,
                            attribute=&quot;crop_coordinates&quot;,
                        )
                    )
                    if tuple(current_crop_coordinates) != tuple(crop_coordinates):
                        logger.debug(
                            f&quot;Should be updating crop_coordinates for {filepath} from {current_crop_coordinates} to {crop_coordinates}. But we won&apos;t..&quot;
                        )
            self.debug_log(
                f&quot;Completed processing gathered {len(output_values)} output values.&quot;
            )
        except Exception as e:
            logger.error(
                f&quot;Error processing images {filepaths if len(filepaths) &gt; 0 else image_paths}: {e}&quot;
            )
            self.debug_log(f&quot;Error traceback: {traceback.format_exc()}&quot;)
            raise e
        return output_values
    def _encode_images_in_batch(
        self, image_pixel_values: list = None, disable_queue: bool = False
    ) -&gt; None:
        &quot;&quot;&quot;Encode the batched Image objects using the VAE model.
        Raises:
            ValueError: If we receive any invalid results.
        &quot;&quot;&quot;
        try:
            if image_pixel_values is not None:
                qlen = len(image_pixel_values)
                if self.vae_batch_size != len(image_pixel_values):
                    self.vae_batch_size = len(image_pixel_values)
            else:
                qlen = self.vae_input_queue.qsize()
            if qlen == 0:
                return
            output_values = []
            while qlen &gt; 0:
                vae_input_images, vae_input_filepaths, vae_output_filepaths = [], [], []
                batch_aspect_bucket = None
                count_to_process = min(qlen, self.vae_batch_size)
                for idx in range(0, count_to_process):
                    if image_pixel_values:
                        pixel_values, filepath, aspect_bucket, is_final_sample = (
                            image_pixel_values.pop()
                        )
                    else:
                        pixel_values, filepath, aspect_bucket, is_final_sample = (
                            self.vae_input_queue.get()
                        )
                    if batch_aspect_bucket is None:
                        batch_aspect_bucket = aspect_bucket
                    vae_input_images.append(pixel_values)
                    vae_input_filepaths.append(filepath)
                    vae_output_filepaths.append(
                        self.generate_vae_cache_filename(filepath)[0]
                    )
                    if is_final_sample:
                        # When we have fewer samples in a bucket than our VAE batch size might indicate,
                        # we need to respect is_final_sample value and not retrieve the *next* element yet.
                        break
                latents = self.encode_images(
                    [
                        sample.to(dtype=StateTracker.get_vae_dtype())
                        for sample in vae_input_images
                    ],
                    vae_input_filepaths,
                    load_from_cache=False,
                )
                if latents is None:
                    raise ValueError(&quot;Received None from encode_images&quot;)
                for output_file, latent_vector, filepath in zip(
                    vae_output_filepaths, latents, vae_input_filepaths
                ):
                    if latent_vector is None:
                        raise ValueError(
                            f&quot;Latent vector is None for filepath {filepath}&quot;
                        )
                    output_value = (output_file, filepath, latent_vector)
                    output_values.append(output_value)
                    if not disable_queue:
                        logger.debug(&quot;Adding outputs to write queue&quot;)
                        self.write_queue.put(output_value)
                if image_pixel_values is not None:
                    qlen = len(image_pixel_values)
                else:
                    qlen = self.vae_input_queue.qsize()
        except Exception as e:
            logger.error(f&quot;Error encoding images {vae_input_filepaths}: {e}&quot;)
            if &quot;out of memory&quot; in str(e).lower():
                import sys
                sys.exit(1)
            # Remove all of the errored images from the bucket. They will be captured on restart.
            for filepath in vae_input_filepaths:
                self.metadata_backend.remove_image(filepath)
            self.debug_log(f&quot;Error traceback: {traceback.format_exc()}&quot;)
            raise Exception(
                f&quot;Error encoding images {vae_input_filepaths}: {e}, traceback: {traceback.format_exc()}&quot;
            )
        return output_values
    def _read_from_storage_concurrently(self, paths, hide_errors: bool = False):
        &quot;&quot;&quot;
        A helper method to read files from storage concurrently, without Queues.
        Args:
            paths (List[str]): A list of file paths to read.
        Returns:
            Generator[Tuple[str, Any], None, None]: Yields file path and contents.
        &quot;&quot;&quot;
        def read_file(path):
            try:
                return path, self._read_from_storage(path, hide_errors=hide_errors)
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error reading {path}: {e}, traceback: {traceback.format_exc()}&quot;
                )
                # If --delete_problematic_images is supplied, we remove the image now:
                if self.delete_problematic_images:
                    self.metadata_backend.remove_image(path)
                    self.image_data_backend.delete(path)
                return path, None
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Map read_file operation over all paths
            future_to_path = {executor.submit(read_file, path): path for path in paths}
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    yield future.result()
                except Exception as exc:
                    logger.error(f&quot;{path} generated an exception: {exc}&quot;)
    def read_images_in_batch(self) -&gt; None:
        &quot;&quot;&quot;Immediately read a batch of images.
        The images are added to a Queue, for later processing.
        Args:
            filepaths (list): A list of image file paths.
        Returns:
            None
        &quot;&quot;&quot;
        filepaths = []
        qlen = self.read_queue.qsize()
        for idx in range(0, qlen):
            read_queue_item = self.read_queue.get()
            path, aspect_bucket = read_queue_item
            filepaths.append(path)
        available_filepaths, batch_output = self.image_data_backend.read_image_batch(
            filepaths, delete_problematic_images=self.delete_problematic_images
        )
        missing_image_count = len(filepaths) - len(available_filepaths)
        if len(available_filepaths) != len(filepaths):
            logging.warning(
                f&quot;Failed to request {missing_image_count} sample{&apos;s&apos; if missing_image_count &gt; 1 else &apos;&apos;} during batched read, out of {len(filepaths)} total samples requested.&quot;
                &quot; These samples likely do not exist in the storage pool any longer.&quot;
            )
        for filepath, element in zip(available_filepaths, batch_output):
            if type(filepath) != str:
                raise ValueError(
                    f&quot;Received unknown filepath type ({type(filepath)}) value: {filepath}&quot;
                )
            # Add the element to the queue for later processing.
            # This allows us to have separate read and processing queue size limits.
            self.process_queue.put((filepath, element, aspect_bucket))
    def _process_raw_filepath(self, raw_filepath: str):
        if type(raw_filepath) == str or len(raw_filepath) == 1:
            filepath = raw_filepath
        elif len(raw_filepath) == 2:
            basename, filepath = raw_filepath
        elif type(raw_filepath) == Path or type(raw_filepath) == numpy_str:
            filepath = str(raw_filepath)
        else:
            raise ValueError(
                f&quot;Received unknown filepath type ({type(raw_filepath)}) value: {raw_filepath}&quot;
            )
        return filepath
    def _accumulate_read_queue(self, filepath, aspect_bucket):
        self.read_queue.put((filepath, aspect_bucket))
    def _process_futures(self, futures: list, executor: ThreadPoolExecutor):
        completed_futures = []
        for future in as_completed(futures):
            try:
                future.result()
                completed_futures.append(future)
            except Exception as e:
                logging.error(
                    f&quot;An error occurred in a future: {e}, file {e.__traceback__.tb_frame}, {e.__traceback__.tb_lineno}, future traceback {traceback.format_exc()}&quot;
                )
                completed_futures.append(future)
        return [f for f in futures if f not in completed_futures]
    def process_buckets(self):
        futures = []
        processed_images = self._list_cached_images()
        aspect_bucket_cache = self.metadata_backend.read_cache().copy()
        # Extract and shuffle the keys of the dictionary
        do_shuffle = (
            os.environ.get(&quot;SIMPLETUNER_SHUFFLE_ASPECTS&quot;, &quot;true&quot;).lower() == &quot;true&quot;
        )
        if do_shuffle:
            shuffled_keys = list(aspect_bucket_cache.keys())
            shuffle(shuffled_keys)
        if self.webhook_handler is not None:
            total_count = len(
                [item for sublist in aspect_bucket_cache.values() for item in sublist]
            )
            self.send_progress_update(
                type=&quot;init_cache_vae_processing_started&quot;,
                progress=int(len(processed_images) / total_count * 100),
                total=total_count,
                current=len(processed_images),
            )
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for bucket in shuffled_keys:
                relevant_files = self._reduce_bucket(
                    bucket, aspect_bucket_cache, processed_images, do_shuffle
                )
                if len(relevant_files) == 0:
                    continue
                statistics = {
                    &quot;not_local&quot;: 0,
                    &quot;already_cached&quot;: 0,
                    &quot;cached&quot;: 0,
                    &quot;total&quot;: 0,
                }
                last_reported_index = 0
                for raw_filepath in tqdm(
                    relevant_files,
                    desc=f&quot;Processing bucket {bucket}&quot;,
                    position=get_rank(),
                    ncols=125,
                    leave=False,
                ):
                    statistics[&quot;total&quot;] += 1
                    filepath = self._process_raw_filepath(raw_filepath)
                    test_filepath = self._image_filename_from_vaecache_filename(
                        filepath
                    )
                    if test_filepath is None:
                        continue
                    if test_filepath not in self.local_unprocessed_files:
                        statistics[&quot;not_local&quot;] += 1
                        continue
                    try:
                        # Convert whatever we have, into the VAE cache basename.
                        filepath = self._process_raw_filepath(raw_filepath)
                        # Does it exist on the backend?
                        if self.already_cached(filepath):
                            statistics[&quot;already_cached&quot;] += 1
                            continue
                        # It does not exist. We can add it to the read queue.
                        self._accumulate_read_queue(filepath, aspect_bucket=bucket)
                        # We will check to see whether the queue is ready.
                        if self.read_queue.qsize() &gt;= self.read_batch_size:
                            # We have an adequate number of samples to read. Let&apos;s now do that in a batch, to reduce I/O wait.
                            future_to_read = executor.submit(self.read_images_in_batch)
                            futures.append(future_to_read)
                        # Now we try and process the images, if we have a process batch size large enough.
                        if self.process_queue.qsize() &gt;= self.process_queue_size:
                            future_to_process = executor.submit(
                                self._process_images_in_batch
                            )
                            futures.append(future_to_process)
                        # Now we encode the images.
                        if self.vae_input_queue.qsize() &gt;= self.vae_batch_size:
                            statistics[&quot;cached&quot;] += 1
                            future_to_process = executor.submit(
                                self._encode_images_in_batch
                            )
                            futures.append(future_to_process)
                            if (
                                self.webhook_handler is not None
                                and int(
                                    statistics[&quot;total&quot;]
                                    // self.webhook_progress_interval
                                )
                                &gt; last_reported_index
                            ):
                                last_reported_index = (
                                    statistics[&quot;total&quot;]
                                    // self.webhook_progress_interval
                                )
                                self.send_progress_update(
                                    type=&quot;vaecache&quot;,
                                    progress=int(
                                        statistics[&quot;total&quot;] / len(relevant_files) * 100
                                    ),
                                    total=len(relevant_files),
                                    current=statistics[&quot;total&quot;],
                                )
                        # If we have accumulated enough write objects, we can write them to disk at once.
                        if self.write_queue.qsize() &gt;= self.write_batch_size:
                            future_to_write = executor.submit(
                                self._write_latents_in_batch
                            )
                            futures.append(future_to_write)
                    except ValueError as e:
                        logger.error(f&quot;Received fatal error: {e}&quot;)
                        raise e
                    except Exception as e:
                        logger.error(f&quot;Error processing image {filepath}: {e}&quot;)
                        self.debug_log(f&quot;Error traceback: {traceback.format_exc()}&quot;)
                        raise e
                    # Now, see if we have any futures to complete, and execute them.
                    # Cleanly removes futures from the list, once they are completed.
                    futures = self._process_futures(futures, executor)
                try:
                    # Handle remainders after processing the bucket
                    if self.read_queue.qsize() &gt; 0:
                        # We have an adequate number of samples to read. Let&apos;s now do that in a batch, to reduce I/O wait.
                        future_to_read = executor.submit(self.read_images_in_batch)
                        futures.append(future_to_read)
                    futures = self._process_futures(futures, executor)
                    # Now we try and process the images, if we have a process batch size large enough.
                    if self.process_queue.qsize() &gt; 0:
                        future_to_process = executor.submit(
                            self._process_images_in_batch
                        )
                        futures.append(future_to_process)
                    futures = self._process_futures(futures, executor)
                    if self.vae_input_queue.qsize() &gt; 0:
                        future_to_process = executor.submit(
                            self._encode_images_in_batch
                        )
                        futures.append(future_to_process)
                    futures = self._process_futures(futures, executor)
                    # Write the remaining batches. This is not strictly necessary, since they do not need to be written with matching dimensions.
                    # However, it&apos;s simply easiest to do this now, even if we have less-than a single batch size.
                    if self.write_queue.qsize() &gt; 0:
                        future_to_write = executor.submit(self._write_latents_in_batch)
                        futures.append(future_to_write)
                    futures = self._process_futures(futures, executor)
                    log_msg = (
                        f&quot;(id={self.id}) Bucket {bucket} caching results: {statistics}&quot;
                    )
                    if get_rank() == 0:
                        logger.debug(log_msg)
                        tqdm.write(log_msg)
                    if self.webhook_handler is not None:
                        self.send_progress_update(
                            type=&quot;init_cache_vae_processing_complete&quot;,
                            progress=100,
                            total=statistics[&quot;total&quot;],
                            current=statistics[&quot;total&quot;],
                        )
                    self.debug_log(
                        &quot;Completed process_buckets, all futures have been returned.&quot;
                    )
                except Exception as e:
                    logger.error(f&quot;Fatal error when processing bucket {bucket}: {e}&quot;)
                    continue
    def scan_cache_contents(self):
        &quot;&quot;&quot;
        A generator method that iterates over the VAE cache, yielding each cache file&apos;s path and its contents
        using multi-threading for improved performance.
        Yields:
            Tuple[str, Any]: A tuple containing the file path and its contents.
        &quot;&quot;&quot;
        try:
            all_cache_files = StateTracker.get_vae_cache_files(data_backend_id=self.id)
            try:
                yield from self._read_from_storage_concurrently(
                    all_cache_files, hide_errors=True
                )
            except FileNotFoundError:
                yield (None, None)
        except Exception as e:
            if &quot;is not iterable&quot; not in str(e):
                logger.error(f&quot;Error in scan_cache_contents: {e}&quot;)
                self.debug_log(f&quot;Error traceback: {traceback.format_exc()}&quot;)</file><file path="helpers/configuration/cmd_args.py">from datetime import timedelta
from accelerate.utils import ProjectConfiguration
from accelerate import InitProcessGroupKwargs
import argparse
import os
from typing import Dict, List, Optional, Tuple
import random
import time
import json
import logging
import sys
import torch
from helpers.models.smoldit import SmolDiTConfigurationNames
from helpers.training import quantised_precision_levels
from helpers.training.optimizer_param import (
    is_optimizer_deprecated,
    is_optimizer_grad_fp32,
    map_deprecated_optimizer_parameter,
    optimizer_choices,
)
logger = logging.getLogger(&quot;ArgsParser&quot;)
# Are we the primary process?
is_primary_process = True
if os.environ.get(&quot;RANK&quot;) is not None:
    if int(os.environ.get(&quot;RANK&quot;)) != 0:
        is_primary_process = False
logger.setLevel(
    os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot; if is_primary_process else &quot;ERROR&quot;)
)
if torch.cuda.is_available():
    os.environ[&quot;NCCL_SOCKET_NTIMEO&quot;] = &quot;2000000&quot;
def print_on_main_thread(message):
    if is_primary_process:
        print(message)
def info_log(message):
    if is_primary_process:
        logger.info(message)
def warning_log(message):
    if is_primary_process:
        logger.warning(message)
def error_log(message):
    if is_primary_process:
        logger.error(message)
def get_argument_parser():
    parser = argparse.ArgumentParser(
        description=&quot;The following SimpleTuner command-line options are available:&quot;,
        exit_on_error=False,
    )
    parser.add_argument(
        &quot;--snr_gamma&quot;,
        type=float,
        default=None,
        help=(
            &quot;SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0.&quot;
            &quot; More details here: https://arxiv.org/abs/2303.09556.&quot;
        ),
    )
    parser.add_argument(
        &quot;--use_soft_min_snr&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, will use the soft min SNR calculation method. This method uses the sigma_data parameter.&quot;
            &quot; If not provided, the method will raise an error.&quot;
        ),
    )
    parser.add_argument(
        &quot;--soft_min_snr_sigma_data&quot;,
        default=None,
        type=float,
        help=(
            &quot;The standard deviation of the data used in the soft min weighting method.&quot;
            &quot; This is required when using the soft min SNR calculation method.&quot;
        ),
    )
    parser.add_argument(
        &quot;--model_family&quot;,
        choices=[
            &quot;pixart_sigma&quot;,
            &quot;sana&quot;,
            &quot;kolors&quot;,
            &quot;sd3&quot;,
            &quot;flux&quot;,
            &quot;smoldit&quot;,
            &quot;sdxl&quot;,
            &quot;ltxvideo&quot;,
            &quot;legacy&quot;,
        ],
        default=None,
        required=True,
        help=(&quot;The model family to train. This option is required.&quot;),
    )
    parser.add_argument(
        &quot;--model_type&quot;,
        type=str,
        choices=[
            &quot;full&quot;,
            &quot;lora&quot;,
            &quot;deepfloyd-full&quot;,
            &quot;deepfloyd-lora&quot;,
            &quot;deepfloyd-stage2&quot;,
            &quot;deepfloyd-stage2-lora&quot;,
        ],
        default=&quot;full&quot;,
        help=(
            &quot;The training type to use. &apos;full&apos; will train the full model, while &apos;lora&apos; will train the LoRA model.&quot;
            &quot; LoRA is a smaller model that can be used for faster training.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_lora_target&quot;,
        type=str,
        choices=[
            &quot;mmdit&quot;,
            &quot;context&quot;,
            &quot;context+ffs&quot;,
            &quot;all&quot;,
            &quot;all+ffs&quot;,
            &quot;ai-toolkit&quot;,
            &quot;tiny&quot;,
            &quot;nano&quot;,
        ],
        default=&quot;all&quot;,
        help=(
            &quot;This option only applies to Standard LoRA, not Lycoris. Flux has single and joint attention blocks.&quot;
            &quot; By default, all attention layers are trained, but not the feed-forward layers&quot;
            &quot; If &apos;mmdit&apos; is provided, the text input layers will not be trained.&quot;
            &quot; If &apos;context&apos; is provided, then ONLY the text attention layers are trained&quot;
            &quot; If &apos;context+ffs&apos; is provided, then text attention and text feed-forward layers are trained. This is somewhat similar to text-encoder-only training in earlier SD versions.&quot;
            &quot; If &apos;all&apos; is provided, all layers will be trained, minus feed-forward.&quot;
            &quot; If &apos;all+ffs&apos; is provided, all layers will be trained including feed-forward.&quot;
            &quot; If &apos;ai-toolkit&apos; is provided, all layers will be trained including feed-forward and norms (based on ostris/ai-toolkit).&quot;
            &quot; If &apos;tiny&apos; is provided, only two layers will be trained.&quot;
            &quot; If &apos;nano&apos; is provided, only one layers will be trained.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flow_matching_sigmoid_scale&quot;,
        type=float,
        default=None,
        help=&quot;Deprecated option. Replaced with --flow_sigmoid_scale.&quot;,
    )
    parser.add_argument(
        &quot;--flow_sigmoid_scale&quot;,
        type=float,
        default=1.0,
        help=&quot;Scale factor for sigmoid timestep sampling for flow-matching models.&quot;,
    )
    parser.add_argument(
        &quot;--flux_fast_schedule&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;An experimental feature to train Flux.1S using a noise schedule closer to what it was trained with,&quot;
            &quot; which has improved results in short experiments. Thanks to @mhirki for the contribution.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_use_uniform_schedule&quot;,
        default=None,
        action=&quot;store_true&quot;,
        help=&quot;Deprecated option. Replaced with --flow_use_uniform_schedule.&quot;,
    )
    parser.add_argument(
        &quot;--flow_use_uniform_schedule&quot;,
        default=False,
        action=&quot;store_true&quot;,
        help=(
            &quot;Whether or not to use a uniform schedule instead of sigmoid for flow-matching noise schedule.&quot;
            &quot; Using uniform sampling may cause a bias toward dark images, and should be used with caution.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_use_beta_schedule&quot;,
        action=&quot;store_true&quot;,
        default=None,
        help=&quot;Deprecated option. Replaced with --flow_use_beta_schedule.&quot;,
    )
    parser.add_argument(
        &quot;--flow_use_beta_schedule&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Whether or not to use a beta schedule instead of sigmoid for flow-matching.&quot;
            &quot; The default values of alpha and beta approximate a sigmoid.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_beta_schedule_alpha&quot;,
        type=float,
        default=None,
        help=(&quot;Deprecated option. Replaced with --flux_beta_schedule_alpha.&quot;),
    )
    parser.add_argument(
        &quot;--flow_beta_schedule_alpha&quot;,
        type=float,
        default=2.0,
        help=(&quot;The alpha value of the flow-matching beta schedule. Default is 2.0&quot;),
    )
    parser.add_argument(
        &quot;--flux_beta_schedule_beta&quot;,
        type=float,
        default=None,
        help=(&quot;Deprecated option. Replaced with --flow_beta_schedule_beta.&quot;),
    )
    parser.add_argument(
        &quot;--flow_beta_schedule_beta&quot;,
        type=float,
        default=2.0,
        help=(&quot;The beta value of the flow-matching beta schedule. Default is 2.0&quot;),
    )
    parser.add_argument(
        &quot;--flux_schedule_shift&quot;,
        type=float,
        default=None,
        help=(&quot;Deprecated option. Replaced with --flow_schedule_shift.&quot;),
    )
    parser.add_argument(
        &quot;--flow_schedule_shift&quot;,
        type=float,
        default=3,
        help=(
            &quot;Shift the noise schedule. This is a value between 0 and ~4.0, where 0 disables the timestep-dependent shift,&quot;
            &quot; and anything greater than 0 will shift the timestep sampling accordingly. Sana and SD3 were trained with&quot;
            &quot; a shift value of 3. This value can change how contrast/brightness are learnt by the model, and whether fine&quot;
            &quot; details are ignored or accentuated. A higher value will focus more on large compositional features,&quot;
            &quot; and a lower value will focus on the high frequency fine details.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_schedule_auto_shift&quot;,
        action=&quot;store_true&quot;,
        default=None,
        help=&quot;Deprecated option. Replaced with --flow_schedule_auto_shift.&quot;,
    )
    parser.add_argument(
        &quot;--flow_schedule_auto_shift&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Shift the noise schedule depending on image resolution. The shift value calculation is taken from the official&quot;
            &quot; Flux inference code. Shift value is math.exp(1.15) = 3.1581 for a pixel count of 1024px * 1024px. The shift&quot;
            &quot; value grows exponentially with higher pixel counts. It is a good idea to train on a mix of different resolutions&quot;
            &quot; when this option is enabled. You may need to lower your learning rate with this enabled.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_guidance_mode&quot;,
        type=str,
        choices=[&quot;constant&quot;, &quot;random-range&quot;],
        default=&quot;constant&quot;,
        help=(
            &quot;Flux has a &apos;guidance&apos; value used during training time that reflects the CFG range of your training samples.&quot;
            &quot; The default mode &apos;constant&apos; will use a single value for every sample.&quot;
            &quot; The mode &apos;random-range&apos; will randomly select a value from the range of the CFG for each sample.&quot;
            &quot; Set the range using --flux_guidance_min and --flux_guidance_max.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_guidance_value&quot;,
        type=float,
        default=1.0,
        help=(
            &quot;When using --flux_guidance_mode=constant, this value will be used for every input sample.&quot;
            &quot; Using a value of 1.0 seems to preserve the CFG distillation for the Dev model,&quot;
            &quot; and using any other value will result in the resulting LoRA requiring CFG at inference time.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flux_guidance_min&quot;,
        type=float,
        default=0.0,
    )
    parser.add_argument(
        &quot;--flux_guidance_max&quot;,
        type=float,
        default=4.0,
    )
    parser.add_argument(
        &quot;--flux_attention_masked_training&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Use attention masking while training flux. This can be a destructive operation,&quot;
            &quot; unless finetuning a model which was already trained with it.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ltx_train_mode&quot;,
        choices=[&quot;t2v&quot;, &quot;i2v&quot;],
        default=&quot;i2v&quot;,
        help=(
            &quot;This value will be the default for all video datasets that do not have their own i2v settings defined.&quot;
            &quot; By default, we enable i2v mode, but it can be switched to t2v for your convenience.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ltx_i2v_prob&quot;,
        type=float,
        default=0.1,
        help=(
            &quot;Probability in [0,1] of applying i2v (image-to-video) style training. &quot;
            &quot;If random.random() &lt; i2v_prob during training, partial or complete first-frame protection &quot;
            &quot;will be triggered (depending on --ltx_protect_first_frame). &quot;
            &quot;If set to 0.0, no i2v logic is applied (pure t2v). Default: 0.1 (from finetuners project)&quot;
        ),
    )
    parser.add_argument(
        &quot;--ltx_protect_first_frame&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If specified, fully protect the first frame whenever i2v logic is triggered (see --ltx_i2v_prob). &quot;
            &quot;This means the first frame is never noised or denoised, effectively pinned to the original content.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ltx_partial_noise_fraction&quot;,
        type=float,
        default=0.05,
        help=(
            &quot;Maximum fraction of noise to introduce into the first frame when i2v is triggered and &quot;
            &quot;the first frame is not fully protected. For instance, a value of 0.05 means the first frame &quot;
            &quot;can have up to 5 percent random noise mixed in, preserving 95 percent of the original content. &quot;
            &quot;Ignored if --ltx_protect_first_frame is set.&quot;
        ),
    )
    parser.add_argument(
        &quot;--t5_padding&quot;,
        choices=[&quot;zero&quot;, &quot;unmodified&quot;],
        default=&quot;unmodified&quot;,
        help=(
            &quot;The padding behaviour for Flux and SD3. &apos;zero&apos; will pad the input with zeros.&quot;
            &quot; The default is &apos;unmodified&apos;, which will not pad the input.&quot;
        ),
    )
    parser.add_argument(
        &quot;--smoldit&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(&quot;Use the experimental SmolDiT model architecture.&quot;),
    )
    parser.add_argument(
        &quot;--smoldit_config&quot;,
        type=str,
        choices=SmolDiTConfigurationNames,
        default=&quot;smoldit-base&quot;,
        help=(
            &quot;The SmolDiT configuration to use. This is a list of pre-configured models.&quot;
            &quot; The default is &apos;smoldit-base&apos;.&quot;
        ),
    )
    parser.add_argument(
        &quot;--flow_matching_loss&quot;,
        type=str,
        choices=[&quot;diffusers&quot;, &quot;compatible&quot;, &quot;diffusion&quot;, &quot;sd35&quot;],
        default=&quot;compatible&quot;,
        help=(
            &quot;A discrepancy exists between the Diffusers implementation of flow matching and the minimal implementation provided&quot;
            &quot; by StabilityAI. This experimental option allows switching loss calculations to be compatible with those.&quot;
            &quot; Additionally, &apos;diffusion&apos; is offered as an option to reparameterise a model to v_prediction loss.&quot;
            &quot; sd35 provides the ability to train on SD3.5&apos;s flow-matching target, which is the denoised sample.&quot;
        ),
    )
    parser.add_argument(
        &quot;--sd3_clip_uncond_behaviour&quot;,
        type=str,
        choices=[&quot;empty_string&quot;, &quot;zero&quot;],
        default=&quot;empty_string&quot;,
        help=(
            &quot;SD3 can be trained using zeroed prompt embeds during unconditional dropout,&quot;
            &quot; or an encoded empty string may be used instead (the default). Changing this value may stabilise or&quot;
            &quot; destabilise training. The default is &apos;empty_string&apos;.&quot;
        ),
    )
    parser.add_argument(
        &quot;--sd3_t5_uncond_behaviour&quot;,
        type=str,
        choices=[&quot;empty_string&quot;, &quot;zero&quot;],
        default=None,
        help=(
            &quot;Override the value of unconditional prompts from T5 embeds.&quot;
            &quot; The default is to follow the value of --sd3_clip_uncond_behaviour.&quot;
        ),
    )
    parser.add_argument(
        &quot;--lora_type&quot;,
        type=str.lower,
        choices=[&quot;standard&quot;, &quot;lycoris&quot;],
        default=&quot;standard&quot;,
        help=(
            &quot;When training using --model_type=lora, you may specify a different type of LoRA to train here.&quot;
            &quot; standard refers to training a vanilla LoRA via PEFT, lycoris refers to training with KohakuBlueleaf&apos;s library of the same name.&quot;
        ),
    )
    parser.add_argument(
        &quot;--lora_init_type&quot;,
        type=str,
        choices=[&quot;default&quot;, &quot;gaussian&quot;, &quot;loftq&quot;, &quot;olora&quot;, &quot;pissa&quot;],
        default=&quot;default&quot;,
        help=(
            &quot;The initialization type for the LoRA model. &apos;default&apos; will use Microsoft&apos;s initialization method,&quot;
            &quot; &apos;gaussian&apos; will use a Gaussian scaled distribution, and &apos;loftq&apos; will use LoftQ initialization.&quot;
            &quot; In short experiments, &apos;default&apos; produced accurate results earlier in training, &apos;gaussian&apos; had slightly more&quot;
            &quot; creative outputs, and LoftQ produces an entirely different result with worse quality at first, taking&quot;
            &quot; potentially longer to converge than the other methods.&quot;
        ),
    )
    parser.add_argument(
        &quot;--init_lora&quot;,
        type=str,
        default=None,
        help=&quot;Specify an existing LoRA or LyCORIS safetensors file to initialize the adapter and continue training, if a full checkpoint is not available.&quot;,
    )
    parser.add_argument(
        &quot;--lora_rank&quot;,
        type=int,
        default=16,
        help=(&quot;The dimension of the LoRA update matrices.&quot;),
    )
    parser.add_argument(
        &quot;--lora_alpha&quot;,
        type=float,
        required=False,
        default=None,
        help=(
            &quot;The alpha value for the LoRA model. This is the learning rate for the LoRA update matrices.&quot;
        ),
    )
    parser.add_argument(
        &quot;--lora_dropout&quot;,
        type=float,
        default=0.1,
        help=(
            &quot;LoRA dropout randomly ignores neurons during training. This can help prevent overfitting.&quot;
        ),
    )
    parser.add_argument(
        &quot;--lycoris_config&quot;,
        type=str,
        default=&quot;config/lycoris_config.json&quot;,
        help=(&quot;The location for the JSON file of the Lycoris configuration.&quot;),
    )
    parser.add_argument(
        &quot;--init_lokr_norm&quot;,
        type=float,
        required=False,
        default=None,
        help=(
            &quot;Setting this turns on perturbed normal initialization of the LyCORIS LoKr PEFT layers. A good value is between 1e-4 and 1e-2.&quot;
        ),
    )
    parser.add_argument(
        &quot;--controlnet&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, ControlNet style training will be used, where a conditioning input image is required alongside the training data.&quot;
        ),
    )
    parser.add_argument(
        &quot;--controlnet_model_name_or_path&quot;,
        action=&quot;store_true&quot;,
        default=None,
        help=(
            &quot;When provided alongside --controlnet, this will specify ControlNet model weights to preload from the hub.&quot;
        ),
    )
    parser.add_argument(
        &quot;--pretrained_model_name_or_path&quot;,
        type=str,
        default=None,
        required=True,
        help=(
            &quot;Path to pretrained model or model identifier from huggingface.co/models.&quot;
            &quot; Some model architectures support loading single-file .safetensors directly.&quot;
            &quot; Note that when using single-file safetensors, the tokeniser and noise schedule configs&quot;
            &quot; will be used from the vanilla upstream Huggingface repository, which requires&quot;
            &quot; network access. If you are training on a machine without network access, you should&quot;
            &quot; pre-download the entire Huggingface model repository instead of using single-file loader.&quot;
        ),
    )
    parser.add_argument(
        &quot;--pretrained_transformer_model_name_or_path&quot;,
        type=str,
        default=None,
        help=&quot;Path to pretrained transformer model or model identifier from huggingface.co/models.&quot;,
    )
    parser.add_argument(
        &quot;--pretrained_transformer_subfolder&quot;,
        type=str,
        default=&quot;transformer&quot;,
        help=&quot;The subfolder to load the transformer model from. Use &apos;none&apos; for a flat directory.&quot;,
    )
    parser.add_argument(
        &quot;--pretrained_unet_model_name_or_path&quot;,
        type=str,
        default=None,
        help=&quot;Path to pretrained unet model or model identifier from huggingface.co/models.&quot;,
    )
    parser.add_argument(
        &quot;--pretrained_unet_subfolder&quot;,
        type=str,
        default=&quot;unet&quot;,
        help=&quot;The subfolder to load the unet model from. Use &apos;none&apos; for a flat directory.&quot;,
    )
    parser.add_argument(
        &quot;--pretrained_vae_model_name_or_path&quot;,
        type=str,
        default=&quot;madebyollin/sdxl-vae-fp16-fix&quot;,
        help=&quot;Path to an improved VAE to stabilize training. For more details check out: https://github.com/huggingface/diffusers/pull/4038.&quot;,
    )
    parser.add_argument(
        &quot;--pretrained_t5_model_name_or_path&quot;,
        type=str,
        default=None,
        help=(
            &quot;T5-XXL is a huge model, and starting from many different models will download a separate one each time.&quot;
            &quot; This option allows you to specify a specific location to retrieve T5-XXL v1.1 from, so that it only downloads once..&quot;
        ),
    )
    parser.add_argument(
        &quot;--prediction_type&quot;,
        type=str,
        default=&quot;epsilon&quot;,
        choices=[&quot;epsilon&quot;, &quot;v_prediction&quot;, &quot;sample&quot;],
        help=(
            &quot;The type of prediction to use for the u-net. Choose between [&apos;epsilon&apos;, &apos;v_prediction&apos;, &apos;sample&apos;].&quot;
            &quot; For SD 2.1-v, this is v_prediction. For 2.1-base, it is epsilon. SDXL is generally epsilon.&quot;
            &quot; SD 1.5 is epsilon.&quot;
        ),
    )
    parser.add_argument(
        &quot;--snr_weight&quot;,
        type=float,
        default=1.0,
        help=(
            &quot;When training a model using `--prediction_type=sample`, one can supply an SNR weight value to augment the loss with.&quot;
            &quot; If a value of 0.5 is provided here, the loss is taken half from the SNR and half from the MSE.&quot;
        ),
    )
    parser.add_argument(
        &quot;--training_scheduler_timestep_spacing&quot;,
        type=str,
        default=&quot;trailing&quot;,
        choices=[&quot;leading&quot;, &quot;linspace&quot;, &quot;trailing&quot;],
        help=(
            &quot;(SDXL Only) Spacing timesteps can fundamentally alter the course of history. Er, I mean, your model weights.&quot;
            &quot; For all training, including epsilon, it would seem that &apos;trailing&apos; is the right choice. SD 2.x always uses &apos;trailing&apos;,&quot;
            &quot; but SDXL may do better in its default state when using &apos;leading&apos;.&quot;
        ),
    )
    parser.add_argument(
        &quot;--inference_scheduler_timestep_spacing&quot;,
        type=str,
        default=&quot;trailing&quot;,
        choices=[&quot;leading&quot;, &quot;linspace&quot;, &quot;trailing&quot;],
        help=(
            &quot;(SDXL Only) The Bytedance paper on zero terminal SNR recommends inference using &apos;trailing&apos;. SD 2.x always uses &apos;trailing&apos;,&quot;
            &quot; but SDXL may do better in its default state when using &apos;leading&apos;.&quot;
        ),
    )
    parser.add_argument(
        &quot;--refiner_training&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When training or adapting a model into a mixture-of-experts 2nd stage / refiner model, this option should be set.&quot;
            &quot; This will slice the timestep schedule defined by --refiner_training_strength proportion value (default 0.2)&quot;
        ),
    )
    parser.add_argument(
        &quot;--refiner_training_invert_schedule&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;While the refiner training strength is applied to the end of the schedule, this option will invert the result&quot;
            &quot; for training a **base** model, eg. the first model in a mixture-of-experts series.&quot;
            &quot; A --refiner_training_strength of 0.35 will result in the refiner learning timesteps 349-0.&quot;
            &quot; Setting --refiner_training_invert_schedule then would result in the base model learning timesteps 999-350.&quot;
        ),
    )
    parser.add_argument(
        &quot;--refiner_training_strength&quot;,
        default=0.2,
        type=float,
        help=(
            &quot;When training a refiner / 2nd stage mixture of experts model, the refiner training strength&quot;
            &quot; indicates how much of the *end* of the schedule it will be trained on. A value of 0.2 means&quot;
            &quot; timesteps 199-0 will be the focus of this model, and 0.3 would be 299-0 and so on.&quot;
            &quot; The default value is 0.2, in line with the SDXL refiner pretraining.&quot;
        ),
    )
    parser.add_argument(
        &quot;--timestep_bias_strategy&quot;,
        type=str,
        default=&quot;none&quot;,
        choices=[&quot;earlier&quot;, &quot;later&quot;, &quot;range&quot;, &quot;none&quot;],
        help=(
            &quot;The timestep bias strategy, which may help direct the model toward learning low or frequency details.&quot;
            &quot; Choices: [&apos;earlier&apos;, &apos;later&apos;, &apos;none&apos;].&quot;
            &quot; The default is &apos;none&apos;, which means no bias is applied, and training proceeds normally.&quot;
            &quot; The value of &apos;later&apos; will prefer to generate samples for later timesteps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--timestep_bias_multiplier&quot;,
        type=float,
        default=1.0,
        help=(
            &quot;The multiplier for the bias. Defaults to 1.0, which means no bias is applied.&quot;
            &quot; A value of 2.0 will double the weight of the bias, and a value of 0.5 will halve it.&quot;
        ),
    )
    parser.add_argument(
        &quot;--timestep_bias_begin&quot;,
        type=int,
        default=0,
        help=(
            &quot;When using `--timestep_bias_strategy=range`, the beginning timestep to bias.&quot;
            &quot; Defaults to zero, which equates to having no specific bias.&quot;
        ),
    )
    parser.add_argument(
        &quot;--timestep_bias_end&quot;,
        type=int,
        default=1000,
        help=(
            &quot;When using `--timestep_bias_strategy=range`, the final timestep to bias.&quot;
            &quot; Defaults to 1000, which is the number of timesteps that SDXL Base and SD 2.x were trained on.&quot;
        ),
    )
    parser.add_argument(
        &quot;--timestep_bias_portion&quot;,
        type=float,
        default=0.25,
        help=(
            &quot;The portion of timesteps to bias. Defaults to 0.25, which 25 percent of timesteps will be biased.&quot;
            &quot; A value of 0.5 will bias one half of the timesteps. The value provided for `--timestep_bias_strategy` determines&quot;
            &quot; whether the biased portions are in the earlier or later timesteps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_segmented_timestep_sampling&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;By default, the timestep schedule is divided into roughly `train_batch_size` number of segments, and then&quot;
            &quot; each of those are sampled from separately. This improves the selection distribution, but may not&quot;
            &quot; be desired in certain training scenarios, eg. when limiting the timestep selection range.&quot;
        ),
    )
    parser.add_argument(
        &quot;--rescale_betas_zero_snr&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, will rescale the betas to zero terminal SNR. This is recommended for training with v_prediction.&quot;
            &quot; For epsilon, this might help with fine details, but will not result in contrast improvements.&quot;
        ),
    )
    parser.add_argument(
        &quot;--vae_dtype&quot;,
        type=str,
        default=&quot;bf16&quot;,
        choices=[&quot;default&quot;, &quot;fp16&quot;, &quot;fp32&quot;, &quot;bf16&quot;],
        required=False,
        help=(
            &quot;The dtype of the VAE model. Choose between [&apos;default&apos;, &apos;fp16&apos;, &apos;fp32&apos;, &apos;bf16&apos;].&quot;
            &quot; The default VAE dtype is bfloat16, due to NaN issues in SDXL 1.0.&quot;
            &quot; Using fp16 is not recommended.&quot;
        ),
    )
    parser.add_argument(
        &quot;--vae_batch_size&quot;,
        type=int,
        default=4,
        help=(
            &quot;When pre-caching latent vectors, this is the batch size to use. Decreasing this may help with VRAM issues,&quot;
            &quot; but if you are at that point of contention, it&apos;s possible that your GPU has too little RAM. Default: 4.&quot;
        ),
    )
    parser.add_argument(
        &quot;--vae_enable_tiling&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, will enable tiling for VAE caching. This is useful for very large images when VRAM is limited.&quot;
            &quot; This may be required for 2048px VAE caching on 24G accelerators, in addition to reducing --vae_batch_size.&quot;
        ),
    )
    parser.add_argument(
        &quot;--vae_cache_scan_behaviour&quot;,
        type=str,
        choices=[&quot;recreate&quot;, &quot;sync&quot;],
        default=&quot;recreate&quot;,
        help=(
            &quot;When a mismatched latent vector is detected, a scan will be initiated to locate inconsistencies and resolve them.&quot;
            &quot; The default setting &apos;recreate&apos; will delete any inconsistent cache entries and rebuild it.&quot;
            &quot; Alternatively, &apos;sync&apos; will update the bucket configuration so that the image is in a bucket that matches its latent size.&quot;
            &quot; The recommended behaviour is to use the default value and allow the cache to be recreated.&quot;
        ),
    )
    parser.add_argument(
        &quot;--vae_cache_ondemand&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;By default, will batch-encode images before training. For some situations, ondemand may be desired, but it greatly slows training and increases memory pressure.&quot;
        ),
    )
    parser.add_argument(
        &quot;--compress_disk_cache&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, will gzip-compress the disk cache for Pytorch files. This will save substantial disk space, but may slow down the training process.&quot;
        ),
    )
    parser.add_argument(
        &quot;--aspect_bucket_disable_rebuild&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When using a randomised aspect bucket list, the VAE and aspect cache are rebuilt on each epoch.&quot;
            &quot; With a large and diverse enough dataset, rebuilding the aspect list may take a long time, and this may be undesirable.&quot;
            &quot; This option will not override vae_cache_clear_each_epoch. If both options are provided, only the VAE cache will be rebuilt.&quot;
        ),
    )
    parser.add_argument(
        &quot;--keep_vae_loaded&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;If set, will keep the VAE loaded in memory. This can reduce disk churn, but consumes VRAM during the forward pass.&quot;,
    )
    parser.add_argument(
        &quot;--skip_file_discovery&quot;,
        type=str,
        default=&quot;&quot;,
        help=(
            &quot;Comma-separated values of which stages to skip discovery for. Skipping any stage will speed up resumption,&quot;
            &quot; but will increase the risk of errors, as missing images or incorrectly bucketed images may not be caught.&quot;
            &quot; &apos;vae&apos; will skip the VAE cache process, &apos;aspect&apos; will not build any aspect buckets, and &apos;text&apos; will avoid text embed management.&quot;
            &quot; Valid options: aspect, vae, text, metadata.&quot;
        ),
    )
    parser.add_argument(
        &quot;--revision&quot;,
        type=str,
        default=None,
        required=False,
        help=(
            &quot;Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be&quot;
            &quot; at least bfloat16 precision.&quot;
        ),
    )
    parser.add_argument(
        &quot;--variant&quot;,
        type=str,
        default=None,
        required=False,
        help=(
            &quot;Variant of pretrained model identifier from huggingface.co/models. Trainable model components should be&quot;
            &quot; at least bfloat16 precision.&quot;
        ),
    )
    parser.add_argument(
        &quot;--preserve_data_backend_cache&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;For very large cloud storage buckets that will never change, enabling this option will prevent the trainer&quot;
            &quot; from scanning it at startup, by preserving the cache files that we generate. Be careful when using this,&quot;
            &quot; as, switching datasets can result in the preserved cache being used, which would be problematic.&quot;
            &quot; Currently, cache is not stored in the dataset itself but rather, locally. This may change in a future release.&quot;
        ),
    )
    parser.add_argument(
        &quot;--use_dora&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, will use the DoRA-enhanced LoRA training. This is an experimental feature, may slow down training,&quot;
            &quot; and is not recommended for general use.&quot;
        ),
    )
    parser.add_argument(
        &quot;--override_dataset_config&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When provided, the dataset&apos;s config will not be checked against the live backend config.&quot;
            &quot; This is useful if you want to simply update the behaviour of an existing dataset,&quot;
            &quot; but the recommendation is to not change the dataset configuration after caching has begun,&quot;
            &quot; as most options cannot be changed without unexpected behaviour later on. Additionally, it prevents&quot;
            &quot; accidentally loading an SDXL configuration on a SD 2.x model and vice versa.&quot;
        ),
    )
    parser.add_argument(
        &quot;--cache_dir_text&quot;,
        type=str,
        default=&quot;cache&quot;,
        help=(
            &quot;This is the path to a local directory that will contain your text embed cache.&quot;
        ),
    )
    parser.add_argument(
        &quot;--cache_dir_vae&quot;,
        type=str,
        default=&quot;&quot;,
        help=(
            &quot;This is the path to a local directory that will contain your VAE outputs.&quot;
            &quot; Unlike the text embed cache, your VAE latents will be stored in the AWS data backend.&quot;
            &quot; Each backend can have its own value, but if that is not provided, this will be the default value.&quot;
        ),
    )
    parser.add_argument(
        &quot;--data_backend_config&quot;,
        type=str,
        default=None,
        help=(
            &quot;The relative or fully-qualified path for your data backend config.&quot;
            &quot; See multidatabackend.json.example for an example.&quot;
        ),
    )
    parser.add_argument(
        &quot;--data_backend_sampling&quot;,
        type=str,
        choices=[&quot;uniform&quot;, &quot;auto-weighting&quot;],
        default=&quot;auto-weighting&quot;,
        help=(
            &quot;When using multiple data backends, the sampling weighting can be set to &apos;uniform&apos; or &apos;auto-weighting&apos;.&quot;
            &quot; The default value is &apos;auto-weighting&apos;, which will automatically adjust the sampling weights based on the&quot;
            &quot; number of images in each backend. &apos;uniform&apos; will sample from each backend equally.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ignore_missing_files&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;This option will disable the check for files that have been deleted or removed from your data directory.&quot;
            &quot; This would allow training on large datasets without keeping the associated images on disk, though it&apos;s&quot;
            &quot; not recommended and is not a supported feature. Use with caution, as it mostly exists for experimentation.&quot;
        ),
    )
    parser.add_argument(
        &quot;--write_batch_size&quot;,
        type=int,
        default=128,
        help=(
            &quot;When using certain storage backends, it is better to batch smaller writes rather than continuous dispatching.&quot;
            &quot; In SimpleTuner, write batching is currently applied during VAE caching, when many small objects are written.&quot;
            &quot; This mostly applies to S3, but some shared server filesystems may benefit as well, eg. Ceph. Default: 64.&quot;
        ),
    )
    parser.add_argument(
        &quot;--read_batch_size&quot;,
        type=int,
        default=25,
        help=(
            &quot;Used by the VAE cache to prefetch image data. This is the number of images to read ahead.&quot;
        ),
    )
    parser.add_argument(
        &quot;--image_processing_batch_size&quot;,
        type=int,
        default=32,
        help=(
            &quot;When resizing and cropping images, we do it in parallel using processes or threads.&quot;
            &quot; This defines how many images will be read into the queue before they are processed.&quot;
        ),
    )
    parser.add_argument(
        &quot;--enable_multiprocessing&quot;,
        default=False,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, will use processes instead of threads during metadata caching operations.&quot;
            &quot; For some systems, multiprocessing may be faster than threading, but will consume a lot more memory.&quot;
            &quot; Use this option with caution, and monitor your system&apos;s memory usage.&quot;
        ),
    )
    parser.add_argument(
        &quot;--max_workers&quot;,
        default=32,
        type=int,
        help=(&quot;How many active threads or processes to run during VAE caching.&quot;),
    )
    parser.add_argument(
        &quot;--aws_max_pool_connections&quot;,
        type=int,
        default=128,
        help=(
            &quot;When using AWS backends, the maximum number of connections to keep open to the S3 bucket at a single time.&quot;
            &quot; This should be greater or equal to the max_workers and aspect bucket worker count values.&quot;
        ),
    )
    parser.add_argument(
        &quot;--torch_num_threads&quot;,
        type=int,
        default=8,
        help=(
            &quot;The number of threads to use for PyTorch operations. This is not the same as the number of workers.&quot;
            &quot; Default: 8.&quot;
        ),
    )
    parser.add_argument(
        &quot;--dataloader_prefetch&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When provided, the dataloader will read-ahead and attempt to retrieve latents, text embeds, and other metadata&quot;
            &quot; ahead of the time when the batch is required, so that it can be immediately available.&quot;
        ),
    )
    parser.add_argument(
        &quot;--dataloader_prefetch_qlen&quot;,
        type=int,
        default=10,
        help=(&quot;Set the number of prefetched batches.&quot;),
    )
    parser.add_argument(
        &quot;--aspect_bucket_worker_count&quot;,
        type=int,
        default=12,
        help=(
            &quot;The number of workers to use for aspect bucketing. This is a CPU-bound task, so the number of workers&quot;
            &quot; should be set to the number of CPU threads available. If you use an I/O bound backend, an even higher&quot;
            &quot; value may make sense. Default: 12.&quot;
        ),
    )
    parser.add_argument(
        &quot;--cache_dir&quot;,
        type=str,
        default=None,
        help=&quot;The directory where the downloaded models and datasets will be stored.&quot;,
    )
    parser.add_argument(
        &quot;--cache_clear_validation_prompts&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When provided, any validation prompt entries in the text embed cache will be recreated.&quot;
            &quot; This is useful if you&apos;ve modified any of the existing prompts, or, disabled/enabled Compel,&quot;
            &quot; via `--disable_compel`&quot;
        ),
    )
    parser.add_argument(
        &quot;--caption_strategy&quot;,
        type=str,
        default=&quot;filename&quot;,
        choices=[&quot;filename&quot;, &quot;textfile&quot;, &quot;instance_prompt&quot;, &quot;parquet&quot;],
        help=(
            &quot;The default captioning strategy, &apos;filename&apos;, will use the filename as the caption, after stripping some characters like underscores.&quot;
            &quot; The &apos;textfile&apos; strategy will use the contents of a text file with the same name as the image.&quot;
            &quot; The &apos;parquet&apos; strategy requires a parquet file with the same name as the image, containing a &apos;caption&apos; column.&quot;
        ),
    )
    parser.add_argument(
        &quot;--parquet_caption_column&quot;,
        type=str,
        default=None,
        help=(
            &quot;When using caption_strategy=parquet, this option will allow you to globally set the default caption field across all datasets&quot;
            &quot; that do not have an override set.&quot;
        ),
    )
    parser.add_argument(
        &quot;--parquet_filename_column&quot;,
        type=str,
        default=None,
        help=(
            &quot;When using caption_strategy=parquet, this option will allow you to globally set the default filename field across all datasets&quot;
            &quot; that do not have an override set.&quot;
        ),
    )
    parser.add_argument(
        &quot;--instance_prompt&quot;,
        type=str,
        default=None,
        required=False,
        help=&quot;This is unused. Filenames will be the captions instead.&quot;,
    )
    parser.add_argument(
        &quot;--output_dir&quot;,
        type=str,
        default=&quot;simpletuner-results&quot;,
        help=&quot;The output directory where the model predictions and checkpoints will be written.&quot;,
    )
    parser.add_argument(
        &quot;--seed&quot;, type=int, default=None, help=&quot;A seed for reproducible training.&quot;
    )
    parser.add_argument(
        &quot;--seed_for_each_device&quot;,
        type=bool,
        default=True,
        help=(
            &quot;By default, a unique seed will be used for each GPU.&quot;
            &quot; This is done deterministically, so that each GPU will receive the same seed across invocations.&quot;
            &quot; If --seed_for_each_device=false is provided, then we will use the same seed across all GPUs,&quot;
            &quot; which will almost certainly result in the over-sampling of inputs on larger datasets.&quot;
        ),
    )
    parser.add_argument(
        &quot;--framerate&quot;,
        default=None,
        help=(
            &quot;By default, SimpleTuner will use a framerate of 25 for training and inference on video models.&quot;
            &quot; You are on your own if you modify this value, but it is provided for your convenience.&quot;
        ),
    )
    parser.add_argument(
        &quot;--resolution&quot;,
        type=float,
        default=1024,
        help=(
            &quot;The resolution for input images, all the images in the train/validation dataset will be resized to this&quot;
            &quot; resolution. If using --resolution_type=area, this float value represents megapixels.&quot;
        ),
    )
    parser.add_argument(
        &quot;--resolution_type&quot;,
        type=str,
        default=&quot;pixel_area&quot;,
        choices=[&quot;pixel&quot;, &quot;area&quot;, &quot;pixel_area&quot;],
        help=(
            &quot;Resizing images maintains aspect ratio. This defines the resizing strategy.&quot;
            &quot; If &apos;pixel&apos;, the images will be resized to the resolution by the shortest pixel edge, if the target size does not match the current size.&quot;
            &quot; If &apos;area&apos;, the images will be resized so the pixel area is this many megapixels. Common rounded values such as `0.5` and `1.0` will be implicitly adjusted to their squared size equivalents.&quot;
            &quot; If &apos;pixel_area&apos;, the pixel value (eg. 1024) will be converted to the proper value for &apos;area&apos;, and then calculate everything the same as &apos;area&apos; would.&quot;
        ),
    )
    parser.add_argument(
        &quot;--aspect_bucket_rounding&quot;,
        type=int,
        default=None,
        choices=range(1, 10),
        help=(
            &quot;The number of decimal places to round the aspect ratio to. This is used to create buckets for aspect ratios.&quot;
            &quot; For higher precision, ensure the image sizes remain compatible. Higher precision levels result in a&quot;
            &quot; greater number of buckets, which may not be a desirable outcome.&quot;
        ),
    )
    parser.add_argument(
        &quot;--aspect_bucket_alignment&quot;,
        type=int,
        choices=[8, 64],
        default=64,
        help=(
            &quot;When training diffusion models, the image sizes generally must align to a 64 pixel interval.&quot;
            &quot; This is an exception when training models like DeepFloyd that use a base resolution of 64 pixels,&quot;
            &quot; as aligning to 64 pixels would result in a 1:1 or 2:1 aspect ratio, overly distorting images.&quot;
            &quot; For DeepFloyd, this value is set to 8, but all other training defaults to 64. You may experiment&quot;
            &quot; with this value, but it is not recommended.&quot;
        ),
    )
    parser.add_argument(
        &quot;--minimum_image_size&quot;,
        type=float,
        default=None,
        help=(
            &quot;The minimum resolution for both sides of input images.&quot;
            &quot; If --delete_unwanted_images is set, images smaller than this will be DELETED.&quot;
            &quot; The default value is None, which means no minimum resolution is enforced.&quot;
            &quot; If this option is not provided, it is possible that images will be destructively upsampled, harming model performance.&quot;
        ),
    )
    parser.add_argument(
        &quot;--maximum_image_size&quot;,
        type=float,
        default=None,
        help=(
            &quot;When cropping images that are excessively large, the entire scene context may be lost, eg. the crop might just&quot;
            &quot; end up being a portion of the background. To avoid this, a maximum image size may be provided, which will&quot;
            &quot; result in very-large images being downsampled before cropping them. This value uses --resolution_type to determine&quot;
            &quot; whether it is a pixel edge or megapixel value.&quot;
        ),
    )
    parser.add_argument(
        &quot;--target_downsample_size&quot;,
        type=float,
        default=None,
        help=(
            &quot;When using --maximum_image_size, very-large images exceeding that value will be downsampled to this target&quot;
            &quot; size before cropping. If --resolution_type=area and --maximum_image_size=4.0, --target_downsample_size=2.0&quot;
            &quot; would result in a 4 megapixel image being resized to 2 megapixel before cropping to 1 megapixel.&quot;
        ),
    )
    parser.add_argument(
        &quot;--train_text_encoder&quot;,
        action=&quot;store_true&quot;,
        help=&quot;(SD 2.x only) Whether to train the text encoder. If set, the text encoder should be float32 precision.&quot;,
    )
    # DeepFloyd
    parser.add_argument(
        &quot;--tokenizer_max_length&quot;,
        type=int,
        default=None,
        required=False,
        help=&quot;The maximum length of the tokenizer. If not set, will default to the tokenizer&apos;s max length.&quot;,
    )
    # End DeepFloyd-specific settings
    parser.add_argument(
        &quot;--train_batch_size&quot;,
        type=int,
        default=4,
        help=&quot;Batch size (per device) for the training dataloader.&quot;,
    )
    parser.add_argument(&quot;--num_train_epochs&quot;, type=int, default=1)
    parser.add_argument(
        &quot;--max_train_steps&quot;,
        type=int,
        default=None,
        help=&quot;Total number of training steps to perform.  If provided, overrides num_train_epochs.&quot;,
    )
    parser.add_argument(
        &quot;--ignore_final_epochs&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When provided, the max epoch counter will not determine the end of the training run.&quot;
            &quot; Instead, it will end when it hits --max_train_steps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--checkpointing_steps&quot;,
        type=int,
        default=500,
        help=(
            &quot;Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training via `--resume_from_checkpoint`. &quot;
            &quot;In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for inference.&quot;
            &quot;Using a checkpoint for inference requires separate loading of the original pipeline and the individual checkpointed model components.&quot;
            &quot;See https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint for step by step&quot;
            &quot;instructions.&quot;
        ),
    )
    parser.add_argument(
        &quot;--checkpoints_total_limit&quot;,
        type=int,
        default=None,
        help=&quot;Max number of checkpoints to store.&quot;,
    )
    parser.add_argument(
        &quot;--resume_from_checkpoint&quot;,
        type=str,
        default=None,
        help=(
            &quot;Whether training should be resumed from a previous checkpoint. Use a path saved by&quot;
            &apos; `--checkpointing_steps`, or `&quot;latest&quot;` to automatically select the last available checkpoint.&apos;
        ),
    )
    parser.add_argument(
        &quot;--gradient_accumulation_steps&quot;,
        type=int,
        default=1,
        help=&quot;Number of updates steps to accumulate before performing a backward/update pass.&quot;,
    )
    parser.add_argument(
        &quot;--gradient_checkpointing&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.&quot;,
    )
    parser.add_argument(
        &quot;--gradient_checkpointing_interval&quot;,
        default=None,
        type=int,
        help=(
            &quot;Some models (Flux, SDXL, SD1.x/2.x, SD3) can have their gradient checkpointing limited to every nth block.&quot;
            &quot; This can speed up training but will use more memory with larger intervals.&quot;
        ),
    )
    parser.add_argument(
        &quot;--learning_rate&quot;,
        type=float,
        default=4e-7,
        help=(
            &quot;Initial learning rate (after the potential warmup period) to use.&quot;
            &quot; When using a cosine or sine schedule, --learning_rate defines the maximum learning rate.&quot;
        ),
    )
    parser.add_argument(
        &quot;--text_encoder_lr&quot;,
        type=float,
        default=None,
        help=&quot;Learning rate for the text encoder. If not provided, the value of --learning_rate will be used.&quot;,
    )
    parser.add_argument(
        &quot;--lr_scale&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.&quot;,
    )
    parser.add_argument(
        &quot;--lr_scheduler&quot;,
        type=str,
        default=&quot;sine&quot;,
        choices=[
            &quot;linear&quot;,
            &quot;sine&quot;,
            &quot;cosine&quot;,
            &quot;cosine_with_restarts&quot;,
            &quot;polynomial&quot;,
            &quot;constant&quot;,
            &quot;constant_with_warmup&quot;,
        ],
        help=(&quot;The scheduler type to use. Default: sine&quot;),
    )
    parser.add_argument(
        &quot;--lr_warmup_steps&quot;,
        type=int,
        default=500,
        help=&quot;Number of steps for the warmup in the lr scheduler.&quot;,
    )
    parser.add_argument(
        &quot;--lr_num_cycles&quot;,
        type=int,
        default=1,
        help=&quot;Number of hard resets of the lr in cosine_with_restarts scheduler.&quot;,
    )
    parser.add_argument(
        &quot;--lr_power&quot;,
        type=float,
        default=0.8,
        help=&quot;Power factor of the polynomial scheduler.&quot;,
    )
    parser.add_argument(
        &quot;--use_ema&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Whether to use EMA (exponential moving average) model. Works with LoRA, Lycoris, and full training.&quot;,
    )
    parser.add_argument(
        &quot;--ema_device&quot;,
        choices=[&quot;cpu&quot;, &quot;accelerator&quot;],
        default=&quot;cpu&quot;,
        help=(
            &quot;The device to use for the EMA model. If set to &apos;accelerator&apos;, the EMA model will be placed on the accelerator.&quot;
            &quot; This provides the fastest EMA update times, but is not ultimately necessary for EMA to function.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ema_validation&quot;,
        choices=[&quot;none&quot;, &quot;ema_only&quot;, &quot;comparison&quot;],
        default=&quot;comparison&quot;,
        help=(
            &quot;When &apos;none&apos; is set, no EMA validation will be done.&quot;
            &quot; When using &apos;ema_only&apos;, the validations will rely mostly on the EMA weights.&quot;
            &quot; When using &apos;comparison&apos; (default) mode, the validations will first run on the checkpoint before also running for&quot;
            &quot; the EMA weights. In comparison mode, the resulting images will be provided side-by-side.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ema_cpu_only&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When using EMA, the shadow model is moved to the accelerator before we update its parameters.&quot;
            &quot; When provided, this option will disable the moving of the EMA model to the accelerator.&quot;
            &quot; This will save a lot of VRAM at the cost of a lot of time for updates. It is recommended to also supply&quot;
            &quot; --ema_update_interval to reduce the number of updates to eg. every 100 steps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ema_foreach_disable&quot;,
        action=&quot;store_true&quot;,
        default=True,
        help=(
            &quot;By default, we use torch._foreach functions for updating the shadow parameters, which should be fast.&quot;
            &quot; When provided, this option will disable the foreach methods and use vanilla EMA updates.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ema_update_interval&quot;,
        type=int,
        default=None,
        help=(
            &quot;The number of optimization steps between EMA updates. If not provided, EMA network will update on every step.&quot;
        ),
    )
    parser.add_argument(
        &quot;--ema_decay&quot;,
        type=float,
        default=0.995,
        help=(
            &quot;The closer to 0.9999 this gets, the less updates will occur over time. Setting it to a lower value, such as 0.990,&quot;
            &quot; will allow greater influence of later updates.&quot;
        ),
    )
    parser.add_argument(
        &quot;--non_ema_revision&quot;,
        type=str,
        default=None,
        required=False,
        help=(
            &quot;Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or&quot;
            &quot; remote repository specified with --pretrained_model_name_or_path.&quot;
        ),
    )
    parser.add_argument(
        &quot;--offload_param_path&quot;,
        type=str,
        default=None,
        help=(
            &quot;When using DeepSpeed ZeRo stage 2 or 3 with NVMe offload, this may be specified to provide a path for the offload.&quot;
        ),
    )
    parser.add_argument(
        &quot;--optimizer&quot;,
        type=str,
        choices=optimizer_choices.keys(),
        required=True,
        default=None,
    )
    parser.add_argument(
        &quot;--optimizer_config&quot;,
        type=str,
        default=None,
        help=(
            &quot;When setting a given optimizer, this allows a comma-separated list of key-value pairs to be provided that will override the optimizer defaults.&quot;
            &quot; For example, `--optimizer_config=decouple_lr=True,weight_decay=0.01`.&quot;
        ),
    )
    parser.add_argument(
        &quot;--optimizer_cpu_offload_method&quot;,
        choices=[&quot;none&quot;],  # , &quot;torchao&quot;],
        default=&quot;none&quot;,
        help=(
            &quot;This option is a placeholder. In the future, it will allow for the selection of different CPU offload methods.&quot;
        ),
    )
    parser.add_argument(
        &quot;--optimizer_offload_gradients&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When creating a CPU-offloaded optimiser, the gradients can be offloaded to the CPU to save more memory.&quot;
        ),
    )
    parser.add_argument(
        &quot;--fuse_optimizer&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When creating a CPU-offloaded optimiser, the fused optimiser could be used to save on memory, while running slightly slower.&quot;
        ),
    )
    parser.add_argument(
        &quot;--optimizer_beta1&quot;,
        type=float,
        default=None,
        help=&quot;The value to use for the first beta value in the optimiser, which is used for the first moment estimate. A range of 0.8-0.9 is common.&quot;,
    )
    parser.add_argument(
        &quot;--optimizer_beta2&quot;,
        type=float,
        default=None,
        help=&quot;The value to use for the second beta value in the optimiser, which is used for the second moment estimate. A range of 0.999-0.9999 is common.&quot;,
    )
    parser.add_argument(
        &quot;--optimizer_release_gradients&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When using Optimi optimizers, this option will release the gradients after the optimizer step.&quot;
            &quot; This can save memory, but may slow down training. With Quanto, there may be no benefit.&quot;
        ),
    )
    parser.add_argument(
        &quot;--adam_beta1&quot;,
        type=float,
        default=0.9,
        help=&quot;The beta1 parameter for the Adam and other optimizers.&quot;,
    )
    parser.add_argument(
        &quot;--adam_beta2&quot;,
        type=float,
        default=0.999,
        help=&quot;The beta2 parameter for the Adam and other optimizers.&quot;,
    )
    parser.add_argument(
        &quot;--adam_weight_decay&quot;, type=float, default=1e-2, help=&quot;Weight decay to use.&quot;
    )
    parser.add_argument(
        &quot;--adam_epsilon&quot;,
        type=float,
        default=1e-08,
        help=&quot;Epsilon value for the Adam optimizer&quot;,
    )
    parser.add_argument(
        &quot;--prodigy_steps&quot;,
        type=int,
        default=None,
        help=(
            &quot;When training with Prodigy, this defines how many steps it should be adjusting its learning rate for.&quot;
            &quot; It seems to be that Diffusion models benefit from a capping off of the adjustments after 25 percent&quot;
            &quot; of the training run (dependent on batch size, repeats, and epochs).&quot;
            &quot; It this value is not supplied, it will be calculated at 25 percent of your training steps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--max_grad_norm&quot;,
        default=2.0,
        type=float,
        help=(
            &quot;Clipping the max gradient norm can help prevent exploding gradients, but&quot;
            &quot; may also harm training by introducing artifacts or making it hard to train artifacts away.&quot;
        ),
    )
    parser.add_argument(
        &quot;--grad_clip_method&quot;,
        default=&quot;value&quot;,
        choices=[&quot;value&quot;, &quot;norm&quot;],
        help=(
            &quot;When applying --max_grad_norm, the method to use for clipping the gradients.&quot;
            &quot; The previous default option &apos;norm&apos; will scale ALL gradient values when any outliers in the gradient are encountered, which can reduce training precision.&quot;
            &quot; The new default option &apos;value&apos; will clip individual gradient values using this value as a maximum, which may preserve precision while avoiding outliers, enhancing convergence.&quot;
            &quot; In simple terms, the default will help the model learn faster without blowing up (SD3.5 Medium was the main test model).&quot;
            &quot; Use &apos;norm&apos; to return to the old behaviour.&quot;
        ),
    )
    parser.add_argument(
        &quot;--push_to_hub&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Whether or not to push the model to the Hub.&quot;,
    )
    parser.add_argument(
        &quot;--push_checkpoints_to_hub&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When set along with --push_to_hub, all intermediary checkpoints will be pushed to the hub as if they were a final checkpoint.&quot;
        ),
    )
    parser.add_argument(
        &quot;--hub_model_id&quot;,
        type=str,
        default=None,
        help=&quot;The name of the repository to keep in sync with the local `output_dir`.&quot;,
    )
    parser.add_argument(
        &quot;--model_card_note&quot;,
        type=str,
        default=None,
        help=(
            &quot;Add a string to the top of your model card to provide users with some additional context.&quot;
        ),
    )
    parser.add_argument(
        &quot;--model_card_safe_for_work&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Hugging Face Hub requires a warning to be added to models that may generate NSFW content.&quot;
            &quot; This is done by default in SimpleTuner for safety purposes, but can be disabled with this option.&quot;
            &quot; Additionally, removing the not-for-all-audiences tag from the README.md in the repo will also disable this warning&quot;
            &quot; on previously-uploaded models.&quot;
        ),
    )
    parser.add_argument(
        &quot;--logging_dir&quot;,
        type=str,
        default=&quot;logs&quot;,
        help=(
            &quot;[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to&quot;
            &quot; *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.&quot;
        ),
    )
    parser.add_argument(
        &quot;--benchmark_base_model&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Deprecated option, benchmarks are now enabled by default. Use --disable_benchmark to disable.&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_benchmark&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;By default, the model will be benchmarked on the first batch of the first epoch.&quot;
            &quot; This can be disabled with this option.&quot;
        ),
    )
    parser.add_argument(
        &quot;--evaluation_type&quot;,
        type=str,
        default=None,
        choices=[&quot;clip&quot;, &quot;none&quot;],
        help=(
            &quot;Validations must be enabled for model evaluation to function. The default is to use no evaluator,&quot;
            &quot; and &apos;clip&apos; will use a CLIP model to evaluate the resulting model&apos;s performance during validations.&quot;
        ),
    )
    parser.add_argument(
        &quot;--eval_dataset_pooling&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When provided, only the pooled evaluation results will be returned in a single chart from all eval sets.&quot;
            &quot; Without this option, all eval sets will have separate charts.&quot;
        ),
    )
    parser.add_argument(
        &quot;--pretrained_evaluation_model_name_or_path&quot;,
        type=str,
        default=&quot;openai/clip-vit-large-patch14-336&quot;,
        help=(
            &quot;Optionally provide a custom model to use for ViT evaluations.&quot;
            &quot; The default is currently clip-vit-large-patch14-336, allowing for lower patch sizes (greater accuracy)&quot;
            &quot; and an input resolution of 336x336.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_on_startup&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;When training begins, the starting model will have validation prompts run through it, for later comparison.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_seed_source&quot;,
        type=str,
        default=&quot;cpu&quot;,
        choices=[&quot;gpu&quot;, &quot;cpu&quot;],
        help=(
            &quot;Some systems may benefit from using CPU-based seeds for reproducibility. On other systems, this may cause a TypeError.&quot;
            &quot; Setting this option to &apos;cpu&apos; may cause validation errors. If so, please set SIMPLETUNER_LOG_LEVEL=DEBUG&quot;
            &quot; and submit debug.log to a new Github issue report.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_lycoris_strength&quot;,
        type=float,
        default=1.0,
        help=(
            &quot;When inferencing for validations, the Lycoris model will by default be run at its training strength, 1.0.&quot;
            &quot; However, this value can be increased to a value of around 1.3 or 1.5 to get a stronger effect from the model.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_torch_compile&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Supply `--validation_torch_compile=true` to enable the use of torch.compile() on the validation pipeline.&quot;
            &quot; For some setups, torch.compile() may error out. This is dependent on PyTorch version, phase of the moon,&quot;
            &quot; but if it works, you should leave it enabled for a great speed-up.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_torch_compile_mode&quot;,
        type=str,
        default=&quot;max-autotune&quot;,
        choices=[&quot;max-autotune&quot;, &quot;reduce-overhead&quot;, &quot;default&quot;],
        help=(
            &quot;PyTorch provides different modes for the Torch Inductor when compiling graphs. max-autotune,&quot;
            &quot; the default mode, provides the most benefit.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_guidance_skip_layers&quot;,
        type=str,
        default=None,
        help=(
            &quot;StabilityAI recommends a value of [7, 8, 9] for Stable Diffusion 3.5 Medium.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_guidance_skip_layers_start&quot;,
        type=float,
        default=0.01,
        help=(&quot;StabilityAI recommends a value of 0.01 for SLG start.&quot;),
    )
    parser.add_argument(
        &quot;--validation_guidance_skip_layers_stop&quot;,
        type=float,
        default=0.01,
        help=(&quot;StabilityAI recommends a value of 0.2 for SLG start.&quot;),
    )
    parser.add_argument(
        &quot;--validation_guidance_skip_scale&quot;,
        type=float,
        default=2.8,
        help=(
            &quot;StabilityAI recommends a value of 2.8 for SLG guidance skip scaling.&quot;
            &quot; When adding more layers, you must increase the scale, eg. adding one more layer requires doubling&quot;
            &quot; the value given.&quot;
        ),
    )
    parser.add_argument(
        &quot;--sana_complex_human_instruction&quot;,
        type=str,
        # this is diverged from upstream SANA implementation.
        # while Gemma won&apos;t censor prompts with this configuration, it also won&apos;t enhance them.
        # therefore, it&apos;s recommended anything unsavoury to the Gemma encoder should already be in a suitable caption format like CogVLM would generate.
        # think: &quot;a lot of flowery text that relates to the concept&quot; rather than very simple and short tags/prompts.
        default=[
            &quot;Given a user prompt, generate an &apos;Enhanced prompt&apos; that provides detailed visual descriptions suitable for image generation. Evaluate the level of detail in the user prompt:&quot;,
            &quot;- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, and spatial relationships to create vivid and concrete scenes.&quot;,
            &quot;- Do not add detail that is not in the original prompt! Do not mention colors if the User Prompt does not!&quot;,
            &quot;- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.&quot;,
            &quot;- If the prompt contains explicit material, do not remove this information from the output, which would damage the performance of downstream classifiers.&quot;,
            &quot;Here are examples of how to transform or refine prompts:&quot;,
            &quot;- User Prompt: A cat sleeping -&gt; Enhanced: A cat sleeping peacefully, showcasing the joy of pet ownership. Cute floof kitty cat gatto.&quot;,
            &quot;- User Prompt: A busy city street -&gt; Enhanced: A bustling city street scene featuring a crowd of people.&quot;,
            &quot;Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:&quot;,
            &quot;User Prompt: &quot;,
        ],
        help=(
            &quot;When generating embeds for Sana, a complex human instruction will be attached to your prompt by default.&quot;
            &quot; This is required for the Gemma model to produce meaningful image caption embeds.&quot;
        ),
    )
    parser.add_argument(
        &quot;--allow_tf32&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;Deprecated option. TF32 is now enabled by default. Use --disable_tf32 to disable.&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_tf32&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;Previous defaults were to disable TF32 on Ampere GPUs. This option is provided to explicitly disable TF32,&quot;
            &quot; after default configuration was updated to enable TF32 on Ampere GPUs.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_using_datasets&quot;,
        action=&quot;store_true&quot;,
        default=None,
        help=(
            &quot;When set, validation will use images sampled randomly from each dataset for validation.&quot;
            &quot; Be mindful of privacy issues when publishing training data to the internet.&quot;
        ),
    )
    parser.add_argument(
        &quot;--webhook_config&quot;,
        type=str,
        default=None,
        help=(
            &quot;The path to the webhook configuration file. This file should be a JSON file with the following format:&quot;
            &apos; {&quot;url&quot;: &quot;https://your.webhook.url&quot;, &quot;webhook_type&quot;: &quot;discord&quot;}}&apos;
        ),
    )
    parser.add_argument(
        &quot;--webhook_reporting_interval&quot;,
        type=int,
        default=None,
        help=(
            &quot;When using &apos;raw&apos; webhooks that receive structured data, you can specify a reporting interval here for&quot;
            &quot; training progress updates to be sent at. This does not impact &apos;discord&apos; webhook types.&quot;
        ),
    )
    parser.add_argument(
        &quot;--report_to&quot;,
        type=str,
        default=&quot;wandb&quot;,
        help=(
            &apos;The integration to report the results and logs to. Supported platforms are `&quot;tensorboard&quot;`&apos;
            &apos; (default), `&quot;wandb&quot;` and `&quot;comet_ml&quot;`. Use `&quot;all&quot;` to report to all integrations,&apos;
            &apos; or `&quot;none&quot;` to disable logging.&apos;
        ),
    )
    parser.add_argument(
        &quot;--tracker_run_name&quot;,
        type=str,
        default=&quot;simpletuner-testing&quot;,
        help=&quot;The name of the run to track with the tracker.&quot;,
    )
    parser.add_argument(
        &quot;--tracker_project_name&quot;,
        type=str,
        default=&quot;simpletuner&quot;,
        help=&quot;The name of the project for WandB or Tensorboard.&quot;,
    )
    parser.add_argument(
        &quot;--tracker_image_layout&quot;,
        choices=[&quot;gallery&quot;, &quot;table&quot;],
        default=&quot;gallery&quot;,
        help=(
            &quot;When running validations with multiple images, you may want them all placed together in a table, row-wise.&quot;
            &quot; Gallery mode, the default, will allow use of a slider to view the historical images easily.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_prompt&quot;,
        type=str,
        default=None,
        help=&quot;A prompt that is used during validation to verify that the model is learning.&quot;,
    )
    parser.add_argument(
        &quot;--validation_prompt_library&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If this is provided, the SimpleTuner prompt library will be used to generate multiple images.&quot;,
    )
    parser.add_argument(
        &quot;--user_prompt_library&quot;,
        type=str,
        default=None,
        help=&quot;This should be a path to the JSON file containing your prompt library. See user_prompt_library.json.example.&quot;,
    )
    parser.add_argument(
        &quot;--validation_negative_prompt&quot;,
        type=str,
        default=&quot;blurry, cropped, ugly&quot;,
        help=(
            &quot;When validating images, a negative prompt may be used to guide the model away from certain features.&quot;
            &quot; When this value is set to --validation_negative_prompt=&apos;&apos;, no negative guidance will be applied.&quot;
            &quot; Default: blurry, cropped, ugly&quot;
        ),
    )
    parser.add_argument(
        &quot;--num_validation_images&quot;,
        type=int,
        default=1,
        help=&quot;Number of images that should be generated during validation with `validation_prompt`.&quot;,
    )
    parser.add_argument(
        &quot;--validation_steps&quot;,
        type=int,
        default=100,
        help=(
            &quot;Run validation every X steps. Validation consists of running the prompt&quot;
            &quot; `args.validation_prompt` multiple times: `args.num_validation_images`&quot;
            &quot; and logging the images.&quot;
        ),
    )
    parser.add_argument(
        &quot;--eval_steps_interval&quot;,
        type=int,
        default=None,
        help=(
            &quot;When set, the model will be evaluated every X steps. This is useful for&quot;
            &quot; monitoring the model&apos;s progress during training, but it requires an eval set&quot;
            &quot; configured in your dataloader.&quot;
        ),
    )
    parser.add_argument(
        &quot;--eval_timesteps&quot;,
        type=int,
        default=28,
        help=(
            &quot;Defines how many timesteps to sample during eval.&quot;
            &quot; You can emulate inference by setting this to the value of --validation_num_inference_steps.&quot;
        ),
    )
    parser.add_argument(
        &quot;--num_eval_images&quot;,
        type=int,
        default=4,
        help=(
            &quot;If possible, this many eval images will be selected from each dataset.&quot;
            &quot; This is used when training super-resolution models such as DeepFloyd Stage II,&quot;
            &quot; which will upscale input images from the training set during validation.&quot;
            &quot; If using --eval_steps_interval, this will be the number of batches sampled&quot;
            &quot; for loss calculations.&quot;
        ),
    )
    parser.add_argument(
        &quot;--eval_dataset_id&quot;,
        type=str,
        default=None,
        help=(
            &quot;When provided, only this dataset&apos;s images will be used as the eval set, to keep&quot;
            &quot; the training and eval images split. This option only applies for img2img validations,&quot;
            &quot; not validation loss calculations.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_num_inference_steps&quot;,
        type=int,
        default=30,
        help=(
            &quot;The default scheduler, DDIM, benefits from more steps. UniPC can do well with just 10-15.&quot;
            &quot; For more speed during validations, reduce this value. For better quality, increase it.&quot;
            &quot; For model distilation, you will likely want to keep this low.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_resolution&quot;,
        type=str,
        default=256,
        help=&quot;Square resolution images will be output at this resolution (256x256).&quot;,
    )
    parser.add_argument(
        &quot;--validation_noise_scheduler&quot;,
        type=str,
        choices=[&quot;ddim&quot;, &quot;ddpm&quot;, &quot;euler&quot;, &quot;euler-a&quot;, &quot;unipc&quot;],
        default=None,
        help=(
            &quot;When validating the model at inference time, a different scheduler may be chosen.&quot;
            &quot; UniPC can offer better speed, and Euler A can put up with instabilities a bit better.&quot;
            &quot; For zero-terminal SNR models, DDIM is the best choice. Choices: [&apos;ddim&apos;, &apos;ddpm&apos;, &apos;euler&apos;, &apos;euler-a&apos;, &apos;unipc&apos;],&quot;
            &quot; Default: None (use the model default)&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_disable_unconditional&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When set, the validation pipeline will not generate unconditional samples.&quot;
            &quot; This is useful to speed up validations with a single prompt on slower systems, or if you are not&quot;
            &quot; interested in unconditional space generations.&quot;
        ),
    )
    parser.add_argument(
        &quot;--enable_watermark&quot;,
        default=False,
        action=&quot;store_true&quot;,
        help=(
            &quot;The SDXL 0.9 and 1.0 licenses both require a watermark be used to identify any images created to be shared.&quot;
            &quot; Since the images created during validation typically are not shared, and we want the most accurate results,&quot;
            &quot; this watermarker is disabled by default. If you are sharing the validation images, it is up to you&quot;
            &quot; to ensure that you are complying with the license, whether that is through this watermarker, or another.&quot;
        ),
    )
    parser.add_argument(
        &quot;--mixed_precision&quot;,
        type=str,
        default=&quot;bf16&quot;,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;, &quot;no&quot;],
        help=(
            &quot;SimpleTuner only supports bf16 training. Bf16 requires PyTorch &gt;=&quot;
            &quot; 1.10. on an Nvidia Ampere or later GPU, and PyTorch 2.3 or newer for Apple Silicon.&quot;
            &quot; Default to the value of accelerate config of the current system or the&quot;
            &quot; flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.&quot;
            &quot; fp16 is offered as an experimental option, but is not recommended as it is less-tested and you will likely encounter errors.&quot;
        ),
    )
    parser.add_argument(
        &quot;--gradient_precision&quot;,
        type=str,
        choices=[&quot;unmodified&quot;, &quot;fp32&quot;],
        default=None,
        help=(
            &quot;One of the hallmark discoveries of the Llama 3.1 paper is numeric instability when calculating&quot;
            &quot; gradients in bf16 precision. The default behaviour when gradient accumulation steps are enabled&quot;
            &quot; is now to use fp32 gradients, which is slower, but provides more accurate updates.&quot;
        ),
    )
    parser.add_argument(
        &quot;--quantize_via&quot;,
        type=str,
        choices=[&quot;cpu&quot;, &quot;accelerator&quot;],
        default=&quot;accelerator&quot;,
        help=(
            &quot;When quantising the model, the quantisation process can be done on the CPU or the accelerator.&quot;
            &quot; When done on the accelerator (default), slightly more VRAM is required, but the process completes in milliseconds.&quot;
            &quot; When done on the CPU, the process may take upwards of 60 seconds, but can complete without OOM on 16G cards.&quot;
        ),
    )
    parser.add_argument(
        &quot;--base_model_precision&quot;,
        type=str,
        default=&quot;no_change&quot;,
        choices=quantised_precision_levels,
        help=(
            &quot;When training a LoRA, you might want to quantise the base model to a lower precision to save more VRAM.&quot;
            &quot; The default value, &apos;no_change&apos;, does not quantise any weights.&quot;
            &quot; Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require Bits n Bytes for quantisation (NVIDIA, maybe AMD).&quot;
            &quot; Using &apos;fp8-quanto&apos; will require Quanto for quantisation (Apple Silicon, NVIDIA, AMD).&quot;
        ),
    )
    parser.add_argument(
        &quot;--quantize_activations&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;(EXPERIMENTAL) This option is currently unsupported, and exists solely for development purposes.&quot;
        ),
    )
    parser.add_argument(
        &quot;--base_model_default_dtype&quot;,
        type=str,
        default=&quot;bf16&quot;,
        choices=[&quot;bf16&quot;, &quot;fp32&quot;],
        help=(
            &quot;Unlike --mixed_precision, this value applies specifically for the default weights of your quantised base model.&quot;
            &quot; When quantised, not every parameter can or should be quantised down to the target precision.&quot;
            &quot; By default, we use bf16 weights for the base model - but this can be changed to fp32 to enable&quot;
            &quot; the use of other optimizers than adamw_bf16. However, this uses marginally more memory,&quot;
            &quot; and may not be necessary for your use case.&quot;
        ),
    )
    for i in range(1, 4):
        parser.add_argument(
            f&quot;--text_encoder_{i}_precision&quot;,
            type=str,
            default=&quot;no_change&quot;,
            choices=quantised_precision_levels,
            help=(
                f&quot;When training a LoRA, you might want to quantise text encoder {i} to a lower precision to save more VRAM.&quot;
                &quot; The default value is to follow base_model_precision (no_change).&quot;
                &quot; Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require Bits n Bytes for quantisation (NVIDIA, maybe AMD).&quot;
                &quot; Using &apos;fp8-quanto&apos; will require Quanto for quantisation (Apple Silicon, NVIDIA, AMD).&quot;
            ),
        )
    parser.add_argument(
        &quot;--local_rank&quot;,
        type=int,
        default=-1,
        help=&quot;For distributed training: local_rank&quot;,
    )
    parser.add_argument(
        &quot;--attention_mechanism&quot;,
        type=str,
        choices=[
            &quot;diffusers&quot;,
            &quot;xformers&quot;,
            &quot;sageattention&quot;,
            &quot;sageattention-int8-fp16-triton&quot;,
            &quot;sageattention-int8-fp16-cuda&quot;,
            &quot;sageattention-int8-fp8-cuda&quot;,
        ],
        default=&quot;diffusers&quot;,
        help=(
            &quot;On NVIDIA CUDA devices, alternative flash attention implementations are offered, with the default being native pytorch SDPA.&quot;
            &quot; SageAttention has multiple backends to select from.&quot;
            &quot; The recommended value, &apos;sageattention&apos;, guesses what would be the &apos;best&apos; option for SageAttention on your hardware&quot;
            &quot; (usually this is the int8-fp16-cuda backend). However, manually setting this value to int8-fp16-triton&quot;
            &quot; may provide better averages for per-step training and inference performance while the cuda backend&quot;
            &quot; may provide the highest maximum speed (with also a lower minimum speed). NOTE: SageAttention training quality&quot;
            &quot; has not been validated.&quot;
        ),
    )
    parser.add_argument(
        &quot;--sageattention_usage&quot;,
        type=str,
        choices=[&quot;training&quot;, &quot;inference&quot;, &quot;training+inference&quot;],
        default=&quot;inference&quot;,
        help=(
            &quot;SageAttention breaks gradient tracking through the backward pass, leading to untrained QKV layers.&quot;
            &quot; This can result in substantial problems for training, so it is recommended to use SageAttention only for inference (default behaviour).&quot;
            &quot; If you are confident in your training setup or do not wish to train QKV layers, you may use &apos;training&apos; to enable SageAttention for training.&quot;
        ),
    )
    parser.add_argument(
        &quot;--enable_xformers_memory_efficient_attention&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Whether or not to use xformers. Deprecated and slated for future removal. Use --attention_mechanism.&quot;,
    )
    parser.add_argument(
        &quot;--set_grads_to_none&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain&quot;
            &quot; behaviors, so disable this argument if it causes any problems. More info:&quot;
            &quot; https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html&quot;
        ),
    )
    parser.add_argument(
        &quot;--noise_offset&quot;,
        type=float,
        default=0.1,
        help=&quot;The scale of noise offset. Default: 0.1&quot;,
    )
    parser.add_argument(
        &quot;--noise_offset_probability&quot;,
        type=float,
        default=0.25,
        help=(
            &quot;When training with --offset_noise, the value of --noise_offset will only be applied probabilistically.&quot;
            &quot; The default behaviour is for offset noise (if enabled) to be applied 25 percent of the time.&quot;
        ),
    )
    parser.add_argument(
        &quot;--validation_guidance&quot;,
        type=float,
        default=7.5,
        help=&quot;CFG value for validation images. Default: 7.5&quot;,
    )
    parser.add_argument(
        &quot;--validation_guidance_real&quot;,
        type=float,
        default=1.0,
        help=&quot;Use real CFG sampling for Flux validation images. Default: 1.0 (no CFG)&quot;,
    )
    parser.add_argument(
        &quot;--validation_no_cfg_until_timestep&quot;,
        type=int,
        default=2,
        help=&quot;When using real CFG sampling for Flux validation images, skip doing CFG on these timesteps. Default: 2&quot;,
    )
    parser.add_argument(
        &quot;--validation_guidance_rescale&quot;,
        type=float,
        default=0.0,
        help=&quot;CFG rescale value for validation images. Default: 0.0, max 1.0&quot;,
    )
    parser.add_argument(
        &quot;--validation_randomize&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;If supplied, validations will be random, ignoring any seeds.&quot;,
    )
    parser.add_argument(
        &quot;--validation_seed&quot;,
        type=int,
        default=None,
        help=(
            &quot;If not supplied, the value for --seed will be used.&quot;
            &quot; If neither those nor --validation_randomize are supplied, a seed of zero is used.&quot;
        ),
    )
    parser.add_argument(
        &quot;--fully_unload_text_encoder&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, will fully unload the text_encoder from memory when not in use.&quot;
            &quot; This currently has the side effect of crashing validations, but it is useful&quot;
            &quot; for initiating VAE caching on GPUs that would otherwise be too small.&quot;
        ),
    )
    parser.add_argument(
        &quot;--freeze_encoder_before&quot;,
        type=int,
        default=12,
        help=&quot;When using &apos;before&apos; strategy, we will freeze layers earlier than this.&quot;,
    )
    parser.add_argument(
        &quot;--freeze_encoder_after&quot;,
        type=int,
        default=17,
        help=&quot;When using &apos;after&apos; strategy, we will freeze layers later than this.&quot;,
    )
    parser.add_argument(
        &quot;--freeze_encoder_strategy&quot;,
        type=str,
        default=&quot;after&quot;,
        help=(
            &quot;When freezing the text_encoder, we can use the &apos;before&apos;, &apos;between&apos;, or &apos;after&apos; strategy.&quot;
            &quot; The &apos;between&apos; strategy will freeze layers between those two values, leaving the outer layers unfrozen.&quot;
            &quot; The default strategy is to freeze all layers from 17 up.&quot;
            &quot; This can be helpful when fine-tuning Stable Diffusion 2.1 on a new style.&quot;
        ),
    )
    parser.add_argument(
        &quot;--layer_freeze_strategy&quot;,
        type=str,
        choices=[&quot;none&quot;, &quot;bitfit&quot;],
        default=&quot;none&quot;,
        help=(
            &quot;When freezing parameters, we can use the &apos;none&apos; or &apos;bitfit&apos; strategy.&quot;
            &quot; The &apos;bitfit&apos; strategy will freeze all weights, and leave bias in a trainable state.&quot;
            &quot; The default strategy is to leave all parameters in a trainable state.&quot;
            &quot; Freezing the weights can improve convergence for finetuning.&quot;
            &quot; Using bitfit only moderately reduces VRAM consumption, but substantially reduces the count of trainable parameters.&quot;
        ),
    )
    parser.add_argument(
        &quot;--unet_attention_slice&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, will use attention slicing for the SDXL UNet. This is an experimental feature and is not recommended for general use.&quot;
            &quot; SD 2.x makes use of attention slicing on Apple MPS platform to avoid a NDArray size crash, but SDXL does not&quot;
            &quot; seem to require attention slicing on MPS. If memory constrained, try enabling it anyway.&quot;
        ),
    )
    parser.add_argument(
        &quot;--print_filenames&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If any image files are stopping the process eg. due to corruption or truncation, this will help identify which is at fault.&quot;
        ),
    )
    parser.add_argument(
        &quot;--print_sampler_statistics&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If provided, will print statistics about the dataset sampler. This is useful for debugging.&quot;
            &quot; The default behaviour is to not print sampler statistics.&quot;
        ),
    )
    parser.add_argument(
        &quot;--metadata_update_interval&quot;,
        type=int,
        default=3600,
        help=(
            &quot;When generating the aspect bucket indicies, we want to save it every X seconds.&quot;
            &quot; The default is to save it every 1 hour, such that progress is not lost on clusters&quot;
            &quot; where runtime is limited to 6-hour increments (e.g. the JUWELS Supercomputer).&quot;
            &quot; The minimum value is 60 seconds.&quot;
        ),
    )
    parser.add_argument(
        &quot;--debug_aspect_buckets&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, will print excessive debugging for aspect bucket operations.&quot;,
    )
    parser.add_argument(
        &quot;--debug_dataset_loader&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, will print excessive debugging for data loader operations.&quot;,
    )
    parser.add_argument(
        &quot;--freeze_encoder&quot;,
        type=bool,
        default=True,
        help=&quot;Whether or not to freeze the text_encoder. The default is true.&quot;,
    )
    parser.add_argument(
        &quot;--save_text_encoder&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, will save the text_encoder after training.&quot;
            &quot; This is useful if you&apos;re using --push_to_hub so that the final pipeline contains all necessary components to run.&quot;
        ),
    )
    parser.add_argument(
        &quot;--text_encoder_limit&quot;,
        type=int,
        default=25,
        help=(
            &quot;When training the text_encoder, we want to limit how long it trains for to avoid catastrophic loss.&quot;
        ),
    )
    parser.add_argument(
        &quot;--prepend_instance_prompt&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When determining the captions from the filename, prepend the instance prompt as an enforced keyword.&quot;
        ),
    )
    parser.add_argument(
        &quot;--only_instance_prompt&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Use the instance prompt instead of the caption from filename.&quot;,
    )
    parser.add_argument(
        &quot;--data_aesthetic_score&quot;,
        type=float,
        default=7.0,
        help=(
            &quot;Since currently we do not calculate aesthetic scores for data, we will statically set it to one value. This is only used by the SDXL Refiner.&quot;
        ),
    )
    parser.add_argument(
        &quot;--sdxl_refiner_uses_full_range&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;If set, the SDXL Refiner will use the full range of the model, rather than the design value of 20 percent.&quot;
            &quot; This is useful for training models that will be used for inference from end-to-end of the noise schedule.&quot;
            &quot; You may use this for example, to turn the SDXL refiner into a full text-to-image model.&quot;
        ),
    )
    parser.add_argument(
        &quot;--caption_dropout_probability&quot;,
        type=float,
        default=None,
        help=(
            &quot;Caption dropout will randomly drop captions and, for SDXL, size conditioning inputs based on this probability.&quot;
            &quot; When set to a value of 0.1, it will drop approximately 10 percent of the inputs.&quot;
            &quot; Maximum recommended value is probably less than 0.5, or 50 percent of the inputs. Maximum technical value is 1.0.&quot;
            &quot; The default is to use zero caption dropout, though for better generalisation, a value of 0.1 is recommended.&quot;
        ),
    )
    parser.add_argument(
        &quot;--delete_unwanted_images&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, will delete images that are not of a minimum size to save on disk space for large training runs.&quot;
            &quot; Default behaviour: Unset, remove images from bucket only.&quot;
        ),
    )
    parser.add_argument(
        &quot;--delete_problematic_images&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;If set, any images that error out during load will be removed from the underlying storage medium.&quot;
            &quot; This is useful to prevent repeatedly attempting to cache bad files on a cloud bucket.&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_bucket_pruning&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;When training on very small datasets, you might not care that the batch sizes will outpace your image count.&quot;
            &quot; Setting this option will prevent SimpleTuner from deleting your bucket lists that do not meet&quot;
            &quot; the minimum image count requirements. Use at your own risk, it may end up throwing off your statistics or epoch tracking.&quot;
        ),
    )
    parser.add_argument(
        &quot;--offset_noise&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Fine-tuning against a modified noise&quot;
            &quot; See: https://www.crosslabs.org//blog/diffusion-with-offset-noise for more information.&quot;
        ),
    )
    parser.add_argument(
        &quot;--input_perturbation&quot;,
        type=float,
        default=0.0,
        help=(
            &quot;Add additional noise only to the inputs fed to the model during training.&quot;
            &quot; This will make the training converge faster. A value of 0.1 is suggested if you want to enable this.&quot;
            &quot; Input perturbation seems to also work with flow-matching (e.g. SD3 and Flux).&quot;
        ),
    )
    parser.add_argument(
        &quot;--input_perturbation_steps&quot;,
        type=float,
        default=0,
        help=(
            &quot;Only apply input perturbation over the first N steps with linear decay.&quot;
            &quot; This should prevent artifacts from showing up in longer training runs.&quot;
        ),
    )
    parser.add_argument(
        &quot;--lr_end&quot;,
        type=str,
        default=&quot;4e-7&quot;,
        help=(
            &quot;A polynomial learning rate will end up at this value after the specified number of warmup steps.&quot;
            &quot; A sine or cosine wave will use this value as its lower bound for the learning rate.&quot;
        ),
    )
    parser.add_argument(
        &quot;--i_know_what_i_am_doing&quot;,
        action=&quot;store_true&quot;,
        help=(
            &quot;This flag allows you to override some safety checks.&quot;
            &quot; It&apos;s not recommended to use this unless you are developing the platform.&quot;
            &quot; Generally speaking, issue reports submitted with this flag enabled will go to the bottom of the queue.&quot;
        ),
    )
    parser.add_argument(
        &quot;--accelerator_cache_clear_interval&quot;,
        default=None,
        type=int,
        help=(
            &quot;Clear the cache from VRAM every X steps. This can help prevent memory leaks, but may slow down training.&quot;
        ),
    )
    return parser
def get_default_config():
    parser = get_argument_parser()
    default_config = {}
    for action in parser.__dict__[&quot;_actions&quot;]:
        if action.dest:
            default_config[action.dest] = action.default
    return default_config
def parse_cmdline_args(input_args=None, exit_on_error: bool = False):
    parser = get_argument_parser()
    args = None
    if input_args is not None:
        for key_val in input_args:
            print_on_main_thread(f&quot;{key_val}&quot;)
        try:
            args = parser.parse_args(input_args)
        except:
            logger.error(f&quot;Could not parse input: {input_args}&quot;)
            import traceback
            logger.error(traceback.format_exc())
    else:
        args = parser.parse_args()
    if args is None and exit_on_error:
        sys.exit(1)
    if args.optimizer == &quot;adam_bfloat16&quot; and args.mixed_precision != &quot;bf16&quot;:
        if not torch.backends.mps.is_available():
            logging.error(
                &quot;You cannot use --adam_bfloat16 without --mixed_precision=bf16.&quot;
            )
            sys.exit(1)
    env_local_rank = int(os.environ.get(&quot;LOCAL_RANK&quot;, -1))
    if env_local_rank != -1 and env_local_rank != args.local_rank:
        args.local_rank = env_local_rank
    if args.seed is not None:
        if args.seed == 0:
            # the current time should be used if value is zero, providing a rolling seed.
            args.seed = int(time.time())
        elif args.seed == -1:
            # more random seed if value is -1, it will be very different on each startup.
            args.seed = int(random.randint(0, 2**30))
    # default to using the same revision for the non-ema model if not specified
    if args.non_ema_revision is None:
        args.non_ema_revision = args.revision
    if args.cache_dir is None or args.cache_dir == &quot;&quot;:
        args.cache_dir = os.path.join(args.output_dir, &quot;cache&quot;)
    if args.maximum_image_size is not None and not args.target_downsample_size:
        raise ValueError(
            &quot;When providing --maximum_image_size, you must also provide a value for --target_downsample_size.&quot;
        )
    if (
        args.maximum_image_size is not None
        and args.resolution_type == &quot;area&quot;
        and args.maximum_image_size &gt; 5
        and not os.environ.get(&quot;SIMPLETUNER_MAXIMUM_IMAGE_SIZE_OVERRIDE&quot;, False)
    ):
        raise ValueError(
            f&quot;When using --resolution_type=area, --maximum_image_size must be less than 5 megapixels. You may have accidentally entered {args.maximum_image_size} pixels, instead of megapixels.&quot;
        )
    elif (
        args.maximum_image_size is not None
        and args.resolution_type == &quot;pixel&quot;
        and args.maximum_image_size &lt; 512
    ):
        raise ValueError(
            f&quot;When using --resolution_type=pixel, --maximum_image_size must be at least 512 pixels. You may have accidentally entered {args.maximum_image_size} megapixels, instead of pixels.&quot;
        )
    if (
        args.target_downsample_size is not None
        and args.resolution_type == &quot;area&quot;
        and args.target_downsample_size &gt; 5
        and not os.environ.get(&quot;SIMPLETUNER_MAXIMUM_IMAGE_SIZE_OVERRIDE&quot;, False)
    ):
        raise ValueError(
            f&quot;When using --resolution_type=area, --target_downsample_size must be less than 5 megapixels. You may have accidentally entered {args.target_downsample_size} pixels, instead of megapixels.&quot;
        )
    elif (
        args.target_downsample_size is not None
        and args.resolution_type == &quot;pixel&quot;
        and args.target_downsample_size &lt; 512
    ):
        raise ValueError(
            f&quot;When using --resolution_type=pixel, --target_downsample_size must be at least 512 pixels. You may have accidentally entered {args.target_downsample_size} megapixels, instead of pixels.&quot;
        )
    model_is_bf16 = (
        args.base_model_precision == &quot;no_change&quot;
        and (args.mixed_precision == &quot;bf16&quot; or torch.backends.mps.is_available())
    ) or (
        args.base_model_precision != &quot;no_change&quot;
        and args.base_model_default_dtype == &quot;bf16&quot;
    )
    model_is_quantized = args.base_model_precision != &quot;no_change&quot;
    # check optimiser validity
    chosen_optimizer = args.optimizer
    is_optimizer_deprecated(chosen_optimizer)
    from helpers.training.optimizer_param import optimizer_parameters
    optimizer_cls, optimizer_details = optimizer_parameters(chosen_optimizer, args)
    using_bf16_optimizer = optimizer_details.get(&quot;default_settings&quot;, {}).get(
        &quot;precision&quot;
    ) in [&quot;any&quot;, &quot;bf16&quot;]
    if using_bf16_optimizer and not model_is_bf16:
        raise ValueError(
            f&quot;Model is not using bf16 precision, but the optimizer {chosen_optimizer} requires it.&quot;
        )
    if is_optimizer_grad_fp32(args.optimizer):
        warning_log(
            &quot;Using an optimizer that requires fp32 gradients. Training will potentially run more slowly.&quot;
        )
        if args.gradient_precision != &quot;fp32&quot;:
            args.gradient_precision = &quot;fp32&quot;
    else:
        if args.gradient_precision == &quot;fp32&quot;:
            args.gradient_precision = &quot;unmodified&quot;
    if torch.backends.mps.is_available():
        if (
            args.model_family.lower() not in [&quot;sd3&quot;, &quot;flux&quot;, &quot;legacy&quot;]
            and not args.unet_attention_slice
        ):
            warning_log(
                &quot;MPS may benefit from the use of --unet_attention_slice for memory savings at the cost of speed.&quot;
            )
        if args.model_family != &quot;smoldit&quot; and args.train_batch_size &gt; 16:
            error_log(
                &quot;An M3 Max 128G will use 12 seconds per step at a batch size of 1 and 65 seconds per step at a batch size of 12.&quot;
                &quot; Any higher values will result in NDArray size errors or other unstable training results and crashes.&quot;
                &quot;\nPlease reduce the batch size to 12 or lower.&quot;
            )
            sys.exit(1)
        if args.quantize_via == &quot;accelerator&quot;:
            error_log(
                &quot;MPS does not benefit from models being quantized on the accelerator device. Overriding --quantize_via to &apos;cpu&apos;.&quot;
            )
            args.quantize_via = &quot;cpu&quot;
    if (
        args.max_train_steps is not None
        and args.max_train_steps &gt; 0
        and args.num_train_epochs &gt; 0
    ):
        error_log(
            &quot;When using --max_train_steps (MAX_NUM_STEPS), you must set --num_train_epochs (NUM_EPOCHS) to 0.&quot;
        )
        sys.exit(1)
    if (
        args.pretrained_vae_model_name_or_path is not None
        and args.model_family in [&quot;legacy&quot;, &quot;flux&quot;, &quot;sd3&quot;, &quot;sana&quot;, &quot;ltxvideo&quot;]
        and &quot;sdxl&quot; in args.pretrained_vae_model_name_or_path
        and &quot;deepfloyd&quot; not in args.model_type
    ):
        warning_log(
            f&quot;The VAE model {args.pretrained_vae_model_name_or_path} is not compatible. Please use a compatible VAE to eliminate this warning. The baked-in VAE will be used, instead.&quot;
        )
        args.pretrained_vae_model_name_or_path = None
    if (
        args.pretrained_vae_model_name_or_path == &quot;&quot;
        or args.pretrained_vae_model_name_or_path == &quot;&apos;&apos;&quot;
    ):
        args.pretrained_vae_model_name_or_path = None
    if &quot;deepfloyd&quot; not in args.model_type:
        info_log(
            f&quot;VAE Model: {args.pretrained_vae_model_name_or_path or args.pretrained_model_name_or_path}&quot;
        )
        info_log(f&quot;Default VAE Cache location: {args.cache_dir_vae}&quot;)
        info_log(f&quot;Text Cache location: {args.cache_dir_text}&quot;)
    if args.model_family == &quot;sd3&quot;:
        warning_log(
            &quot;MM-DiT requires an alignment value of 64px. Overriding the value of --aspect_bucket_alignment.&quot;
        )
        args.aspect_bucket_alignment = 64
        if args.sd3_t5_uncond_behaviour is None:
            args.sd3_t5_uncond_behaviour = args.sd3_clip_uncond_behaviour
        info_log(
            f&quot;SD3 embeds for unconditional captions: t5={args.sd3_t5_uncond_behaviour}, clip={args.sd3_clip_uncond_behaviour}&quot;
        )
    elif &quot;deepfloyd&quot; in args.model_type:
        deepfloyd_pixel_alignment = 8
        if args.aspect_bucket_alignment != deepfloyd_pixel_alignment:
            warning_log(
                f&quot;Overriding aspect bucket alignment pixel interval to {deepfloyd_pixel_alignment}px instead of {args.aspect_bucket_alignment}px.&quot;
            )
            args.aspect_bucket_alignment = deepfloyd_pixel_alignment
    if &quot;deepfloyd-stage2&quot; in args.model_type and args.resolution &lt; 256:
        warning_log(
            &quot;DeepFloyd Stage II requires a resolution of at least 256. Setting to 256.&quot;
        )
        args.resolution = 256
        args.aspect_bucket_alignment = 64
        args.resolution_type = &quot;pixel&quot;
    validation_resolution_is_float = False
    if &quot;.&quot; in str(args.validation_resolution):
        try:
            # this makes handling for int() conversion easier later.
            args.validation_resolution = float(args.validation_resolution)
            validation_resolution_is_float = True
        except ValueError:
            pass
    validation_resolution_is_digit = False
    try:
        int(args.validation_resolution)
        validation_resolution_is_digit = True
    except ValueError:
        pass
    if (
        (validation_resolution_is_digit or validation_resolution_is_float)
        and int(args.validation_resolution) &lt; 128
        and &quot;deepfloyd&quot; not in args.model_type
    ):
        # Convert from megapixels to pixels:
        log_msg = f&quot;It seems that --validation_resolution was given in megapixels ({args.validation_resolution}). Converting to pixel measurement:&quot;
        if int(args.validation_resolution) == 1:
            args.validation_resolution = 1024
        else:
            args.validation_resolution = int(int(args.validation_resolution) * 1e3)
            # Make it divisible by 8:
            args.validation_resolution = int(int(args.validation_resolution) / 8) * 8
        info_log(f&quot;{log_msg} {int(args.validation_resolution)}px&quot;)
    if args.timestep_bias_portion &lt; 0.0 or args.timestep_bias_portion &gt; 1.0:
        raise ValueError(&quot;Timestep bias portion must be between 0.0 and 1.0.&quot;)
    if args.controlnet and &quot;lora&quot; in args.model_type:
        raise ValueError(&quot;ControlNet is not supported for LoRA models.&quot;)
    if args.metadata_update_interval &lt; 60:
        raise ValueError(&quot;Metadata update interval must be at least 60 seconds.&quot;)
    if args.model_family == &quot;sd3&quot;:
        args.pretrained_vae_model_name_or_path = None
        args.disable_compel = True
    t5_max_length = 154
    if args.model_family == &quot;sd3&quot; and (
        args.tokenizer_max_length is None
        or int(args.tokenizer_max_length) &gt; t5_max_length
    ):
        if not args.i_know_what_i_am_doing:
            warning_log(
                f&quot;Updating T5 XXL tokeniser max length to {t5_max_length} for SD3.&quot;
            )
            args.tokenizer_max_length = t5_max_length
        else:
            warning_log(
                f&quot;-!- SD3 supports a max length of {t5_max_length} tokens, but you have supplied `--i_know_what_i_am_doing`, so this limit will not be enforced. -!-&quot;
            )
            warning_log(
                f&quot;The model will begin to collapse after a short period of time, if the model you are continuing from has not been tuned beyond {t5_max_length} tokens.&quot;
            )
    flux_version = &quot;dev&quot;
    model_max_seq_length = 512
    if (
        &quot;schnell&quot; in args.pretrained_model_name_or_path.lower()
        or args.flux_fast_schedule
    ):
        if not args.flux_fast_schedule and not args.i_know_what_i_am_doing:
            error_log(
                &quot;Schnell requires --flux_fast_schedule (or --i_know_what_i_am_doing).&quot;
            )
            sys.exit(1)
        flux_version = &quot;schnell&quot;
        model_max_seq_length = 256
    if args.model_family == &quot;flux&quot;:
        if (
            args.tokenizer_max_length is None
            or int(args.tokenizer_max_length) &gt; model_max_seq_length
        ):
            if not args.i_know_what_i_am_doing:
                warning_log(
                    f&quot;Updating T5 XXL tokeniser max length to {model_max_seq_length} for Flux.&quot;
                )
                args.tokenizer_max_length = model_max_seq_length
            else:
                warning_log(
                    f&quot;-!- Flux supports a max length of {model_max_seq_length} tokens, but you have supplied `--i_know_what_i_am_doing`, so this limit will not be enforced. -!-&quot;
                )
                warning_log(
                    f&quot;The model will begin to collapse after a short period of time, if the model you are continuing from has not been tuned beyond 256 tokens.&quot;
                )
        if flux_version == &quot;dev&quot;:
            if args.validation_num_inference_steps &gt; 28:
                warning_log(
                    &quot;Flux Dev expects around 28 or fewer inference steps. Consider limiting --validation_num_inference_steps to 28.&quot;
                )
            if args.validation_num_inference_steps &lt; 15:
                warning_log(
                    &quot;Flux Dev expects around 15 or more inference steps. Consider increasing --validation_num_inference_steps to 15.&quot;
                )
        if flux_version == &quot;schnell&quot; and args.validation_num_inference_steps &gt; 4:
            warning_log(
                &quot;Flux Schnell requires fewer inference steps. Consider reducing --validation_num_inference_steps to 4.&quot;
            )
        if args.flux_guidance_mode == &quot;mobius&quot;:
            warning_log(
                &quot;Mobius training is only for the most elite. Pardon my English, but this is not for those who don&apos;t like to destroy something beautiful every now and then. If you feel perhaps this is not for you, please consider using a different guidance mode.&quot;
            )
            if args.flux_guidance_min &lt; 1.0:
                warning_log(
                    &quot;Flux minimum guidance value for Mobius training is 1.0. Updating value..&quot;
                )
                args.flux_guidance_min = 1.0
    if args.use_ema and args.ema_cpu_only:
        args.ema_device = &quot;cpu&quot;
    if (args.optimizer_beta1 is not None and args.optimizer_beta2 is None) or (
        args.optimizer_beta1 is None and args.optimizer_beta2 is not None
    ):
        error_log(&quot;Both --optimizer_beta1 and --optimizer_beta2 should be provided.&quot;)
        sys.exit(1)
    if not args.i_know_what_i_am_doing:
        if args.model_family == &quot;pixart_sigma&quot;:
            if args.max_grad_norm is None or float(args.max_grad_norm) &gt; 0.01:
                warning_log(
                    f&quot;PixArt Sigma requires --max_grad_norm=0.01 to prevent model collapse. Overriding value. Set this value manually to disable this warning.&quot;
                )
                args.max_grad_norm = 0.01
    if args.gradient_checkpointing:
        # enable torch compile w/ activation checkpointing :[ slows us down.
        torch._dynamo.config.optimize_ddp = False
    args.logging_dir = os.path.join(args.output_dir, args.logging_dir)
    args.accelerator_project_config = ProjectConfiguration(
        project_dir=args.output_dir, logging_dir=args.logging_dir
    )
    # Create the custom configuration
    args.process_group_kwargs = InitProcessGroupKwargs(
        timeout=timedelta(seconds=5400)
    )  # 1.5 hours
    # Enable TF32 for faster training on Ampere GPUs,
    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        if args.disable_tf32:
            warning_log(
                &quot;--disable_tf32 is provided, not enabling. Training will potentially be much slower.&quot;
            )
            torch.backends.cuda.matmul.allow_tf32 = False
            torch.backends.cudnn.allow_tf32 = False
        else:
            info_log(
                &quot;Enabled NVIDIA TF32 for faster training on Ampere GPUs. Use --disable_tf32 if this causes any problems.&quot;
            )
    args.is_quantized = (
        False
        if (args.base_model_precision == &quot;no_change&quot; or &quot;lora&quot; not in args.model_type)
        else True
    )
    args.weight_dtype = (
        torch.bfloat16
        if (
            args.mixed_precision == &quot;bf16&quot;
            or (args.base_model_default_dtype == &quot;bf16&quot; and args.is_quantized)
        )
        else torch.float16 if args.mixed_precision == &quot;fp16&quot; else torch.float32
    )
    args.disable_accelerator = os.environ.get(&quot;SIMPLETUNER_DISABLE_ACCELERATOR&quot;, False)
    if &quot;lycoris&quot; == args.lora_type.lower():
        from lycoris import create_lycoris
        if args.lycoris_config is None:
            raise ValueError(
                &quot;--lora_type=lycoris requires you to add a JSON &quot;
                + &quot;configuration file location with --lycoris_config&quot;
            )
        # is it readable?
        if not os.path.isfile(args.lycoris_config) or not os.access(
            args.lycoris_config, os.R_OK
        ):
            raise ValueError(
                f&quot;Could not find the JSON configuration file at &apos;{args.lycoris_config}&apos;&quot;
            )
        import json
        with open(args.lycoris_config, &quot;r&quot;) as f:
            lycoris_config = json.load(f)
        assert &quot;algo&quot; in lycoris_config, &quot;lycoris_config JSON must contain algo key&quot;
        assert (
            &quot;multiplier&quot; in lycoris_config
        ), &quot;lycoris_config JSON must contain multiplier key&quot;
        assert (
            &quot;linear_dim&quot; in lycoris_config
        ), &quot;lycoris_config JSON must contain linear_dim key&quot;
        assert (
            &quot;linear_alpha&quot; in lycoris_config
        ), &quot;lycoris_config JSON must contain linear_alpha key&quot;
    elif &quot;standard&quot; == args.lora_type.lower():
        if hasattr(args, &quot;lora_init_type&quot;) and args.lora_init_type is not None:
            if torch.backends.mps.is_available() and args.lora_init_type == &quot;loftq&quot;:
                error_log(
                    &quot;Apple MPS cannot make use of LoftQ initialisation. Overriding to &apos;default&apos;.&quot;
                )
            elif args.is_quantized and args.lora_init_type == &quot;loftq&quot;:
                error_log(
                    &quot;LoftQ initialisation is not supported with quantised models. Overriding to &apos;default&apos;.&quot;
                )
            else:
                args.lora_initialisation_style = (
                    args.lora_init_type if args.lora_init_type != &quot;default&quot; else True
                )
        if args.use_dora:
            if &quot;quanto&quot; in args.base_model_precision:
                error_log(
                    &quot;Quanto does not yet support DoRA training in PEFT. Disabling DoRA. 😴&quot;
                )
                args.use_dora = False
            else:
                warning_log(
                    &quot;DoRA support is experimental and not very thoroughly tested.&quot;
                )
                args.lora_initialisation_style = &quot;default&quot;
    if not args.data_backend_config:
        from helpers.training.state_tracker import StateTracker
        args.data_backend_config = os.path.join(
            StateTracker.get_config_path(), &quot;multidatabackend.json&quot;
        )
        warning_log(
            f&quot;No data backend config provided. Using default config at {args.data_backend_config}.&quot;
        )
    # Check if we have a valid gradient accumulation steps.
    if args.gradient_accumulation_steps &lt; 1:
        raise ValueError(
            f&quot;Invalid gradient_accumulation_steps parameter: {args.gradient_accumulation_steps}, should be &gt;= 1&quot;
        )
    if args.validation_guidance_skip_layers is not None:
        try:
            import json
            args.validation_guidance_skip_layers = json.loads(
                args.validation_guidance_skip_layers
            )
        except Exception as e:
            logger.error(f&quot;Could not load skip layers: {e}&quot;)
            raise
    if args.framerate is None:
        if args.model_family == &quot;ltxvideo&quot;:
            args.framerate = 25
    if (
        args.sana_complex_human_instruction is not None
        and type(args.sana_complex_human_instruction) is str
        and args.sana_complex_human_instruction not in [&quot;&quot;, &quot;None&quot;]
    ):
        try:
            import json
            args.sana_complex_human_instruction = json.loads(
                args.sana_complex_human_instruction
            )
        except Exception as e:
            logger.error(
                f&quot;Could not load complex human instruction ({args.sana_complex_human_instruction}): {e}&quot;
            )
            raise
    elif args.sana_complex_human_instruction == &quot;None&quot;:
        args.sana_complex_human_instruction = None
    if args.enable_xformers_memory_efficient_attention:
        if args.attention_mechanism != &quot;xformers&quot;:
            warning_log(
                &quot;The option --enable_xformers_memory_efficient_attention is deprecated. Please use --attention_mechanism=xformers instead.&quot;
            )
            args.attention_mechanism = &quot;xformers&quot;
    if args.attention_mechanism != &quot;diffusers&quot; and not torch.cuda.is_available():
        warning_log(
            &quot;For non-CUDA systems, only Diffusers attention mechanism is officially supported.&quot;
        )
    deprecated_options = {
        &quot;flux_beta_schedule_alpha&quot;: &quot;flow_beta_schedule_alpha&quot;,
        &quot;flux_beta_schedule_beta&quot;: &quot;flow_beta_schedule_beta&quot;,
        &quot;flux_use_beta_schedule&quot;: &quot;flow_use_beta_schedule&quot;,
        &quot;flux_use_uniform_schedule&quot;: &quot;flow_use_uniform_schedule&quot;,
        &quot;flux_schedule_shift&quot;: &quot;flow_schedule_shift&quot;,
        &quot;flux_schedule_auto_shift&quot;: &quot;flow_schedule_auto_shift&quot;,
        &quot;flow_matching_sigmoid_scale&quot;: &quot;flow_sigmoid_scale&quot;,
    }
    for deprecated_option, replacement_option in deprecated_options.items():
        if (
            getattr(args, replacement_option) is not None
            and getattr(args, deprecated_option) is not None
            and type(getattr(args, deprecated_option)) is not object
        ):
            warning_log(
                f&quot;The option --{deprecated_option} has been replaced with --{replacement_option}.&quot;
            )
            setattr(args, replacement_option, getattr(args, deprecated_option))
        elif getattr(args, deprecated_option) is not None:
            error_log(
                f&quot;The option {deprecated_option} has been deprecated without a replacement option. Please remove it from your configuration.&quot;
            )
            sys.exit(1)
    return args</file><file path="helpers/configuration/env_file.py">import json
env_to_args_map = {
    &quot;RESUME_CHECKPOINT&quot;: &quot;--resume_from_checkpoint&quot;,
    &quot;DATALOADER_CONFIG&quot;: &quot;--data_backend_config&quot;,
    &quot;ASPECT_BUCKET_ROUNDING&quot;: &quot;--aspect_bucket_rounding&quot;,
    &quot;TRAINING_SEED&quot;: &quot;--seed&quot;,
    &quot;USE_EMA&quot;: &quot;--use_ema&quot;,
    &quot;USE_XFORMERS&quot;: &quot;--enable_xformers_memory_efficient_attention&quot;,
    &quot;MINIMUM_RESOLUTION&quot;: &quot;--minimum_image_size&quot;,
    &quot;OUTPUT_DIR&quot;: &quot;--output_dir&quot;,
    &quot;USE_DORA&quot;: &quot;--use_dora&quot;,
    &quot;USE_BITFIT&quot;: &quot;--layer_freeze_strategy=bitfit&quot;,
    &quot;LORA_TYPE&quot;: &quot;--lora_type&quot;,
    &quot;LYCORIS_CONFIG&quot;: &quot;--lycoris_config&quot;,
    &quot;PUSH_TO_HUB&quot;: &quot;--push_to_hub&quot;,
    &quot;PUSH_CHECKPOINTS&quot;: &quot;--push_checkpoints_to_hub&quot;,
    &quot;MAX_NUM_STEPS&quot;: &quot;--max_train_steps&quot;,
    &quot;NUM_EPOCHS&quot;: &quot;--num_train_epochs&quot;,
    &quot;CHECKPOINTING_STEPS&quot;: &quot;--checkpointing_steps&quot;,
    &quot;CHECKPOINTING_LIMIT&quot;: &quot;--checkpoints_total_limit&quot;,
    &quot;HUB_MODEL_NAME&quot;: &quot;--hub_model_id&quot;,
    &quot;MODEL_CARD_SAFE_FOR_WORK&quot;: &quot;--model_card_safe_for_work&quot;,
    &quot;TRACKER_PROJECT_NAME&quot;: &quot;--tracker_project_name&quot;,
    &quot;TRACKER_RUN_NAME&quot;: &quot;--tracker_run_name&quot;,
    &quot;MODEL_TYPE&quot;: &quot;--model_type&quot;,
    &quot;MODEL_NAME&quot;: &quot;--pretrained_model_name_or_path&quot;,
    &quot;MODEL_FAMILY&quot;: &quot;--model_family&quot;,
    &quot;TRAIN_BATCH_SIZE&quot;: &quot;--train_batch_size&quot;,
    &quot;USE_GRADIENT_CHECKPOINTING&quot;: &quot;--gradient_checkpointing&quot;,
    &quot;CAPTION_DROPOUT_PROBABILITY&quot;: &quot;--caption_dropout_probability&quot;,
    &quot;RESOLUTION_TYPE&quot;: &quot;--resolution_type&quot;,
    &quot;RESOLUTION&quot;: &quot;--resolution&quot;,
    &quot;VALIDATION_SEED&quot;: &quot;--validation_seed&quot;,
    &quot;VALIDATION_STEPS&quot;: &quot;--validation_steps&quot;,
    &quot;VALIDATION_RESOLUTION&quot;: &quot;--validation_resolution&quot;,
    &quot;VALIDATION_GUIDANCE&quot;: &quot;--validation_guidance&quot;,
    &quot;VALIDATION_GUIDANCE_RESCALE&quot;: &quot;--validation_guidance_rescale&quot;,
    &quot;VALIDATION_NUM_INFERENCE_STEPS&quot;: &quot;--validation_num_inference_steps&quot;,
    &quot;VALIDATION_PROMPT&quot;: &quot;--validation_prompt&quot;,
    &quot;ALLOW_TF32&quot;: &quot;--allow_tf32&quot;,
    &quot;MIXED_PRECISION&quot;: &quot;--mixed_precision&quot;,
    &quot;OPTIMIZER&quot;: &quot;--optimizer&quot;,
    &quot;LEARNING_RATE&quot;: &quot;--learning_rate&quot;,
    &quot;LR_SCHEDULE&quot;: &quot;--lr_scheduler&quot;,
    &quot;LR_WARMUP_STEPS&quot;: &quot;--lr_warmup_steps&quot;,
    &quot;BASE_MODEL_PRECISION&quot;: &quot;--base_model_precision&quot;,
    &quot;TRAINING_NUM_PROCESSES&quot;: &quot;--num_processes&quot;,
    &quot;TRAINING_NUM_MACHINES&quot;: &quot;--num_machines&quot;,
    &quot;VALIDATION_TORCH_COMPILE&quot;: &quot;--validation_torch_compile&quot;,
    &quot;TRAINER_DYNAMO_BACKEND&quot;: &quot;--dynamo_backend&quot;,
    &quot;VALIDATION_GUIDANCE_REAL&quot;: &quot;--validation_guidance_real&quot;,
    &quot;VALIDATION_NO_CFG_UNTIL_TIMESTEP&quot;: &quot;--validation_no_cfg_until_timestep&quot;,
    &quot;TRAINING_SCHEDULER_TIMESTEP_SPACING&quot;: &quot;--training_scheduler_timestep_spacing&quot;,
    &quot;INFERENCE_SCHEDULER_TIMESTEP_SPACING&quot;: &quot;--inference_scheduler_timestep_spacing&quot;,
    &quot;GRADIENT_ACCUMULATION_STEPS&quot;: &quot;--gradient_accumulation_steps&quot;,
    &quot;TRAINING_DYNAMO_BACKEND&quot;: &quot;--dynamo_backend&quot;,
    &quot;LR_END&quot;: &quot;--lr_end&quot;,
    &quot;FLUX_GUIDANCE_VALUE&quot;: &quot;--flux_guidance_value&quot;,
    &quot;FLUX_LORA_TARGET&quot;: &quot;--flux_lora_target&quot;,
    &quot;VALIDATION_NEGATIVE_PROMPT&quot;: &quot;--validation_negative_prompt&quot;,
    &quot;METADATA_UPDATE_INTERVAL&quot;: &quot;--metadata_update_interval&quot;,
    &quot;READ_BATCH_SIZE&quot;: &quot;--read_batch_size&quot;,
    &quot;WRITE_BATCH_SIZE&quot;: &quot;--write_batch_size&quot;,
    &quot;AWS_MAX_POOL_CONNECTIONS&quot;: &quot;--aws_max_pool_connections&quot;,
    &quot;TORCH_NUM_THREADS&quot;: &quot;--torch_num_threads&quot;,
    &quot;IMAGE_PROCESSING_BATCH_SIZE&quot;: &quot;--image_processing_batch_size&quot;,
    &quot;DISABLE_BENCHMARK&quot;: &quot;--disable_benchmark&quot;,
}
import os
import subprocess
import logging
from helpers.training.multi_process import should_log
logger = logging.getLogger(&quot;SimpleTuner&quot;)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def load_env():
    &quot;&quot;&quot;
    Load environment variables from .env files based on the specified environment.
    &quot;&quot;&quot;
    # Define the paths to the default and environment-specific .env files
    config_env_path = &quot;config/config.env&quot;
    env = os.environ.get(
        &quot;SIMPLETUNER_ENVIRONMENT&quot;,
        os.environ.get(&quot;SIMPLETUNER_ENV&quot;, os.environ.get(&quot;ENV&quot;, None)),
    )
    if env and env != &quot;default&quot;:
        config_env_path = f&quot;config/{env}/config.env&quot;
    # Load default environment variables if the file exists
    config_file_contents = {}
    if os.path.isfile(config_env_path):
        # Loop through, ignoring comments &apos;#&apos; and empty lines, while setting the env variables
        with open(config_env_path, &quot;r&quot;) as f:
            for line in f:
                # Skip comments and empty lines
                if line.startswith(&quot;#&quot;) or line.strip() == &quot;&quot;:
                    continue
                # Remove &apos;export&apos; from the start
                if line.startswith(&quot;export&quot;):
                    line = line[7:]
                # Handle `+=` for appending values
                if &quot;+=&quot; in line:
                    key, value = line.strip().split(&quot;+=&quot;, 1)
                    key, value = (
                        key.strip(),
                        value.strip(&apos;&quot;&apos;).strip(&quot;&apos;&quot;).strip().split(),
                    )
                    # Append each element to the existing key&apos;s list or create a new list
                    if key in config_file_contents:
                        config_file_contents[key].extend(value)
                    else:
                        config_file_contents[key] = value
                else:
                    # Regular `=` assignment
                    c = line.strip().split(&quot;=&quot;, 1)
                    if len(c) == 2:
                        key, value = c
                        config_file_contents[key.strip()] = (
                            value.strip(&apos;&quot;&apos;).strip(&quot;&apos;&quot;).split()
                        )
        # Convert lists to single string values with spaces, if needed
        for key, value in config_file_contents.items():
            if isinstance(value, list):
                if value and &quot;${&quot; in value[0]:
                    continue
                config_file_contents[key] = &quot; &quot;.join(value)
        logger.info(f&quot;[CONFIG.ENV] Loaded environment variables from {config_env_path}&quot;)
    else:
        logger.error(f&quot;Cannot find config file: {config_env_path}&quot;)
    return config_file_contents
def load_env_config():
    &quot;&quot;&quot;
    Map the environment variables to command-line arguments.
    :return: List of command-line arguments.
    &quot;&quot;&quot;
    config_file_contents = load_env()
    mapped_args = []
    # Loop through the environment variable to argument mapping
    ignored_accelerate_kwargs = [
        &quot;--num_processes&quot;,
        &quot;--num_machines&quot;,
        &quot;--dynamo_backend&quot;,
    ]
    for env_var, arg_name in env_to_args_map.items():
        if arg_name in ignored_accelerate_kwargs:
            continue
        value = config_file_contents.get(env_var, None)
        # strip &apos;s from the outside of value
        if value is not None and value.startswith(&quot;&apos;&quot;) and value.endswith(&quot;&apos;&quot;):
            value = value[1:-1]
        if value is not None and value.startswith(&apos;&quot;&apos;) and value.endswith(&apos;&quot;&apos;):
            value = value[1:-1]
        is_numeric = (
            str(value).isnumeric()
            or str(value).isdigit()
            or str(value).replace(&quot;.&quot;, &quot;&quot;).isdigit()
        )
        if value is not None:
            # Handle booleans by checking their string value
            if value.lower() in [&quot;true&quot;, &quot;false&quot;]:
                if value.lower() == &quot;true&quot;:
                    mapped_args.append(f&quot;{arg_name}&quot;)
            elif is_numeric:
                # Handle numeric values
                mapped_args.append(f&quot;{arg_name}={value}&quot;)
            else:
                # Add the argument and its value to the list
                mapped_args.append(f&quot;{arg_name}={value}&quot;)
    # handle TRAINER_EXTRA_ARGS, which is like `TRAINER_EXTRA_ARGS=&quot;--num_processes=1 --num_machines=1 --dynamo_backend=local&quot;`
    extra_args = config_file_contents.get(&quot;TRAINER_EXTRA_ARGS&quot;, None)
    if extra_args:
        print(f&quot;Extra args: {extra_args}&quot;)
        if type(extra_args) is list:
            for value in extra_args:
                if &quot;${&quot; in value:
                    continue
                mapped_args.extend(value.split())
        else:
            mapped_args.extend(extra_args.split())
    logger.info(f&quot;Loaded environment variables: {json.dumps(mapped_args, indent=4)}&quot;)
    return mapped_args</file><file path="helpers/configuration/json_file.py">import os
import json
import logging
# Set up logging
from helpers.training.multi_process import _get_rank
logger = logging.getLogger(&quot;SimpleTuner&quot;)
if _get_rank() &gt; 0:
    logger.setLevel(logging.WARNING)
else:
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def normalize_args(args_dict):
    &quot;&quot;&quot;
    Normalize arguments, ensuring they have &apos;--&apos; at the start if necessary.
    :param args_dict: A dictionary of arguments that may or may not have &apos;--&apos; prefixes.
    :return: A normalized dictionary of arguments.
    &quot;&quot;&quot;
    normalized = []
    for key, value in args_dict.items():
        # Add -- prefix if not present
        if (type(value) is bool and value) or value == &quot;true&quot;:
            if not key.startswith(&quot;--&quot;):
                normalized_key = f&quot;--{key}&quot;
            else:
                normalized_key = key
        elif type(value) is bool and not value or value == &quot;false&quot;:
            logger.warning(f&quot;Skipping false argument: {key}&quot;)
            continue
        else:
            if not key.startswith(&quot;--&quot;):
                normalized_key = f&quot;--{key}={value}&quot;
            else:
                normalized_key = f&quot;{key}={value}&quot;
        normalized.append(normalized_key)
    return normalized
def load_json_config():
    &quot;&quot;&quot;
    Load configuration from a JSON file that directly specifies command-line arguments.
    :param json_path: The path to the JSON file.
    :return: A dictionary containing the configuration.
    &quot;&quot;&quot;
    config_json_path = &quot;config/config.json&quot;
    env = os.environ.get(
        &quot;SIMPLETUNER_ENVIRONMENT&quot;,
        os.environ.get(&quot;SIMPLETUNER_ENV&quot;, os.environ.get(&quot;ENV&quot;, None)),
    )
    if env and env != &quot;default&quot;:
        config_json_path = f&quot;config/{env}/config.json&quot;
    if not os.path.isfile(config_json_path):
        raise ValueError(f&quot;JSON configuration file not found: {config_json_path}&quot;)
    with open(config_json_path, &quot;r&quot;) as file:
        try:
            config = json.load(file)
            logger.info(f&quot;[CONFIG.JSON] Loaded configuration from {config_json_path}&quot;)
            return normalize_args(config)
        except json.JSONDecodeError as e:
            raise ValueError(f&quot;Failed to parse JSON file {config_json_path}: {e}&quot;)</file><file path="helpers/configuration/loader.py">import os
import logging
from helpers.configuration import toml_file, json_file, env_file, cmd_args
from helpers.training.state_tracker import StateTracker
import sys
logger = logging.getLogger(&quot;SimpleTuner&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
helpers = {
    &quot;json&quot;: json_file.load_json_config,
    &quot;toml&quot;: toml_file.load_toml_config,
    &quot;env&quot;: env_file.load_env_config,
    &quot;cmd&quot;: cmd_args.parse_cmdline_args,
}
default_config_paths = {
    &quot;json&quot;: &quot;config.json&quot;,
    &quot;toml&quot;: &quot;config.toml&quot;,
    &quot;env&quot;: &quot;config.env&quot;,
}
def attach_env_to_path_if_not_present(backend: str, env: str = None):
    backend_cfg_path = default_config_paths.get(backend)
    if env and env != &quot;default&quot;:
        return f&quot;config/{env}/{backend_cfg_path}&quot;
    return f&quot;config/{backend_cfg_path}&quot;
def load_config(args: dict = None, exit_on_error: bool = False):
    # Check if help is requested; bypass configuration loading if true
    if &quot;-h&quot; in sys.argv or &quot;--help&quot; in sys.argv:
        return helpers[&quot;cmd&quot;]()
    mapped_config = args
    if mapped_config is None or not mapped_config:
        config_backend = os.environ.get(
            &quot;SIMPLETUNER_CONFIG_BACKEND&quot;,
            os.environ.get(&quot;CONFIG_BACKEND&quot;, os.environ.get(&quot;CONFIG_TYPE&quot;, &quot;env&quot;)),
        ).lower()
        config_env = os.environ.get(
            &quot;SIMPLETUNER_ENVIRONMENT&quot;,
            os.environ.get(&quot;SIMPLETUNER_ENV&quot;, os.environ.get(&quot;ENV&quot;, &quot;default&quot;)),
        )
        config_backend_path = &quot;config&quot;
        if config_env and config_env != &quot;default&quot; and config_env is not None:
            config_backend_path = os.path.join(&quot;config&quot;, config_env)
        StateTracker.set_config_path(config_backend_path)
        logger.info(&quot;Using {} configuration backend.&quot;.format(config_backend))
        mapped_config = helpers[config_backend]()
        if config_backend == &quot;cmd&quot;:
            return mapped_config
    # Other configs need to be passed through parse_cmdline_args to be made whole and have complete defaults and safety checks applied.
    configuration = helpers[&quot;cmd&quot;](
        input_args=mapped_config, exit_on_error=exit_on_error
    )
    return configuration</file><file path="helpers/configuration/toml_file.py">import os
import toml
import logging
# Set up logging
from helpers.training.multi_process import _get_rank
logger = logging.getLogger(&quot;SimpleTuner&quot;)
if _get_rank() &gt; 0:
    logger.setLevel(logging.WARNING)
else:
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def normalize_args(args_dict):
    &quot;&quot;&quot;
    Normalize arguments, ensuring they have &apos;--&apos; at the start if necessary.
    :param args_dict: A dictionary of arguments that may or may not have &apos;--&apos; prefixes.
    :return: A normalized dictionary of arguments.
    &quot;&quot;&quot;
    normalized = []
    for key, value in args_dict.items():
        # Add -- prefix if not present
        if type(value) is bool and value or value == &quot;true&quot;:
            if not key.startswith(&quot;--&quot;):
                normalized_key = f&quot;--{key}&quot;
            else:
                normalized_key = key
        elif type(value) is bool and not value or value == &quot;false&quot;:
            logger.warning(f&quot;Skipping false argument: {key}&quot;)
            continue
        else:
            print(f&quot;Value: {value}, type: {type(value)}&quot;)
            if not key.startswith(&quot;--&quot;):
                normalized_key = f&quot;--{key}={value}&quot;
            else:
                normalized_key = f&quot;{key}={value}&quot;
        normalized.append(normalized_key)
    return normalized
def load_toml_config():
    &quot;&quot;&quot;
    Load configuration from a TOML file that directly specifies command-line arguments.
    :param toml_path: The path to the TOML file.
    :return: A dictionary containing the configuration.
    &quot;&quot;&quot;
    config_toml_path = &quot;config/config.toml&quot;
    env = os.environ.get(
        &quot;SIMPLETUNER_ENVIRONMENT&quot;,
        os.environ.get(&quot;SIMPLETUNER_ENV&quot;, os.environ.get(&quot;ENV&quot;, None)),
    )
    if env and env != &quot;default&quot;:
        config_toml_path = f&quot;config/{env}/config.toml&quot;
    if not os.path.isfile(config_toml_path):
        raise ValueError(f&quot;Can not find config file: {config_toml_path}&quot;)
    with open(config_toml_path, &quot;r&quot;) as file:
        try:
            config = toml.load(file)
            logger.info(f&quot;[CONFIG.TOML] Loaded configuration from {config_toml_path}&quot;)
            toml_config = config
        except toml.TomlDecodeError as e:
            logger.error(f&quot;Failed to parse TOML file {config_toml_path}: {e}&quot;)
            toml_config = {}
    normalized_config = normalize_args(toml_config)
    logger.info(
        f&quot;[CONFIG] Loaded and normalized TOML configuration: {normalized_config}&quot;
    )
    return normalized_config</file><file path="helpers/data_backend/aws.py">import boto3
import os
from os.path import splitext
import time
from botocore.exceptions import (
    NoCredentialsError,
    PartialCredentialsError,
)
import fnmatch
import logging
import torch
from torch import Tensor
import concurrent.futures
from botocore.config import Config
from helpers.data_backend.base import BaseDataBackend
from helpers.training.multi_process import _get_rank as get_rank
from helpers.image_manipulation.load import load_image, load_video
from helpers.training import video_file_extensions, image_file_extensions
from io import BytesIO
from typing import Union
loggers_to_silence = [
    &quot;botocore.hooks&quot;,
    &quot;botocore.auth&quot;,
    &quot;botocore.httpsession&quot;,
    &quot;botocore.parsers&quot;,
    &quot;botocore.retryhandler&quot;,
    &quot;botocore.loaders&quot;,
    &quot;botocore.regions&quot;,
    &quot;botocore.utils&quot;,
    &quot;botocore.client&quot;,
    &quot;botocore.handler&quot;,
    &quot;botocore.handlers&quot;,
    &quot;botocore.awsrequest&quot;,
]
for logger_name in loggers_to_silence:
    logger = logging.getLogger(logger_name)
    logger.setLevel(&quot;ERROR&quot;)
boto_logger = logging.getLogger(&quot;botocore.endpoint&quot;)
boto_logger.setLevel(os.environ.get(&quot;SIMPLETUNER_AWS_LOG_LEVEL&quot;, &quot;ERROR&quot;))
logger = logging.getLogger(&quot;S3DataBackend&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class S3DataBackend(BaseDataBackend):
    # Storing the list_files output in a local dict.
    _list_cache: dict = {}
    def __init__(
        self,
        id: str,
        bucket_name,
        accelerator,
        region_name=&quot;us-east-1&quot;,
        endpoint_url: str = None,
        aws_access_key_id: str = None,
        aws_secret_access_key: str = None,
        read_retry_limit: int = 5,
        write_retry_limit: int = 5,
        read_retry_interval: int = 5,
        write_retry_interval: int = 5,
        compress_cache: bool = False,
        max_pool_connections: int = 128,
    ):
        self.id = id
        self.accelerator = accelerator
        self.bucket_name = bucket_name
        self.read_retry_limit = read_retry_limit
        self.read_retry_interval = read_retry_interval
        self.write_retry_limit = write_retry_limit
        self.write_retry_interval = write_retry_interval
        self.compress_cache = compress_cache
        self.max_pool_connections = max_pool_connections
        self.type = &quot;aws&quot;
        extra_args = {&quot;region_name&quot;: region_name}
        if endpoint_url:
            # If using endpoint_url, we do not use region
            extra_args = {&quot;endpoint_url&quot;: endpoint_url}
        s3_config = Config(max_pool_connections=self.max_pool_connections)
        self.client = boto3.client(
            &quot;s3&quot;,
            aws_access_key_id=aws_access_key_id,
            aws_secret_access_key=aws_secret_access_key,
            config=s3_config,
            **extra_args,
        )
    def exists(self, s3_key):
        &quot;&quot;&quot;Check if the key exists in S3, with retries for transient errors.&quot;&quot;&quot;
        for i in range(self.read_retry_limit):
            try:
                self.client.head_object(Bucket=self.bucket_name, Key=str(s3_key))
                return True
            except self.client.exceptions.NoSuchKey:
                logger.debug(
                    f&quot;File {s3_key} does not exist in S3 bucket ({self.bucket_name})&quot;
                )
                return False
            except (NoCredentialsError, PartialCredentialsError) as e:
                raise e  # Raise credential errors to the caller
            except Exception as e:
                if (
                    &quot;An error occurred (404) when calling the HeadObject operation: Not Found&quot;
                    in str(e)
                ):
                    return False
                logger.error(f&apos;Error checking existence of S3 key &quot;{s3_key}&quot;: {e}&apos;)
                if i == self.read_retry_limit - 1:
                    raise e
                else:
                    time.sleep(self.read_retry_interval)
    def read(self, s3_key):
        &quot;&quot;&quot;Retrieve and return the content of the file from S3.&quot;&quot;&quot;
        for i in range(self.read_retry_limit):
            try:
                response = self.client.get_object(
                    Bucket=self.bucket_name, Key=str(s3_key)
                )
                return response[&quot;Body&quot;].read()
            except self.client.exceptions.NoSuchKey:
                logger.debug(
                    f&quot;File {s3_key} does not exist in S3 bucket ({self.bucket_name})&quot;
                )
                return None
            except (NoCredentialsError, PartialCredentialsError) as e:
                raise e  # Raise credential errors to the caller
            except Exception as e:
                logger.error(f&apos;Error reading S3 key &quot;{s3_key}&quot;: {e}&apos;)
                if i == self.read_retry_limit - 1:
                    raise e
                else:
                    time.sleep(self.read_retry_interval)
    def open_file(self, s3_key, mode):
        &quot;&quot;&quot;Open the file in the specified mode.&quot;&quot;&quot;
        return self.read(s3_key)
    def write(self, s3_key, data):
        &quot;&quot;&quot;
        Upload data to the specified S3 key.
        &quot;&quot;&quot;
        real_key = str(s3_key)
        for i in range(self.write_retry_limit):
            try:
                if isinstance(data, (dict, torch.Tensor)):
                    return self.torch_save(data, real_key)
                response = self.client.put_object(
                    Body=data, Bucket=self.bucket_name, Key=real_key
                )
                return response
            except Exception as e:
                logger.error(f&apos;Error writing S3 key &quot;{real_key}&quot;: {e}&apos;)
                if i == self.write_retry_limit - 1:
                    raise e
                else:
                    time.sleep(self.write_retry_interval)
    def delete(self, s3_key):
        &quot;&quot;&quot;Delete the specified file from S3.&quot;&quot;&quot;
        for i in range(self.write_retry_limit):
            try:
                logger.debug(f&apos;Deleting S3 key &quot;{s3_key}&quot;&apos;)
                response = self.client.delete_object(
                    Bucket=self.bucket_name, Key=str(s3_key)
                )
                return response
            except Exception as e:
                logger.error(f&apos;Error deleting S3 key &quot;{s3_key}&quot;: {e}&apos;)
                if i == self.write_retry_limit - 1:
                    raise e
                else:
                    time.sleep(self.write_retry_interval)
    def list_by_prefix(self, prefix=&quot;&quot;):
        &quot;&quot;&quot;List all files under a specific path (prefix) in the S3 bucket.&quot;&quot;&quot;
        response = self.client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
        bucket_prefix = f&quot;{self.bucket_name}/&quot;
        return [
            (
                item[&quot;Key&quot;][len(bucket_prefix) :]
                if item[&quot;Key&quot;].startswith(bucket_prefix)
                else item[&quot;Key&quot;]
            )
            for item in response.get(&quot;Contents&quot;, [])
        ]
    def list_files(self, file_extensions: list, instance_data_dir: str = None):
        results = []
        def splitext_(path):
            ext = splitext(path)[1].lower()
            return ext[1:] if ext else ext
        start_time = time.time()
        paginator = self.client.get_paginator(&quot;list_objects_v2&quot;)
        prefix_dict = {}
        logger.debug(
            f&quot;Listing files in S3 bucket {self.bucket_name} in prefix {instance_data_dir} with extensions: {file_extensions}&quot;
        )
        for page in paginator.paginate(Bucket=self.bucket_name, MaxKeys=1000):
            for obj in page.get(&quot;Contents&quot;, []):
                ext = splitext_(obj[&quot;Key&quot;])
                if file_extensions and ext not in file_extensions:
                    continue
                parts = obj[&quot;Key&quot;].split(&quot;/&quot;)
                subdir = &quot;/&quot;.join(parts[:-1])
                filename = parts[-1]
                if subdir not in prefix_dict:
                    prefix_dict[subdir] = []
                prefix_dict[subdir].append(obj[&quot;Key&quot;])
        for subdir, files in prefix_dict.items():
            results.append((subdir, [], files))
        end_time = time.time()
        total_time = end_time - start_time
        if total_time &gt; 120:
            logger.debug(f&quot;Completed file list in {total_time/60} minutes.&quot;)
        elif total_time &lt; 60:
            logger.debug(f&quot;Completed file list in {total_time} seconds.&quot;)
        return results
    def read_image(self, s3_key):
        &quot;&quot;&quot;
        Read an image OR a video from S3.
        &quot;&quot;&quot;
        data = self.read(s3_key)
        if data is None:
            return None
        buffer = BytesIO(data)
        # Check extension
        ext = s3_key.rsplit(&quot;.&quot;, 1)[-1].lower() if &quot;.&quot; in s3_key else &quot;&quot;
        if ext in video_file_extensions:
            return load_video(buffer)
        else:
            return load_image(buffer)
    def read_image_batch(self, s3_keys: list, delete_problematic_images: bool = False):
        &quot;&quot;&quot;
        Return a list of Image (or video) objects, given a list of S3 keys.
        This uses read_batch(...) for potential concurrency.
        &quot;&quot;&quot;
        batch = self.read_batch(s3_keys)
        output_images = []
        available_keys = []
        for s3_key, data in zip(s3_keys, batch):
            if data is None:
                logger.warning(f&quot;Unable to load image &apos;{s3_key}&apos;, skipping (no data).&quot;)
                continue
            try:
                # Check extension to decide loader
                ext = s3_key.rsplit(&quot;.&quot;, 1)[-1].lower() if &quot;.&quot; in s3_key else &quot;&quot;
                buffer = BytesIO(data)
                if ext in video_file_extensions:
                    image_data = load_video(buffer)
                else:
                    image_data = load_image(buffer)
                if image_data is None:
                    logger.warning(f&quot;Unable to load image &apos;{s3_key}&apos;, skipping.&quot;)
                    continue
                output_images.append(image_data)
                available_keys.append(s3_key)
            except Exception as e:
                if delete_problematic_images:
                    logger.warning(f&quot;Deleting image &apos;{s3_key}&apos; due to error: {e}&quot;)
                    self.delete(s3_key)
                else:
                    logger.warning(f&quot;Problematic image/video {s3_key} encountered: {e}&quot;)
        return (available_keys, output_images)
    def create_directory(self, directory_path):
        &quot;&quot;&quot;
        S3 doesn&apos;t have a directory structure, so this is a no-op.
        &quot;&quot;&quot;
        pass
    def _detect_file_format(self, fileobj):
        fileobj.seek(0)
        magic_number = fileobj.read(4)
        fileobj.seek(0)
        logger.debug(f&quot;Magic number: {magic_number}&quot;)
        if magic_number[:2] == b&quot;\x80\x04&quot; or b&quot;PK&quot; in magic_number:
            try:
                obj = torch.load(fileobj, map_location=&quot;cpu&quot;)
                if isinstance(obj, bytes):
                    return &quot;incorrect&quot;
                else:
                    return &quot;correct_uncompressed&quot;
            except Exception:
                return &quot;correct_compressed&quot;
        elif magic_number[:2] == b&quot;\x1f\x8b&quot;:
            return &quot;correct_compressed&quot;
        else:
            return &quot;unknown&quot;
    def torch_load(self, s3_key):
        for i in range(self.read_retry_limit):
            try:
                data = self.read(s3_key)
                if data is None:
                    return None
                stored_data = BytesIO(data)
                stored_data.seek(0)
                file_format = self._detect_file_format(stored_data)
                logger.debug(f&quot;File format: {file_format}&quot;)
                if file_format == &quot;incorrect&quot;:
                    stored_data.seek(0)
                    compressed_data = BytesIO(
                        torch.load(stored_data, map_location=&quot;cpu&quot;)
                    )
                    stored_tensor = self._decompress_torch(compressed_data)
                elif file_format == &quot;correct_compressed&quot;:
                    stored_tensor = self._decompress_torch(data)
                else:
                    stored_tensor = stored_data
                if hasattr(stored_tensor, &quot;seek&quot;):
                    stored_tensor.seek(0)
                obj = torch.load(stored_tensor, map_location=&quot;cpu&quot;)
                if isinstance(obj, tuple):
                    obj = tuple(o.to(torch.float32) for o in obj)
                elif isinstance(obj, torch.Tensor):
                    obj = obj.to(torch.float32)
                return obj
            except Exception as e:
                logging.error(f&quot;Failed to load tensor from {s3_key}: {e}&quot;)
                if i == self.read_retry_limit - 1:
                    raise
                else:
                    logging.info(f&quot;Retrying... ({i+1}/{self.read_retry_limit})&quot;)
    def torch_save(self, data, s3_key):
        &quot;&quot;&quot;
        Save a torch object (tensor or dict) to S3 using possible compression.
        &quot;&quot;&quot;
        import torch
        from io import BytesIO
        for i in range(self.write_retry_limit):
            try:
                buffer = BytesIO()
                if self.compress_cache:
                    compressed_data = self._compress_torch(data)
                    buffer.write(compressed_data)
                else:
                    torch.save(data, buffer)
                buffer.seek(0)
                logger.debug(f&quot;Writing torch file: {s3_key}&quot;)
                result = self.write(s3_key, buffer.getvalue())
                logger.debug(f&quot;Write completed: {s3_key}&quot;)
                return result
            except Exception as e:
                logger.error(f&quot;Could not torch save to backend: {e}&quot;)
                if i == self.write_retry_limit - 1:
                    raise e
                else:
                    time.sleep(self.write_retry_interval)
    def write_batch(self, s3_keys, data_list):
        &quot;&quot;&quot;Write a batch of files to the specified S3 keys concurrently.&quot;&quot;&quot;
        with concurrent.futures.ThreadPoolExecutor() as executor:
            executor.map(self.write, s3_keys, data_list)
    def read_batch(self, s3_keys):
        &quot;&quot;&quot;Read a batch of files from the specified S3 keys concurrently.&quot;&quot;&quot;
        with concurrent.futures.ThreadPoolExecutor() as executor:
            return list(executor.map(self.read, s3_keys))
    def bulk_exists(self, s3_keys, prefix=&quot;&quot;):
        &quot;&quot;&quot;Check the existence of a list of S3 keys in bulk.&quot;&quot;&quot;
        objects = self.client.list_objects_v2(Bucket=self.bucket_name, Prefix=prefix)
        existing_keys = set(obj[&quot;Key&quot;] for obj in objects.get(&quot;Contents&quot;, []))
        return [key in existing_keys for key in s3_keys]</file><file path="helpers/data_backend/base.py">from abc import ABC, abstractmethod
from io import BytesIO
import gzip
import torch
class BaseDataBackend(ABC):
    @abstractmethod
    def read(self, identifier):
        &quot;&quot;&quot;
        Read data based on the identifier.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def write(self, identifier, data):
        &quot;&quot;&quot;
        Write data to the specified identifier.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def delete(self, identifier):
        &quot;&quot;&quot;
        Delete data associated with the identifier.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def exists(self, identifier):
        &quot;&quot;&quot;
        Check if the identifier exists.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def open_file(self, identifier, mode):
        &quot;&quot;&quot;
        Open the identifier (file or object) in the specified mode.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def list_files(self, file_extensions: list, instance_data_dir: str = None) -&gt; tuple:
        &quot;&quot;&quot;
        List all files matching the pattern.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def read_image(self, filepath: str, delete_problematic_images: bool = False):
        &quot;&quot;&quot;
        Read an image from the backend and return a PIL Image.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def read_image_batch(self, filepaths: str, delete_problematic_images: bool = False):
        &quot;&quot;&quot;
        Read a batch of images from the backend and return a list of PIL Images.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def create_directory(self, directory_path):
        &quot;&quot;&quot;
        Creates a directory in the backend.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def torch_load(self, filename):
        &quot;&quot;&quot;
        Reads content from the backend and loads it with torch.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def torch_save(self, data, filename):
        &quot;&quot;&quot;
        Saves the data using torch to the backend.
        &quot;&quot;&quot;
        pass
    @abstractmethod
    def write_batch(self, identifiers, files):
        &quot;&quot;&quot;
        Write a batch of files to the specified identifiers.
        &quot;&quot;&quot;
        pass
    def _decompress_torch(self, gzip_data: BytesIO):
        &quot;&quot;&quot;
        We&apos;ve read the gzip from disk. Just decompress it.
        &quot;&quot;&quot;
        # bytes object might not have seek. workaround:
        if not hasattr(gzip_data, &quot;seek&quot;):
            gzip_data = BytesIO(gzip_data)
        gzip_data.seek(0)
        with gzip.GzipFile(fileobj=gzip_data, mode=&quot;rb&quot;) as file:
            decompressed_data = file.read()
        return BytesIO(decompressed_data)
    def _compress_torch(self, data):
        &quot;&quot;&quot;
        Compress the torch data before writing it to disk.
        &quot;&quot;&quot;
        output_data_container = BytesIO()
        torch.save(data, output_data_container)
        output_data_container.seek(0)
        with BytesIO() as compressed_output:
            with gzip.GzipFile(fileobj=compressed_output, mode=&quot;wb&quot;) as file:
                file.write(output_data_container.getvalue())
            return compressed_output.getvalue()</file><file path="helpers/data_backend/csv_url_list.py">import fnmatch
import hashlib
import pandas as pd
import requests
from helpers.data_backend.base import BaseDataBackend
from helpers.image_manipulation.load import load_image, load_video
from helpers.training.multi_process import should_log
from helpers.training import video_file_extensions, image_file_extensions
from pathlib import Path
from io import BytesIO
import os
import logging
import torch
from typing import Any, Union, Optional, BinaryIO
logger = logging.getLogger(&quot;CSVDataBackend&quot;)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(&quot;ERROR&quot;)
def url_to_filename(url: str) -&gt; str:
    return url.split(&quot;/&quot;)[-1]
def str_hash(filename: str) -&gt; str:
    return str(hashlib.sha256(str(filename).encode()).hexdigest())
def path_to_hashed_path(path: Path, hash_filenames: bool) -&gt; Path:
    path = Path(path).resolve()
    if hash_filenames:
        return path.parent.joinpath(str_hash(path.stem) + path.suffix)
    return path
def html_to_file_loc(parent_directory: Path, url: str, hash_filenames: bool) -&gt; str:
    filename = url_to_filename(url)
    cached_loc = path_to_hashed_path(
        parent_directory.joinpath(filename), hash_filenames
    )
    return str(cached_loc.resolve())
class CSVDataBackend(BaseDataBackend):
    def __init__(
        self,
        accelerator,
        id: str,
        csv_file: Path,
        compress_cache: bool = False,
        url_column: str = &quot;url&quot;,
        caption_column: str = &quot;caption&quot;,
        image_cache_loc: Optional[str] = None,
        hash_filenames: bool = True,
    ):
        self.id = id
        self.type = &quot;csv&quot;
        self.compress_cache = compress_cache
        self.hash_filenames = hash_filenames
        self.csv_file = csv_file
        self.accelerator = accelerator
        self.url_column = url_column
        self.df = pd.read_csv(csv_file, index_col=url_column)
        self.df = self.df.groupby(level=0).last()  # deduplicate by index (image loc)
        self.caption_column = caption_column
        self.image_cache_loc = (
            Path(image_cache_loc) if image_cache_loc is not None else None
        )
    def read(self, location, as_byteIO: bool = False):
        &quot;&quot;&quot;Read and return the content of the file.&quot;&quot;&quot;
        already_hashed = False
        if isinstance(location, Path):
            location = str(location.resolve())
        if location.startswith(&quot;http&quot;):
            if self.image_cache_loc is not None:
                # check for cache
                cached_loc = html_to_file_loc(
                    self.image_cache_loc, location, self.hash_filenames
                )
                if os.path.exists(cached_loc):
                    # found cache
                    location = cached_loc
                    already_hashed = True
                else:
                    # actually go to website
                    data = requests.get(location, stream=True).raw.data
                    with open(cached_loc, &quot;wb&quot;) as f:
                        f.write(data)
            else:
                data = requests.get(location, stream=True).raw.data
        if not location.startswith(&quot;http&quot;):
            # read from local file
            hashed_location = path_to_hashed_path(
                location, hash_filenames=self.hash_filenames and not already_hashed
            )
            try:
                with open(hashed_location, &quot;rb&quot;) as file:
                    data = file.read()
            except FileNotFoundError as e:
                from tqdm import tqdm
                tqdm.write(f&quot;ask was for file {location} bound to {hashed_location}&quot;)
                raise e
        if not as_byteIO:
            return data
        return BytesIO(data)
    def write(self, filepath: Union[str, Path], data: Any) -&gt; None:
        &quot;&quot;&quot;
        Write the provided data to the specified filepath.
        &quot;&quot;&quot;
        if isinstance(filepath, str):
            assert not filepath.startswith(
                &quot;http&quot;
            ), f&quot;writing to {filepath} is not allowed as it has http in it&quot;
            filepath = Path(filepath)
        filepath = path_to_hashed_path(filepath, self.hash_filenames)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, &quot;wb&quot;) as file:
            if isinstance(data, (dict, torch.Tensor)):
                return self.torch_save(data, file)
            if isinstance(data, str):
                data = data.encode(&quot;utf-8&quot;)
            else:
                logger.debug(
                    f&quot;Received an unknown data type to write to disk. Doing our best: {type(data)}&quot;
                )
            file.write(data)
    def delete(self, filepath):
        &quot;&quot;&quot;Delete the specified file.&quot;&quot;&quot;
        if filepath in self.df.index:
            self.df.drop(filepath, inplace=True)
        filepath = path_to_hashed_path(filepath, self.hash_filenames)
        if os.path.exists(filepath):
            logger.debug(f&quot;Deleting file: {filepath}&quot;)
            os.remove(filepath)
        if self.exists(filepath) or filepath in self.df.index:
            raise Exception(f&quot;Failed to delete {filepath}&quot;)
    def exists(self, filepath):
        &quot;&quot;&quot;Check if the file exists.&quot;&quot;&quot;
        if isinstance(filepath, str) and &quot;http&quot; in filepath:
            return filepath in self.df.index
        else:
            filepath = path_to_hashed_path(filepath, self.hash_filenames)
            return os.path.exists(filepath)
    def open_file(self, filepath, mode):
        &quot;&quot;&quot;Open the file in the specified mode.&quot;&quot;&quot;
        return open(path_to_hashed_path(filepath, self.hash_filenames), mode)
    def list_files(
        self, file_extensions: list = None, instance_data_dir: str = None
    ) -&gt; tuple:
        &quot;&quot;&quot;
        List all files matching the file extensions.
        Creates Path objects of each file found.
        &quot;&quot;&quot;
        logger.debug(
            f&quot;CSVDataBackend.list_files: file_extensions={file_extensions}, instance_data_dir={instance_data_dir}&quot;
        )
        if instance_data_dir is None:
            filtered_paths = set(self.df.index)
            filtered_ids = set(filtered_paths)
        else:
            # Convert file extensions to patterns
            if file_extensions:
                patterns = [f&quot;*.{ext.lower()}&quot; for ext in file_extensions]
            else:
                patterns = [&quot;*&quot;]
            filtered_ids = set()
            for pattern in patterns:
                filtered_ids.update(
                    filter(lambda id: fnmatch.fnmatch(id, pattern), list(self.df.index))
                )
            filtered_paths = set(
                filter(lambda id: &quot;http&quot; not in id and os.path.exists(id), filtered_ids)
            )
        path_dict = {}
        for path in filtered_paths:
            if hasattr(path, &quot;parent&quot;):
                parent = str(Path(path).parent)
                if parent not in path_dict:
                    path_dict[parent] = []
                path_dict[parent].append(str(Path(path).absolute()))
            else:
                if &quot;/&quot; not in path_dict:
                    path_dict[&quot;/&quot;] = []
                if os.path.splitext(str(path))[1] not in [&quot;.json&quot;, &quot;.csv&quot;, &quot;.parquet&quot;]:
                    path_dict[&quot;/&quot;].append(str(path))
        results = [(subdir, [], files) for subdir, files in path_dict.items()]
        results += [(&quot;&quot;, [], filtered_ids - filtered_paths)]
        return results
    def read_image(self, filepath: str, delete_problematic_images: bool = False):
        &quot;&quot;&quot;
        Read an image or video from the specified filepath.
        &quot;&quot;&quot;
        if isinstance(filepath, str):
            filepath = filepath.replace(&quot;\x00&quot;, &quot;&quot;)
        try:
            image_data = self.read(filepath, as_byteIO=True)
            ext = os.path.splitext(filepath)[1].lower().strip(&quot;.&quot;)
            if ext in video_file_extensions:
                image = load_video(image_data)
            else:
                image = load_image(image_data)
            return image
        except Exception as e:
            import traceback
            logger.error(
                f&quot;Encountered error opening image {filepath}: {e}, traceback: {traceback.format_exc()}&quot;
            )
            if delete_problematic_images:
                logger.error(
                    &quot;Deleting image, because --delete_problematic_images is provided.&quot;
                )
                self.delete(filepath)
            else:
                exit(1)
                raise e
    def read_image_batch(
        self, filepaths: list, delete_problematic_images: bool = False
    ) -&gt; list:
        &quot;&quot;&quot;Read a batch of images (or videos) from the specified filepaths.&quot;&quot;&quot;
        if not isinstance(filepaths, list):
            raise ValueError(
                f&quot;read_image_batch must be given a list of image filepaths. we received: {filepaths}&quot;
            )
        output_images = []
        available_keys = []
        for filepath in filepaths:
            try:
                image_data = self.read_image(filepath, delete_problematic_images)
                if image_data is None:
                    logger.warning(f&quot;Unable to load image &apos;{filepath}&apos;, skipping.&quot;)
                    continue
                output_images.append(image_data)
                available_keys.append(filepath)
            except Exception as e:
                if delete_problematic_images:
                    logger.error(
                        f&quot;Deleting image &apos;{filepath}&apos;, because --delete_problematic_images is provided. Error: {e}&quot;
                    )
                else:
                    logger.warning(
                        f&quot;A problematic image {filepath} is detected, but we are not allowed to remove it. Error: {e}&quot;
                    )
        return (available_keys, output_images)
    def create_directory(self, directory_path):
        if os.path.exists(directory_path):
            return
        logger.debug(f&quot;Creating directory: {directory_path}&quot;)
        os.makedirs(directory_path, exist_ok=True)
    def torch_load(self, filename):
        &quot;&quot;&quot;
        Load a torch tensor from a file.
        &quot;&quot;&quot;
        stored_tensor = self.read(filename, as_byteIO=True)
        if self.compress_cache:
            try:
                stored_tensor = self._decompress_torch(stored_tensor)
            except Exception as e:
                logger.error(
                    f&quot;Failed to decompress torch file, falling back to passthrough: {e}&quot;
                )
        if hasattr(stored_tensor, &quot;seek&quot;):
            stored_tensor.seek(0)
        try:
            loaded_tensor = torch.load(stored_tensor, map_location=&quot;cpu&quot;)
        except Exception as e:
            logger.error(f&quot;Failed to load corrupt torch file &apos;{filename}&apos;: {e}&quot;)
            if &quot;invalid load key&quot; in str(e):
                self.delete(filename)
            raise e
        return loaded_tensor
    def torch_save(self, data, location: Union[str, Path, BytesIO]):
        &quot;&quot;&quot;
        Save a torch object (tensor or dict) to a file or file-like object.
        &quot;&quot;&quot;
        if isinstance(location, (str, Path)):
            location = path_to_hashed_path(location, self.hash_filenames)
            location = self.open_file(location, &quot;wb&quot;)
        if self.compress_cache:
            compressed_data = self._compress_torch(data)
            location.write(compressed_data)
        else:
            torch.save(data, location)
        location.close()
    def write_batch(self, filepaths: list, data_list: list) -&gt; None:
        &quot;&quot;&quot;Write a batch of data to the specified filepaths.&quot;&quot;&quot;
        for filepath, data in zip(filepaths, data_list):
            self.write(filepath, data)
    def save_state(self):
        self.df.to_csv(self.csv_file, index_label=self.url_column)
    def get_caption(self, image_path: str) -&gt; str:
        if self.caption_column is None:
            raise ValueError(&quot;Cannot retrieve caption from csv, as one is not set.&quot;)
        return self.df.loc[image_path, self.caption_column]</file><file path="helpers/data_backend/factory.py">from helpers.data_backend.local import LocalDataBackend
from helpers.data_backend.aws import S3DataBackend
from helpers.data_backend.csv_url_list import CSVDataBackend
from helpers.data_backend.base import BaseDataBackend
from helpers.training.default_settings import default, latest_config_version
from helpers.caching.text_embeds import TextEmbeddingCache
from helpers.training.exceptions import MultiDatasetExhausted
from helpers.multiaspect.dataset import MultiAspectDataset
from helpers.multiaspect.sampler import MultiAspectSampler
from helpers.prompts import PromptHandler, CaptionNotFoundError
from helpers.caching.vae import VAECache
from helpers.training.multi_process import should_log, rank_info, _get_rank as get_rank
from helpers.training.collate import collate_fn
from helpers.training.state_tracker import StateTracker
import json
import os
import torch
import logging
import io
import time
import threading
from tqdm import tqdm
import queue
from math import sqrt
import pandas as pd
import numpy as np
logger = logging.getLogger(&quot;DataBackendFactory&quot;)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(logging.ERROR)
prefetch_log = logging.getLogger(&quot;DataBackendPrefetch&quot;)
if should_log():
    prefetch_log.setLevel(os.environ.get(&quot;SIMPLETUNER_PREFETCH_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    prefetch_log.setLevel(logging.ERROR)
# For prefetching.
def prefetch_log_debug(message):
    prefetch_log.debug(f&quot;{rank_info()} {message}&quot;)
def info_log(message):
    if StateTracker.get_accelerator().is_main_process:
        logger.info(message)
def warning_log(message):
    if StateTracker.get_accelerator().is_main_process:
        logger.warning(message)
def check_column_values(
    column_data, column_name, parquet_path, fallback_caption_column=False
):
    # Determine if the column contains arrays or scalar values
    non_null_values = column_data.dropna()
    if non_null_values.empty:
        # All values are null
        raise ValueError(
            f&quot;Parquet file {parquet_path} contains only null values in the &apos;{column_name}&apos; column.&quot;
        )
    first_non_null = non_null_values.iloc[0]
    if isinstance(first_non_null, (list, tuple, np.ndarray, pd.Series)):
        # Column contains arrays
        # Check for null arrays
        if column_data.isnull().any() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains null arrays in the &apos;{column_name}&apos; column.&quot;
            )
        # Check for empty arrays
        empty_arrays = column_data.apply(lambda x: len(x) == 0)
        if empty_arrays.any() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains empty arrays in the &apos;{column_name}&apos; column.&quot;
            )
        # Check for null elements within arrays
        null_elements_in_arrays = column_data.apply(
            lambda arr: any(pd.isnull(s) for s in arr)
        )
        if null_elements_in_arrays.any() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains null values within arrays in the &apos;{column_name}&apos; column.&quot;
            )
        # Check for empty strings within arrays
        empty_strings_in_arrays = column_data.apply(
            lambda arr: any(s == &quot;&quot; for s in arr)
        )
        if empty_strings_in_arrays.all() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains only empty strings within arrays in the &apos;{column_name}&apos; column.&quot;
            )
    elif isinstance(first_non_null, str):
        # Column contains scalar strings
        # Check for null values
        if column_data.isnull().any() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains null values in the &apos;{column_name}&apos; column.&quot;
            )
        # Check for empty strings
        if (column_data == &quot;&quot;).any() and not fallback_caption_column:
            raise ValueError(
                f&quot;Parquet file {parquet_path} contains empty strings in the &apos;{column_name}&apos; column.&quot;
            )
    else:
        raise TypeError(
            f&quot;Unsupported data type in column &apos;{column_name}&apos;. Expected strings or arrays of strings.&quot;
        )
def init_backend_config(backend: dict, args: dict, accelerator) -&gt; dict:
    output = {&quot;id&quot;: backend[&quot;id&quot;], &quot;config&quot;: {}}
    if backend.get(&quot;dataset_type&quot;, None) == &quot;text_embeds&quot;:
        if &quot;caption_filter_list&quot; in backend:
            output[&quot;config&quot;][&quot;caption_filter_list&quot;] = backend[&quot;caption_filter_list&quot;]
        output[&quot;dataset_type&quot;] = &quot;text_embeds&quot;
        return output
    elif backend.get(&quot;dataset_type&quot;, None) == &quot;image_embeds&quot;:
        # no overrides for image embed backends
        return output
    else:
        ## Check for settings we shouldn&apos;t have for non-text datasets.
        if &quot;caption_filter_list&quot; in backend:
            raise ValueError(
                f&quot;caption_filter_list is only a valid setting for text datasets. It is currently set for the {backend.get(&apos;dataset_type&apos;, &apos;image&apos;)} dataset {backend[&apos;id&apos;]}.&quot;
            )
    # Image backend config
    output[&quot;dataset_type&quot;] = backend.get(&quot;dataset_type&quot;, &quot;image&quot;)
    choices = [&quot;image&quot;, &quot;conditioning&quot;, &quot;eval&quot;, &quot;video&quot;]
    if (
        StateTracker.get_args().controlnet
        and output[&quot;dataset_type&quot;] == &quot;image&quot;
        and backend.get(&quot;conditioning_data&quot;, None) is None
    ):
        raise ValueError(
            &quot;Image datasets require a corresponding conditioning_data set configured in your dataloader.&quot;
        )
    if output[&quot;dataset_type&quot;] not in choices:
        raise ValueError(f&quot;(id={backend[&apos;id&apos;]}) dataset_type must be one of {choices}.&quot;)
    if &quot;vae_cache_clear_each_epoch&quot; in backend:
        output[&quot;config&quot;][&quot;vae_cache_clear_each_epoch&quot;] = backend[
            &quot;vae_cache_clear_each_epoch&quot;
        ]
    if &quot;probability&quot; in backend:
        output[&quot;config&quot;][&quot;probability&quot;] = (
            float(backend[&quot;probability&quot;]) if backend[&quot;probability&quot;] else 1.0
        )
    if &quot;ignore_epochs&quot; in backend:
        logger.error(
            &quot;ignore_epochs is deprecated, and will do nothing. This can be safely removed from your configuration.&quot;
        )
    if &quot;repeats&quot; in backend:
        output[&quot;config&quot;][&quot;repeats&quot;] = (
            int(backend[&quot;repeats&quot;]) if backend[&quot;repeats&quot;] else 0
        )
    if &quot;crop&quot; in backend:
        output[&quot;config&quot;][&quot;crop&quot;] = backend[&quot;crop&quot;]
    else:
        output[&quot;config&quot;][&quot;crop&quot;] = False
    if backend.get(&quot;type&quot;) == &quot;csv&quot;:
        if &quot;csv_cache_dir&quot; in backend:
            output[&quot;config&quot;][&quot;csv_cache_dir&quot;] = backend[&quot;csv_cache_dir&quot;]
        if &quot;csv_file&quot; in backend:
            output[&quot;config&quot;][&quot;csv_file&quot;] = backend[&quot;csv_file&quot;]
        if &quot;csv_caption_column&quot; in backend:
            output[&quot;config&quot;][&quot;csv_caption_column&quot;] = backend[&quot;csv_caption_column&quot;]
        if &quot;csv_url_column&quot; in backend:
            output[&quot;config&quot;][&quot;csv_url_column&quot;] = backend[&quot;csv_url_column&quot;]
    if &quot;crop_aspect&quot; in backend:
        choices = [&quot;square&quot;, &quot;preserve&quot;, &quot;random&quot;, &quot;closest&quot;]
        if backend.get(&quot;crop_aspect&quot;, None) not in choices:
            raise ValueError(
                f&quot;(id={backend[&apos;id&apos;]}) crop_aspect must be one of {choices}.&quot;
            )
        output[&quot;config&quot;][&quot;crop_aspect&quot;] = backend[&quot;crop_aspect&quot;]
        if (
            output[&quot;config&quot;][&quot;crop_aspect&quot;] == &quot;random&quot;
            or output[&quot;config&quot;][&quot;crop_aspect&quot;] == &quot;closest&quot;
        ):
            if &quot;crop_aspect_buckets&quot; not in backend or not isinstance(
                backend[&quot;crop_aspect_buckets&quot;], list
            ):
                raise ValueError(
                    f&quot;(id={backend[&apos;id&apos;]}) crop_aspect_buckets must be provided when crop_aspect is set to &apos;random&apos;.&quot;
                    &quot; This should be a list of float values or a list of dictionaries following the format: {&apos;aspect_bucket&apos;: float, &apos;weight&apos;: float}.&quot;
                    &quot; The weight represents how likely this bucket is to be chosen, and all weights should add up to 1.0 collectively.&quot;
                )
            for bucket in backend.get(&quot;crop_aspect_buckets&quot;):
                if type(bucket) not in [float, int, dict]:
                    raise ValueError(
                        f&quot;(id={backend[&apos;id&apos;]}) crop_aspect_buckets must be a list of float values or a list of dictionaries following the format: {&apos;aspect_bucket&apos;: float, &apos;weight&apos;: float}.&quot;
                        &quot; The weight represents how likely this bucket is to be chosen, and all weights should add up to 1.0 collectively.&quot;
                    )
        output[&quot;config&quot;][&quot;crop_aspect_buckets&quot;] = backend.get(&quot;crop_aspect_buckets&quot;)
    else:
        output[&quot;config&quot;][&quot;crop_aspect&quot;] = &quot;square&quot;
    if &quot;crop_style&quot; in backend:
        crop_styles = [&quot;random&quot;, &quot;corner&quot;, &quot;center&quot;, &quot;centre&quot;, &quot;face&quot;]
        if backend[&quot;crop_style&quot;] not in crop_styles:
            raise ValueError(
                f&quot;(id={backend[&apos;id&apos;]}) crop_style must be one of {crop_styles}.&quot;
            )
        output[&quot;config&quot;][&quot;crop_style&quot;] = backend[&quot;crop_style&quot;]
    else:
        output[&quot;config&quot;][&quot;crop_style&quot;] = &quot;random&quot;
    output[&quot;config&quot;][&quot;disable_validation&quot;] = backend.get(&quot;disable_validation&quot;, False)
    if &quot;resolution&quot; in backend:
        output[&quot;config&quot;][&quot;resolution&quot;] = backend[&quot;resolution&quot;]
    else:
        output[&quot;config&quot;][&quot;resolution&quot;] = args.resolution
    if &quot;resolution_type&quot; in backend:
        output[&quot;config&quot;][&quot;resolution_type&quot;] = backend[&quot;resolution_type&quot;]
    else:
        output[&quot;config&quot;][&quot;resolution_type&quot;] = args.resolution_type
    if &quot;parquet&quot; in backend:
        output[&quot;config&quot;][&quot;parquet&quot;] = backend[&quot;parquet&quot;]
    if &quot;caption_strategy&quot; in backend:
        output[&quot;config&quot;][&quot;caption_strategy&quot;] = backend[&quot;caption_strategy&quot;]
    else:
        output[&quot;config&quot;][&quot;caption_strategy&quot;] = args.caption_strategy
    output[&quot;config&quot;][&quot;instance_data_dir&quot;] = backend.get(
        &quot;instance_data_dir&quot;, backend.get(&quot;aws_data_prefix&quot;, &quot;&quot;)
    )
    if &quot;hash_filenames&quot; in backend:
        output[&quot;config&quot;][&quot;hash_filenames&quot;] = backend[&quot;hash_filenames&quot;]
    if &quot;hash_filenames&quot; in backend and backend.get(&quot;type&quot;) == &quot;csv&quot;:
        output[&quot;config&quot;][&quot;hash_filenames&quot;] = backend[&quot;hash_filenames&quot;]
    # check if caption_strategy=parquet with metadata_backend=json
    current_metadata_backend_type = backend.get(&quot;metadata_backend&quot;, &quot;discovery&quot;)
    if output[&quot;config&quot;][&quot;caption_strategy&quot;] == &quot;parquet&quot; and (
        current_metadata_backend_type == &quot;json&quot;
        or current_metadata_backend_type == &quot;discovery&quot;
    ):
        raise ValueError(
            f&quot;(id={backend[&apos;id&apos;]}) Cannot use caption_strategy=parquet with metadata_backend={current_metadata_backend_type}. Instead, it is recommended to use the textfile strategy and extract your captions into txt files.&quot;
        )
    maximum_image_size = backend.get(&quot;maximum_image_size&quot;, args.maximum_image_size)
    target_downsample_size = backend.get(
        &quot;target_downsample_size&quot;, args.target_downsample_size
    )
    output[&quot;config&quot;][&quot;maximum_image_size&quot;] = maximum_image_size
    output[&quot;config&quot;][&quot;target_downsample_size&quot;] = target_downsample_size
    if maximum_image_size and not target_downsample_size:
        raise ValueError(
            &quot;When a data backend is configured to use `maximum_image_size`, you must also provide a value for `target_downsample_size`.&quot;
        )
    if (
        maximum_image_size
        and output[&quot;config&quot;][&quot;resolution_type&quot;] == &quot;area&quot;
        and maximum_image_size &gt; 10
        and not os.environ.get(&quot;SIMPLETUNER_MAXIMUM_IMAGE_SIZE_OVERRIDE&quot;, False)
    ):
        raise ValueError(
            f&quot;When a data backend is configured to use `&apos;resolution_type&apos;:area`, `maximum_image_size` must be less than 10 megapixels. You may have accidentally entered {maximum_image_size} pixels, instead of megapixels.&quot;
        )
    elif (
        maximum_image_size
        and output[&quot;config&quot;][&quot;resolution_type&quot;] == &quot;pixel&quot;
        and maximum_image_size &lt; 512
        and &quot;deepfloyd&quot; not in args.model_type
        and args.model_family != &quot;smoldit&quot;
    ):
        raise ValueError(
            f&quot;When a data backend is configured to use `&apos;resolution_type&apos;:pixel`, `maximum_image_size` must be at least 512 pixels. You may have accidentally entered {maximum_image_size} megapixels, instead of pixels.&quot;
        )
    if (
        target_downsample_size
        and output[&quot;config&quot;][&quot;resolution_type&quot;] == &quot;area&quot;
        and target_downsample_size &gt; 10
        and not os.environ.get(&quot;SIMPLETUNER_MAXIMUM_IMAGE_SIZE_OVERRIDE&quot;, False)
    ):
        raise ValueError(
            f&quot;When a data backend is configured to use `&apos;resolution_type&apos;:area`, `target_downsample_size` must be less than 10 megapixels. You may have accidentally entered {target_downsample_size} pixels, instead of megapixels.&quot;
        )
    elif (
        target_downsample_size
        and output[&quot;config&quot;][&quot;resolution_type&quot;] == &quot;pixel&quot;
        and target_downsample_size &lt; 512
        and &quot;deepfloyd&quot; not in args.model_type
        and args.model_family != &quot;smoldit&quot;
    ):
        raise ValueError(
            f&quot;When a data backend is configured to use `&apos;resolution_type&apos;:pixel`, `target_downsample_size` must be at least 512 pixels. You may have accidentally entered {target_downsample_size} megapixels, instead of pixels.&quot;
        )
    if backend.get(&quot;dataset_type&quot;, None) == &quot;video&quot;:
        output[&quot;config&quot;][&quot;video&quot;] = {}
        if &quot;video&quot; in backend:
            output[&quot;config&quot;][&quot;video&quot;].update(backend[&quot;video&quot;])
        if &quot;num_frames&quot; not in output[&quot;config&quot;][&quot;video&quot;]:
            warning_log(
                f&quot;No `num_frames` was provided for video backend. Defaulting to 125 (5 seconds @ 25fps) to avoid memory implosion/explosion. Reduce value further for lower memory use.&quot;
            )
            output[&quot;config&quot;][&quot;video&quot;][&quot;num_frames&quot;] = 125
        if &quot;min_frames&quot; not in output[&quot;config&quot;][&quot;video&quot;]:
            warning_log(
                f&quot;No `min_frames` was provided for video backend. Defaulting to {output[&apos;config&apos;][&apos;video&apos;][&apos;num_frames&apos;]} frames (num_frames). Reduce num_frames further for lower memory use.&quot;
            )
            output[&quot;config&quot;][&quot;video&quot;][&quot;min_frames&quot;] = output[&quot;config&quot;][&quot;video&quot;][
                &quot;num_frames&quot;
            ]
        if &quot;max_frames&quot; not in output[&quot;config&quot;][&quot;video&quot;]:
            warning_log(
                f&quot;No `max_frames` was provided for video backend. Set this value to avoid scanning huge video files.&quot;
            )
        if &quot;is_i2v&quot; not in output[&quot;config&quot;][&quot;video&quot;]:
            if args.model_family in [&quot;ltxvideo&quot;]:
                warning_log(
                    f&quot;Setting is_i2v to True for model_family={args.model_family}. Set this manually to false to override.&quot;
                )
                output[&quot;config&quot;][&quot;video&quot;][&quot;is_i2v&quot;] = True
            else:
                warning_log(
                    f&quot;No value for is_i2v was supplied for your dataset. Assuming it is disabled.&quot;
                )
                output[&quot;config&quot;][&quot;video&quot;][&quot;is_i2v&quot;] = False
        min_frames = output[&quot;config&quot;][&quot;video&quot;][&quot;min_frames&quot;]
        num_frames = output[&quot;config&quot;][&quot;video&quot;][&quot;num_frames&quot;]
        # both should be integers
        if not any([isinstance(min_frames, int), isinstance(num_frames, int)]):
            raise ValueError(
                f&quot;video-&gt;min_frames and video-&gt;num_frames must be integers. Received min_frames={min_frames} and num_frames={num_frames}.&quot;
            )
        if min_frames &lt; 1 or num_frames &lt; 1:
            raise ValueError(
                f&quot;video-&gt;min_frames and video-&gt;num_frames must be greater than 0. Received min_frames={min_frames} and num_frames={num_frames}.&quot;
            )
        if min_frames &lt; num_frames:
            raise ValueError(
                f&quot;video-&gt;min_frames must be greater than or equal to video-&gt;num_frames. Received min_frames={min_frames} and num_frames={num_frames}.&quot;
            )
    return output
def print_bucket_info(metadata_backend, dataset_type: str = &quot;image&quot;):
    # Print table header
    if get_rank() == 0:
        tqdm.write(
            f&quot;{rank_info()} | {&apos;bucket&apos;:&lt;10} | {f&apos;{dataset_type} count (per-GPU)&apos;:&lt;12}&quot;
        )
        # Print separator
        tqdm.write(&quot;-&quot; * 30)
        # Print each bucket&apos;s information
        for bucket in metadata_backend.aspect_ratio_bucket_indices:
            image_count = len(metadata_backend.aspect_ratio_bucket_indices[bucket])
            if image_count == 0:
                continue
            tqdm.write(f&quot;{rank_info()} | {bucket:&lt;10} | {image_count:&lt;12}&quot;)
def configure_parquet_database(backend: dict, args, data_backend: BaseDataBackend):
    &quot;&quot;&quot;When given a backend config dictionary, configure a parquet database.&quot;&quot;&quot;
    parquet_config = backend.get(&quot;parquet&quot;, None)
    if not parquet_config:
        raise ValueError(
            &quot;Parquet backend must have a &apos;parquet&apos; field in the backend config containing required fields for configuration.&quot;
        )
    parquet_path = parquet_config.get(&quot;path&quot;, None)
    if not parquet_path:
        raise ValueError(
            &quot;Parquet backend must have a &apos;path&apos; field in the backend config under the &apos;parquet&apos; key.&quot;
        )
    if not data_backend.exists(parquet_path):
        raise FileNotFoundError(f&quot;Parquet file {parquet_path} not found.&quot;)
    # Load the dataframe
    import pandas as pd
    bytes_string = data_backend.read(parquet_path)
    pq = io.BytesIO(bytes_string)
    if parquet_path.endswith(&quot;.jsonl&quot;):
        df = pd.read_json(pq, lines=True)
    else:
        df = pd.read_parquet(pq)
    caption_column = parquet_config.get(
        &quot;caption_column&quot;, args.parquet_caption_column or &quot;description&quot;
    )
    fallback_caption_column = parquet_config.get(&quot;fallback_caption_column&quot;, None)
    filename_column = parquet_config.get(
        &quot;filename_column&quot;, args.parquet_filename_column or &quot;id&quot;
    )
    identifier_includes_extension = parquet_config.get(
        &quot;identifier_includes_extension&quot;, False
    )
    # Check the columns exist
    if caption_column not in df.columns:
        raise ValueError(
            f&quot;Parquet file {parquet_path} does not contain a column named &apos;{caption_column}&apos;.&quot;
        )
    if filename_column not in df.columns:
        raise ValueError(
            f&quot;Parquet file {parquet_path} does not contain a column named &apos;{filename_column}&apos;.&quot;
        )
    # Apply the function to the caption_column.
    check_column_values(
        df[caption_column],
        caption_column,
        parquet_path,
        fallback_caption_column=fallback_caption_column,
    )
    # Apply the function to the filename_column.
    check_column_values(
        df[filename_column],
        filename_column,
        parquet_path,
        fallback_caption_column=False,  # Always check filename_column
    )
    # Store the database in StateTracker
    StateTracker.set_parquet_database(
        backend[&quot;id&quot;],
        (
            df,
            filename_column,
            caption_column,
            fallback_caption_column,
            identifier_includes_extension,
        ),
    )
    info_log(
        f&quot;Configured parquet database for backend {backend[&apos;id&apos;]}. Caption column: {caption_column}. Filename column: {filename_column}.&quot;
    )
def configure_multi_databackend(args: dict, accelerator, text_encoders, tokenizers):
    &quot;&quot;&quot;
    Configure a multiple dataloaders based on the provided commandline args.
    &quot;&quot;&quot;
    StateTracker.clear_data_backends()
    logger.setLevel(
        os.environ.get(
            &quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot; if accelerator.is_main_process else &quot;ERROR&quot;
        )
    )
    if args.data_backend_config is None:
        raise ValueError(
            &quot;Must provide a data backend config file via --data_backend_config&quot;
        )
    if not os.path.exists(args.data_backend_config):
        raise FileNotFoundError(
            f&quot;Data backend config file {args.data_backend_config} not found.&quot;
        )
    info_log(f&quot;Loading data backend config from {args.data_backend_config}&quot;)
    with open(args.data_backend_config, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        data_backend_config = json.load(f)
    if len(data_backend_config) == 0:
        raise ValueError(
            &quot;Must provide at least one data backend in the data backend config file.&quot;
        )
    text_embed_backends = {}
    image_embed_backends = {}
    ###                                            ###
    #    now we configure the text embed backends    #
    ###                                            ###
    default_text_embed_backend_id = None
    text_embed_cache_dir_paths = []
    for backend in data_backend_config:
        dataset_type = backend.get(&quot;dataset_type&quot;, None)
        if dataset_type is None or dataset_type != &quot;text_embeds&quot;:
            # Skip configuration of image data backends. It is done later.
            continue
        if (&quot;disabled&quot; in backend and backend[&quot;disabled&quot;]) or (
            &quot;disable&quot; in backend and backend[&quot;disable&quot;]
        ):
            info_log(f&quot;Skipping disabled data backend {backend[&apos;id&apos;]} in config file.&quot;)
            continue
        info_log(f&apos;Configuring text embed backend: {backend[&quot;id&quot;]}&apos;)
        if backend.get(&quot;default&quot;, None):
            if default_text_embed_backend_id is not None:
                raise ValueError(
                    &quot;Only one text embed backend can be marked as default.&quot;
                )
            default_text_embed_backend_id = backend[&quot;id&quot;]
        # Retrieve some config file overrides for commandline arguments,
        #  there currently isn&apos;t much for text embeds.
        init_backend = init_backend_config(backend, args, accelerator)
        StateTracker.set_data_backend_config(init_backend[&quot;id&quot;], init_backend[&quot;config&quot;])
        if backend[&quot;type&quot;] == &quot;local&quot;:
            text_embed_cache_dir_paths.append(
                backend.get(&quot;cache_dir&quot;, args.cache_dir_text)
            )
            init_backend[&quot;data_backend&quot;] = get_local_backend(
                accelerator, init_backend[&quot;id&quot;], compress_cache=args.compress_disk_cache
            )
            init_backend[&quot;cache_dir&quot;] = backend[&quot;cache_dir&quot;]
        elif backend[&quot;type&quot;] == &quot;aws&quot;:
            check_aws_config(backend)
            init_backend[&quot;data_backend&quot;] = get_aws_backend(
                identifier=init_backend[&quot;id&quot;],
                aws_bucket_name=backend[&quot;aws_bucket_name&quot;],
                aws_region_name=backend[&quot;aws_region_name&quot;],
                aws_endpoint_url=backend[&quot;aws_endpoint_url&quot;],
                aws_access_key_id=backend[&quot;aws_access_key_id&quot;],
                aws_secret_access_key=backend[&quot;aws_secret_access_key&quot;],
                accelerator=accelerator,
                max_pool_connections=backend.get(
                    &quot;max_pool_connections&quot;, args.aws_max_pool_connections
                ),
            )
            # S3 buckets use the aws_data_prefix as their prefix/ for all data.
            # Ensure we have a trailing slash on the prefix:
            init_backend[&quot;cache_dir&quot;] = backend.get(
                &quot;aws_data_prefix&quot;, backend.get(&quot;cache_dir&quot;, args.cache_dir_text)
            )
        elif backend[&quot;type&quot;] == &quot;csv&quot;:
            raise ValueError(&quot;Cannot use CSV backend for text embed storage.&quot;)
        else:
            raise ValueError(f&quot;Unknown data backend type: {backend[&apos;type&apos;]}&quot;)
        preserve_data_backend_cache = backend.get(&quot;preserve_data_backend_cache&quot;, False)
        if not preserve_data_backend_cache and accelerator.is_local_main_process:
            StateTracker.delete_cache_files(
                data_backend_id=init_backend[&quot;id&quot;],
                preserve_data_backend_cache=preserve_data_backend_cache,
            )
        # Generate a TextEmbeddingCache object
        logger.debug(f&quot;rank {get_rank()} is creating TextEmbeddingCache&quot;)
        init_backend[&quot;text_embed_cache&quot;] = TextEmbeddingCache(
            id=init_backend[&quot;id&quot;],
            data_backend=init_backend[&quot;data_backend&quot;],
            text_encoders=text_encoders,
            tokenizers=tokenizers,
            accelerator=accelerator,
            cache_dir=init_backend.get(&quot;cache_dir&quot;, args.cache_dir_text),
            model_type=StateTracker.get_model_family(),
            write_batch_size=backend.get(&quot;write_batch_size&quot;, args.write_batch_size),
        )
        logger.debug(f&quot;rank {get_rank()} completed creation of TextEmbeddingCache&quot;)
        init_backend[&quot;text_embed_cache&quot;].set_webhook_handler(
            StateTracker.get_webhook_handler()
        )
        logger.debug(f&quot;rank {get_rank()} might skip discovery..&quot;)
        with accelerator.main_process_first():
            logger.debug(f&quot;rank {get_rank()} is discovering all files&quot;)
            init_backend[&quot;text_embed_cache&quot;].discover_all_files()
        logger.debug(f&quot;rank {get_rank()} is waiting for other processes&quot;)
        accelerator.wait_for_everyone()
        if backend.get(&quot;default&quot;, False):
            # The default embed cache will be used for eg. validation prompts.
            StateTracker.set_default_text_embed_cache(init_backend[&quot;text_embed_cache&quot;])
            logger.debug(f&quot;Set the default text embed cache to {init_backend[&apos;id&apos;]}.&quot;)
            # We will compute the null embedding for caption dropout here.
            info_log(&quot;Pre-computing null embedding&quot;)
            logger.debug(f&quot;rank {get_rank()} may skip computing the embedding..&quot;)
            with accelerator.main_process_first():
                logger.debug(f&quot;rank {get_rank()} is computing the null embed&quot;)
                init_backend[&quot;text_embed_cache&quot;].compute_embeddings_for_prompts(
                    [&quot;&quot;], return_concat=False, load_from_cache=False
                )
                logger.debug(
                    f&quot;rank {get_rank()} has completed computing the null embed&quot;
                )
            logger.debug(f&quot;rank {get_rank()} is waiting for other processes&quot;)
            accelerator.wait_for_everyone()
            logger.debug(f&quot;rank {get_rank()} is continuing&quot;)
        if args.caption_dropout_probability == 0.0:
            warning_log(
                &quot;Not using caption dropout will potentially lead to overfitting on captions, eg. CFG will not work very well. Set --caption_dropout_probability=0.1 as a recommended value.&quot;
            )
        # We don&apos;t compute the text embeds at this time, because we do not really have any captions available yet.
        text_embed_backends[init_backend[&quot;id&quot;]] = init_backend
    if not text_embed_backends:
        raise ValueError(
            &quot;Your dataloader config must contain at least one image dataset AND at least one text_embed dataset.&quot;
            &quot; See this link for more information about dataset_type: https://github.com/bghira/SimpleTuner/blob/main/documentation/DATALOADER.md#configuration-options&quot;
        )
    if not default_text_embed_backend_id and len(text_embed_backends) &gt; 1:
        raise ValueError(
            f&quot;You have {len(text_embed_backends)} text_embed dataset{&apos;s&apos; if len(text_embed_backends) &gt; 1 else &apos;&apos;}, but no default text embed was defined.&quot;
            &quot;\nPlease set default: true on one of the text_embed datasets, as this will be the location of global embeds (validation prompts, etc).&quot;
            &quot;\nSee this link for more information on how to configure a default text embed dataset: https://github.com/bghira/SimpleTuner/blob/main/documentation/DATALOADER.md#configuration-options&quot;
        )
    elif not default_text_embed_backend_id:
        warning_log(
            f&quot;No default text embed was defined, using {list(text_embed_backends.keys())[0]} as the default.&quot;
            &quot; See this page for information about the default text embed backend: https://github.com/bghira/SimpleTuner/blob/main/documentation/DATALOADER.md#configuration-options&quot;
        )
        default_text_embed_backend_id = list(text_embed_backends.keys())[0]
    info_log(&quot;Completed loading text embed services.&quot;)
    ###                                             ###
    #    now we configure the image embed backends    #
    ###                                             ###
    for backend in data_backend_config:
        dataset_type = backend.get(&quot;dataset_type&quot;, None)
        if dataset_type is None or dataset_type not in [&quot;image_embeds&quot;]:
            continue
        if (&quot;disabled&quot; in backend and backend[&quot;disabled&quot;]) or (
            &quot;disable&quot; in backend and backend[&quot;disable&quot;]
        ):
            info_log(f&quot;Skipping disabled data backend {backend[&apos;id&apos;]} in config file.&quot;)
            continue
        info_log(f&apos;Configuring VAE image embed backend: {backend[&quot;id&quot;]}&apos;)
        # Retrieve some config file overrides for commandline arguments,
        #  there currently isn&apos;t much for text embeds.
        init_backend = init_backend_config(backend, args, accelerator)
        existing_config = StateTracker.get_data_backend_config(init_backend[&quot;id&quot;])
        if existing_config is not None and existing_config != {}:
            raise ValueError(
                f&quot;You can only have one backend named {init_backend[&apos;id&apos;]}&quot;
            )
        StateTracker.set_data_backend_config(init_backend[&quot;id&quot;], init_backend[&quot;config&quot;])
        if backend[&quot;type&quot;] == &quot;local&quot;:
            init_backend[&quot;data_backend&quot;] = get_local_backend(
                accelerator, init_backend[&quot;id&quot;], compress_cache=args.compress_disk_cache
            )
        elif backend[&quot;type&quot;] == &quot;aws&quot;:
            check_aws_config(backend)
            init_backend[&quot;data_backend&quot;] = get_aws_backend(
                identifier=init_backend[&quot;id&quot;],
                aws_bucket_name=backend[&quot;aws_bucket_name&quot;],
                aws_region_name=backend[&quot;aws_region_name&quot;],
                aws_endpoint_url=backend[&quot;aws_endpoint_url&quot;],
                aws_access_key_id=backend[&quot;aws_access_key_id&quot;],
                aws_secret_access_key=backend[&quot;aws_secret_access_key&quot;],
                accelerator=accelerator,
                max_pool_connections=backend.get(
                    &quot;max_pool_connections&quot;, args.aws_max_pool_connections
                ),
            )
            # S3 buckets use the aws_data_prefix as their prefix/ for all data.
            # Ensure we have a trailing slash on the prefix:
            init_backend[&quot;cache_dir&quot;] = backend.get(&quot;aws_data_prefix&quot;, None)
        elif backend[&quot;type&quot;] == &quot;csv&quot;:
            raise ValueError(&quot;Cannot use CSV backend for image embed storage.&quot;)
        else:
            raise ValueError(f&quot;Unknown data backend type: {backend[&apos;type&apos;]}&quot;)
        preserve_data_backend_cache = backend.get(&quot;preserve_data_backend_cache&quot;, False)
        if not preserve_data_backend_cache and accelerator.is_local_main_process:
            StateTracker.delete_cache_files(
                data_backend_id=init_backend[&quot;id&quot;],
                preserve_data_backend_cache=preserve_data_backend_cache,
            )
        image_embed_backends[init_backend[&quot;id&quot;]] = init_backend
    ###                                       ###
    #    now we configure the image backends    #
    ###                                       ###
    vae_cache_dir_paths = []  # tracking for duplicates
    for backend in data_backend_config:
        dataset_type = backend.get(&quot;dataset_type&quot;, None)
        if dataset_type is not None and dataset_type not in [
            &quot;image&quot;,
            &quot;conditioning&quot;,
            &quot;eval&quot;,
            &quot;video&quot;,
        ]:
            # image, conditioning, and eval sets are all included in this
            continue
        if (&quot;disabled&quot; in backend and backend[&quot;disabled&quot;]) or (
            &quot;disable&quot; in backend and backend[&quot;disable&quot;]
        ):
            info_log(f&quot;Skipping disabled data backend {backend[&apos;id&apos;]} in config file.&quot;)
            continue
        # For each backend, we will create a dict to store all of its components in.
        if (
            &quot;id&quot; not in backend
            or backend[&quot;id&quot;] == &quot;&quot;
            or backend[&quot;id&quot;]
            in StateTracker.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;])
        ):
            raise ValueError(&quot;Each dataset needs a unique &apos;id&apos; field.&quot;)
        info_log(f&quot;Configuring data backend: {backend[&apos;id&apos;]}&quot;)
        conditioning_type = backend.get(&quot;conditioning_type&quot;)
        if (
            backend.get(&quot;dataset_type&quot;) == &quot;conditioning&quot;
            or conditioning_type is not None
        ):
            backend[&quot;dataset_type&quot;] = &quot;conditioning&quot;
        resolution_type = backend.get(&quot;resolution_type&quot;, args.resolution_type)
        if resolution_type == &quot;pixel_area&quot;:
            pixel_edge_length = backend.get(&quot;resolution&quot;, int(args.resolution))
            if pixel_edge_length is None or (
                type(pixel_edge_length) is not int
                or not str(pixel_edge_length).isdigit()
            ):
                raise ValueError(
                    f&quot;Resolution type &apos;pixel_area&apos; requires a &apos;resolution&apos; field to be set in the backend config using an integer in the format: 1024, but {pixel_edge_length} was given&quot;
                )
            # we&apos;ll convert pixel_area to area
            backend[&quot;resolution_type&quot;] = &quot;area&quot;
            backend[&quot;resolution&quot;] = (pixel_edge_length * pixel_edge_length) / (1000**2)
            # convert the other megapixel values.
            if (
                backend.get(&quot;maximum_image_size&quot;, None) is not None
                and backend[&quot;maximum_image_size&quot;] &gt; 0
            ):
                backend[&quot;maximum_image_size&quot;] = (
                    backend[&quot;maximum_image_size&quot;] * backend[&quot;maximum_image_size&quot;]
                ) / 1_000_000
            if (
                backend.get(&quot;target_downsample_size&quot;, None) is not None
                and backend[&quot;target_downsample_size&quot;] &gt; 0
            ):
                backend[&quot;target_downsample_size&quot;] = (
                    backend[&quot;target_downsample_size&quot;]
                    * backend[&quot;target_downsample_size&quot;]
                ) / 1_000_000
            if (
                backend.get(&quot;minimum_image_size&quot;, None) is not None
                and backend[&quot;minimum_image_size&quot;] &gt; 0
            ):
                backend[&quot;minimum_image_size&quot;] = (
                    backend[&quot;minimum_image_size&quot;] * backend[&quot;minimum_image_size&quot;]
                ) / 1_000_000
        # Retrieve some config file overrides for commandline arguments, eg. cropping
        init_backend = init_backend_config(backend, args, accelerator)
        StateTracker.set_data_backend_config(
            data_backend_id=init_backend[&quot;id&quot;],
            config=init_backend[&quot;config&quot;],
        )
        preserve_data_backend_cache = backend.get(&quot;preserve_data_backend_cache&quot;, False)
        if not preserve_data_backend_cache:
            StateTracker.delete_cache_files(
                data_backend_id=init_backend[&quot;id&quot;],
                preserve_data_backend_cache=preserve_data_backend_cache,
            )
        StateTracker.load_aspect_resolution_map(
            dataloader_resolution=init_backend[&quot;config&quot;][&quot;resolution&quot;],
        )
        if backend[&quot;type&quot;] == &quot;local&quot;:
            init_backend[&quot;data_backend&quot;] = get_local_backend(
                accelerator, init_backend[&quot;id&quot;], compress_cache=args.compress_disk_cache
            )
            init_backend[&quot;instance_data_dir&quot;] = backend.get(
                &quot;instance_data_dir&quot;, backend.get(&quot;instance_data_root&quot;)
            )
            if init_backend[&quot;instance_data_dir&quot;] is None:
                raise ValueError(
                    &quot;A local backend requires instance_data_dir be defined and pointing to the image data directory.&quot;
                )
            # Remove trailing slash
            if (
                init_backend[&quot;instance_data_dir&quot;] is not None
                and init_backend[&quot;instance_data_dir&quot;][-1] == &quot;/&quot;
            ):
                init_backend[&quot;instance_data_dir&quot;] = init_backend[&quot;instance_data_dir&quot;][
                    :-1
                ]
        elif backend[&quot;type&quot;] == &quot;aws&quot;:
            check_aws_config(backend)
            init_backend[&quot;data_backend&quot;] = get_aws_backend(
                identifier=init_backend[&quot;id&quot;],
                aws_bucket_name=backend[&quot;aws_bucket_name&quot;],
                aws_region_name=backend[&quot;aws_region_name&quot;],
                aws_endpoint_url=backend[&quot;aws_endpoint_url&quot;],
                aws_access_key_id=backend[&quot;aws_access_key_id&quot;],
                aws_secret_access_key=backend[&quot;aws_secret_access_key&quot;],
                accelerator=accelerator,
                compress_cache=args.compress_disk_cache,
                max_pool_connections=backend.get(
                    &quot;max_pool_connections&quot;, args.aws_max_pool_connections
                ),
            )
            # S3 buckets use the aws_data_prefix as their prefix/ for all data.
            init_backend[&quot;instance_data_dir&quot;] = backend.get(&quot;aws_data_prefix&quot;, &quot;&quot;)
        elif backend[&quot;type&quot;] == &quot;csv&quot;:
            check_csv_config(backend=backend, args=args)
            init_backend[&quot;data_backend&quot;] = get_csv_backend(
                accelerator=accelerator,
                id=backend[&quot;id&quot;],
                csv_file=backend[&quot;csv_file&quot;],
                csv_cache_dir=backend[&quot;csv_cache_dir&quot;],
                compress_cache=args.compress_disk_cache,
                hash_filenames=backend.get(&quot;hash_filenames&quot;, False),
            )
            # init_backend[&quot;instance_data_dir&quot;] = backend.get(&quot;instance_data_dir&quot;, backend.get(&quot;instance_data_root&quot;, backend.get(&quot;csv_cache_dir&quot;)))
            init_backend[&quot;instance_data_dir&quot;] = None
            # if init_backend[&quot;instance_data_dir&quot;] is None:
            #     raise ValueError(&quot;CSV backend requires one of instance_data_dir, instance_data_root or csv_cache_dir to be set, as we require a location to place metadata lists.&quot;)
            # Remove trailing slash
            if (
                init_backend[&quot;instance_data_dir&quot;] is not None
                and init_backend[&quot;instance_data_dir&quot;][-1] == &quot;/&quot;
            ):
                init_backend[&quot;instance_data_dir&quot;] = init_backend[&quot;instance_data_dir&quot;][
                    :-1
                ]
        else:
            raise ValueError(f&quot;Unknown data backend type: {backend[&apos;type&apos;]}&quot;)
        # Assign a TextEmbeddingCache to this dataset. it might be undefined.
        text_embed_id = backend.get(
            &quot;text_embeds&quot;,
            backend.get(&quot;text_embed_cache&quot;, default_text_embed_backend_id),
        )
        if text_embed_id not in text_embed_backends:
            raise ValueError(
                f&quot;Text embed backend {text_embed_id} not found in data backend config file.&quot;
            )
        # Do we have a specific VAE embed backend?
        image_embed_backend_id = backend.get(&quot;image_embeds&quot;, None)
        image_embed_data_backend = init_backend
        if image_embed_backend_id is not None:
            if image_embed_backend_id not in image_embed_backends:
                raise ValueError(
                    f&quot;Could not find image embed backend ID in multidatabackend config: {image_embed_backend_id}&quot;
                )
            image_embed_data_backend = image_embed_backends[image_embed_backend_id]
        info_log(f&quot;(id={init_backend[&apos;id&apos;]}) Loading bucket manager.&quot;)
        metadata_backend_args = {}
        metadata_backend = backend.get(&quot;metadata_backend&quot;, &quot;discovery&quot;)
        if metadata_backend == &quot;json&quot; or metadata_backend == &quot;discovery&quot;:
            from helpers.metadata.backends.discovery import DiscoveryMetadataBackend
            MetadataBackendCls = DiscoveryMetadataBackend
        elif metadata_backend == &quot;parquet&quot;:
            from helpers.metadata.backends.parquet import ParquetMetadataBackend
            MetadataBackendCls = ParquetMetadataBackend
            metadata_backend_args[&quot;parquet_config&quot;] = backend.get(&quot;parquet&quot;, None)
            if not metadata_backend_args[&quot;parquet_config&quot;]:
                raise ValueError(
                    &quot;Parquet metadata backend requires a &apos;parquet&apos; field in the backend config containing required fields for configuration.&quot;
                )
        else:
            raise ValueError(f&quot;Unknown metadata backend type: {metadata_backend}&quot;)
        video_config = init_backend[&quot;config&quot;].get(&quot;video&quot;, {})
        init_backend[&quot;metadata_backend&quot;] = MetadataBackendCls(
            id=init_backend[&quot;id&quot;],
            instance_data_dir=init_backend[&quot;instance_data_dir&quot;],
            data_backend=init_backend[&quot;data_backend&quot;],
            accelerator=accelerator,
            resolution=backend.get(&quot;resolution&quot;, args.resolution),
            minimum_image_size=backend.get(
                &quot;minimum_image_size&quot;, args.minimum_image_size
            ),
            minimum_aspect_ratio=backend.get(&quot;minimum_aspect_ratio&quot;, None),
            maximum_aspect_ratio=backend.get(&quot;maximum_aspect_ratio&quot;, None),
            minimum_num_frames=video_config.get(&quot;min_frames&quot;, None),
            maximum_num_frames=video_config.get(&quot;max_frames&quot;, None),
            num_frames=video_config.get(&quot;num_frames&quot;, None),
            resolution_type=backend.get(&quot;resolution_type&quot;, args.resolution_type),
            batch_size=args.train_batch_size,
            metadata_update_interval=backend.get(
                &quot;metadata_update_interval&quot;, args.metadata_update_interval
            ),
            cache_file=os.path.join(
                backend.get(
                    &quot;instance_data_dir&quot;,
                    backend.get(&quot;csv_cache_dir&quot;, backend.get(&quot;aws_data_prefix&quot;, &quot;&quot;)),
                ),
                &quot;aspect_ratio_bucket_indices&quot;,
            ),
            metadata_file=os.path.join(
                backend.get(
                    &quot;instance_data_dir&quot;,
                    backend.get(&quot;csv_cache_dir&quot;, backend.get(&quot;aws_data_prefix&quot;, &quot;&quot;)),
                ),
                &quot;aspect_ratio_bucket_metadata&quot;,
            ),
            delete_problematic_images=args.delete_problematic_images or False,
            delete_unwanted_images=backend.get(
                &quot;delete_unwanted_images&quot;, args.delete_unwanted_images
            ),
            cache_file_suffix=backend.get(&quot;cache_file_suffix&quot;, init_backend[&quot;id&quot;]),
            repeats=init_backend[&quot;config&quot;].get(&quot;repeats&quot;, 0),
            **metadata_backend_args,
        )
        if (
            &quot;aspect&quot; not in args.skip_file_discovery
            and &quot;aspect&quot; not in backend.get(&quot;skip_file_discovery&quot;, &quot;&quot;)
            and conditioning_type not in [&quot;mask&quot;, &quot;controlnet&quot;]
        ):
            if accelerator.is_local_main_process:
                info_log(
                    f&quot;(id={init_backend[&apos;id&apos;]}) Refreshing aspect buckets on main process.&quot;
                )
                init_backend[&quot;metadata_backend&quot;].refresh_buckets(rank_info())
        accelerator.wait_for_everyone()
        if not accelerator.is_main_process:
            info_log(
                f&quot;(id={init_backend[&apos;id&apos;]}) Reloading bucket manager cache on subprocesses.&quot;
            )
            init_backend[&quot;metadata_backend&quot;].reload_cache()
        accelerator.wait_for_everyone()
        if init_backend[&quot;metadata_backend&quot;].has_single_underfilled_bucket():
            raise Exception(
                f&quot;Cannot train using a dataset that has a single bucket with fewer than {args.train_batch_size} images.&quot;
                f&quot; You have to reduce your batch size, or increase your dataset size (id={init_backend[&apos;id&apos;]}).&quot;
            )
        # Now split the contents of these buckets between all processes
        init_backend[&quot;metadata_backend&quot;].split_buckets_between_processes(
            gradient_accumulation_steps=args.gradient_accumulation_steps,
        )
        # Check if there is an existing &apos;config&apos; in the metadata_backend.config
        excluded_keys = [
            &quot;probability&quot;,
            &quot;repeats&quot;,
            &quot;ignore_epochs&quot;,
            &quot;caption_filter_list&quot;,
            &quot;vae_cache_clear_each_epoch&quot;,
            &quot;caption_strategy&quot;,
            &quot;maximum_image_size&quot;,
            &quot;target_downsample_size&quot;,
            &quot;parquet&quot;,
        ]
        # we will set the latest version by default.
        current_config_version = latest_config_version()
        if init_backend[&quot;metadata_backend&quot;].config != {}:
            prev_config = init_backend[&quot;metadata_backend&quot;].config
            # if the prev config used an old default config version, we will update defaults here.
            current_config_version = prev_config.get(&quot;config_version&quot;, None)
            if current_config_version is None:
                # backwards compatibility for non-versioned config files, so that we do not enable life-changing options.
                current_config_version = 1
            logger.debug(
                f&quot;Found existing config (version={current_config_version}): {prev_config}&quot;
            )
            logger.debug(f&quot;Comparing against new config: {init_backend[&apos;config&apos;]}&quot;)
            # Check if any values differ between the &apos;backend&apos; values and the &apos;config&apos; values:
            for key, _ in prev_config.items():
                logger.debug(f&quot;Checking config key: {key}&quot;)
                if key not in excluded_keys:
                    if key in backend and prev_config[key] != backend[key]:
                        if not args.override_dataset_config:
                            raise Exception(
                                f&quot;Dataset {init_backend[&apos;id&apos;]} has inconsistent config, and --override_dataset_config was not provided.&quot;
                                f&quot;\n-&gt; Expected value {key}={prev_config.get(key)} differs from current value={backend.get(key)}.&quot;
                                f&quot;\n-&gt; Recommended action is to correct the current config values to match the values that were used to create this dataset:&quot;
                                f&quot;\n{prev_config}&quot;
                            )
                        else:
                            warning_log(
                                f&quot;Overriding config value {key}={prev_config[key]} with {backend[key]}&quot;
                            )
                            prev_config[key] = backend[key]
                    elif key not in backend:
                        if should_log():
                            warning_log(
                                f&quot;Key {key} not found in the current backend config, using the existing value &apos;{prev_config[key]}&apos;.&quot;
                            )
                        init_backend[&quot;config&quot;][key] = prev_config[key]
        init_backend[&quot;config&quot;][&quot;config_version&quot;] = current_config_version
        StateTracker.set_data_backend_config(init_backend[&quot;id&quot;], init_backend[&quot;config&quot;])
        info_log(f&quot;Configured backend: {init_backend}&quot;)
        if len(init_backend[&quot;metadata_backend&quot;]) == 0 and conditioning_type is None:
            raise Exception(
                f&quot;No images were discovered by the bucket manager in the dataset: {init_backend[&apos;id&apos;]}.&quot;
            )
        print_bucket_info(
            init_backend[&quot;metadata_backend&quot;], init_backend.get(&quot;dataset_type&quot;)
        )
        use_captions = True
        is_regularisation_data = backend.get(
            &quot;is_regularisation_data&quot;, backend.get(&quot;is_regularization_data&quot;, False)
        )
        is_i2v_data = backend.get(&quot;video&quot;, {}).get(
            &quot;is_i2v&quot;, True if args.ltx_train_mode == &quot;i2v&quot; else False
        )
        if &quot;only_instance_prompt&quot; in backend and backend[&quot;only_instance_prompt&quot;]:
            use_captions = False
        elif args.only_instance_prompt:
            use_captions = False
        init_backend[&quot;train_dataset&quot;] = MultiAspectDataset(
            id=init_backend[&quot;id&quot;],
            datasets=[init_backend[&quot;metadata_backend&quot;]],
            is_regularisation_data=is_regularisation_data,
            is_i2v_data=is_i2v_data,
        )
        if &quot;deepfloyd&quot; in args.model_type:
            if init_backend[&quot;metadata_backend&quot;].resolution_type == &quot;area&quot;:
                warning_log(
                    &quot;Resolution type is &apos;area&apos;, but should be &apos;pixel&apos; for DeepFloyd. Unexpected results may occur.&quot;
                )
                if init_backend[&quot;metadata_backend&quot;].resolution &gt; 0.25:
                    warning_log(
                        &quot;Resolution is greater than 0.25 megapixels. This may lead to unconstrained memory requirements.&quot;
                    )
            if init_backend[&quot;metadata_backend&quot;].resolution_type == &quot;pixel&quot;:
                if (
                    &quot;stage2&quot; not in args.model_type
                    and init_backend[&quot;metadata_backend&quot;].resolution &gt; 64
                ):
                    warning_log(
                        &quot;Resolution is greater than 64 pixels, which will possibly lead to poor quality results.&quot;
                    )
        if &quot;deepfloyd-stage2&quot; in args.model_type:
            # Resolution must be at least 256 for Stage II.
            if init_backend[&quot;metadata_backend&quot;].resolution &lt; 256:
                warning_log(
                    &quot;Increasing resolution to 256, as is required for DF Stage II.&quot;
                )
        init_backend[&quot;sampler&quot;] = MultiAspectSampler(
            id=init_backend[&quot;id&quot;],
            metadata_backend=init_backend[&quot;metadata_backend&quot;],
            data_backend=init_backend[&quot;data_backend&quot;],
            accelerator=accelerator,
            batch_size=args.train_batch_size,
            debug_aspect_buckets=args.debug_aspect_buckets,
            delete_unwanted_images=backend.get(
                &quot;delete_unwanted_images&quot;, args.delete_unwanted_images
            ),
            resolution=backend.get(&quot;resolution&quot;, args.resolution),
            resolution_type=backend.get(&quot;resolution_type&quot;, args.resolution_type),
            caption_strategy=backend.get(&quot;caption_strategy&quot;, args.caption_strategy),
            use_captions=use_captions,
            prepend_instance_prompt=backend.get(
                &quot;prepend_instance_prompt&quot;, args.prepend_instance_prompt
            ),
            instance_prompt=backend.get(&quot;instance_prompt&quot;, args.instance_prompt),
            conditioning_type=conditioning_type,
            is_regularisation_data=is_regularisation_data,
            dataset_type=backend.get(&quot;dataset_type&quot;),
        )
        if init_backend[&quot;sampler&quot;].caption_strategy == &quot;parquet&quot;:
            configure_parquet_database(backend, args, init_backend[&quot;data_backend&quot;])
        init_backend[&quot;train_dataloader&quot;] = torch.utils.data.DataLoader(
            init_backend[&quot;train_dataset&quot;],
            batch_size=1,  # The sampler handles batching
            shuffle=False,  # The sampler handles shuffling
            sampler=init_backend[&quot;sampler&quot;],
            collate_fn=lambda examples: collate_fn(examples),
            num_workers=0,
            persistent_workers=False,
        )
        init_backend[&quot;text_embed_cache&quot;] = text_embed_backends[text_embed_id][
            &quot;text_embed_cache&quot;
        ]
        prepend_instance_prompt = backend.get(
            &quot;prepend_instance_prompt&quot;, args.prepend_instance_prompt
        )
        instance_prompt = backend.get(&quot;instance_prompt&quot;, args.instance_prompt)
        if prepend_instance_prompt and instance_prompt is None:
            raise ValueError(
                f&quot;Backend {init_backend[&apos;id&apos;]} has prepend_instance_prompt=True, but no instance_prompt was provided. You must provide an instance_prompt, or disable this option.&quot;
            )
        # Update the backend registration here so the metadata backend can be found.
        StateTracker.register_data_backend(init_backend)
        # We get captions from the IMAGE dataset. Not the text embeds dataset.
        if (
            conditioning_type != &quot;mask&quot;
            and &quot;text&quot; not in args.skip_file_discovery
            and &quot;text&quot; not in backend.get(&quot;skip_file_discovery&quot;, &quot;&quot;)
        ):
            info_log(f&quot;(id={init_backend[&apos;id&apos;]}) Collecting captions.&quot;)
            captions, images_missing_captions = PromptHandler.get_all_captions(
                data_backend=init_backend[&quot;data_backend&quot;],
                instance_data_dir=init_backend[&quot;instance_data_dir&quot;],
                prepend_instance_prompt=prepend_instance_prompt,
                instance_prompt=instance_prompt,
                use_captions=use_captions,
                caption_strategy=backend.get(&quot;caption_strategy&quot;, args.caption_strategy),
            )
            logger.debug(
                f&quot;Pre-computing text embeds / updating cache. We have {len(captions)} captions to process, though these will be filtered next.&quot;
            )
            logger.debug(f&quot;Data missing captions: {images_missing_captions}&quot;)
            if len(images_missing_captions) &gt; 0 and hasattr(
                init_backend[&quot;metadata_backend&quot;], &quot;remove_images&quot;
            ):
                # we&apos;ll tell the aspect bucket manager to remove these images.
                init_backend[&quot;metadata_backend&quot;].remove_images(images_missing_captions)
            caption_strategy = backend.get(&quot;caption_strategy&quot;, args.caption_strategy)
            info_log(
                f&quot;(id={init_backend[&apos;id&apos;]}) Initialise text embed pre-computation using the {caption_strategy} caption strategy. We have {len(captions)} captions to process.&quot;
            )
            init_backend[&quot;text_embed_cache&quot;].compute_embeddings_for_prompts(
                captions, return_concat=False, load_from_cache=False
            )
            info_log(
                f&quot;(id={init_backend[&apos;id&apos;]}) Completed processing {len(captions)} captions.&quot;
            )
        # Register the backend here so the sampler can be found.
        StateTracker.register_data_backend(init_backend)
        default_hash_option = True
        hash_filenames = init_backend[&quot;config&quot;].get(
            &quot;hash_filenames&quot;, default_hash_option
        )
        init_backend[&quot;config&quot;][&quot;hash_filenames&quot;] = hash_filenames
        StateTracker.set_data_backend_config(init_backend[&quot;id&quot;], init_backend[&quot;config&quot;])
        logger.debug(f&quot;Hashing filenames: {hash_filenames}&quot;)
        if (
            &quot;deepfloyd&quot; not in StateTracker.get_args().model_type
            and conditioning_type not in [&quot;mask&quot;, &quot;controlnet&quot;]
        ):
            info_log(f&quot;(id={init_backend[&apos;id&apos;]}) Creating VAE latent cache.&quot;)
            vae_cache_dir = backend.get(&quot;cache_dir_vae&quot;, None)
            if vae_cache_dir in vae_cache_dir_paths:
                raise ValueError(
                    f&quot;VAE image embed cache directory {backend.get(&apos;cache_dir_vae&apos;)} is the same as another VAE image embed cache directory. This is not allowed, the trainer will get confused and sleepy and wake up in a distant place with no memory and no money for a taxi ride back home, forever looking in the mirror and wondering who they are. This should be avoided.&quot;
                )
            vae_cache_dir_paths.append(vae_cache_dir)
            if (
                vae_cache_dir is not None
                and vae_cache_dir in text_embed_cache_dir_paths
            ):
                raise ValueError(
                    f&quot;VAE image embed cache directory {backend.get(&apos;cache_dir_vae&apos;)} is the same as the text embed cache directory. This is not allowed, the trainer will get confused.&quot;
                )
            if backend[&quot;type&quot;] == &quot;local&quot; and (
                vae_cache_dir is None or vae_cache_dir == &quot;&quot;
            ):
                raise ValueError(
                    f&quot;VAE image embed cache directory {backend.get(&apos;cache_dir_vae&apos;)} is not set. This is required for the VAE image embed cache.&quot;
                )
            init_backend[&quot;vaecache&quot;] = VAECache(
                id=init_backend[&quot;id&quot;],
                dataset_type=init_backend[&quot;dataset_type&quot;],
                vae=StateTracker.get_vae(),
                accelerator=accelerator,
                metadata_backend=init_backend[&quot;metadata_backend&quot;],
                image_data_backend=init_backend[&quot;data_backend&quot;],
                cache_data_backend=image_embed_data_backend[&quot;data_backend&quot;],
                instance_data_dir=init_backend[&quot;instance_data_dir&quot;],
                delete_problematic_images=backend.get(
                    &quot;delete_problematic_images&quot;, args.delete_problematic_images
                ),
                resolution=backend.get(&quot;resolution&quot;, args.resolution),
                resolution_type=backend.get(&quot;resolution_type&quot;, args.resolution_type),
                num_video_frames=video_config.get(&quot;num_frames&quot;, None),
                maximum_image_size=backend.get(
                    &quot;maximum_image_size&quot;,
                    args.maximum_image_size
                    or backend.get(&quot;resolution&quot;, args.resolution) * 1.5,
                ),
                target_downsample_size=backend.get(
                    &quot;target_downsample_size&quot;,
                    args.target_downsample_size
                    or backend.get(&quot;resolution&quot;, args.resolution) * 1.25,
                ),
                minimum_image_size=backend.get(
                    &quot;minimum_image_size&quot;,
                    args.minimum_image_size,
                ),
                vae_batch_size=backend.get(&quot;vae_batch_size&quot;, args.vae_batch_size),
                write_batch_size=backend.get(&quot;write_batch_size&quot;, args.write_batch_size),
                read_batch_size=backend.get(&quot;read_batch_size&quot;, args.read_batch_size),
                cache_dir=backend.get(&quot;cache_dir_vae&quot;, args.cache_dir_vae),
                max_workers=backend.get(&quot;max_workers&quot;, args.max_workers),
                process_queue_size=backend.get(
                    &quot;image_processing_batch_size&quot;, args.image_processing_batch_size
                ),
                vae_cache_ondemand=args.vae_cache_ondemand,
                hash_filenames=hash_filenames,
            )
            init_backend[&quot;vaecache&quot;].set_webhook_handler(
                StateTracker.get_webhook_handler()
            )
            if not args.vae_cache_ondemand:
                info_log(f&quot;(id={init_backend[&apos;id&apos;]}) Discovering cache objects..&quot;)
                if accelerator.is_local_main_process:
                    init_backend[&quot;vaecache&quot;].discover_all_files()
                accelerator.wait_for_everyone()
            all_image_files = StateTracker.get_image_files(
                data_backend_id=init_backend[&quot;id&quot;]
            )
            init_backend[&quot;vaecache&quot;].build_vae_cache_filename_map(
                all_image_files=all_image_files
            )
        if (
            (
                &quot;metadata&quot; not in args.skip_file_discovery
                or &quot;metadata&quot; not in backend.get(&quot;skip_file_discovery&quot;, &quot;&quot;)
            )
            and accelerator.is_main_process
            and backend.get(&quot;scan_for_errors&quot;, False)
            and &quot;deepfloyd&quot; not in StateTracker.get_args().model_type
            and conditioning_type not in [&quot;mask&quot;, &quot;controlnet&quot;]
        ):
            info_log(
                f&quot;Beginning error scan for dataset {init_backend[&apos;id&apos;]}. Set &apos;scan_for_errors&apos; to False in the dataset config to disable this.&quot;
            )
            init_backend[&quot;metadata_backend&quot;].handle_vae_cache_inconsistencies(
                vae_cache=init_backend[&quot;vaecache&quot;],
                vae_cache_behavior=backend.get(
                    &quot;vae_cache_scan_behaviour&quot;, args.vae_cache_scan_behaviour
                ),
            )
            init_backend[&quot;metadata_backend&quot;].scan_for_metadata()
        accelerator.wait_for_everyone()
        if not accelerator.is_main_process:
            init_backend[&quot;metadata_backend&quot;].load_image_metadata()
        accelerator.wait_for_everyone()
        if (
            not args.vae_cache_ondemand
            and &quot;vaecache&quot; in init_backend
            and &quot;vae&quot; not in args.skip_file_discovery
            and &quot;vae&quot; not in backend.get(&quot;skip_file_discovery&quot;, &quot;&quot;)
            and &quot;deepfloyd&quot; not in StateTracker.get_args().model_type
            and conditioning_type not in [&quot;mask&quot;, &quot;controlnet&quot;]
        ):
            init_backend[&quot;vaecache&quot;].discover_unprocessed_files()
            if not args.vae_cache_ondemand:
                init_backend[&quot;vaecache&quot;].process_buckets()
            logger.debug(f&quot;Encoding images during training: {args.vae_cache_ondemand}&quot;)
            accelerator.wait_for_everyone()
        info_log(f&quot;Configured backend: {init_backend}&quot;)
        StateTracker.register_data_backend(init_backend)
        init_backend[&quot;metadata_backend&quot;].save_cache()
    # For each image backend, connect it to its conditioning backend.
    for backend in data_backend_config:
        dataset_type = backend.get(&quot;dataset_type&quot;, &quot;image&quot;)
        if dataset_type is not None and dataset_type != &quot;image&quot;:
            # Skip configuration of conditioning/text data backends. It is done earlier.
            continue
        if (&quot;disabled&quot; in backend and backend[&quot;disabled&quot;]) or (
            &quot;disable&quot; in backend and backend[&quot;disable&quot;]
        ):
            info_log(f&quot;Skipping disabled data backend {backend[&apos;id&apos;]} in config file.&quot;)
            continue
        if &quot;conditioning_data&quot; in backend and backend[
            &quot;conditioning_data&quot;
        ] not in StateTracker.get_data_backends(_type=&quot;conditioning&quot;):
            raise ValueError(
                f&quot;Conditioning data backend {backend[&apos;conditioning_data&apos;]} not found in data backend list: {StateTracker.get_data_backends(_type=&apos;conditionin&apos;)}.&quot;
            )
        if &quot;conditioning_data&quot; in backend:
            StateTracker.set_conditioning_dataset(
                backend[&quot;id&quot;], backend[&quot;conditioning_data&quot;]
            )
            info_log(
                f&quot;Successfully configured conditioning image dataset for {backend[&apos;id&apos;]}&quot;
            )
    if len(StateTracker.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;])) == 0:
        raise ValueError(
            &quot;Must provide at least one data backend in the data backend config file.&quot;
        )
    return StateTracker.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;])
def get_local_backend(
    accelerator, identifier: str, compress_cache: bool = False
) -&gt; LocalDataBackend:
    &quot;&quot;&quot;
    Get a local disk backend.
    Args:
        accelerator (Accelerator): A Huggingface Accelerate object.
        identifier (str): An identifier that links this data backend to its other components.
    Returns:
        LocalDataBackend: A LocalDataBackend object.
    &quot;&quot;&quot;
    return LocalDataBackend(
        accelerator=accelerator, id=identifier, compress_cache=compress_cache
    )
def get_csv_backend(
    accelerator,
    id: str,
    csv_file: str,
    csv_cache_dir: str,
    url_column: str,
    caption_column: str,
    compress_cache: bool = False,
    hash_filenames: bool = False,
    shorten_filenames: bool = False,
) -&gt; CSVDataBackend:
    from pathlib import Path
    return CSVDataBackend(
        accelerator=accelerator,
        id=id,
        csv_file=Path(csv_file),
        image_cache_loc=csv_cache_dir,
        url_column=url_column,
        caption_column=caption_column,
        compress_cache=compress_cache,
        shorten_filenames=shorten_filenames,
        hash_filenames=hash_filenames,
    )
def check_csv_config(backend: dict, args) -&gt; None:
    required_keys = {
        &quot;csv_file&quot;: &quot;This is the path to the CSV file containing your image URLs.&quot;,
        &quot;csv_cache_dir&quot;: &quot;This is the path to your temporary cache files where images will be stored. This can grow quite large.&quot;,
        &quot;csv_caption_column&quot;: &quot;This is the column in your csv which contains the caption(s) for the samples.&quot;,
        &quot;csv_url_column&quot;: &quot;This is the column in your csv that contains image urls or paths.&quot;,
    }
    for key in required_keys.keys():
        if key not in backend:
            raise ValueError(
                f&quot;Missing required key {key} in CSV backend config: {required_keys[key]}&quot;
            )
    if not args.compress_disk_cache:
        warning_log(
            &quot;You can save more disk space for cache objects by providing --compress_disk_cache and recreating its contents&quot;
        )
    caption_strategy = backend.get(&quot;caption_strategy&quot;)
    if caption_strategy is None or caption_strategy != &quot;csv&quot;:
        raise ValueError(&quot;CSV backend requires a caption_strategy of &apos;csv&apos;.&quot;)
def check_aws_config(backend: dict) -&gt; None:
    &quot;&quot;&quot;
    Check the configuration for an AWS backend.
    Args:
        backend (dict): A dictionary of the backend configuration.
    Returns:
        None
    &quot;&quot;&quot;
    required_keys = [
        &quot;aws_bucket_name&quot;,
        &quot;aws_region_name&quot;,
        &quot;aws_endpoint_url&quot;,
        &quot;aws_access_key_id&quot;,
        &quot;aws_secret_access_key&quot;,
    ]
    for key in required_keys:
        if key not in backend:
            raise ValueError(f&quot;Missing required key {key} in AWS backend config.&quot;)
def get_aws_backend(
    aws_bucket_name: str,
    aws_region_name: str,
    aws_endpoint_url: str,
    aws_access_key_id: str,
    aws_secret_access_key: str,
    accelerator,
    identifier: str,
    compress_cache: bool = False,
    max_pool_connections: int = 128,
) -&gt; S3DataBackend:
    return S3DataBackend(
        id=identifier,
        bucket_name=aws_bucket_name,
        accelerator=accelerator,
        region_name=aws_region_name,
        endpoint_url=aws_endpoint_url,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        compress_cache=compress_cache,
        max_pool_connections=max_pool_connections,
    )
def select_dataloader_index(step, backends):
    # Generate weights for each backend based on some criteria
    weights = []
    backend_ids = []
    for backend_id, backend in backends.items():
        weight = get_backend_weight(backend_id, backend, step)
        weights.append(weight)
        backend_ids.append(backend_id)
    # Convert to a torch tensor for easy sampling
    weights = torch.tensor(weights, dtype=torch.float32)
    weights /= weights.sum()  # Normalize the weights
    if weights.sum() == 0:
        return None
    # Sample a backend index based on the weights
    chosen_index = torch.multinomial(weights, 1).item()
    chosen_backend_id = backend_ids[chosen_index]
    return chosen_backend_id
def get_backend_weight(backend_id, backend, step):
    backend_config = StateTracker.get_data_backend_config(backend_id)
    prob = backend_config.get(&quot;probability&quot;, 1)
    if StateTracker.get_args().data_backend_sampling == &quot;uniform&quot;:
        return prob
    elif StateTracker.get_args().data_backend_sampling == &quot;auto-weighting&quot;:
        # Get the dataset length (assuming you have a method or property to retrieve it)
        dataset_length = StateTracker.get_dataset_size(backend_id)
        # Calculate the weight based on dataset length
        length_factor = dataset_length / sum(
            StateTracker.get_dataset_size(b)
            for b in StateTracker.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;])
        )
        # Adjust the probability by length factor
        adjusted_prob = prob * length_factor
        disable_step = backend_config.get(&quot;disable_after_epoch_step&quot;, None)
        if disable_step:
            disable_step = int(disable_step)
        else:
            disable_step = float(&quot;inf&quot;)
        adjusted_prob = (
            0
            if int(step) &gt; disable_step
            else max(0, adjusted_prob * (1 - step / disable_step))
        )
        return adjusted_prob
    else:
        raise ValueError(
            f&quot;Unknown sampling weighting method: {StateTracker.get_args().data_backend_sampling}&quot;
        )
def random_dataloader_iterator(step, backends: dict):
    prefetch_log_debug(&quot;Random dataloader iterator launched.&quot;)
    gradient_accumulation_steps = StateTracker.get_args().gradient_accumulation_steps
    logger.debug(f&quot;Backends to select from {backends}&quot;)
    if backends == {}:
        logger.debug(
            &quot;All dataloaders exhausted. Moving to next epoch in main training loop.&quot;
        )
        StateTracker.clear_exhausted_buckets()
        StateTracker.set_repeats(repeats=0)
        return False
    while backends:
        epoch_step = int(step / gradient_accumulation_steps)
        StateTracker.set_epoch_step(epoch_step)
        chosen_backend_id = select_dataloader_index(step, backends)
        if chosen_backend_id is None:
            logger.debug(&quot;No dataloader iterators were available.&quot;)
            break
        chosen_iter = iter(backends[chosen_backend_id])
        try:
            return next(chosen_iter)
        except MultiDatasetExhausted:
            # We may want to repeat the same dataset multiple times in a single epoch.
            # If so, we can just reset the iterator and keep going.
            repeats = StateTracker.get_data_backend_config(chosen_backend_id).get(
                &quot;repeats&quot;, False
            )
            if (
                repeats
                and repeats &gt; 0
                and StateTracker.get_repeats(chosen_backend_id) &lt; repeats
            ):
                StateTracker.increment_repeats(chosen_backend_id)
                logger.debug(
                    f&quot;Dataset (name={chosen_backend_id}) is now sampling its {StateTracker.get_repeats(chosen_backend_id)} repeat out of {repeats} total allowed.&quot;
                )
                continue
            logger.debug(
                f&quot;Dataset (name={chosen_backend_id}) is now exhausted after {StateTracker.get_repeats(chosen_backend_id)} repeat(s). Removing from list.&quot;
            )
            del backends[chosen_backend_id]
            StateTracker.backend_exhausted(chosen_backend_id)
            StateTracker.set_repeats(data_backend_id=chosen_backend_id, repeats=0)
        finally:
            if not backends:
                logger.debug(
                    &quot;All dataloaders exhausted. Moving to next epoch in main training loop.&quot;
                )
                StateTracker.clear_exhausted_buckets()
                return False
class BatchFetcher:
    def __init__(self, step, max_size=10, datasets={}):
        self.queue = queue.Queue(max_size)
        self.datasets = datasets
        self.keep_running = True
        self.step = step
    def start_fetching(self):
        thread = threading.Thread(target=self.fetch_responses)
        thread.start()
        return thread
    def fetch_responses(self):
        prefetch_log_debug(&quot;Launching retrieval thread.&quot;)
        while self.keep_running:
            if self.queue.qsize() &lt; self.queue.maxsize:
                prefetch_log_debug(
                    f&quot;Queue size: {self.queue.qsize()}. Fetching more data.&quot;
                )
                self.queue.put(random_dataloader_iterator(self.step, self.datasets))
                if self.queue.qsize() &gt;= self.queue.maxsize:
                    prefetch_log_debug(&quot;Completed fetching data. Queue is full.&quot;)
                    continue
            else:
                time.sleep(0.5)
        prefetch_log_debug(&quot;Exiting retrieval thread.&quot;)
    def next_response(self, step: int):
        self.step = step
        if self.queue.empty():
            prefetch_log_debug(&quot;Queue is empty. Waiting for data.&quot;)
        while self.queue.empty():
            continue
        prefetch_log_debug(&quot;Queue has data. Yielding next item.&quot;)
        return self.queue.get()
    def stop_fetching(self):
        self.keep_running = False</file><file path="helpers/data_backend/local.py">from helpers.data_backend.base import BaseDataBackend
from helpers.image_manipulation.load import load_image, load_video
from helpers.training import video_file_extensions, image_file_extensions
from pathlib import Path
from io import BytesIO
import os
import logging
import torch
from typing import Any, List, Tuple, Union
from atomicwrites import atomic_write
logger = logging.getLogger(&quot;LocalDataBackend&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class LocalDataBackend(BaseDataBackend):
    def __init__(self, accelerator, id: str, compress_cache: bool = False):
        self.accelerator = accelerator
        self.id = id
        self.type = &quot;local&quot;
        self.compress_cache = compress_cache
    def read(self, filepath: str, as_byteIO: bool = False) -&gt; Any:
        &quot;&quot;&quot;Read and return the content of the file.&quot;&quot;&quot;
        try:
            with open(filepath, &quot;rb&quot;) as file:
                data = file.read()
                if not as_byteIO:
                    return data
                return BytesIO(data)
        except FileNotFoundError:
            logger.error(f&quot;File not found: {filepath}&quot;)
            raise
        except Exception as e:
            logger.error(f&quot;Error reading file {filepath}: {e}&quot;)
            raise
    def write(self, filepath: str, data: Any) -&gt; None:
        &quot;&quot;&quot;Write the provided data to the specified filepath atomically.&quot;&quot;&quot;
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        mode = &quot;wb&quot;
        try:
            with atomic_write(
                filepath, mode=mode, overwrite=True, encoding=None
            ) as temp_file:
                if isinstance(data, Union[dict, torch.Tensor]):
                    self.torch_save(data, temp_file)
                elif isinstance(data, str):
                    temp_file.write(data.encode(&quot;utf-8&quot;))
                elif isinstance(data, bytes):
                    temp_file.write(data)
                else:
                    logger.debug(
                        f&quot;Received an unknown data type to write to disk. Attempting to write as bytes: {type(data)}&quot;
                    )
                    temp_file.write(data)
        except Exception as e:
            logger.error(f&quot;Failed to write data to {filepath}: {e}&quot;)
            raise
    def delete(self, filepath: str) -&gt; None:
        &quot;&quot;&quot;Delete the specified file.&quot;&quot;&quot;
        try:
            if os.path.exists(filepath):
                logger.debug(f&quot;Deleting file: {filepath}&quot;)
                os.remove(filepath)
                logger.info(f&quot;Successfully deleted file: {filepath}&quot;)
            else:
                raise FileNotFoundError(f&quot;{filepath} not found.&quot;)
        except Exception as e:
            logger.error(f&quot;Error deleting file {filepath}: {e}&quot;)
            raise
        # Verify deletion
        if self.exists(filepath):
            error_msg = f&quot;Failed to delete {filepath}&quot;
            logger.error(error_msg)
            raise Exception(error_msg)
    def exists(self, filepath: str) -&gt; bool:
        &quot;&quot;&quot;Check if the file exists.&quot;&quot;&quot;
        return os.path.exists(filepath)
    def open_file(self, filepath: str, mode: str):
        &quot;&quot;&quot;Open the file in the specified mode.&quot;&quot;&quot;
        try:
            return open(filepath, mode)
        except Exception as e:
            logger.error(f&quot;Error opening file {filepath} with mode {mode}: {e}&quot;)
            raise
    def list_files(
        self, file_extensions: List[str], instance_data_dir: str
    ) -&gt; List[Tuple[str, List, List[str]]]:
        &quot;&quot;&quot;
        List all files matching the given file extensions.
        Creates Path objects of each file found.
        &quot;&quot;&quot;
        logger.debug(
            f&quot;LocalDataBackend.list_files: file_extensions={file_extensions}, instance_data_dir={instance_data_dir}&quot;
        )
        if not instance_data_dir:
            raise ValueError(&quot;instance_data_dir must be specified.&quot;)
        def _rglob_follow_symlinks(path: Path, extensions: List[str]):
            # Skip Spotlight and Jupyter directories
            forbidden_directories = {
                &quot;.Spotlight-V100&quot;,
                &quot;.Trashes&quot;,
                &quot;.fseventsd&quot;,
                &quot;.TemporaryItems&quot;,
                &quot;.zfs&quot;,
                &quot;.ipynb_checkpoints&quot;,
            }
            if path.name in forbidden_directories:
                return
            # If no extensions are provided, list all files
            if not extensions:
                for p in path.rglob(&quot;*&quot;):
                    if p.is_file():
                        yield p
            else:
                for ext in extensions:
                    for p in path.rglob(ext):
                        if p.is_file():
                            yield p
            for p in path.iterdir():
                if p.is_dir() and not p.is_symlink():
                    yield from _rglob_follow_symlinks(p, extensions)
                elif p.is_symlink():
                    try:
                        real_path = p.resolve()
                        if real_path.is_dir():
                            yield from _rglob_follow_symlinks(real_path, extensions)
                    except Exception as e:
                        logger.warning(f&quot;Broken symlink encountered: {p} - {e}&quot;)
        # Prepare the extensions for globbing
        extensions = (
            [f&quot;*.{ext.lower()}&quot; for ext in file_extensions] if file_extensions else None
        )
        paths = list(_rglob_follow_symlinks(Path(instance_data_dir), extensions))
        # Group files by their parent directory
        path_dict = {}
        for path in paths:
            parent = str(path.parent)
            path_dict.setdefault(parent, []).append(str(path.absolute()))
        results = [(subdir, [], files) for subdir, files in path_dict.items()]
        return results
    def read_image(self, filepath: str, delete_problematic_images: bool = False) -&gt; Any:
        &quot;&quot;&quot;Read an image from the specified filepath.&quot;&quot;&quot;
        filepath = filepath.replace(&quot;\x00&quot;, &quot;&quot;)
        file_extension = os.path.splitext(filepath)[1].lower().strip(&quot;.&quot;)
        file_loader = load_image
        if file_extension in video_file_extensions:
            file_loader = load_video
        try:
            image = file_loader(filepath)
            return image
        except Exception as e:
            logger.error(
                f&quot;Encountered error opening image {filepath}: {e}&quot;, exc_info=True
            )
            if delete_problematic_images:
                try:
                    logger.error(
                        &quot;Deleting image, because --delete_problematic_images is provided.&quot;
                    )
                    self.delete(filepath)
                except Exception as del_e:
                    logger.error(
                        f&quot;Failed to delete problematic image {filepath}: {del_e}&quot;
                    )
            else:
                raise e
    def read_image_batch(
        self, filepaths: List[str], delete_problematic_images: bool = False
    ) -&gt; Tuple[List[str], List[Any]]:
        &quot;&quot;&quot;Read a batch of images from the specified filepaths.&quot;&quot;&quot;
        if not isinstance(filepaths, list):
            raise ValueError(
                f&quot;read_image_batch must be given a list of image filepaths. Received type: {type(filepaths)}&quot;
            )
        output_images = []
        available_keys = []
        for filepath in filepaths:
            try:
                image_data = self.read_image(filepath, delete_problematic_images)
                if image_data is None:
                    logger.warning(f&quot;Unable to load image &apos;{filepath}&apos;, skipping.&quot;)
                    continue
                output_images.append(image_data)
                available_keys.append(filepath)
            except Exception as e:
                if delete_problematic_images:
                    logger.error(
                        f&quot;Deleting image &apos;{filepath}&apos;, because --delete_problematic_images is provided. Error: {e}&quot;
                    )
                    try:
                        self.delete(filepath)
                    except Exception as del_e:
                        logger.error(
                            f&quot;Failed to delete problematic image {filepath}: {del_e}&quot;
                        )
                else:
                    logger.warning(
                        f&quot;A problematic image {filepath} is detected, but we are not allowed to remove it, because --delete_problematic_images is not provided.&quot;
                        f&quot; Please correct this manually. Error: {e}&quot;
                    )
        return available_keys, output_images
    def create_directory(self, directory_path: str) -&gt; None:
        &quot;&quot;&quot;Create a directory if it does not exist.&quot;&quot;&quot;
        if os.path.exists(directory_path):
            return
        try:
            logger.debug(f&quot;Creating directory: {directory_path}&quot;)
            os.makedirs(directory_path, exist_ok=True)
            logger.info(f&quot;Directory created: {directory_path}&quot;)
        except Exception as e:
            logger.error(f&quot;Failed to create directory {directory_path}: {e}&quot;)
            raise
    def torch_load(self, filename: str) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Load a torch tensor from a file.
        &quot;&quot;&quot;
        if not self.exists(filename):
            error_msg = f&quot;{filename} not found.&quot;
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        try:
            with self.read(filename, as_byteIO=True) as stored_tensor:
                if self.compress_cache:
                    stored_tensor = self._decompress_torch(stored_tensor)
                stored_tensor.seek(0)
                loaded_tensor = torch.load(stored_tensor, map_location=&quot;cpu&quot;)
            return loaded_tensor
        except Exception as e:
            logger.error(f&quot;Failed to load torch file &apos;{filename}&apos;: {e}&quot;, exc_info=True)
            if &quot;invalid load key&quot; in str(e):
                try:
                    self.delete(filename)
                    logger.info(f&quot;Deleted corrupt torch file: {filename}&quot;)
                except Exception as del_e:
                    logger.error(
                        f&quot;Failed to delete corrupt torch file {filename}: {del_e}&quot;
                    )
            raise e
    def torch_save(self, data: torch.Tensor, original_location: Any) -&gt; None:
        &quot;&quot;&quot;
        Save a torch tensor to a file object or filepath.
        &quot;&quot;&quot;
        try:
            if isinstance(original_location, str):
                # original_location is a filepath
                with atomic_write(
                    original_location, mode=&quot;wb&quot;, overwrite=True, encoding=None
                ) as temp_file:
                    if self.compress_cache:
                        compressed_data = self._compress_torch(data)
                        temp_file.write(compressed_data)
                    else:
                        torch.save(data, temp_file)
            else:
                # original_location is a file-like object
                if self.compress_cache:
                    compressed_data = self._compress_torch(data)
                    original_location.write(compressed_data)
                else:
                    torch.save(data, original_location)
                original_location.flush()
                os.fsync(original_location.fileno())
        except Exception as e:
            logger.error(f&quot;Failed to save torch tensor: {e}&quot;, exc_info=True)
            raise
    def write_batch(self, filepaths: List[str], data_list: List[Any]) -&gt; None:
        &quot;&quot;&quot;Write a batch of data to the specified filepaths atomically.&quot;&quot;&quot;
        if len(filepaths) != len(data_list):
            error_msg = &quot;filepaths and data_list must have the same length.&quot;
            logger.error(error_msg)
            raise ValueError(error_msg)
        for filepath, data in zip(filepaths, data_list):
            try:
                self.write(filepath, data)
                logger.debug(f&quot;Successfully wrote to {filepath}&quot;)
            except Exception as e:
                logger.error(f&quot;Failed to write to {filepath}: {e}&quot;)
                raise</file><file path="helpers/image_manipulation/brightness.py">import multiprocessing
import numpy as np
from PIL import Image
def calculate_luminance(img: Image.Image):
    if isinstance(img, np.ndarray):
        np_img = img
    elif isinstance(img, Image.Image):
        np_img = np.asarray(img.convert(&quot;RGB&quot;))
    else:
        raise ValueError(
            f&quot;Unexpected image type for luminance calculation: {type(img)}&quot;
        )
    r, g, b = np_img[:, :, 0], np_img[:, :, 1], np_img[:, :, 2]
    luminance = 0.299 * r + 0.587 * g + 0.114 * b
    avg_luminance = np.mean(luminance)
    return avg_luminance
def worker_batch_luminance(imgs: list):
    return [calculate_luminance(img) for img in imgs]
def calculate_batch_luminance(imgs: list):
    num_processes = multiprocessing.cpu_count()
    with multiprocessing.Pool(num_processes) as pool:
        # Splitting images into batches for each process
        img_batches = [imgs[i::num_processes] for i in range(num_processes)]
        results = pool.map(worker_batch_luminance, img_batches)
    # Flatten the results and calculate average luminance
    all_luminance_values = [lum for sublist in results for lum in sublist]
    return sum(all_luminance_values) / len(all_luminance_values)</file><file path="helpers/image_manipulation/cropping.py">from PIL import Image
import logging
import os
from typing import Union
import numpy as np
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class BaseCropping:
    def __init__(
        self, image: Union[Image.Image, np.ndarray] = None, image_metadata: dict = None
    ):
        self.original_height = None
        self.original_width = None
        self.intermediary_height = None
        self.intermediary_width = None
        self.image = image
        self.image_metadata = image_metadata
        # When we&apos;ve only got metadata, we can&apos;t crop the image.
        self.meta_crop = False
        if self.image is not None:
            if isinstance(self.image, Image.Image):
                self.original_width, self.original_height = self.image.size
            elif isinstance(self.image, np.ndarray):
                # Support both single image (3D) and video (4D)
                if self.image.ndim == 4:  # video: (num_frames, height, width, channels)
                    _, h, w, _ = self.image.shape
                    self.original_width, self.original_height = w, h
                elif self.image.ndim == 3:  # single image: (height, width, channels)
                    h, w = self.image.shape[:2]
                    self.original_width, self.original_height = w, h
                else:
                    raise ValueError(
                        f&quot;Unexpected shape for training sample: {self.image.shape}&quot;
                    )
        elif self.image_metadata:
            self.original_width, self.original_height = self.image_metadata[
                &quot;original_size&quot;
            ]
    def crop(self, target_width, target_height):
        raise NotImplementedError(&quot;Subclasses must implement this method&quot;)
    def set_image(self, image: Union[Image.Image, np.ndarray]):
        if type(image) not in [Image.Image, np.ndarray]:
            raise TypeError(&quot;Image must be a PIL Image or a NumPy ndarray&quot;)
        self.image = image
        return self
    def set_intermediary_size(self, width, height):
        self.intermediary_width = width
        self.intermediary_height = height
        return self
class CornerCropping(BaseCropping):
    def crop(self, target_width, target_height):
        left = max(0, self.intermediary_width - target_width)
        top = max(0, self.intermediary_height - target_height)
        right = self.intermediary_width
        bottom = self.intermediary_height
        if self.image is not None:
            if isinstance(self.image, Image.Image):
                return self.image.crop((left, top, right, bottom)), (top, left)
            elif isinstance(self.image, np.ndarray):
                # Handle both video (4D) and single image (3D)
                if self.image.ndim == 4:
                    cropped = self.image[:, top:bottom, left:right, :]
                else:
                    cropped = self.image[top:bottom, left:right, :]
                return cropped, (top, left)
        elif self.image_metadata:
            return None, (top, left)
class CenterCropping(BaseCropping):
    def crop(self, target_width, target_height):
        left = int((self.intermediary_width - target_width) / 2)
        top = int((self.intermediary_height - target_height) / 2)
        right = left + target_width
        bottom = top + target_height
        if self.image is not None:
            if isinstance(self.image, Image.Image):
                return self.image.crop((left, top, right, bottom)), (top, left)
            elif isinstance(self.image, np.ndarray):
                if self.image.ndim == 4:
                    cropped = self.image[:, top:bottom, left:right, :]
                else:
                    cropped = self.image[top:bottom, left:right, :]
                return cropped, (top, left)
        elif self.image_metadata:
            return None, (top, left)
class RandomCropping(BaseCropping):
    def crop(self, target_width, target_height):
        import random
        left = random.randint(0, max(0, self.intermediary_width - target_width))
        top = random.randint(0, max(0, self.intermediary_height - target_height))
        right = left + target_width
        bottom = top + target_height
        if self.image is not None:
            if isinstance(self.image, Image.Image):
                return self.image.crop((left, top, right, bottom)), (top, left)
            elif isinstance(self.image, np.ndarray):
                if self.image.ndim == 4:
                    cropped = self.image[:, top:bottom, left:right, :]
                else:
                    cropped = self.image[top:bottom, left:right, :]
                return cropped, (top, left)
        elif self.image_metadata:
            return None, (top, left)
class FaceCropping(RandomCropping):
    def crop(
        self,
        image: Union[Image.Image, np.ndarray],
        target_width: int,
        target_height: int,
    ):
        import cv2
        import numpy as np
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + &quot;haarcascade_frontalface_default.xml&quot;
        )
        if isinstance(image, np.ndarray):
            # Assume it&apos;s a video (4D) and use the first frame for face detection.
            sample_frame = image[0]
            gray = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            if len(faces) &gt; 0:
                # Get the largest face.
                face = max(faces, key=lambda f: f[2] * f[3])
                x, y, w, h = face
                left = int(max(0, x - 0.5 * w))
                top = int(max(0, y - 0.5 * h))
                right = int(min(sample_frame.shape[1], x + 1.5 * w))
                bottom = int(min(sample_frame.shape[0], y + 1.5 * h))
            else:
                # Fallback to random cropping on the sample frame.
                import random
                left = random.randint(0, max(0, sample_frame.shape[1] - target_width))
                top = random.randint(0, max(0, sample_frame.shape[0] - target_height))
                right = left + target_width
                bottom = top + target_height
            # Crop all frames in the video.
            cropped = image[:, top:bottom, left:right, :]
            return cropped, (top, left)
        elif isinstance(image, Image.Image):
            image_rgb = image.convert(&quot;RGB&quot;)
            image_np = np.array(image_rgb)
            gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
            if len(faces) &gt; 0:
                face = max(faces, key=lambda f: f[2] * f[3])
                x, y, w, h = face
                left = max(0, x - 0.5 * w)
                top = max(0, y - 0.5 * h)
                right = min(image_np.shape[1], x + 1.5 * w)
                bottom = min(image_np.shape[0], y + 1.5 * h)
                return image.crop((left, top, right, bottom)), (top, left)
            else:
                return super().crop(target_width, target_height)
# Dictionary mapping crop types to classes.
crop_handlers = {
    &quot;corner&quot;: CornerCropping,
    &quot;centre&quot;: CenterCropping,
    &quot;center&quot;: CenterCropping,
    &quot;random&quot;: RandomCropping,
    &quot;face&quot;: FaceCropping,
}</file><file path="helpers/image_manipulation/load.py">import logging
import tempfile
import os
import numpy as np
from io import BytesIO
from typing import Union, IO, Any
from PIL import Image, PngImagePlugin
logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)
try:
    import cv2
except Exception as e:
    if &quot;libGL&quot; in str(e):
        print(
            &quot;An error occurred while importing OpenCV2 due to a missing LibGL dependency on your system or container.&quot;
            &quot; Unfortunately, this is not a dependency that SimpleTuner can include during install time.&quot;
            &quot;\nFor Ubuntu systems, you can typically resolve this by running the following command:\n&quot;
            &quot;sudo apt-get install libgl1-mesa-glx&quot;
            &quot;\nor, if that does not work:\n&quot;
            &quot;sudo apt-get install libgl1-mesa-dri&quot;
            &quot;\nIf all else fails, you may need to contact the support department for your chosen platform.&quot;
            &quot; You can find the full error message at the end of debug.log inside the SimpleTuner directory.&quot;
        )
        from sys import exit
        exit(1)
    else:
        raise e
LARGE_ENOUGH_NUMBER = 100
PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)
def decode_image_with_opencv(nparr: np.ndarray) -&gt; Union[Image.Image, None]:
    img_cv = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img_cv is not None:
        img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)
        # Ensuring we only convert to RGB if needed.
        if len(img_cv.shape) == 2 or (img_cv.shape[2] != 3 and img_cv.shape[2] == 1):
            img_cv = cv2.cvtColor(img_cv, cv2.COLOR_GRAY2RGB)
    return img_cv if img_cv is None else Image.fromarray(img_cv)
def decode_image_with_pil(img_data: bytes) -&gt; Image.Image:
    try:
        if isinstance(img_data, bytes):
            img_pil = Image.open(BytesIO(img_data))
        else:
            img_pil = Image.open(img_data)
        if img_pil.mode not in [&quot;RGB&quot;, &quot;RGBA&quot;] and &quot;transparency&quot; in img_pil.info:
            img_pil = img_pil.convert(&quot;RGBA&quot;)
        # For transparent images, add a white background as this is correct
        # most of the time.
        if img_pil.mode == &quot;RGBA&quot;:
            canvas = Image.new(&quot;RGBA&quot;, img_pil.size, (255, 255, 255))
            canvas.alpha_composite(img_pil)
            img_pil = canvas.convert(&quot;RGB&quot;)
        else:
            img_pil = img_pil.convert(&quot;RGB&quot;)
    except (OSError, Image.DecompressionBombError, ValueError) as e:
        logger.warning(f&quot;Error decoding image: {e}&quot;)
        raise
    return img_pil
def load_image(img_data: Union[bytes, IO[Any], str]) -&gt; Image.Image:
    &quot;&quot;&quot;
    Load an image using CV2. If that fails, fall back to PIL.
    The image is returned as a PIL object.
    &quot;&quot;&quot;
    if isinstance(img_data, str):
        with open(img_data, &quot;rb&quot;) as file:
            img_data = file.read()
    elif hasattr(img_data, &quot;read&quot;):
        # Check if it&apos;s file-like object.
        img_data = img_data.read()
    # Preload the image bytes with channels unchanged and ensure determine
    # if the image has an alpha channel. If it does we should add a white
    # background to it using PIL.
    nparr = np.frombuffer(img_data, np.uint8)
    image_preload = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)
    has_alpha = False
    if (
        image_preload is not None
        and len(image_preload.shape) &gt;= 3
        and image_preload.shape[2] == 4
    ):
        has_alpha = True
    del image_preload
    img = None
    if not has_alpha:
        img = decode_image_with_opencv(nparr)
    if img is None:
        img = decode_image_with_pil(img_data)
    return img
def load_video(vid_data: Union[bytes, IO[Any], str]) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Load a video using OpenCV&apos;s VideoCapture.
    Accepts a file path (str), a file-like object, or raw bytes.
    Reads all frames from the video and returns them as a NumPy array.
    Raises:
        ValueError: If the video cannot be opened or no frames are read.
        TypeError: If the input type is not supported.
    &quot;&quot;&quot;
    tmp_path = None
    # If it&apos;s a file path, use it directly.
    if isinstance(vid_data, str):
        video_path = vid_data
    # If it&apos;s a file-like object.
    elif hasattr(vid_data, &quot;read&quot;):
        data = vid_data.read()
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=&quot;.mp4&quot;)
        try:
            tmp.write(data)
            video_path = tmp.name
            tmp_path = video_path
        finally:
            tmp.close()
    # If it&apos;s raw bytes.
    elif isinstance(vid_data, bytes):
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=&quot;.mp4&quot;)
        try:
            tmp.write(vid_data)
            video_path = tmp.name
            tmp_path = video_path
        finally:
            tmp.close()
    else:
        raise TypeError(
            &quot;Unsupported type for vid_data. Expected str, bytes, or file-like object.&quot;
        )
    # Open the video using VideoCapture.
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        if tmp_path:
            os.remove(tmp_path)
        raise ValueError(&quot;Failed to open video.&quot;)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame_rgb)
    cap.release()
    # Clean up temporary file if one was used.
    if tmp_path:
        os.remove(tmp_path)
    if not frames:
        raise ValueError(&quot;No frames were read from the video.&quot;)
    # Stack frames into a numpy array: shape (num_frames, height, width, channels)
    video_array = np.stack(frames, axis=0)
    return video_array</file><file path="helpers/image_manipulation/training_sample.py">from PIL import Image
from PIL.ImageOps import exif_transpose
from helpers.multiaspect.image import MultiaspectImage, resize_helpers
from helpers.multiaspect.video import resize_video_frames
from helpers.image_manipulation.cropping import crop_handlers
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import should_log
from diffusers.utils.export_utils import export_to_gif
import logging
import os, cv2
from tqdm import tqdm
from math import sqrt
import random
import time
import numpy as np
logger = logging.getLogger(__name__)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(&quot;ERROR&quot;)
class TrainingSample:
    def __init__(
        self,
        image: Image.Image,
        data_backend_id: str,
        image_metadata: dict = None,
        image_path: str = None,
        conditioning_type: str = None,
    ):
        &quot;&quot;&quot;
        Initializes a new TrainingSample instance with a provided PIL.Image object and a data backend identifier.
        Args:
            image (Image.Image): A PIL Image object.
            data_backend_id (str): Identifier for the data backend used for additional operations.
            metadata (dict): Optional metadata associated with the image.
        &quot;&quot;&quot;
        self.image = image
        self.target_size = None
        self.intermediary_size = None
        self.original_size = None
        self.conditioning_type = conditioning_type
        self.data_backend_id = data_backend_id
        self.image_metadata = (
            image_metadata
            if image_metadata
            else StateTracker.get_metadata_by_filepath(image_path, data_backend_id)
        )
        if isinstance(image, np.ndarray):
            if len(image.shape) == 4:
                logger.debug(f&quot;Received 4D Shape: {image.shape}&quot;)
                self.original_size = (image.shape[2], image.shape[1])
            elif len(image.shape) == 5:
                raise ValueError(
                    f&quot;Received invalid shape: {image.shape}, expected 4D item instead&quot;
                )
            logger.debug(
                f&quot;Checking on {type(image)}: {self.original_size[0]}x{self.original_size[1]}&quot;
            )
            self.original_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
                self.original_size
            )
        elif hasattr(image, &quot;size&quot;):
            self.original_size = self.image.size
            self.original_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
                self.original_size
            )
        elif image_metadata is not None:
            self.original_size = image_metadata.get(&quot;original_size&quot;)
            self.original_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
                self.original_size
            )
        self.current_size = self.original_size
        if not self.original_size:
            raise Exception(&quot;Original size not found in metadata.&quot;)
        # Torchvision transforms turn the pixels into a Tensor and normalize them for the VAE.
        self.transforms = MultiaspectImage.get_image_transforms()
        # Backend config details
        self.data_backend_config = StateTracker.get_data_backend_config(data_backend_id)
        self.crop_enabled = self.data_backend_config.get(&quot;crop&quot;, False)
        self.crop_style = self.data_backend_config.get(&quot;crop_style&quot;, &quot;random&quot;)
        self.crop_aspect = self.data_backend_config.get(&quot;crop_aspect&quot;, &quot;square&quot;)
        self.crop_aspect_buckets = self.data_backend_config.get(
            &quot;crop_aspect_buckets&quot;, []
        )
        self.crop_coordinates = (0, 0)
        crop_handler_cls = crop_handlers.get(self.crop_style)
        if not crop_handler_cls:
            raise ValueError(f&quot;Unknown crop style: {self.crop_style}&quot;)
        self.cropper = crop_handler_cls(image=self.image, image_metadata=image_metadata)
        self.resolution = self.data_backend_config.get(&quot;resolution&quot;)
        self.resolution_type = self.data_backend_config.get(&quot;resolution_type&quot;)
        self.target_size_calculator = resize_helpers.get(self.resolution_type)
        if self.target_size_calculator is None and conditioning_type not in [
            &quot;mask&quot;,
            &quot;controlnet&quot;,
        ]:
            raise ValueError(f&quot;Unknown resolution type: {self.resolution_type}&quot;)
        self._set_resolution()
        self.target_downsample_size = self.data_backend_config.get(
            &quot;target_downsample_size&quot;, None
        )
        self.maximum_image_size = self.data_backend_config.get(
            &quot;maximum_image_size&quot;, None
        )
        self._image_path = image_path
        # RGB/EXIF conversions.
        self.correct_image()
        self._validate_image_metadata()
    def save_debug_image(self, path: str):
        if self.image is not None:
            if os.environ.get(&quot;SIMPLETUNER_DEBUG_IMAGE_PREP&quot;, &quot;&quot;) == &quot;true&quot;:
                if hasattr(self.image, &quot;save&quot;):
                    self.image.save(path)
                else:
                    # switch .png to .mp4
                    if path.endswith(&quot;.png&quot;):
                        path = path.replace(&quot;.png&quot;, &quot;.mp4&quot;)
                    logger.debug(f&quot;Not saving debug video output: {path}&quot;)
                    # write to path
                    import imageio
                    from io import BytesIO
                    video_byte_array = BytesIO()
                    imageio.v3.imwrite(
                        video_byte_array,
                        self.image,  # a list of NumPy arrays
                        plugin=&quot;pyav&quot;,  # or &quot;ffmpeg&quot;
                        fps=StateTracker.get_args().framerate,
                        extension=&quot;.mp4&quot;,
                        codec=&quot;libx264&quot;,
                    )
                    video_byte_array.seek(0)
                    with open(path, &quot;wb&quot;) as f:
                        f.write(video_byte_array.read())
        return self
    @staticmethod
    def from_image_path(image_path: str, data_backend_id: str):
        &quot;&quot;&quot;
        Create a new TrainingSample instance from an image path.
        Args:
            image_path (str): The path to the image.
            data_backend_id (str): Identifier for the data backend used for additional operations.
        Returns:
            TrainingSample: A new TrainingSample instance.
        &quot;&quot;&quot;
        data_backend = StateTracker.get_data_backend(data_backend_id)
        image = data_backend[&quot;data_backend&quot;].read_image(image_path)
        return TrainingSample(image, data_backend_id, image_path=image_path)
    def _validate_image_metadata(self) -&gt; bool:
        &quot;&quot;&quot;
        Determine whether all required keys exist for prepare() to skip calculations.
        This is useful because randomised aspect buckets must be preserved across runs to avoid mismatched tensor dimensions.
        Returns:
            bool: True if the metadata is valid, False otherwise.
        &quot;&quot;&quot;
        required_keys = [
            &quot;original_size&quot;,
            &quot;target_size&quot;,
            &quot;intermediary_size&quot;,
            &quot;crop_coordinates&quot;,
            &quot;aspect_ratio&quot;,
        ]
        if type(self.image_metadata) is not dict:
            self.valid_metadata = False
        else:
            self.valid_metadata = all(
                key in self.image_metadata for key in required_keys
            )
        if self.valid_metadata:
            self.original_size = self.image_metadata[&quot;original_size&quot;]
            self.target_size = self.image_metadata[&quot;target_size&quot;]
            self.intermediary_size = self.image_metadata[&quot;intermediary_size&quot;]
            self.crop_coordinates = self.image_metadata[&quot;crop_coordinates&quot;]
            self.aspect_ratio = self.image_metadata[&quot;aspect_ratio&quot;]
        self.original_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            self.original_size
        )
        if (
            not self.valid_metadata
            and hasattr(self.image, &quot;size&quot;)
            and isinstance(self.image, Image.Image)
        ):
            self.original_size = self.image.size
        return self.valid_metadata
    def _set_resolution(self):
        if self.resolution_type == &quot;pixel&quot;:
            self.target_area = self.resolution
            # Store the pixel value, eg. 1024
            self.pixel_resolution = int(self.resolution)
            # Store the megapixel value, eg. 1.0
            self.megapixel_resolution = self.resolution / 1e3
        elif self.resolution_type == &quot;area&quot;:
            # Convert pixel area to megapixels, remapping commonly used round values
            # to their pixel_area equivalents for compatibility purposes.
            self.target_area = {
                0.25: 512**2,
                0.5: 768**2,
                1.0: 1024**2,
                2.0: 1536**2,
                4.0: 2048**2,
            }.get(self.resolution, self.resolution * 1e6)
            # Store the pixel value, eg. 1024
            self.pixel_resolution = int(
                MultiaspectImage._round_to_nearest_multiple(
                    sqrt(self.resolution * (1024**2))
                )
            )
            # Store the megapixel value, eg. 1.0
            self.megapixel_resolution = self.resolution
        else:
            raise Exception(f&quot;Unknown resolution type: {self.resolution_type}&quot;)
    def _trim_aspect_bucket_list(self):
        &quot;&quot;&quot;
        Momentarily return a temporarily list of pruned buckets that&apos;ll work for this image.
        An aspect bucket will &quot;work&quot; if the image must be upscaled less than 20% to fit into it.
        Returns:
            list[float]: The list of available aspect buckets
        &quot;&quot;&quot;
        available_buckets = []
        for bucket in self.crop_aspect_buckets:
            # We want to ensure we don&apos;t upscale images beyond about 20% of their original size.
            # If any of the aspect buckets will result in that, we&apos;ll ignore it.
            if type(bucket) is dict:
                aspect = bucket[&quot;aspect_ratio&quot;]
            elif type(bucket) is float or type(bucket) is int:
                aspect = bucket
            else:
                raise ValueError(
                    &quot;Aspect buckets must be a list of floats or dictionaries.&quot;
                )
            # Calculate new size
            target_size, intermediary_size, aspect_ratio = self.target_size_calculator(
                aspect, self.resolution, self.original_size
            )
            # Check the size vs a 20% threshold
            if (
                target_size[0] * 1.2 &lt; self.original_size[0]
                and target_size[1] * 1.2 &lt; self.original_size[1]
            ):
                available_buckets.append(aspect)
        return available_buckets
    def _select_random_aspect(self):
        &quot;&quot;&quot;
        This method returns an aspect bucket based on the crop_aspect configuration.
        If crop_aspect is &quot;closest&quot;, it returns the closest aspect ratio.
        If crop_aspect is &quot;random&quot;, it returns a random aspect ratio based on weights.
        Returns:
            float: The selected aspect ratio.
        &quot;&quot;&quot;
        if not self.crop_aspect_buckets:
            raise ValueError(
                &quot;Aspect buckets are not defined in the data backend config.&quot;
            )
        if self.valid_metadata:
            self.aspect_ratio = self.image_metadata[&quot;aspect_ratio&quot;]
            return self.aspect_ratio
        # Handle &apos;preserve&apos; crop_aspect mode by picking the closest aspect ratio
        if self.crop_aspect == &quot;closest&quot;:
            closest_aspect = min(
                self.crop_aspect_buckets,
                key=lambda bucket: abs(
                    (bucket[&quot;aspect&quot;] if isinstance(bucket, dict) else bucket)
                    - self.aspect_ratio
                ),
            )
            closest_aspect_value = (
                closest_aspect[&quot;aspect&quot;]
                if isinstance(closest_aspect, dict)
                else closest_aspect
            )
            # logger.debug(f&quot;Selected closest aspect: {closest_aspect_value} for aspect ratio: {self.aspect_ratio}&quot;)
            return closest_aspect_value
        # Handle &apos;random&apos; crop_aspect mode by picking a random aspect ratio based on weights
        if self.crop_aspect == &quot;random&quot;:
            if (
                len(self.crop_aspect_buckets) &gt; 0
                and type(self.crop_aspect_buckets[0]) is dict
            ):
                has_portrait_buckets = any(
                    bucket[&quot;aspect&quot;] &lt; 1.0 for bucket in self.crop_aspect_buckets
                )
                has_landscape_buckets = any(
                    bucket[&quot;aspect&quot;] &gt; 1.0 for bucket in self.crop_aspect_buckets
                )
                logger.error(
                    f&quot;has_portrait_buckets: {has_portrait_buckets}, has_landscape_buckets: {has_landscape_buckets}&quot;
                )
                # Instead of defaulting to 1.0, use whatever buckets are available
                aspects = [bucket[&quot;aspect&quot;] for bucket in self.crop_aspect_buckets]
                weights = [bucket[&quot;weight&quot;] for bucket in self.crop_aspect_buckets]
                # Ensure that the weights add up to 1.0
                total_weight = sum(weights)
                if total_weight != 1.0:
                    raise ValueError(&quot;The weights of aspect buckets must add up to 1.&quot;)
                selected_aspect = random.choices(aspects, weights)[0]
                return selected_aspect
            elif (
                len(self.crop_aspect_buckets) &gt; 0
                and type(self.crop_aspect_buckets[0]) is float
            ):
                available_aspects = self._trim_aspect_bucket_list()
                if len(available_aspects) == 0:
                    selected_aspect = 1.0
                    if should_log():
                        tqdm.write(
                            &quot;[WARNING] Image dimensions do not fit into the configured aspect buckets. Using square crop.&quot;
                        )
                else:
                    selected_aspect = random.choice(available_aspects)
                return selected_aspect
            else:
                raise ValueError(
                    &quot;Aspect buckets must be a list of floats or dictionaries.&quot;
                    &quot; If using a dictionary, it is expected to be in the format {&apos;aspect&apos;: 1.0, &apos;weight&apos;: 0.5}.&quot;
                    &quot; To provide multiple aspect ratios, use a list of dictionaries: [{&apos;aspect&apos;: 1.0, &apos;weight&apos;: 0.5}, {&apos;aspect&apos;: 1.5, &apos;weight&apos;: 0.5}].&quot;
                )
        # Default to 1.0 if none of the conditions above match
        return 1.0
    def prepare_like(self, other_sample, return_tensor=False):
        &quot;&quot;&quot;
        Prepare the current TrainingSample in the same way as other_sample.
        Args:
            other_sample (TrainingSample): The sample to mimic.
            return_tensors (bool): Whether to return tensors.
        Returns:
            PreparedSample: The prepared sample.
        &quot;&quot;&quot;
        # Copy over the image metadata from the other sample
        self.image_metadata = (
            other_sample.image_metadata.copy() if other_sample.image_metadata else {}
        )
        # Validate the metadata to set internal attributes
        self._validate_image_metadata()
        # Proceed to prepare the image
        return self.prepare(return_tensor=return_tensor)
    def prepare(self, return_tensor: bool = False):
        &quot;&quot;&quot;
        Perform initial image preparations such as converting to RGB and applying EXIF transformations.
        Args:
            image (Image.Image): The image to prepare.
        Returns: tuple
            - image data (PIL.Image)
            - crop_coordinates (tuple)
            - aspect_ratio (float)
        &quot;&quot;&quot;
        self.save_debug_image(f&quot;images/{time.time()}-0-original.png&quot;)
        self.crop()
        self.save_debug_image(f&quot;images/{time.time()}-1-cropped.png&quot;)
        if not self.crop_enabled:
            self.save_debug_image(f&quot;images/{time.time()}-1b-nocrop-resize.png&quot;)
            self.resize()
            self.save_debug_image(f&quot;images/{time.time()}-2-final-output.png&quot;)
        image = self.image
        if return_tensor:
            # Return normalised tensor.
            image = self.transforms(image)
        webhook_handler = StateTracker.get_webhook_handler()
        prepared_sample = PreparedSample(
            image=image,
            original_size=self.original_size,
            crop_coordinates=self.crop_coordinates,
            aspect_ratio=self.aspect_ratio,
            image_metadata=self.image_metadata,
            target_size=self.target_size,
            intermediary_size=self.intermediary_size,
        )
        if webhook_handler:
            webhook_handler.send(
                message=f&quot;Debug info for prepared sample, {str(prepared_sample)}&quot;,
                images=[self.image],
                message_level=&quot;debug&quot;,
            )
        return prepared_sample
    def area(self) -&gt; int:
        &quot;&quot;&quot;
        Calculate the area of the image.
        Returns:
            int: The area of the image.
        &quot;&quot;&quot;
        if self.image is not None:
            return self.image.size[0] * self.image.size[1]
        if self.original_size:
            return self.original_size[0] * self.original_size[1]
    def _should_resize_before_crop(self) -&gt; bool:
        &quot;&quot;&quot;
        If the options to do so are enabled, or, the image require it; we will resize before cropping.
        Returns:
            bool: True if the image should be resized before cropping, False otherwise.
        &quot;&quot;&quot;
        if (
            not self.crop_enabled
            or not self.maximum_image_size
            or not self.target_downsample_size
        ):
            return False
        if self.data_backend_config.get(&quot;resolution_type&quot;) == &quot;pixel&quot;:
            return (
                self.current_size[0] &gt; self.pixel_resolution
                or self.current_size[1] &gt; self.pixel_resolution
            ) or (
                self.current_size[0] &lt; self.pixel_resolution
                or self.current_size[1] &lt; self.pixel_resolution
            )
        elif self.data_backend_config.get(&quot;resolution_type&quot;) == &quot;area&quot;:
            should_resize = (
                self.area() &gt; self.target_area
                or self.area() &lt; self.target_area
                or self.current_size[0] &lt; self.target_size[0]
                or self.current_size[1] &lt; self.target_size[1]
            )
            logger.debug(f&quot;Should resize? {should_resize}&quot;)
            return should_resize
        else:
            raise ValueError(
                f&quot;Unknown resolution type: {self.data_backend_config.get(&apos;resolution_type&apos;)}&quot;
            )
    def _calculate_target_downsample_size(self):
        &quot;&quot;&quot;
        When cropping images, it is optional to disturb them with a resize before the crop.
        This is desirable when a large image is being cropped to a small size, as it will preserve scene details and maintain aspect ratio.
        Returns:
            tuple: The target downsample size as (width, height).
        &quot;&quot;&quot;
        # We&apos;ll run the target size calculator logic without updating any of the object attributes.
        # This will prevent contamination of the final values that the image will represent.
        _, calculated_intermediary_size, _ = self.target_size_calculator(
            self.original_aspect_ratio, self.target_downsample_size, self.original_size
        )
        # The calculated_intermediary_size&apos;s purpose is to resize to this value before cropping to target_size.
        # If the intermediary size is smaller than target_size on either edge, the cropping will result in black bars.
        # We have to calculate the scale factor and adjust the image edges proportionally to avoid squishing it.
        if calculated_intermediary_size[0] &lt; self.target_size[0]:
            scale_factor = self.target_size[0] / calculated_intermediary_size[0]
            calculated_intermediary_size = (
                self.target_size[0],
                int(calculated_intermediary_size[1] * scale_factor),
            )
        elif calculated_intermediary_size[1] &lt; self.target_size[1]:
            scale_factor = self.target_size[1] / calculated_intermediary_size[1]
            calculated_intermediary_size = (
                int(calculated_intermediary_size[0] * scale_factor),
                self.target_size[1],
            )
        return calculated_intermediary_size
    def _downsample_before_crop(self):
        &quot;&quot;&quot;
        Downsample the image before cropping, to preserve scene details and maintain aspect ratio.
        Returns:
            TrainingSample: The current TrainingSample instance.
        &quot;&quot;&quot;
        if self._should_resize_before_crop():
            target_downsample_size = self._calculate_target_downsample_size()
            logger.debug(
                f&quot;Calculated target_downsample_size, resizing to {target_downsample_size}&quot;
            )
            self.resize(target_downsample_size)
        return self
    def correct_intermediary_square_size(self):
        &quot;&quot;&quot;
        When an intermediary size is calculated, we don&apos;t adjust it to be divisible by 8 or 64.
        However, the aspect ratio 1.0 needs special consideration for our base resolutions 512, 768, and 1024, because they typically result in 500x500, 750x750, and 1000x1000 images.
        Returns:
            TrainingSample: The current TrainingSample instance.
        &quot;&quot;&quot;
        if (
            self.aspect_ratio == 1.0
            and self.intermediary_size[0] &lt; self.pixel_resolution
        ):
            self.intermediary_size = (
                self.pixel_resolution,
                self.pixel_resolution,
            )
            self.crop_coordinates = (0, 0)
        return self
    def calculate_target_size(self):
        &quot;&quot;&quot;
        This method will populate the values for self.{target_size,intermediary_size,aspect_ratio} based on the image&apos;s original size and the data backend configuration.
        Returns:
            tuple:
                - The target size as (width, height).
                - The intermediary size as (width, height).
                - The aspect ratio of the target size. This will likely be different from the original aspect ratio.
        &quot;&quot;&quot;
        self.aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            self.original_size
        )
        if self.crop_enabled:
            if self.crop_aspect == &quot;square&quot;:
                self.target_size = (self.pixel_resolution, self.pixel_resolution)
                _, self.intermediary_size, _ = self.target_size_calculator(
                    self.aspect_ratio, self.resolution, self.original_size
                )
                self.aspect_ratio = 1.0
                self.correct_intermediary_square_size()
                square_crop_metadata = (
                    self.target_size,
                    self.intermediary_size,
                    self.aspect_ratio,
                )
                logger.debug(f&quot;Square crop metadata: {square_crop_metadata}&quot;)
                return square_crop_metadata
        if self.crop_enabled and (
            self.crop_aspect == &quot;random&quot; or self.crop_aspect == &quot;closest&quot;
        ):
            # Grab a random aspect ratio from a list.
            self.aspect_ratio = self._select_random_aspect()
        self.target_size, calculated_intermediary_size, self.aspect_ratio = (
            self.target_size_calculator(
                self.aspect_ratio, self.resolution, self.original_size
            )
        )
        if (
            self.crop_enabled and self.crop_aspect != &quot;random&quot;
        ) or not self.valid_metadata:
            self.intermediary_size = calculated_intermediary_size
        self.aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            self.target_size
        )
        self.correct_intermediary_square_size()
        if self.aspect_ratio == 1.0:
            self.target_size = (self.pixel_resolution, self.pixel_resolution)
        return (
            self.target_size,
            (int(self.intermediary_size[0]), int(self.intermediary_size[1])),
            self.aspect_ratio,
        )
    def correct_image(self):
        &quot;&quot;&quot;
        Apply a series of transformations to the image to &quot;correct&quot; it, such as EXIF rotation and conversion to RGB.
        Returns:
            TrainingSample: The current TrainingSample instance.
        &quot;&quot;&quot;
        if self.image is not None and hasattr(self.image, &quot;convert&quot;):
            # Convert image to RGB to remove any alpha channel and apply EXIF data transformations
            self.image = self.image.convert(&quot;RGB&quot;)
            self.image = exif_transpose(self.image)
        return self
    def crop(self):
        &quot;&quot;&quot;
        Crop the image using the detected crop handler class.
        If cropping is not enabled, we do nothing.
        Returns:
            TrainingSample: The current TrainingSample instance.
        &quot;&quot;&quot;
        if not self.crop_enabled:
            return self
        # Too-big of an image, resize before we crop.
        self.calculate_target_size()
        self._downsample_before_crop()
        self.save_debug_image(f&quot;images/{time.time()}-0.5-downsampled.png&quot;)
        if self.image is not None:
            logger.debug(f&quot;setting image: {self.image.size}&quot;)
            self.cropper.set_image(self.image)
        logger.debug(f&quot;Cropper size updating to {self.current_size}&quot;)
        self.cropper.set_intermediary_size(self.current_size[0], self.current_size[1])
        self.image, self.crop_coordinates = self.cropper.crop(
            self.target_size[0], self.target_size[1]
        )
        self.current_size = self.target_size
        logger.debug(
            f&quot;Cropped to {self.image.size if self.image is not None else self.current_size} via crop coordinates {self.crop_coordinates} {&apos;resulting in current_size of&apos; if self.image is not None else &apos;&apos;} {self.current_size if self.image is not None else &apos;&apos;}&quot;
        )
        return self
    def resize(self, size: tuple = None):
        &quot;&quot;&quot;
        Resize the image to a new size. If one is not provided, we will use the precalculated self.target_size
        Args:
            (optional) target_size (tuple): The target size as (width, height).
        Returns:
            TrainingSample: The current TrainingSample instance.
        &quot;&quot;&quot;
        current_size = self.image.size if self.image is not None else self.original_size
        if size is None:
            if not self.valid_metadata:
                self.target_size, self.intermediary_size, self.target_aspect_ratio = (
                    self.calculate_target_size()
                )
            size = self.target_size
            if self.target_size != self.intermediary_size:
                logger.debug(
                    f&quot;we have to crop because target size {self.target_size} != intermediary size {self.intermediary_size}&quot;
                )
                # Now we can resize the image to the intermediary size.
                self.current_size = self.intermediary_size
                if self.image is not None:
                    if isinstance(self.image, Image.Image):
                        self.image = self.image.resize(
                            self.intermediary_size, Image.Resampling.LANCZOS
                        )
                        self.current_size = self.image.size
                    elif isinstance(self.image, np.ndarray):
                        # we have a video to resize
                        logger.debug(
                            f&quot;Resizing {self.image.shape} to {self.intermediary_size}, &quot;
                        )
                        self.image = resize_video_frames(
                            self.image,
                            (self.intermediary_size[0], self.intermediary_size[1]),
                        )
                        width, height = self.image.shape[2], self.image.shape[1]
                        self.current_size = (width, height)
                        logger.debug(
                            f&quot;Post resize: {self.current_size} / {self.image.shape}&quot;
                        )
                if self.image is not None and self.cropper:
                    self.cropper.set_image(self.image)
                self.cropper.set_intermediary_size(
                    self.intermediary_size[0], self.intermediary_size[1]
                )
                self.image, self.crop_coordinates = self.cropper.crop(
                    self.target_size[0], self.target_size[1]
                )
                logger.debug(
                    f&quot;Cropped to {self.target_size} via crop coordinates {self.crop_coordinates} (resulting in current_size of {self.current_size})&quot;
                )
                self.current_size = self.target_size
                logger.debug(f&quot;crop coordinates: {self.crop_coordinates}&quot;)
                return self
        if self.image is not None and hasattr(self.image, &quot;resize&quot;):
            logger.debug(f&quot;Resize ({type(self.image)}) to {size}&quot;)
            if isinstance(self.image, Image.Image):
                self.image = self.image.resize(size, Image.Resampling.LANCZOS)
                self.aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
                    self.image.size
                )
            elif isinstance(self.image, np.ndarray):
                # we have a video to resize
                logger.debug(f&quot;Resizing {self.image.shape} to {size}, &quot;)
                self.image = resize_video_frames(self.image, (size[1], size[0]))
                self.aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(size)
                logger.debug(f&quot;Now {self.image.shape} @ {self.aspect_ratio}&quot;)
        self.current_size = size
        logger.debug(
            f&quot;Resized to {self.current_size} (aspect ratio: {self.aspect_ratio})&quot;
        )
        return self
    def get_image(self):
        &quot;&quot;&quot;
        Returns the current state of the image.
        If using the `parquet` metadata backend, this value may be None during the initial aspect bucketing phase.
        Returns:
            Image.Image: The current image.
        &quot;&quot;&quot;
        return self.image
    def is_conditioning_sample(self):
        return self.conditioning_type is not None
    def get_conditioning_type(self):
        return self.conditioning_type
    def cache_path(self):
        &quot;&quot;&quot;
        Given an image path, manipulate the prefix and suffix to return its counterpart cache path.
        The image extension will be stripped and replaced with the appropriate value (.pt).
        If the instance_data_dir is found in the path, it will be replaced with the cache_dir.
        Returns:
            str: The cache path for the image.
        &quot;&quot;&quot;
        vae_cache = StateTracker.get_data_backend(self.data_backend_id)[&quot;vaecache&quot;]
        return vae_cache.image_path_to_vae_path.get(self._image_path, None)
    def image_path(self, basename_only=False):
        &quot;&quot;&quot;
        Returns the absolute or basename path for the current training sample.
        Args:
            basename_only (bool): Whether to return the basename only.
        Returns:
            str: The image path
        &quot;&quot;&quot;
        if basename_only:
            return os.path.basename(self._image_path)
        return self._image_path
class PreparedSample:
    def __init__(
        self,
        image: Image.Image,
        image_metadata: dict,
        original_size: tuple,
        intermediary_size: tuple,
        target_size: tuple,
        aspect_ratio: float,
        crop_coordinates: tuple,
    ):
        &quot;&quot;&quot;
        Initializes a new PreparedSample instance with a provided PIL.Image object and optional metadata.
        Args:
        image (Image.Image): A PIL Image object.
        metadata (dict): Optional metadata associated with the image.
        &quot;&quot;&quot;
        self.image = image
        self.image_metadata = image_metadata if image_metadata else {}
        self.original_size = original_size
        self.intermediary_size = intermediary_size
        self.target_size = target_size
        if image is not None and hasattr(image, &quot;size&quot;) and type(image.size) is tuple:
            self.aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
                image.size[0] / image.size[1]
            )
        else:
            self.aspect_ratio = aspect_ratio
        self.crop_coordinates = crop_coordinates
    def __str__(self):
        return f&quot;PreparedSample(image={self.image}, original_size={self.original_size}, intermediary_size={self.intermediary_size}, target_size={self.target_size}, aspect_ratio={self.aspect_ratio}, crop_coordinates={self.crop_coordinates})&quot;
    def to_dict(self):
        return {
            &quot;image&quot;: self.image,
            &quot;original_size&quot;: self.original_size,
            &quot;intermediary_size&quot;: self.intermediary_size,
            &quot;target_size&quot;: self.target_size,
            &quot;aspect_ratio&quot;: self.aspect_ratio,
            &quot;crop_coordinates&quot;: self.crop_coordinates,
        }</file><file path="helpers/kolors/pipeline.py"># Copyright 2024 Stability AI, Kwai-Kolors Team and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import PIL.Image
import torch
from diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback
from diffusers.image_processor import PipelineImageInput, VaeImageProcessor
from diffusers.loaders import StableDiffusionXLLoraLoaderMixin
from diffusers.models import AutoencoderKL, UNet2DConditionModel
from diffusers.models.attention_processor import (
    AttnProcessor2_0,
    FusedAttnProcessor2_0,
    XFormersAttnProcessor,
)
from diffusers.image_processor import VaeImageProcessor
from diffusers.schedulers import KarrasDiffusionSchedulers
from diffusers.utils import is_torch_xla_available, logging, replace_example_docstring
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline, StableDiffusionMixin
from diffusers.pipelines.kolors.pipeline_output import KolorsPipelineOutput
from diffusers.pipelines.kolors.text_encoder import ChatGLMModel
from diffusers.pipelines.kolors.tokenizer import ChatGLMTokenizer
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm
    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import KolorsImg2ImgPipeline
        &gt;&gt;&gt; from diffusers.utils import load_image
        &gt;&gt;&gt; pipe = KolorsImg2ImgPipeline.from_pretrained(
        ...     &quot;Kwai-Kolors/Kolors-diffusers&quot;, variant=&quot;fp16&quot;, torch_dtype=torch.float16
        ... )
        &gt;&gt;&gt; pipe = pipe.to(&quot;cuda&quot;)
        &gt;&gt;&gt; url = (
        ...     &quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/kolors/bunny_source.png&quot;
        ... )
        &gt;&gt;&gt; init_image = load_image(url)
        &gt;&gt;&gt; prompt = &quot;high quality image of a capybara wearing sunglasses. In the background of the image there are trees, poles, grass and other objects. At the bottom of the object there is the road., 8k, highly detailed.&quot;
        &gt;&gt;&gt; image = pipe(prompt, image=init_image).images[0]
        ```
&quot;&quot;&quot;
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents
def retrieve_latents(
    encoder_output: torch.Tensor,
    generator: Optional[torch.Generator] = None,
    sample_mode: str = &quot;sample&quot;,
):
    if hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;sample&quot;:
        return encoder_output.latent_dist.sample(generator)
    elif hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;argmax&quot;:
        return encoder_output.latent_dist.mode()
    elif hasattr(encoder_output, &quot;latents&quot;):
        return encoder_output.latents
    else:
        raise AttributeError(&quot;Could not access latents of provided encoder_output&quot;)
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class KolorsImg2ImgPipeline(
    DiffusionPipeline, StableDiffusionMixin, StableDiffusionXLLoraLoaderMixin
):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using Kolors.
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)
    The pipeline also inherits the following loading methods:
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights
    Args:
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`ChatGLMModel`]):
            Frozen text-encoder. Kolors uses [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b).
        tokenizer (`ChatGLMTokenizer`):
            Tokenizer of class
            [ChatGLMTokenizer](https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenization_chatglm.py).
        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.
        scheduler ([`SchedulerMixin`]):
            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of
            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].
        force_zeros_for_empty_prompt (`bool`, *optional*, defaults to `&quot;False&quot;`):
            Whether the negative prompt embeddings shall be forced to always be set to 0. Also see the config of
            `Kwai-Kolors/Kolors-diffusers`.
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;unet-&gt;vae&quot;
    _optional_components = [
        &quot;tokenizer&quot;,
        &quot;text_encoder&quot;,
    ]
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;add_text_embeds&quot;,
        &quot;add_time_ids&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
        &quot;negative_add_time_ids&quot;,
    ]
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: ChatGLMModel,
        tokenizer: ChatGLMTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        force_zeros_for_empty_prompt: bool = False,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            unet=unet,
            scheduler=scheduler,
        )
        self.register_to_config(
            force_zeros_for_empty_prompt=force_zeros_for_empty_prompt
        )
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels) - 1)
            if hasattr(self, &quot;vae&quot;) and self.vae is not None
            else 8
        )
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.default_sample_size = self.unet.config.sample_size
    # Copied from diffusers.pipelines.kolors.pipeline_kolors.KolorsPipeline.encode_prompt
    def encode_prompt(
        self,
        prompt,
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt=None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        max_sequence_length: int = 256,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
        &quot;&quot;&quot;
        # from IPython import embed; embed(); exit()
        device = device or self._execution_device
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        # Define tokenizers and text encoders
        tokenizers = [self.tokenizer]
        text_encoders = [self.text_encoder]
        if prompt_embeds is None:
            prompt_embeds_list = []
            for tokenizer, text_encoder in zip(tokenizers, text_encoders):
                text_inputs = tokenizer(
                    prompt,
                    padding=&quot;max_length&quot;,
                    max_length=max_sequence_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                ).to(device)
                output = text_encoder(
                    input_ids=text_inputs[&quot;input_ids&quot;],
                    attention_mask=text_inputs[&quot;attention_mask&quot;],
                    position_ids=text_inputs[&quot;position_ids&quot;],
                    output_hidden_states=True,
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, max_sequence_length, hidden_size]
                # clone to have a contiguous tensor
                prompt_embeds = output.hidden_states[-2].permute(1, 0, 2).clone()
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, hidden_size]
                pooled_prompt_embeds = output.hidden_states[-1][-1, :, :].clone()
                bs_embed, seq_len, _ = prompt_embeds.shape
                prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
                prompt_embeds = prompt_embeds.view(
                    bs_embed * num_images_per_prompt, seq_len, -1
                )
                prompt_embeds_list.append(prompt_embeds)
            prompt_embeds = prompt_embeds_list[0]
        # get unconditional embeddings for classifier free guidance
        zero_out_negative_prompt = (
            negative_prompt is None and self.config.force_zeros_for_empty_prompt
        )
        if (
            do_classifier_free_guidance
            and negative_prompt_embeds is None
            and zero_out_negative_prompt
        ):
            negative_prompt_embeds = torch.zeros_like(prompt_embeds)
        elif do_classifier_free_guidance and negative_prompt_embeds is None:
            uncond_tokens: List[str]
            if negative_prompt is None:
                uncond_tokens = [&quot;&quot;] * batch_size
            elif prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif isinstance(negative_prompt, str):
                uncond_tokens = [negative_prompt]
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            else:
                uncond_tokens = negative_prompt
            negative_prompt_embeds_list = []
            for tokenizer, text_encoder in zip(tokenizers, text_encoders):
                uncond_input = tokenizer(
                    uncond_tokens,
                    padding=&quot;max_length&quot;,
                    max_length=max_sequence_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                ).to(device)
                output = text_encoder(
                    input_ids=uncond_input[&quot;input_ids&quot;],
                    attention_mask=uncond_input[&quot;attention_mask&quot;],
                    position_ids=uncond_input[&quot;position_ids&quot;],
                    output_hidden_states=True,
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, max_sequence_length, hidden_size]
                # clone to have a contiguous tensor
                negative_prompt_embeds = (
                    output.hidden_states[-2].permute(1, 0, 2).clone()
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, hidden_size]
                negative_pooled_prompt_embeds = output.hidden_states[-1][
                    -1, :, :
                ].clone()
                if do_classifier_free_guidance:
                    # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
                    seq_len = negative_prompt_embeds.shape[1]
                    negative_prompt_embeds = negative_prompt_embeds.to(
                        dtype=text_encoder.dtype, device=device
                    )
                    negative_prompt_embeds = negative_prompt_embeds.repeat(
                        1, num_images_per_prompt, 1
                    )
                    negative_prompt_embeds = negative_prompt_embeds.view(
                        batch_size * num_images_per_prompt, seq_len, -1
                    )
                negative_prompt_embeds_list.append(negative_prompt_embeds)
            negative_prompt_embeds = negative_prompt_embeds_list[0]
        bs_embed = pooled_prompt_embeds.shape[0]
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)
        if do_classifier_free_guidance:
            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(
                1, num_images_per_prompt
            ).view(bs_embed * num_images_per_prompt, -1)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        strength,
        height,
        width,
        negative_prompt=None,
        prompt_embeds=None,
        pooled_prompt_embeds=None,
        negative_prompt_embeds=None,
        negative_pooled_prompt_embeds=None,
        callback_on_step_end_tensor_inputs=None,
        max_sequence_length=None,
    ):
        if strength &lt; 0 or strength &gt; 1:
            raise ValueError(
                f&quot;The value of strength should in [0.0, 1.0] but is {strength}&quot;
            )
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.&quot;
            )
        if max_sequence_length is not None and max_sequence_length &gt; 256:
            raise ValueError(
                f&quot;`max_sequence_length` cannot be greater than 256 but is {max_sequence_length}&quot;
            )
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl_img2img.StableDiffusionXLImg2ImgPipeline.get_timesteps
    def get_timesteps(
        self, num_inference_steps, strength, device, denoising_start=None
    ):
        # get the original timestep using init_timestep
        if denoising_start is None:
            init_timestep = min(
                int(num_inference_steps * strength), num_inference_steps
            )
            t_start = max(num_inference_steps - init_timestep, 0)
        else:
            t_start = 0
        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]
        # Strength is irrelevant if we directly request a timestep to start at;
        # that is, strength is determined by the denoising_start instead.
        if denoising_start is not None:
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (denoising_start * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = (timesteps &lt; discrete_timestep_cutoff).sum().item()
            if self.scheduler.order == 2 and num_inference_steps % 2 == 0:
                # if the scheduler is a 2nd order scheduler we might have to do +1
                # because `num_inference_steps` might be even given that every timestep
                # (except the highest one) is duplicated. If `num_inference_steps` is even it would
                # mean that we cut the timesteps in the middle of the denoising step
                # (between 1st and 2nd derivative) which leads to incorrect results. By adding 1
                # we ensure that the denoising process always ends after the 2nd derivate step of the scheduler
                num_inference_steps = num_inference_steps + 1
            # because t_n+1 &gt;= t_n, we slice the timesteps starting from the end
            timesteps = timesteps[-num_inference_steps:]
            return timesteps, num_inference_steps
        return timesteps, num_inference_steps - t_start
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl_img2img.StableDiffusionXLImg2ImgPipeline.prepare_latents
    def prepare_latents(
        self,
        image,
        timestep,
        batch_size,
        num_images_per_prompt,
        dtype,
        device,
        generator=None,
        add_noise=True,
    ):
        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):
            raise ValueError(
                f&quot;`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}&quot;
            )
        latents_mean = latents_std = None
        if (
            hasattr(self.vae.config, &quot;latents_mean&quot;)
            and self.vae.config.latents_mean is not None
        ):
            latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1)
        if (
            hasattr(self.vae.config, &quot;latents_std&quot;)
            and self.vae.config.latents_std is not None
        ):
            latents_std = torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1)
        # Offload text encoder if `enable_model_cpu_offload` was enabled
        if hasattr(self, &quot;final_offload_hook&quot;) and self.final_offload_hook is not None:
            self.text_encoder_2.to(&quot;cpu&quot;)
            torch.cuda.empty_cache()
        image = image.to(device=device, dtype=dtype)
        batch_size = batch_size * num_images_per_prompt
        if image.shape[1] == 4:
            init_latents = image
        else:
            # make sure the VAE is in float32 mode, as it overflows in float16
            if self.vae.config.force_upcast:
                image = image.float()
                self.vae.to(dtype=torch.float32)
            if isinstance(generator, list) and len(generator) != batch_size:
                raise ValueError(
                    f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                    f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
                )
            elif isinstance(generator, list):
                if image.shape[0] &lt; batch_size and batch_size % image.shape[0] == 0:
                    image = torch.cat([image] * (batch_size // image.shape[0]), dim=0)
                elif image.shape[0] &lt; batch_size and batch_size % image.shape[0] != 0:
                    raise ValueError(
                        f&quot;Cannot duplicate `image` of batch size {image.shape[0]} to effective batch_size {batch_size} &quot;
                    )
                init_latents = [
                    retrieve_latents(
                        self.vae.encode(image[i : i + 1]), generator=generator[i]
                    )
                    for i in range(batch_size)
                ]
                init_latents = torch.cat(init_latents, dim=0)
            else:
                init_latents = retrieve_latents(
                    self.vae.encode(image), generator=generator
                )
            if self.vae.config.force_upcast:
                self.vae.to(dtype)
            init_latents = init_latents.to(dtype)
            if latents_mean is not None and latents_std is not None:
                latents_mean = latents_mean.to(device=device, dtype=dtype)
                latents_std = latents_std.to(device=device, dtype=dtype)
                init_latents = (
                    (init_latents - latents_mean)
                    * self.vae.config.scaling_factor
                    / latents_std
                )
            else:
                init_latents = self.vae.config.scaling_factor * init_latents
        if (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] == 0
        ):
            # expand init_latents for batch_size
            additional_image_per_prompt = batch_size // init_latents.shape[0]
            init_latents = torch.cat(
                [init_latents] * additional_image_per_prompt, dim=0
            )
        elif (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] != 0
        ):
            raise ValueError(
                f&quot;Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.&quot;
            )
        else:
            init_latents = torch.cat([init_latents], dim=0)
        if add_noise:
            shape = init_latents.shape
            noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
            # get latents
            init_latents = self.scheduler.add_noise(init_latents, noise, timestep)
        latents = init_latents
        return latents
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline._get_add_time_ids
    def _get_add_time_ids(
        self,
        original_size,
        crops_coords_top_left,
        target_size,
        dtype,
        text_encoder_projection_dim=None,
    ):
        add_time_ids = list(original_size + crops_coords_top_left + target_size)
        passed_add_embed_dim = (
            self.unet.config.addition_time_embed_dim * len(add_time_ids)
            + text_encoder_projection_dim
        )
        expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features
        if expected_add_embed_dim != passed_add_embed_dim:
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.&quot;
            )
        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
        return add_time_ids
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline.upcast_vae
    def upcast_vae(self):
        dtype = self.vae.dtype
        self.vae.to(dtype=torch.float32)
        use_torch_2_0_or_xformers = isinstance(
            self.vae.decoder.mid_block.attentions[0].processor,
            (
                AttnProcessor2_0,
                XFormersAttnProcessor,
                FusedAttnProcessor2_0,
            ),
        )
        # if xformers or torch_2_0 is used attention block does not need
        # to be in float32 which can save lots of memory
        if use_torch_2_0_or_xformers:
            self.vae.post_quant_conv.to(dtype)
            self.vae.decoder.conv_in.to(dtype)
            self.vae.decoder.mid_block.to(dtype)
    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
    def get_guidance_scale_embedding(
        self,
        w: torch.Tensor,
        embedding_dim: int = 512,
        dtype: torch.dtype = torch.float32,
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
        Args:
            w (`torch.Tensor`):
                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
            embedding_dim (`int`, *optional*, defaults to 512):
                Dimension of the embeddings to generate.
            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
                Data type of the generated embeddings.
        Returns:
            `torch.Tensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
        &quot;&quot;&quot;
        assert len(w.shape) == 1
        w = w * 1000.0
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
        emb = w.to(dtype)[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1))
        assert emb.shape == (w.shape[0], embedding_dim)
        return emb
    @property
    def guidance_scale(self):
        return self._guidance_scale
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1 and self.unet.config.time_cond_proj_dim is None
    @property
    def cross_attention_kwargs(self):
        return self._cross_attention_kwargs
    @property
    def denoising_start(self):
        return self._denoising_start
    @property
    def denoising_end(self):
        return self._denoising_end
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        image: PipelineImageInput = None,
        strength: float = 0.3,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        denoising_start: Optional[float] = None,
        denoising_end: Optional[float] = None,
        guidance_scale: float = 5.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        original_size: Optional[Tuple[int, int]] = None,
        crops_coords_top_left: Tuple[int, int] = (0, 0),
        target_size: Optional[Tuple[int, int]] = None,
        negative_original_size: Optional[Tuple[int, int]] = None,
        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),
        negative_target_size: Optional[Tuple[int, int]] = None,
        callback_on_step_end: Optional[
            Union[
                Callable[[int, int, Dict], None],
                PipelineCallback,
                MultiPipelineCallbacks,
            ]
        ] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        max_sequence_length: int = 256,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            image (`torch.Tensor` or `PIL.Image.Image` or `np.ndarray` or `List[torch.Tensor]` or `List[PIL.Image.Image]` or `List[np.ndarray]`):
                The image(s) to modify with the pipeline.
            strength (`float`, *optional*, defaults to 0.3):
                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1. `image`
                will be used as a starting point, adding more noise to it the larger the `strength`. The number of
                denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will
                be maximum and the denoising process will run for the full number of iterations specified in
                `num_inference_steps`. A value of 1, therefore, essentially ignores `image`. Note that in the case of
                `denoising_start` being declared as an integer, the value of `strength` will be ignored.
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [Kwai-Kolors/Kolors-diffusers](https://huggingface.co/Kwai-Kolors/Kolors-diffusers) and checkpoints
                that are not specifically fine-tuned on low resolutions.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [Kwai-Kolors/Kolors-diffusers](https://huggingface.co/Kwai-Kolors/Kolors-diffusers) and checkpoints
                that are not specifically fine-tuned on low resolutions.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            sigmas (`List[float]`, *optional*):
                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
                will be used.
            denoising_start (`float`, *optional*):
                When specified, indicates the fraction (between 0.0 and 1.0) of the total denoising process to be
                bypassed before it is initiated. Consequently, the initial part of the denoising process is skipped and
                it is assumed that the passed `image` is a partly denoised image. Note that when this is specified,
                strength will be ignored. The `denoising_start` parameter is particularly beneficial when this pipeline
                is integrated into a &quot;Mixture of Denoisers&quot; multi-pipeline setup, as detailed in [**Refine Image
                Quality**](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality).
            denoising_end (`float`, *optional*):
                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be
                completed before it is intentionally prematurely terminated. As a result, the returned sample will
                still retain a substantial amount of noise as determined by the discrete timesteps selected by the
                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a
                &quot;Mixture of Denoisers&quot; multi-pipeline setup, as elaborated in [**Refining the Image
                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
            guidance_scale (`float`, *optional*, defaults to 5.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.kolors.KolorsPipelineOutput`] instead of a plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.
                `original_size` defaults to `(height, width)` if not specified. Part of SDXL&apos;s micro-conditioning as
                explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                `crops_coords_top_left` can be used to generate an image that appears to be &quot;cropped&quot; from the position
                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting
                `crops_coords_top_left` to (0, 0). Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                For most cases, `target_size` should be set to the desired height and width of the generated image. If
                not specified it will default to `(height, width)`. Part of SDXL&apos;s micro-conditioning as explained in
                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a specific image resolution. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a target image resolution. It should be as same
                as the `target_size` for most cases. Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):
                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of
                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:
                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
        Examples:
        Returns:
            [`~pipelines.kolors.KolorsPipelineOutput`] or `tuple`: [`~pipelines.kolors.KolorsPipelineOutput`] if
            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is a list with the
            generated images.
        &quot;&quot;&quot;
        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):
            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs
        # 0. Default height and width to unet
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        original_size = original_size or (height, width)
        target_size = target_size or (height, width)
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            strength,
            height,
            width,
            negative_prompt,
            prompt_embeds,
            pooled_prompt_embeds,
            negative_prompt_embeds,
            negative_pooled_prompt_embeds,
            callback_on_step_end_tensor_inputs,
            max_sequence_length=max_sequence_length,
        )
        self._guidance_scale = guidance_scale
        self._cross_attention_kwargs = cross_attention_kwargs
        self._denoising_end = denoising_end
        self._denoising_start = denoising_start
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        # 3. Encode input prompt
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
        )
        # 4. Preprocess image
        image = self.image_processor.preprocess(image)
        # 5. Prepare timesteps
        def denoising_value_valid(dnv):
            return isinstance(dnv, float) and 0 &lt; dnv &lt; 1
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps, sigmas
        )
        timesteps, num_inference_steps = self.get_timesteps(
            num_inference_steps,
            strength,
            device,
            denoising_start=(
                self.denoising_start
                if denoising_value_valid(self.denoising_start)
                else None
            ),
        )
        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)
        add_noise = True if self.denoising_start is None else False
        # 6. Prepare latent variables
        if latents is None:
            latents = self.prepare_latents(
                image,
                latent_timestep,
                batch_size,
                num_images_per_prompt,
                prompt_embeds.dtype,
                device,
                generator,
                add_noise,
            )
        # 7. Prepare extra step kwargs.
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        height, width = latents.shape[-2:]
        height = height * self.vae_scale_factor
        width = width * self.vae_scale_factor
        original_size = original_size or (height, width)
        target_size = target_size or (height, width)
        # 8. Prepare added time ids &amp; embeddings
        add_text_embeds = pooled_prompt_embeds
        text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])
        add_time_ids = self._get_add_time_ids(
            original_size,
            crops_coords_top_left,
            target_size,
            dtype=prompt_embeds.dtype,
            text_encoder_projection_dim=text_encoder_projection_dim,
        )
        if negative_original_size is not None and negative_target_size is not None:
            negative_add_time_ids = self._get_add_time_ids(
                negative_original_size,
                negative_crops_coords_top_left,
                negative_target_size,
                dtype=prompt_embeds.dtype,
                text_encoder_projection_dim=text_encoder_projection_dim,
            )
        else:
            negative_add_time_ids = add_time_ids
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            add_text_embeds = torch.cat(
                [negative_pooled_prompt_embeds, add_text_embeds], dim=0
            )
            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)
        prompt_embeds = prompt_embeds.to(device)
        add_text_embeds = add_text_embeds.to(device)
        add_time_ids = add_time_ids.to(device).repeat(
            batch_size * num_images_per_prompt, 1
        )
        # 9. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        # 9.1 Apply denoising_end
        if (
            self.denoising_end is not None
            and self.denoising_start is not None
            and denoising_value_valid(self.denoising_end)
            and denoising_value_valid(self.denoising_start)
            and self.denoising_start &gt;= self.denoising_end
        ):
            raise ValueError(
                f&quot;`denoising_start`: {self.denoising_start} cannot be larger than or equal to `denoising_end`: &quot;
                + f&quot; {self.denoising_end} when using type float.&quot;
            )
        elif self.denoising_end is not None and denoising_value_valid(
            self.denoising_end
        ):
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = len(
                list(filter(lambda ts: ts &gt;= discrete_timestep_cutoff, timesteps))
            )
            timesteps = timesteps[:num_inference_steps]
        # 9.2 Optionally get Guidance Scale Embedding
        timestep_cond = None
        if self.unet.config.time_cond_proj_dim is not None:
            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(
                batch_size * num_images_per_prompt
            )
            timestep_cond = self.get_guidance_scale_embedding(
                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim
            ).to(device=device, dtype=latents.dtype)
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                # predict the noise residual
                added_cond_kwargs = {
                    &quot;text_embeds&quot;: add_text_embeds,
                    &quot;time_ids&quot;: add_time_ids,
                }
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    timestep_cond=timestep_cond,
                    cross_attention_kwargs=self.cross_attention_kwargs,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    add_text_embeds = callback_outputs.pop(
                        &quot;add_text_embeds&quot;, add_text_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                    add_time_ids = callback_outputs.pop(&quot;add_time_ids&quot;, add_time_ids)
                    negative_add_time_ids = callback_outputs.pop(
                        &quot;negative_add_time_ids&quot;, negative_add_time_ids
                    )
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                if XLA_AVAILABLE:
                    xm.mark_step()
        if not output_type == &quot;latent&quot;:
            # make sure the VAE is in float32 mode, as it overflows in float16
            needs_upcasting = (
                self.vae.dtype == torch.float16 and self.vae.config.force_upcast
            )
            if needs_upcasting:
                self.upcast_vae()
                latents = latents.to(
                    next(iter(self.vae.post_quant_conv.parameters())).dtype
                )
            elif latents.dtype != self.vae.dtype:
                if torch.backends.mps.is_available():
                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                    self.vae = self.vae.to(latents.dtype)
            # unscale/denormalize the latents
            latents = latents / self.vae.config.scaling_factor
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(device=self.vae.device), return_dict=False
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            # cast back to fp16 if needed
            if needs_upcasting:
                self.vae.to(dtype=torch.float16)
        else:
            image = latents
        if not output_type == &quot;latent&quot;:
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return KolorsPipelineOutput(images=image)
class KolorsPipeline(
    DiffusionPipeline, StableDiffusionMixin, StableDiffusionXLLoraLoaderMixin
):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using Kolors.
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)
    The pipeline also inherits the following loading methods:
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights
    Args:
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`ChatGLMModel`]):
            Frozen text-encoder. Kolors uses [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b).
        tokenizer (`ChatGLMTokenizer`):
            Tokenizer of class
            [ChatGLMTokenizer](https://huggingface.co/THUDM/chatglm3-6b/blob/main/tokenization_chatglm.py).
        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.
        scheduler ([`SchedulerMixin`]):
            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of
            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].
        force_zeros_for_empty_prompt (`bool`, *optional*, defaults to `&quot;False&quot;`):
            Whether the negative prompt embeddings shall be forced to always be set to 0. Also see the config of
            `Kwai-Kolors/Kolors-diffusers`.
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;unet-&gt;vae&quot;
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;add_text_embeds&quot;,
        &quot;add_time_ids&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
        &quot;negative_add_time_ids&quot;,
    ]
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: ChatGLMModel,
        tokenizer: ChatGLMTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        force_zeros_for_empty_prompt: bool = False,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            unet=unet,
            scheduler=scheduler,
        )
        self.register_to_config(
            force_zeros_for_empty_prompt=force_zeros_for_empty_prompt
        )
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels) - 1)
            if hasattr(self, &quot;vae&quot;) and self.vae is not None
            else 8
        )
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.default_sample_size = self.unet.config.sample_size
    def encode_prompt(
        self,
        prompt,
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt=None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        max_sequence_length: int = 256,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
        &quot;&quot;&quot;
        # from IPython import embed; embed(); exit()
        device = device or self._execution_device
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        # Define tokenizers and text encoders
        tokenizers = [self.tokenizer]
        text_encoders = [self.text_encoder]
        if prompt_embeds is None:
            prompt_embeds_list = []
            for tokenizer, text_encoder in zip(tokenizers, text_encoders):
                text_inputs = tokenizer(
                    prompt,
                    padding=&quot;max_length&quot;,
                    max_length=max_sequence_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                ).to(device)
                output = text_encoder(
                    input_ids=text_inputs[&quot;input_ids&quot;],
                    attention_mask=text_inputs[&quot;attention_mask&quot;],
                    position_ids=text_inputs[&quot;position_ids&quot;],
                    output_hidden_states=True,
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, max_sequence_length, hidden_size]
                # clone to have a contiguous tensor
                print(f&quot;Shape A: {[o.shape for o in output.hidden_states]}&quot;)
                prompt_embeds = output.hidden_states[-2].permute(1, 0, 2).clone()
                print(f&quot;Shape B: {prompt_embeds.shape}&quot;)
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, hidden_size]
                pooled_prompt_embeds = output.hidden_states[-1][-1, :, :].clone()
                bs_embed, seq_len, _ = prompt_embeds.shape
                prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
                prompt_embeds = prompt_embeds.view(
                    bs_embed * num_images_per_prompt, seq_len, -1
                )
                print(f&quot;Shape C: {prompt_embeds.shape}&quot;)
                prompt_embeds_list.append(prompt_embeds)
            prompt_embeds = prompt_embeds_list[0]
        # get unconditional embeddings for classifier free guidance
        zero_out_negative_prompt = (
            negative_prompt is None and self.config.force_zeros_for_empty_prompt
        )
        if (
            do_classifier_free_guidance
            and negative_prompt_embeds is None
            and zero_out_negative_prompt
        ):
            negative_prompt_embeds = torch.zeros_like(prompt_embeds)
        elif do_classifier_free_guidance and negative_prompt_embeds is None:
            uncond_tokens: List[str]
            if negative_prompt is None:
                uncond_tokens = [&quot;&quot;] * batch_size
            elif prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif isinstance(negative_prompt, str):
                uncond_tokens = [negative_prompt]
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            else:
                uncond_tokens = negative_prompt
            negative_prompt_embeds_list = []
            for tokenizer, text_encoder in zip(tokenizers, text_encoders):
                uncond_input = tokenizer(
                    uncond_tokens,
                    padding=&quot;max_length&quot;,
                    max_length=max_sequence_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                ).to(device)
                output = text_encoder(
                    input_ids=uncond_input[&quot;input_ids&quot;],
                    attention_mask=uncond_input[&quot;attention_mask&quot;],
                    position_ids=uncond_input[&quot;position_ids&quot;],
                    output_hidden_states=True,
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, max_sequence_length, hidden_size]
                # clone to have a contiguous tensor
                negative_prompt_embeds = (
                    output.hidden_states[-2].permute(1, 0, 2).clone()
                )
                # [max_sequence_length, batch, hidden_size] -&gt; [batch, hidden_size]
                negative_pooled_prompt_embeds = output.hidden_states[-1][
                    -1, :, :
                ].clone()
                if do_classifier_free_guidance:
                    # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
                    seq_len = negative_prompt_embeds.shape[1]
                    negative_prompt_embeds = negative_prompt_embeds.to(
                        dtype=text_encoder.dtype, device=device
                    )
                    negative_prompt_embeds = negative_prompt_embeds.repeat(
                        1, num_images_per_prompt, 1
                    )
                    negative_prompt_embeds = negative_prompt_embeds.view(
                        batch_size * num_images_per_prompt, seq_len, -1
                    )
                negative_prompt_embeds_list.append(negative_prompt_embeds)
            negative_prompt_embeds = negative_prompt_embeds_list[0]
        bs_embed = pooled_prompt_embeds.shape[0]
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)
        if do_classifier_free_guidance:
            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(
                1, num_images_per_prompt
            ).view(bs_embed * num_images_per_prompt, -1)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        height,
        width,
        negative_prompt=None,
        prompt_embeds=None,
        pooled_prompt_embeds=None,
        negative_prompt_embeds=None,
        negative_pooled_prompt_embeds=None,
        callback_on_step_end_tensor_inputs=None,
        max_sequence_length=None,
    ):
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.&quot;
            )
        if max_sequence_length is not None and max_sequence_length &gt; 256:
            raise ValueError(
                f&quot;`max_sequence_length` cannot be greater than 256 but is {max_sequence_length}&quot;
            )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        if latents is None:
            latents = randn_tensor(
                shape, generator=generator, device=device, dtype=dtype
            )
        else:
            latents = latents.to(device)
        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline._get_add_time_ids
    def _get_add_time_ids(
        self,
        original_size,
        crops_coords_top_left,
        target_size,
        dtype,
        text_encoder_projection_dim=None,
    ):
        add_time_ids = list(original_size + crops_coords_top_left + target_size)
        passed_add_embed_dim = (
            self.unet.config.addition_time_embed_dim * len(add_time_ids)
            + text_encoder_projection_dim
        )
        expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features
        if expected_add_embed_dim != passed_add_embed_dim:
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.&quot;
            )
        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
        return add_time_ids
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline.upcast_vae
    def upcast_vae(self):
        dtype = self.vae.dtype
        self.vae.to(dtype=torch.float32)
        use_torch_2_0_or_xformers = isinstance(
            self.vae.decoder.mid_block.attentions[0].processor,
            (
                AttnProcessor2_0,
                XFormersAttnProcessor,
                FusedAttnProcessor2_0,
            ),
        )
        # if xformers or torch_2_0 is used attention block does not need
        # to be in float32 which can save lots of memory
        if use_torch_2_0_or_xformers:
            self.vae.post_quant_conv.to(dtype)
            self.vae.decoder.conv_in.to(dtype)
            self.vae.decoder.mid_block.to(dtype)
    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
    def get_guidance_scale_embedding(
        self,
        w: torch.Tensor,
        embedding_dim: int = 512,
        dtype: torch.dtype = torch.float32,
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
        Args:
            w (`torch.Tensor`):
                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
            embedding_dim (`int`, *optional*, defaults to 512):
                Dimension of the embeddings to generate.
            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
                Data type of the generated embeddings.
        Returns:
            `torch.Tensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
        &quot;&quot;&quot;
        assert len(w.shape) == 1
        w = w * 1000.0
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
        emb = w.to(dtype)[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1))
        assert emb.shape == (w.shape[0], embedding_dim)
        return emb
    @property
    def guidance_scale(self):
        return self._guidance_scale
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1 and self.unet.config.time_cond_proj_dim is None
    @property
    def cross_attention_kwargs(self):
        return self._cross_attention_kwargs
    @property
    def denoising_end(self):
        return self._denoising_end
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        denoising_end: Optional[float] = None,
        guidance_scale: float = 5.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        original_size: Optional[Tuple[int, int]] = None,
        crops_coords_top_left: Tuple[int, int] = (0, 0),
        target_size: Optional[Tuple[int, int]] = None,
        negative_original_size: Optional[Tuple[int, int]] = None,
        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),
        negative_target_size: Optional[Tuple[int, int]] = None,
        callback_on_step_end: Optional[
            Union[
                Callable[[int, int, Dict], None],
                PipelineCallback,
                MultiPipelineCallbacks,
            ]
        ] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        max_sequence_length: int = 256,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [Kwai-Kolors/Kolors-diffusers](https://huggingface.co/Kwai-Kolors/Kolors-diffusers) and checkpoints
                that are not specifically fine-tuned on low resolutions.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [Kwai-Kolors/Kolors-diffusers](https://huggingface.co/Kwai-Kolors/Kolors-diffusers) and checkpoints
                that are not specifically fine-tuned on low resolutions.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            sigmas (`List[float]`, *optional*):
                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
                will be used.
            denoising_end (`float`, *optional*):
                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be
                completed before it is intentionally prematurely terminated. As a result, the returned sample will
                still retain a substantial amount of noise as determined by the discrete timesteps selected by the
                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a
                &quot;Mixture of Denoisers&quot; multi-pipeline setup, as elaborated in [**Refining the Image
                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
            guidance_scale (`float`, *optional*, defaults to 5.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.kolors.KolorsPipelineOutput`] instead of a plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.
                `original_size` defaults to `(height, width)` if not specified. Part of SDXL&apos;s micro-conditioning as
                explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                `crops_coords_top_left` can be used to generate an image that appears to be &quot;cropped&quot; from the position
                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting
                `crops_coords_top_left` to (0, 0). Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                For most cases, `target_size` should be set to the desired height and width of the generated image. If
                not specified it will default to `(height, width)`. Part of SDXL&apos;s micro-conditioning as explained in
                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a specific image resolution. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a target image resolution. It should be as same
                as the `target_size` for most cases. Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):
                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of
                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:
                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
        Examples:
        Returns:
            [`~pipelines.kolors.KolorsPipelineOutput`] or `tuple`: [`~pipelines.kolors.KolorsPipelineOutput`] if
            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is a list with the
            generated images.
        &quot;&quot;&quot;
        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):
            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs
        # 0. Default height and width to unet
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        original_size = original_size or (height, width)
        target_size = target_size or (height, width)
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            height,
            width,
            negative_prompt,
            prompt_embeds,
            pooled_prompt_embeds,
            negative_prompt_embeds,
            negative_pooled_prompt_embeds,
            callback_on_step_end_tensor_inputs,
            max_sequence_length=max_sequence_length,
        )
        self._guidance_scale = guidance_scale
        self._cross_attention_kwargs = cross_attention_kwargs
        self._denoising_end = denoising_end
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        # 3. Encode input prompt
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            prompt_embeds=prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
        )
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps, sigmas
        )
        # 5. Prepare latent variables
        num_channels_latents = self.unet.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        # 7. Prepare added time ids &amp; embeddings
        add_text_embeds = pooled_prompt_embeds
        text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])
        add_time_ids = self._get_add_time_ids(
            original_size,
            crops_coords_top_left,
            target_size,
            dtype=prompt_embeds.dtype,
            text_encoder_projection_dim=text_encoder_projection_dim,
        )
        if negative_original_size is not None and negative_target_size is not None:
            negative_add_time_ids = self._get_add_time_ids(
                negative_original_size,
                negative_crops_coords_top_left,
                negative_target_size,
                dtype=prompt_embeds.dtype,
                text_encoder_projection_dim=text_encoder_projection_dim,
            )
        else:
            negative_add_time_ids = add_time_ids
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            add_text_embeds = torch.cat(
                [negative_pooled_prompt_embeds, add_text_embeds], dim=0
            )
            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)
        prompt_embeds = prompt_embeds.to(device)
        add_text_embeds = add_text_embeds.to(device)
        add_time_ids = add_time_ids.to(device).repeat(
            batch_size * num_images_per_prompt, 1
        )
        # 8. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        # 8.1 Apply denoising_end
        if (
            self.denoising_end is not None
            and isinstance(self.denoising_end, float)
            and self.denoising_end &gt; 0
            and self.denoising_end &lt; 1
        ):
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = len(
                list(filter(lambda ts: ts &gt;= discrete_timestep_cutoff, timesteps))
            )
            timesteps = timesteps[:num_inference_steps]
        # 9. Optionally get Guidance Scale Embedding
        timestep_cond = None
        if self.unet.config.time_cond_proj_dim is not None:
            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(
                batch_size * num_images_per_prompt
            )
            timestep_cond = self.get_guidance_scale_embedding(
                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim
            ).to(device=device, dtype=latents.dtype)
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                # predict the noise residual
                added_cond_kwargs = {
                    &quot;text_embeds&quot;: add_text_embeds,
                    &quot;time_ids&quot;: add_time_ids,
                }
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    timestep_cond=timestep_cond,
                    cross_attention_kwargs=self.cross_attention_kwargs,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    add_text_embeds = callback_outputs.pop(
                        &quot;add_text_embeds&quot;, add_text_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                    add_time_ids = callback_outputs.pop(&quot;add_time_ids&quot;, add_time_ids)
                    negative_add_time_ids = callback_outputs.pop(
                        &quot;negative_add_time_ids&quot;, negative_add_time_ids
                    )
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                if XLA_AVAILABLE:
                    xm.mark_step()
        if not output_type == &quot;latent&quot;:
            # make sure the VAE is in float32 mode, as it overflows in float16
            needs_upcasting = (
                self.vae.dtype == torch.float16 and self.vae.config.force_upcast
            )
            if needs_upcasting:
                self.upcast_vae()
                latents = latents.to(
                    next(iter(self.vae.post_quant_conv.parameters())).dtype
                )
            elif latents.dtype != self.vae.dtype:
                if torch.backends.mps.is_available():
                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                    self.vae = self.vae.to(latents.dtype)
            # unscale/denormalize the latents
            latents = latents / self.vae.config.scaling_factor
            image = self.vae.decode(
                latents.to(device=self.vae.device, dtype=self.vae.dtype),
                return_dict=False,
            )[0]
            # cast back to fp16 if needed
            if needs_upcasting:
                self.vae.to(dtype=torch.float16)
        else:
            image = latents
        if not output_type == &quot;latent&quot;:
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return KolorsPipelineOutput(images=image)</file><file path="helpers/legacy/pipeline.py"># Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from typing import Any, Callable, Dict, List, Optional, Union
import torch
from packaging import version
from transformers import (
    CLIPImageProcessor,
    CLIPTextModel,
    CLIPTokenizer,
    CLIPVisionModelWithProjection,
)
from diffusers.configuration_utils import FrozenDict
from diffusers.image_processor import PipelineImageInput, VaeImageProcessor
from diffusers.loaders import (
    FromSingleFileMixin,
    IPAdapterMixin,
    LoraLoaderMixin,
    TextualInversionLoaderMixin,
)
from diffusers.models import AutoencoderKL, ImageProjection, UNet2DConditionModel
from diffusers.models.lora import adjust_lora_scale_text_encoder
from diffusers.schedulers import KarrasDiffusionSchedulers
from diffusers.utils import (
    USE_PEFT_BACKEND,
    deprecate,
    logging,
    replace_example_docstring,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline, StableDiffusionMixin
from diffusers.pipelines.stable_diffusion.pipeline_output import (
    StableDiffusionPipelineOutput,
)
from diffusers.pipelines.stable_diffusion.safety_checker import (
    StableDiffusionSafetyChecker,
)
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import StableDiffusionPipeline
        &gt;&gt;&gt; pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, torch_dtype=torch.float16)
        &gt;&gt;&gt; pipe = pipe.to(&quot;cuda&quot;)
        &gt;&gt;&gt; prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
        &gt;&gt;&gt; image = pipe(prompt).images[0]
        ```
&quot;&quot;&quot;
def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):
    &quot;&quot;&quot;
    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and
    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4
    &quot;&quot;&quot;
    std_text = noise_pred_text.std(
        dim=list(range(1, noise_pred_text.ndim)), keepdim=True
    )
    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)
    # rescale the results from guidance (fixes overexposure)
    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)
    # mix with the original results from guidance by factor guidance_rescale to avoid &quot;plain looking&quot; images
    noise_cfg = (
        guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg
    )
    return noise_cfg
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
                Custom timesteps used to support arbitrary spacing between timesteps. If `None`, then the default
                timestep spacing strategy of the scheduler is used. If `timesteps` is passed, `num_inference_steps`
                must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class StableDiffusionPipeline(
    DiffusionPipeline,
    StableDiffusionMixin,
    TextualInversionLoaderMixin,
    LoraLoaderMixin,
    IPAdapterMixin,
    FromSingleFileMixin,
):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using Stable Diffusion.
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods
    implemented for all pipelines (downloading, saving, running on a particular device, etc.).
    The pipeline also inherits the following loading methods:
        - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings
        - [`~loaders.LoraLoaderMixin.load_lora_weights`] for loading LoRA weights
        - [`~loaders.LoraLoaderMixin.save_lora_weights`] for saving LoRA weights
        - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files
        - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters
    Args:
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.
        text_encoder ([`~transformers.CLIPTextModel`]):
            Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).
        tokenizer ([`~transformers.CLIPTokenizer`]):
            A `CLIPTokenizer` to tokenize text.
        unet ([`UNet2DConditionModel`]):
            A `UNet2DConditionModel` to denoise the encoded image latents.
        scheduler ([`SchedulerMixin`]):
            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of
            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].
        safety_checker ([`StableDiffusionSafetyChecker`]):
            Classification module that estimates whether generated images could be considered offensive or harmful.
            Please refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for more details
            about a model&apos;s potential harms.
        feature_extractor ([`~transformers.CLIPImageProcessor`]):
            A `CLIPImageProcessor` to extract features from generated images; used as inputs to the `safety_checker`.
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;image_encoder-&gt;unet-&gt;vae&quot;
    _optional_components = [&quot;safety_checker&quot;, &quot;feature_extractor&quot;, &quot;image_encoder&quot;]
    _exclude_from_cpu_offload = [&quot;safety_checker&quot;]
    _callback_tensor_inputs = [&quot;latents&quot;, &quot;prompt_embeds&quot;, &quot;negative_prompt_embeds&quot;]
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModel,
        tokenizer: CLIPTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        safety_checker: StableDiffusionSafetyChecker,
        feature_extractor: CLIPImageProcessor,
        image_encoder: CLIPVisionModelWithProjection = None,
        requires_safety_checker: bool = True,
    ):
        super().__init__()
        if (
            hasattr(scheduler.config, &quot;steps_offset&quot;)
            and scheduler.config.steps_offset != 1
        ):
            deprecation_message = (
                f&quot;The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`&quot;
                f&quot; should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure &quot;
                &quot;to update the config accordingly as leaving `steps_offset` might led to incorrect results&quot;
                &quot; in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,&quot;
                &quot; it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`&quot;
                &quot; file&quot;
            )
            deprecate(
                &quot;steps_offset!=1&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False
            )
            new_config = dict(scheduler.config)
            new_config[&quot;steps_offset&quot;] = 1
            scheduler._internal_dict = FrozenDict(new_config)
        if (
            hasattr(scheduler.config, &quot;clip_sample&quot;)
            and scheduler.config.clip_sample is True
        ):
            deprecation_message = (
                f&quot;The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.&quot;
                &quot; `clip_sample` should be set to False in the configuration file. Please make sure to update the&quot;
                &quot; config accordingly as not setting `clip_sample` in the config might lead to incorrect results in&quot;
                &quot; future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very&quot;
                &quot; nice if you could open a Pull request for the `scheduler/scheduler_config.json` file&quot;
            )
            deprecate(
                &quot;clip_sample not set&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False
            )
            new_config = dict(scheduler.config)
            new_config[&quot;clip_sample&quot;] = False
            scheduler._internal_dict = FrozenDict(new_config)
        if safety_checker is None and requires_safety_checker:
            logger.warning(
                f&quot;You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure&quot;
                &quot; that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered&quot;
                &quot; results in services or applications open to the public. Both the diffusers team and Hugging Face&quot;
                &quot; strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling&quot;
                &quot; it only for use-cases that involve analyzing network behavior or auditing its results. For more&quot;
                &quot; information, please have a look at https://github.com/huggingface/diffusers/pull/254 .&quot;
            )
        if safety_checker is not None and feature_extractor is None:
            raise ValueError(
                &quot;Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety&quot;
                &quot; checker. If you do not want to use the safety checker, you can pass `&apos;safety_checker=None&apos;` instead.&quot;
            )
        is_unet_version_less_0_9_0 = hasattr(
            unet.config, &quot;_diffusers_version&quot;
        ) and version.parse(
            version.parse(unet.config._diffusers_version).base_version
        ) &lt; version.parse(
            &quot;0.9.0.dev0&quot;
        )
        is_unet_sample_size_less_64 = (
            hasattr(unet.config, &quot;sample_size&quot;) and unet.config.sample_size &lt; 64
        )
        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:
            deprecation_message = (
                &quot;The configuration file of the unet has set the default `sample_size` to smaller than&quot;
                &quot; 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the&quot;
                &quot; following: \n- CompVis/stable-diffusion-v1-4 \n- CompVis/stable-diffusion-v1-3 \n-&quot;
                &quot; CompVis/stable-diffusion-v1-2 \n- CompVis/stable-diffusion-v1-1 \n- runwayml/stable-diffusion-v1-5&quot;
                &quot; \n- runwayml/stable-diffusion-inpainting \n you should change &apos;sample_size&apos; to 64 in the&quot;
                &quot; configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`&quot;
                &quot; in the config might lead to incorrect results in future versions. If you have downloaded this&quot;
                &quot; checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for&quot;
                &quot; the `unet/config.json` file&quot;
            )
            deprecate(
                &quot;sample_size&lt;64&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False
            )
            new_config = dict(unet.config)
            new_config[&quot;sample_size&quot;] = 64
            unet._internal_dict = FrozenDict(new_config)
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            unet=unet,
            scheduler=scheduler,
            safety_checker=safety_checker,
            feature_extractor=feature_extractor,
            image_encoder=image_encoder,
        )
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.register_to_config(requires_safety_checker=requires_safety_checker)
    def _encode_prompt(
        self,
        prompt,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
        negative_prompt=None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        lora_scale: Optional[float] = None,
        **kwargs,
    ):
        deprecation_message = &quot;`_encode_prompt()` is deprecated and it will be removed in a future version. Use `encode_prompt()` instead. Also, be aware that the output format changed from a concatenated tensor to a tuple.&quot;
        deprecate(&quot;_encode_prompt()&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
        prompt_embeds_tuple = self.encode_prompt(
            prompt=prompt,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            do_classifier_free_guidance=do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            lora_scale=lora_scale,
            **kwargs,
        )
        # concatenate for backwards comp
        prompt_embeds = torch.cat([prompt_embeds_tuple[1], prompt_embeds_tuple[0]])
        return prompt_embeds
    def encode_prompt(
        self,
        prompt,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
        negative_prompt=None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        lora_scale: Optional[float] = None,
        clip_skip: Optional[int] = None,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            lora_scale (`float`, *optional*):
                A LoRA scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
        &quot;&quot;&quot;
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(self, LoraLoaderMixin):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if not USE_PEFT_BACKEND:
                adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)
            else:
                scale_lora_layers(self.text_encoder, lora_scale)
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        if prompt_embeds is None:
            # textual inversion: process multi-vector tokens if necessary
            if isinstance(self, TextualInversionLoaderMixin):
                prompt = self.maybe_convert_prompt(prompt, self.tokenizer)
            text_inputs = self.tokenizer(
                prompt,
                padding=&quot;max_length&quot;,
                max_length=self.tokenizer.model_max_length,
                truncation=True,
                return_tensors=&quot;pt&quot;,
            )
            text_input_ids = text_inputs.input_ids
            untruncated_ids = self.tokenizer(
                prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
            ).input_ids
            if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[
                -1
            ] and not torch.equal(text_input_ids, untruncated_ids):
                removed_text = self.tokenizer.batch_decode(
                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]
                )
                logger.warning(
                    &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
                    f&quot; {self.tokenizer.model_max_length} tokens: {removed_text}&quot;
                )
            if (
                hasattr(self.text_encoder.config, &quot;use_attention_mask&quot;)
                and self.text_encoder.config.use_attention_mask
            ):
                attention_mask = text_inputs.attention_mask.to(device)
            else:
                attention_mask = None
            if clip_skip is None:
                prompt_embeds = self.text_encoder(
                    text_input_ids.to(device), attention_mask=attention_mask
                )
                prompt_embeds = prompt_embeds[0]
            else:
                prompt_embeds = self.text_encoder(
                    text_input_ids.to(device),
                    attention_mask=attention_mask,
                    output_hidden_states=True,
                )
                # Access the `hidden_states` first, that contains a tuple of
                # all the hidden states from the encoder layers. Then index into
                # the tuple to access the hidden states from the desired layer.
                prompt_embeds = prompt_embeds[-1][-(clip_skip + 1)]
                # We also need to apply the final LayerNorm here to not mess with the
                # representations. The `last_hidden_states` that we typically use for
                # obtaining the final prompt representations passes through the LayerNorm
                # layer.
                prompt_embeds = self.text_encoder.text_model.final_layer_norm(
                    prompt_embeds
                )
        if self.text_encoder is not None:
            prompt_embeds_dtype = self.text_encoder.dtype
        elif self.unet is not None:
            prompt_embeds_dtype = self.unet.dtype
        else:
            prompt_embeds_dtype = prompt_embeds.dtype
        prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )
        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance and negative_prompt_embeds is None:
            uncond_tokens: List[str]
            if negative_prompt is None:
                uncond_tokens = [&quot;&quot;] * batch_size
            elif prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif isinstance(negative_prompt, str):
                uncond_tokens = [negative_prompt]
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            else:
                uncond_tokens = negative_prompt
            # textual inversion: process multi-vector tokens if necessary
            if isinstance(self, TextualInversionLoaderMixin):
                uncond_tokens = self.maybe_convert_prompt(uncond_tokens, self.tokenizer)
            max_length = prompt_embeds.shape[1]
            uncond_input = self.tokenizer(
                uncond_tokens,
                padding=&quot;max_length&quot;,
                max_length=max_length,
                truncation=True,
                return_tensors=&quot;pt&quot;,
            )
            if (
                hasattr(self.text_encoder.config, &quot;use_attention_mask&quot;)
                and self.text_encoder.config.use_attention_mask
            ):
                attention_mask = uncond_input.attention_mask.to(device)
            else:
                attention_mask = None
            negative_prompt_embeds = self.text_encoder(
                uncond_input.input_ids.to(device),
                attention_mask=attention_mask,
            )
            negative_prompt_embeds = negative_prompt_embeds[0]
        if do_classifier_free_guidance:
            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = negative_prompt_embeds.shape[1]
            negative_prompt_embeds = negative_prompt_embeds.to(
                dtype=prompt_embeds_dtype, device=device
            )
            negative_prompt_embeds = negative_prompt_embeds.repeat(
                1, num_images_per_prompt, 1
            )
            negative_prompt_embeds = negative_prompt_embeds.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )
        if (
            isinstance(self, LoraLoaderMixin)
            and USE_PEFT_BACKEND
            and self.text_encoder is not None
        ):
            # Retrieve the original scale by scaling back the LoRA layers
            unscale_lora_layers(self.text_encoder, lora_scale)
        return prompt_embeds, negative_prompt_embeds
    def encode_image(
        self, image, device, num_images_per_prompt, output_hidden_states=None
    ):
        dtype = next(self.image_encoder.parameters()).dtype
        if not isinstance(image, torch.Tensor):
            image = self.feature_extractor(image, return_tensors=&quot;pt&quot;).pixel_values
        image = image.to(device=device, dtype=dtype)
        if output_hidden_states:
            image_enc_hidden_states = self.image_encoder(
                image, output_hidden_states=True
            ).hidden_states[-2]
            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(
                num_images_per_prompt, dim=0
            )
            uncond_image_enc_hidden_states = self.image_encoder(
                torch.zeros_like(image), output_hidden_states=True
            ).hidden_states[-2]
            uncond_image_enc_hidden_states = (
                uncond_image_enc_hidden_states.repeat_interleave(
                    num_images_per_prompt, dim=0
                )
            )
            return image_enc_hidden_states, uncond_image_enc_hidden_states
        else:
            image_embeds = self.image_encoder(image).image_embeds
            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)
            uncond_image_embeds = torch.zeros_like(image_embeds)
            return image_embeds, uncond_image_embeds
    def prepare_ip_adapter_image_embeds(
        self,
        ip_adapter_image,
        ip_adapter_image_embeds,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
    ):
        if ip_adapter_image_embeds is None:
            if not isinstance(ip_adapter_image, list):
                ip_adapter_image = [ip_adapter_image]
            if len(ip_adapter_image) != len(
                self.unet.encoder_hid_proj.image_projection_layers
            ):
                raise ValueError(
                    f&quot;`ip_adapter_image` must have same length as the number of IP Adapters. Got {len(ip_adapter_image)} images and {len(self.unet.encoder_hid_proj.image_projection_layers)} IP Adapters.&quot;
                )
            image_embeds = []
            for single_ip_adapter_image, image_proj_layer in zip(
                ip_adapter_image, self.unet.encoder_hid_proj.image_projection_layers
            ):
                output_hidden_state = not isinstance(image_proj_layer, ImageProjection)
                single_image_embeds, single_negative_image_embeds = self.encode_image(
                    single_ip_adapter_image, device, 1, output_hidden_state
                )
                single_image_embeds = torch.stack(
                    [single_image_embeds] * num_images_per_prompt, dim=0
                )
                single_negative_image_embeds = torch.stack(
                    [single_negative_image_embeds] * num_images_per_prompt, dim=0
                )
                if do_classifier_free_guidance:
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                    single_image_embeds = single_image_embeds.to(device)
                image_embeds.append(single_image_embeds)
        else:
            repeat_dims = [1]
            image_embeds = []
            for single_image_embeds in ip_adapter_image_embeds:
                if do_classifier_free_guidance:
                    single_negative_image_embeds, single_image_embeds = (
                        single_image_embeds.chunk(2)
                    )
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                    single_negative_image_embeds = single_negative_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_negative_image_embeds.shape[1:])),
                    )
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                else:
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                image_embeds.append(single_image_embeds)
        return image_embeds
    def run_safety_checker(self, image, device, dtype):
        if self.safety_checker is None:
            has_nsfw_concept = None
        else:
            if torch.is_tensor(image):
                feature_extractor_input = self.image_processor.postprocess(
                    image, output_type=&quot;pil&quot;
                )
            else:
                feature_extractor_input = self.image_processor.numpy_to_pil(image)
            safety_checker_input = self.feature_extractor(
                feature_extractor_input, return_tensors=&quot;pt&quot;
            ).to(device)
            image, has_nsfw_concept = self.safety_checker(
                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)
            )
        return image, has_nsfw_concept
    def decode_latents(self, latents):
        deprecation_message = &quot;The decode_latents method is deprecated and will be removed in 1.0.0. Please use VaeImageProcessor.postprocess(...) instead&quot;
        deprecate(&quot;decode_latents&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
        latents = 1 / self.vae.config.scaling_factor * latents
        image = self.vae.decode(latents, return_dict=False)[0]
        image = (image / 2 + 0.5).clamp(0, 1)
        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16
        image = image.cpu().permute(0, 2, 3, 1).float().numpy()
        return image
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        height,
        width,
        callback_steps,
        negative_prompt=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        ip_adapter_image=None,
        ip_adapter_image_embeds=None,
        callback_on_step_end_tensor_inputs=None,
    ):
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if callback_steps is not None and (
            not isinstance(callback_steps, int) or callback_steps &lt;= 0
        ):
            raise ValueError(
                f&quot;`callback_steps` has to be a positive integer but is {callback_steps} of type&quot;
                f&quot; {type(callback_steps)}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if ip_adapter_image is not None and ip_adapter_image_embeds is not None:
            raise ValueError(
                &quot;Provide either `ip_adapter_image` or `ip_adapter_image_embeds`. Cannot leave both `ip_adapter_image` and `ip_adapter_image_embeds` defined.&quot;
            )
        if ip_adapter_image_embeds is not None:
            if not isinstance(ip_adapter_image_embeds, list):
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be of type `list` but is {type(ip_adapter_image_embeds)}&quot;
                )
            elif ip_adapter_image_embeds[0].ndim not in [3, 4]:
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be a list of 3D or 4D tensors but is {ip_adapter_image_embeds[0].ndim}D&quot;
                )
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        if latents is None:
            latents = randn_tensor(
                shape, generator=generator, device=device, dtype=dtype
            )
        else:
            latents = latents.to(device)
        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents
    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
    def get_guidance_scale_embedding(
        self,
        w: torch.Tensor,
        embedding_dim: int = 512,
        dtype: torch.dtype = torch.float32,
    ) -&gt; torch.FloatTensor:
        &quot;&quot;&quot;
        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
        Args:
            w (`torch.Tensor`):
                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
            embedding_dim (`int`, *optional*, defaults to 512):
                Dimension of the embeddings to generate.
            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
                Data type of the generated embeddings.
        Returns:
            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
        &quot;&quot;&quot;
        assert len(w.shape) == 1
        w = w * 1000.0
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
        emb = w.to(dtype)[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1))
        assert emb.shape == (w.shape[0], embedding_dim)
        return emb
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def guidance_rescale(self):
        return self._guidance_rescale
    @property
    def clip_skip(self):
        return self._clip_skip
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1 and self.unet.config.time_cond_proj_dim is None
    @property
    def cross_attention_kwargs(self):
        return self._cross_attention_kwargs
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        guidance_scale: float = 7.5,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        ip_adapter_image: Optional[PipelineImageInput] = None,
        ip_adapter_image_embeds: Optional[List[torch.FloatTensor]] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        guidance_rescale: float = 0.0,
        clip_skip: Optional[int] = None,
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        **kwargs,
    ):
        r&quot;&quot;&quot;
        The call function to the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.
            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):
                The width in pixels of the generated image.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            guidance_scale (`float`, *optional*, defaults to 7.5):
                A higher guidance scale value encourages the model to generate images closely linked to the text
                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale &gt; 1`.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide what to not include in image generation. If not defined, you need to
                pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale &lt; 1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies
                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor is generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not
                provided, text embeddings are generated from the `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If
                not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.
            ip_adapter_image: (`PipelineImageInput`, *optional*): Optional image input to work with IP Adapters.
            ip_adapter_image_embeds (`List[torch.FloatTensor]`, *optional*):
                Pre-generated image embeddings for IP-Adapter. It should be a list of length same as number of
                IP-adapters. Each element should be a tensor of shape `(batch_size, num_images, emb_dim)`. It should
                contain the negative image embedding if `do_classifier_free_guidance` is set to `True`. If not
                provided, embeddings are computed from the `ip_adapter_image` input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generated image. Choose between `PIL.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
                plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in
                [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            guidance_rescale (`float`, *optional*, defaults to 0.0):
                Guidance rescale factor from [Common Diffusion Noise Schedules and Sample Steps are
                Flawed](https://arxiv.org/pdf/2305.08891.pdf). Guidance rescale factor should fix overexposure when
                using zero terminal SNR.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
        Examples:
        Returns:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:
                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,
                otherwise a `tuple` is returned where the first element is a list with the generated images and the
                second element is a list of `bool`s indicating whether the corresponding generated image contains
                &quot;not-safe-for-work&quot; (nsfw) content.
        &quot;&quot;&quot;
        callback = kwargs.pop(&quot;callback&quot;, None)
        callback_steps = kwargs.pop(&quot;callback_steps&quot;, None)
        if callback is not None:
            deprecate(
                &quot;callback&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`&quot;,
            )
        if callback_steps is not None:
            deprecate(
                &quot;callback_steps&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback_steps` as an input argument to `__call__` is deprecated, consider using `callback_on_step_end`&quot;,
            )
        # 0. Default height and width to unet
        height = height or self.unet.config.sample_size * self.vae_scale_factor
        width = width or self.unet.config.sample_size * self.vae_scale_factor
        # to deal with lora scaling and other possible forward hooks
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            height,
            width,
            callback_steps,
            negative_prompt,
            prompt_embeds,
            negative_prompt_embeds,
            ip_adapter_image,
            ip_adapter_image_embeds,
            callback_on_step_end_tensor_inputs,
        )
        self._guidance_scale = guidance_scale
        self._guidance_rescale = guidance_rescale
        self._clip_skip = clip_skip
        self._cross_attention_kwargs = cross_attention_kwargs
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        # 3. Encode input prompt
        lora_scale = (
            self.cross_attention_kwargs.get(&quot;scale&quot;, None)
            if self.cross_attention_kwargs is not None
            else None
        )
        prompt_embeds, negative_prompt_embeds = self.encode_prompt(
            prompt,
            device,
            num_images_per_prompt,
            self.do_classifier_free_guidance,
            negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            lora_scale=lora_scale,
            clip_skip=self.clip_skip,
        )
        # For classifier free guidance, we need to do two forward passes.
        # Here we concatenate the unconditional and text embeddings into a single batch
        # to avoid doing two forward passes
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:
            image_embeds = self.prepare_ip_adapter_image_embeds(
                ip_adapter_image,
                ip_adapter_image_embeds,
                device,
                batch_size * num_images_per_prompt,
                self.do_classifier_free_guidance,
            )
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps
        )
        # 5. Prepare latent variables
        num_channels_latents = self.unet.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        # 6.1 Add image embeds for IP-Adapter
        added_cond_kwargs = (
            {&quot;image_embeds&quot;: image_embeds}
            if (ip_adapter_image is not None or ip_adapter_image_embeds is not None)
            else None
        )
        # 6.2 Optionally get Guidance Scale Embedding
        timestep_cond = None
        if self.unet.config.time_cond_proj_dim is not None:
            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(
                batch_size * num_images_per_prompt
            )
            timestep_cond = self.get_guidance_scale_embedding(
                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim
            ).to(device=device, dtype=latents.dtype)
        # 7. Denoising loop
        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                # predict the noise residual
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    timestep_cond=timestep_cond,
                    cross_attention_kwargs=self.cross_attention_kwargs,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                if self.do_classifier_free_guidance and self.guidance_rescale &gt; 0.0:
                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                    noise_pred = rescale_noise_cfg(
                        noise_pred,
                        noise_pred_text,
                        guidance_rescale=self.guidance_rescale,
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, &quot;order&quot;, 1)
                        callback(step_idx, t, latents)
        has_nsfw_concept = None
        if not output_type == &quot;latent&quot;:
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype) / self.vae.config.scaling_factor,
                return_dict=False,
                generator=generator,
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
        else:
            image = latents
        if has_nsfw_concept is None:
            do_denormalize = [True] * image.shape[0]
        else:
            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]
        image = self.image_processor.postprocess(
            image, output_type=output_type, do_denormalize=do_denormalize
        )
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image, has_nsfw_concept)
        return StableDiffusionPipelineOutput(
            images=image, nsfw_content_detected=has_nsfw_concept
        )</file><file path="helpers/metadata/backends/base.py">from math import ceil
import os
import time
import logging
import threading
import torch
from helpers.data_backend.base import BaseDataBackend
from helpers.multiaspect.image import MultiaspectImage
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import should_log
from multiprocessing import Process, Queue
from threading import Thread
from pathlib import Path
from tqdm import tqdm
from PIL import Image
from math import floor
import numpy as np
# For semaphore
from threading import Semaphore
logger = logging.getLogger(&quot;BaseMetadataBackend&quot;)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(&quot;ERROR&quot;)
class MetadataBackend:
    def __init__(
        self,
        id: str,
        instance_data_dir: str,
        cache_file: str,
        metadata_file: str,
        data_backend: BaseDataBackend,
        accelerator,
        batch_size: int,
        resolution: float,
        resolution_type: str,
        delete_problematic_images: bool = False,
        delete_unwanted_images: bool = False,
        metadata_update_interval: int = 3600,
        minimum_image_size: int = None,
        minimum_aspect_ratio: int = None,
        maximum_aspect_ratio: int = None,
        num_frames: int = None,
        minimum_num_frames: int = None,
        maximum_num_frames: int = None,
        cache_file_suffix: str = None,
        repeats: int = 0,
    ):
        self.id = id
        if self.id != data_backend.id:
            raise ValueError(
                f&quot;BucketManager ID ({self.id}) must match the DataBackend ID ({data_backend.id}).&quot;
            )
        self.accelerator = accelerator
        self.should_abort = False
        self.data_backend = data_backend
        self.batch_size = int(batch_size)
        self.repeats = int(repeats)
        self.instance_data_dir = instance_data_dir
        if cache_file_suffix is not None:
            cache_file = f&quot;{cache_file}_{cache_file_suffix}&quot;
            metadata_file = f&quot;{metadata_file}_{cache_file_suffix}&quot;
        self.cache_file = Path(f&quot;{cache_file}.json&quot;)
        self.metadata_file = Path(f&quot;{metadata_file}.json&quot;)
        self.aspect_ratio_bucket_indices = {}
        self.image_metadata = {}  # Store image metadata
        self.seen_images = {}
        self.config = {}
        self.reload_cache()
        self.resolution = float(resolution)
        self.resolution_type = resolution_type
        self.delete_problematic_images = delete_problematic_images
        self.delete_unwanted_images = delete_unwanted_images
        self.metadata_update_interval = metadata_update_interval
        self.minimum_image_size = (
            float(minimum_image_size) if minimum_image_size else None
        )
        self.minimum_aspect_ratio = (
            float(minimum_aspect_ratio) if minimum_aspect_ratio else None
        )
        self.maximum_aspect_ratio = (
            float(maximum_aspect_ratio) if maximum_aspect_ratio else None
        )
        self.maximum_num_frames = (
            float(maximum_num_frames) if maximum_num_frames else None
        )
        self.minimum_num_frames = (
            float(minimum_num_frames) if minimum_num_frames else None
        )
        self.num_frames = float(num_frames) if num_frames else None
        self.image_metadata_loaded = False
        self.vae_output_scaling_factor = 8
        self.metadata_semaphor = Semaphore()
        # When a multi-gpu system splits the buckets, we no longer update.
        self.read_only = False
    def load_metadata(self):
        raise NotImplementedError
    def save_metadata(self):
        raise NotImplementedError
    def _bucket_worker(
        self,
        tqdm_queue,
        files,
        aspect_ratio_bucket_indices_queue,
        metadata_updates_queue,
        written_files_queue,
        existing_files_set,
    ):
        &quot;&quot;&quot;
        A worker function to bucket a list of files.
        Args:
            tqdm_queue (Queue): A queue to report progress to.
            files (list): A list of files to bucket.
            aspect_ratio_bucket_indices_queue (Queue): A queue to report the bucket indices to.
            existing_files_set (set): A set of existing files.
        Returns:
            dict: The bucket indices.
        &quot;&quot;&quot;
        local_aspect_ratio_bucket_indices = {}
        local_metadata_updates = {}
        processed_file_list = set()
        processed_file_count = 0
        # Initialize statistics dictionary
        statistics = {
            &quot;total_processed&quot;: 0,
            &quot;skipped&quot;: {
                &quot;already_exists&quot;: 0,
                &quot;metadata_missing&quot;: 0,
                &quot;not_found&quot;: 0,
                &quot;too_small&quot;: 0,
                &quot;other&quot;: 0,  # Add more specific reasons as needed
            },
        }
        for file in files:
            if str(file) not in existing_files_set:
                logger.debug(f&quot;Processing file {file}.&quot;)
                try:
                    local_aspect_ratio_bucket_indices = self._process_for_bucket(
                        file,
                        local_aspect_ratio_bucket_indices,
                        metadata_updates=local_metadata_updates,
                        delete_problematic_images=self.delete_problematic_images,
                        statistics=statistics,
                    )
                except Exception as e:
                    logger.error(
                        f&quot;Error processing file {file}. Reason: {e}. Skipping.&quot;
                    )
                    statistics[&quot;skipped&quot;][&quot;error&quot;] += 1
                logger.debug(
                    f&quot;Statistics: {statistics}, total: {sum([len(bucket) for bucket in local_aspect_ratio_bucket_indices.values()])}&quot;
                )
                processed_file_count += 1
                # Successfully processed
                statistics[&quot;total_processed&quot;] = processed_file_count
                processed_file_list.add(file)
            else:
                statistics[&quot;skipped&quot;][&quot;already_exists&quot;] += 1
            tqdm_queue.put(1)
            if processed_file_count % 500 == 0:
                # Send updates to queues and reset the local dictionaries
                if aspect_ratio_bucket_indices_queue is not None:
                    aspect_ratio_bucket_indices_queue.put(
                        local_aspect_ratio_bucket_indices
                    )
                if written_files_queue is not None:
                    written_files_queue.put(processed_file_list)
                metadata_updates_queue.put(local_metadata_updates)
                local_aspect_ratio_bucket_indices = {}
                local_metadata_updates = {}
                processed_file_list = set()
        if (
            aspect_ratio_bucket_indices_queue is not None
            and local_aspect_ratio_bucket_indices
        ):
            aspect_ratio_bucket_indices_queue.put(local_aspect_ratio_bucket_indices)
        if local_metadata_updates:
            metadata_updates_queue.put(local_metadata_updates)
            # At the end of the _bucket_worker method
            metadata_updates_queue.put((&quot;statistics&quot;, statistics))
        time.sleep(0.001)
        logger.debug(&quot;Bucket worker completed processing. Returning to main thread.&quot;)
    def compute_aspect_ratio_bucket_indices(self, ignore_existing_cache: bool = False):
        &quot;&quot;&quot;
        Compute the aspect ratio bucket indices. The workhorse of this class.
        Arguments:
            ignore_existing_cache (bool): Whether to ignore the existing cache
            and entirely recompute the aspect ratio bucket indices.
        Returns:
            dict: The aspect ratio bucket indices.
        &quot;&quot;&quot;
        logger.info(&quot;Discovering new files...&quot;)
        new_files = self._discover_new_files(
            ignore_existing_cache=ignore_existing_cache
        )
        existing_files_set = set().union(*self.aspect_ratio_bucket_indices.values())
        logger.info(
            f&quot;Compressed {len(existing_files_set)} existing files from {len(self.aspect_ratio_bucket_indices.values())}.&quot;
        )
        # Initialize aggregated statistics
        aggregated_statistics = {
            &quot;total_processed&quot;: 0,
            &quot;skipped&quot;: {
                &quot;already_exists&quot;: len(existing_files_set),
                &quot;metadata_missing&quot;: 0,
                &quot;not_found&quot;: 0,
                &quot;too_small&quot;: 0,
                &quot;other&quot;: 0,
            },
        }
        if not new_files:
            logger.info(&quot;No new files discovered. Doing nothing.&quot;)
            logger.info(f&quot;Statistics: {aggregated_statistics}&quot;)
            return
        num_cpus = (
            StateTracker.get_args().aspect_bucket_worker_count
        )  # Using a fixed number for better control and predictability
        files_split = np.array_split(new_files, num_cpus)
        metadata_updates_queue = Queue()
        written_files_queue = Queue()
        tqdm_queue = Queue()
        aspect_ratio_bucket_indices_queue = Queue()
        try:
            self.load_image_metadata()
        except Exception as e:
            if ignore_existing_cache:
                logger.warning(
                    f&quot;Error loading image metadata, creating new metadata cache: {e}&quot;
                )
                self.image_metadata = {}
            else:
                raise Exception(
                    f&quot;Error loading image metadata. You may have to remove the metadata json file &apos;{self.metadata_file}&apos; and VAE cache manually: {e}&quot;
                )
        worker_cls = (
            Process if StateTracker.get_args().enable_multiprocessing else Thread
        )
        workers = [
            worker_cls(
                target=self._bucket_worker,
                args=(
                    tqdm_queue,
                    file_shard,
                    aspect_ratio_bucket_indices_queue,
                    metadata_updates_queue,
                    written_files_queue,
                    existing_files_set,
                ),
            )
            for file_shard in files_split
        ]
        for worker in workers:
            worker.start()
        last_write_time = time.time()
        written_files = set()
        with tqdm(
            desc=&quot;Generating aspect bucket cache&quot;,
            total=len(new_files),
            leave=False,
            ncols=100,
            miniters=int(len(new_files) / 100),
        ) as pbar:
            if self.should_abort:
                logger.info(&quot;Aborting aspect bucket update.&quot;)
                return
            while (
                any(worker.is_alive() for worker in workers)
                or not tqdm_queue.empty()
                or not aspect_ratio_bucket_indices_queue.empty()
                or not metadata_updates_queue.empty()
                or not written_files_queue.empty()
            ):
                current_time = time.time()
                while not tqdm_queue.empty():
                    pbar.update(tqdm_queue.get())
                while not aspect_ratio_bucket_indices_queue.empty():
                    aspect_ratio_bucket_indices_update = (
                        aspect_ratio_bucket_indices_queue.get()
                    )
                    for key, value in aspect_ratio_bucket_indices_update.items():
                        self.aspect_ratio_bucket_indices.setdefault(key, []).extend(
                            value
                        )
                # Now, pull metadata updates from the queue
                while not metadata_updates_queue.empty():
                    metadata_update = metadata_updates_queue.get()
                    if (
                        type(metadata_update) is tuple
                        and metadata_update[0] == &quot;statistics&quot;
                    ):
                        logger.debug(
                            f&quot;Received statistics update: {metadata_update[1]}&quot;
                        )
                        for reason, count in metadata_update[1][&quot;skipped&quot;].items():
                            aggregated_statistics[&quot;skipped&quot;][reason] += count
                        aggregated_statistics[&quot;total_processed&quot;] += metadata_update[1][
                            &quot;total_processed&quot;
                        ]
                        continue
                    for filepath, meta in metadata_update.items():
                        self.set_metadata_by_filepath(
                            filepath=filepath, metadata=meta, update_json=False
                        )
                # Process the written files queue
                while not written_files_queue.empty():
                    written_files_batch = written_files_queue.get()
                    written_files.update(written_files_batch)  # Use update for sets
                processing_duration = current_time - last_write_time
                if processing_duration &gt;= self.metadata_update_interval:
                    logger.debug(
                        f&quot;In-flight metadata update after {processing_duration} seconds. Saving {len(self.image_metadata)} metadata entries and {len(self.aspect_ratio_bucket_indices)} aspect bucket lists.&quot;
                    )
                    self.save_cache(enforce_constraints=False)
                    self.save_image_metadata()
                    last_write_time = current_time
                time.sleep(0.001)
        for worker in workers:
            worker.join()
        logger.info(f&quot;Image processing statistics: {aggregated_statistics}&quot;)
        self.save_image_metadata()
        self.save_cache(enforce_constraints=True)
        logger.info(&quot;Completed aspect bucket update.&quot;)
    def split_buckets_between_processes(self, gradient_accumulation_steps=1):
        &quot;&quot;&quot;
        Splits the contents of each bucket in aspect_ratio_bucket_indices between the available processes.
        &quot;&quot;&quot;
        new_aspect_ratio_bucket_indices = {}
        total_images = sum(
            [len(bucket) for bucket in self.aspect_ratio_bucket_indices.values()]
        )
        logger.debug(f&quot;Count of items before split: {total_images}&quot;)
        # Determine the effective batch size for all processes considering gradient accumulation
        num_processes = self.accelerator.num_processes
        effective_batch_size = (
            self.batch_size * num_processes * gradient_accumulation_steps
        )
        for bucket, images in self.aspect_ratio_bucket_indices.items():
            # Trim the list to a length that&apos;s divisible by the effective batch size
            total_img_count_incl_repeats = len(images) * (self.repeats + 1)
            num_batches = ceil(total_img_count_incl_repeats / effective_batch_size)
            trimmed_images = images[: num_batches * effective_batch_size]
            if len(trimmed_images) == 0 and should_log():
                logger.error(
                    f&quot;Bucket {bucket} has no images after trimming because {len(images)} images are not enough to satisfy an effective batch size of {effective_batch_size}.&quot;
                    &quot; Lower your batch size, increase repeat count, or increase data pool size.&quot;
                )
            with self.accelerator.split_between_processes(
                trimmed_images, apply_padding=False
            ) as images_split:
                # Now images_split contains only the part of the images list that this process should handle
                new_aspect_ratio_bucket_indices[bucket] = images_split
        # Replace the original aspect_ratio_bucket_indices with the new one containing only this process&apos;s share
        self.aspect_ratio_bucket_indices = new_aspect_ratio_bucket_indices
        post_total = sum(
            [len(bucket) for bucket in self.aspect_ratio_bucket_indices.values()]
        )
        if total_images != post_total:
            self.read_only = True
        logger.debug(f&quot;Count of items after split: {post_total}&quot;)
    def mark_as_seen(self, image_path):
        &quot;&quot;&quot;Mark an image as seen.&quot;&quot;&quot;
        self.seen_images[image_path] = True
    def mark_batch_as_seen(self, image_paths):
        &quot;&quot;&quot;Efficiently extend the Manager with new contents, image_paths
        Args:
            image_paths (list): A list of image paths to mark as seen.
        &quot;&quot;&quot;
        self.seen_images.update({image_path: True for image_path in image_paths})
    def is_seen(self, image_path):
        &quot;&quot;&quot;Check if an image is seen.&quot;&quot;&quot;
        return self.seen_images.get(image_path, False)
    def reset_seen_images(self):
        &quot;&quot;&quot;Reset the seen images.&quot;&quot;&quot;
        self.seen_images.clear()
    def remove_image(self, image_path, bucket: str = None):
        &quot;&quot;&quot;
        Used by other classes to reliably remove images from a bucket.
        Args:
            image_path (str): The path to the image to remove.
            bucket (str): The bucket to remove the image from.
        Returns:
            dict: The aspect ratio bucket indices.
        &quot;&quot;&quot;
        if not bucket:
            for bucket, images in self.aspect_ratio_bucket_indices.items():
                if image_path in images:
                    self.aspect_ratio_bucket_indices[bucket].remove(image_path)
                    break
        if image_path in self.aspect_ratio_bucket_indices[bucket]:
            self.aspect_ratio_bucket_indices[bucket].remove(image_path)
    def update_buckets_with_existing_files(self, existing_files: set):
        &quot;&quot;&quot;
        Update bucket indices to remove entries that no longer exist and remove duplicates.
        Args:
            existing_files (set): A set of existing files.
        &quot;&quot;&quot;
        logger.debug(
            f&quot;Before updating, in all buckets, we had {sum([len(bucket) for bucket in self.aspect_ratio_bucket_indices.values()])}.&quot;
        )
        for bucket, images in self.aspect_ratio_bucket_indices.items():
            # Remove non-existing files and duplicates while preserving order
            filtered_images = list(
                dict.fromkeys(img for img in images if img in existing_files)
            )
            self.aspect_ratio_bucket_indices[bucket] = filtered_images
        logger.debug(
            f&quot;After updating, in all buckets, we had {sum([len(bucket) for bucket in self.aspect_ratio_bucket_indices.values()])}.&quot;
        )
        # Save the updated cache
        self.save_cache()
    def refresh_buckets(self, rank: int = None):
        &quot;&quot;&quot;
        Discover new files and remove images that no longer exist.
        &quot;&quot;&quot;
        # Discover new files and update bucket indices
        self.compute_aspect_ratio_bucket_indices()
        # Get the list of existing files
        logger.debug(
            f&quot;Refreshing buckets for rank {rank} via data_backend id {self.id}.&quot;
        )
        existing_files = StateTracker.get_image_files(data_backend_id=self.id)
        if not StateTracker.get_args().ignore_missing_files:
            # Update bucket indices to remove entries that no longer exist
            self.update_buckets_with_existing_files(existing_files)
        return
    def _enforce_min_bucket_size(self):
        &quot;&quot;&quot;
        Remove buckets that have fewer samples than batch_size and enforce minimum image size constraints.
        &quot;&quot;&quot;
        if self.minimum_image_size is None:
            return
        logger.info(
            f&quot;Enforcing minimum image size of {self.minimum_image_size}.&quot;
            &quot; This could take a while for very-large datasets.&quot;
        )
        for bucket in tqdm(
            list(self.aspect_ratio_bucket_indices.keys()),
            leave=False,
            desc=&quot;Enforcing minimum bucket size&quot;,
        ):  # Safe iteration over keys
            # Prune the smaller buckets so that we don&apos;t enforce resolution constraints on them unnecessarily.
            self._prune_small_buckets(bucket)
            if self.minimum_image_size is not None:
                self._enforce_resolution_constraints(bucket)
                # We do this twice in case there were any new contenders for being too small.
                self._prune_small_buckets(bucket)
    def _enforce_min_aspect_ratio(self):
        &quot;&quot;&quot;
        Remove buckets that have an aspect ratio outside the specified range.
        &quot;&quot;&quot;
        if self.minimum_aspect_ratio is None or self.minimum_aspect_ratio == 0.0:
            return
        logger.info(
            f&quot;Enforcing minimum aspect ratio of {self.minimum_aspect_ratio}.&quot;
            &quot; This could take a while for very-large datasets.&quot;
        )
        for bucket in tqdm(
            list(self.aspect_ratio_bucket_indices.keys()),
            leave=False,
            desc=&quot;Enforcing minimum aspect ratio&quot;,
        ):  # Safe iteration over keys
            if float(bucket) &lt; self.minimum_aspect_ratio:
                logger.info(
                    f&quot;Removing bucket {bucket} due to aspect ratio being less than {self.minimum_aspect_ratio}.&quot;
                )
                del self.aspect_ratio_bucket_indices[bucket]
    def _enforce_max_aspect_ratio(self):
        &quot;&quot;&quot;
        Remove buckets that have an aspect ratio outside the specified range.
        &quot;&quot;&quot;
        if self.maximum_aspect_ratio is None or self.maximum_aspect_ratio == 0.0:
            return
        logger.info(
            f&quot;Enforcing maximum aspect ratio of {self.maximum_aspect_ratio}.&quot;
            &quot; This could take a while for very-large datasets.&quot;
        )
        for bucket in tqdm(
            list(self.aspect_ratio_bucket_indices.keys()),
            leave=False,
            desc=&quot;Enforcing maximum aspect ratio&quot;,
        ):  # Safe iteration over keys
            if float(bucket) &gt; self.maximum_aspect_ratio:
                logger.info(
                    f&quot;Removing bucket {bucket} due to aspect ratio being greater than {self.maximum_aspect_ratio}.&quot;
                )
                del self.aspect_ratio_bucket_indices[bucket]
    def _prune_small_buckets(self, bucket):
        &quot;&quot;&quot;
        Remove buckets with fewer images than the batch size.
        &quot;&quot;&quot;
        if StateTracker.get_args().disable_bucket_pruning:
            logger.warning(
                &quot;Not pruning small buckets, as --disable_bucket_pruning is provided.&quot;
            )
            return
        if (
            bucket in self.aspect_ratio_bucket_indices
            and (
                len(self.aspect_ratio_bucket_indices[bucket]) * (int(self.repeats) + 1)
            )
            &lt; self.batch_size
        ):
            bucket_sample_count = len(self.aspect_ratio_bucket_indices[bucket])
            del self.aspect_ratio_bucket_indices[bucket]
            logger.warning(
                f&quot;Removing bucket {bucket} due to insufficient samples; your batch size may be too large for the small quantity of data (batch_size={self.batch_size} &gt; sample_count={bucket_sample_count}).&quot;
            )
    def _enforce_resolution_constraints(self, bucket):
        &quot;&quot;&quot;
        Enforce resolution constraints on images in a bucket.
        &quot;&quot;&quot;
        if self.minimum_image_size is not None:
            if bucket not in self.aspect_ratio_bucket_indices:
                logger.debug(
                    f&quot;Bucket {bucket} was already removed due to insufficient samples.&quot;
                )
                return
            images = self.aspect_ratio_bucket_indices[bucket]
            total_before = len(images)
            self.aspect_ratio_bucket_indices[bucket] = [
                img
                for img in images
                if self.meets_resolution_requirements(
                    image_path=img,
                    image=None,
                )
            ]
            total_after = len(self.aspect_ratio_bucket_indices[bucket])
            total_lost = total_before - total_after
            if total_lost &gt; 0:
                logger.info(
                    f&quot;Had {total_before} samples before and {total_lost} that did not meet the minimum image size requirement ({self.minimum_image_size}).&quot;
                )
    def meets_resolution_requirements(
        self,
        image_path: str = None,
        image: Image = None,
        image_metadata: dict = None,
    ):
        &quot;&quot;&quot;
        Check if an image meets the resolution requirements.
        &quot;&quot;&quot;
        if image is None and (image_path is not None and image_metadata is None):
            metadata = self.get_metadata_by_filepath(image_path)
            if metadata is None:
                logger.warning(f&quot;Metadata not found for image {image_path}.&quot;)
                return False
            width, height = metadata[&quot;original_size&quot;]
        elif isinstance(image, np.ndarray):
            # we have a video
            width, height = image.shape[2], image.shape[1]
            logger.debug(f&quot;Checking resolution: {width}x{height}&quot;)
            if self.minimum_num_frames is not None:
                num_frames = image.shape[0]
                if num_frames &lt; self.minimum_num_frames:
                    logger.debug(
                        f&quot;Video has {num_frames} frames, which is less than the minimum required {self.minimum_num_frames}.&quot;
                    )
                    return False
            if self.maximum_num_frames is not None:
                num_frames = image.shape[0]
                if num_frames &gt; self.maximum_num_frames:
                    logger.debug(
                        f&quot;Video has {num_frames} frames, which is more than the maximum configured {self.maximum_num_frames}.&quot;
                    )
                    return False
        elif image is not None:
            width, height = image.size
        elif image_metadata is not None:
            width, height = image_metadata[&quot;original_size&quot;]
        else:
            # Unexpected condition
            raise ValueError(
                f&quot;meets_resolution_requirements expects an image_path&quot;
                f&quot; ({image_path}) or Image object ({image}), but received neither.&quot;
            )
        if self.minimum_image_size is None:
            return True
        if self.resolution_type == &quot;pixel&quot;:
            return (
                self.minimum_image_size &lt;= width and self.minimum_image_size &lt;= height
            )
        elif self.resolution_type == &quot;area&quot;:
            # We receive megapixel integer value, and then have to compare here by converting minimum_image_size MP to pixels.
            if self.minimum_image_size &gt; 5:
                raise ValueError(
                    f&quot;--minimum_image_size was given with a value of {self.minimum_image_size} but resolution_type is area, which means this value is most likely too large. Please use a value less than 5.&quot;
                )
            # We need to find the square image length if crop_style = square.
            minimum_image_size = self.minimum_image_size * 1_000_000
            if (
                StateTracker.get_data_backend_config(self.id).get(&quot;crop&quot;, False)
                and StateTracker.get_data_backend_config(self.id).get(
                    &quot;crop_aspect&quot;, &quot;square&quot;
                )
                == &quot;square&quot;
            ):
                # When comparing the &apos;area&apos; of an image but cropping to square area, one side might be too small.
                # So we have to convert our megapixel value to a 1.0 aspect square image size.
                # We do this by taking the square root of the megapixel value.
                pixel_edge_len = floor(np.sqrt(minimum_image_size))
                if not (pixel_edge_len &lt;= width and pixel_edge_len &lt;= height):
                    # If the square edge length is too small, then the image is too small.
                    return False
            # Since we&apos;ve now tested whether a square-cropped image will be adequate, we can calculate the area of the image.
            return minimum_image_size &lt;= width * height
        else:
            raise ValueError(
                f&quot;BucketManager.meets_resolution_requirements received unexpected value for resolution_type: {self.resolution_type}&quot;
            )
    def handle_incorrect_bucket(
        self, image_path: str, bucket: str, actual_bucket: str, save_cache: bool = True
    ):
        &quot;&quot;&quot;
        Used by other classes to move images between buckets, when mis-detected.
        Args:
            image_path (str): The path to the image to move.
            bucket (str): The bucket to move the image from.
            actual_bucket (str): The bucket to move the image to.
        &quot;&quot;&quot;
        logger.warning(
            f&quot;Found an image in bucket {bucket} it doesn&apos;t belong in, when actually it is: {actual_bucket}&quot;
        )
        self.remove_image(image_path, bucket)
        if actual_bucket in self.aspect_ratio_bucket_indices:
            logger.warning(&quot;Moved image to bucket, it already existed.&quot;)
            self.aspect_ratio_bucket_indices[actual_bucket].append(image_path)
        else:
            logger.warning(&quot;Created new bucket for that pesky image.&quot;)
            self.aspect_ratio_bucket_indices[actual_bucket] = [image_path]
        if save_cache:
            self.save_cache()
    def handle_small_image(
        self, image_path: str, bucket: str, delete_unwanted_images: bool
    ):
        &quot;&quot;&quot;
        Used by other classes to remove an image, or DELETE it from disk, depending on parameters.
        Args:
            image_path (str): The path to the image to remove.
            bucket (str): The bucket to remove the image from.
            delete_unwanted_images (bool): Whether to delete the image from disk.
        &quot;&quot;&quot;
        if delete_unwanted_images:
            try:
                logger.warning(
                    f&quot;Image {image_path} too small: DELETING image and continuing search.&quot;
                )
                self.data_backend.delete(image_path)
            except Exception:
                logger.debug(
                    f&quot;Image {image_path} was already deleted. Another GPU must have gotten to it.&quot;
                )
        else:
            logger.warning(
                f&quot;Image {image_path} too small, but --delete_unwanted_images is not provided, so we simply ignore and remove from bucket.&quot;
            )
        self.remove_image(image_path, bucket)
    def has_single_underfilled_bucket(self):
        &quot;&quot;&quot;
        Check if there&apos;s only one active bucket and it has fewer images than the batch size.
        Returns:
            bool: True if there&apos;s a single underfilled bucket, False otherwise.
        &quot;&quot;&quot;
        if len(self.aspect_ratio_bucket_indices) != 1:
            return False
        bucket = list(self.aspect_ratio_bucket_indices.keys())[0]
        if (
            len(self.aspect_ratio_bucket_indices[bucket]) * (int(self.repeats) + 1)
        ) &lt; self.batch_size:
            return True
        return False
    def read_cache(self):
        &quot;&quot;&quot;
        Read the entire bucket cache.
        &quot;&quot;&quot;
        return self.aspect_ratio_bucket_indices
    def get_metadata_attribute_by_filepath(self, filepath: str, attribute: str):
        &quot;&quot;&quot;Use get_metadata_by_filepath to return a specific attribute.
        Args:
            filepath (str): The complete path from the aspect bucket list.
            attribute (str): The attribute you are seeking.
        Returns:
            any type: The attribute value, or None.
        &quot;&quot;&quot;
        metadata = self.get_metadata_by_filepath(filepath)
        if metadata:
            return metadata.get(attribute, None)
        else:
            return None
    def set_metadata_attribute_by_filepath(
        self, filepath: str, attribute: str, value: any, update_json: bool = True
    ):
        &quot;&quot;&quot;Use set_metadata_by_filepath to update the contents of a specific attribute.
        Args:
            filepath (str): The complete path from the aspect bucket list.
            attribute (str): The attribute you are updating.
            value (any type): The value to set.
        &quot;&quot;&quot;
        metadata = self.get_metadata_by_filepath(filepath) or {}
        metadata[attribute] = value
        return self.set_metadata_by_filepath(filepath, metadata, update_json)
    def set_metadata_by_filepath(
        self, filepath: str, metadata: dict, update_json: bool = True
    ):
        &quot;&quot;&quot;Set metadata for a given image file path.
        Args:
            filepath (str): The complete path from the aspect bucket list.
        &quot;&quot;&quot;
        with self.metadata_semaphor:
            logger.debug(f&quot;Setting metadata for {filepath} to {metadata}.&quot;)
            self.image_metadata[filepath] = metadata
            if update_json:
                self.save_image_metadata()
    def get_metadata_by_filepath(self, filepath: str):
        &quot;&quot;&quot;Retrieve metadata for a given image file path.
        Args:
            filepath (str): The complete or basename path from the aspect bucket list.
                            First, we search for the basename as the key, and we fall
                             back to the
        Returns:
            dict: Metadata for the image. Returns None if not found.
        &quot;&quot;&quot;
        if type(filepath) is tuple or type(filepath) is list:
            for path in filepath:
                if path in self.image_metadata:
                    result = self.image_metadata.get(path, None)
                    logger.debug(
                        f&quot;Retrieving metadata for path: {filepath}, result: {result}&quot;
                    )
                    if result is not None:
                        return result
            return None
        return self.image_metadata.get(filepath, None)
    def scan_for_metadata(self):
        &quot;&quot;&quot;
        Update the metadata without modifying the bucket indices.
        &quot;&quot;&quot;
        logger.info(f&quot;Loading metadata from {self.metadata_file}&quot;)
        self.load_image_metadata()
        logger.debug(
            f&quot;A subset of the available metadata: {list(self.image_metadata.keys())[:5]}&quot;
        )
        logger.info(&quot;Discovering new images for metadata scan...&quot;)
        new_files = self._discover_new_files(for_metadata=True)
        if not new_files:
            logger.info(&quot;No new files discovered. Exiting.&quot;)
            return
        existing_files_set = {
            existing_file for existing_file in self.image_metadata.keys()
        }
        num_cpus = 8  # Using a fixed number for better control and predictability
        files_split = np.array_split(new_files, num_cpus)
        metadata_updates_queue = Queue()
        tqdm_queue = Queue()
        worker_cls = (
            Process if StateTracker.get_args().enable_multiprocessing else Thread
        )
        workers = [
            worker_cls(
                target=self._bucket_worker,
                args=(
                    tqdm_queue,
                    file_shard,
                    None,  # Passing None to indicate we don&apos;t want to update the buckets
                    metadata_updates_queue,
                    None,  # Passing None to indicate we don&apos;t want to update the written files list
                    existing_files_set,
                ),
            )
            for file_shard in files_split
        ]
        for worker in workers:
            worker.start()
        with tqdm(
            desc=&quot;Scanning image metadata&quot;,
            total=len(new_files),
            leave=False,
            ncols=100,
        ) as pbar:
            while any(worker.is_alive() for worker in workers):
                while not tqdm_queue.empty():
                    pbar.update(tqdm_queue.get())
                # Only update the metadata
                while not metadata_updates_queue.empty():
                    metadata_update = metadata_updates_queue.get()
                    logger.debug(
                        f&quot;Received type of metadata update: {type(metadata_update)}, contents: {metadata_update}&quot;
                    )
                    if type(metadata_update) == dict:
                        for filepath, meta in metadata_update.items():
                            self.set_metadata_by_filepath(
                                filepath=filepath, metadata=meta, update_json=False
                            )
        for worker in workers:
            worker.join()
        self.save_image_metadata()
        self.save_cache(enforce_constraints=True)
        logger.info(&quot;Completed metadata update.&quot;)
    def handle_vae_cache_inconsistencies(self, vae_cache, vae_cache_behavior: str):
        &quot;&quot;&quot;
        Handles inconsistencies between the aspect buckets and the VAE cache.
        Args:
            vae_cache: The VAECache object.
            vae_cache_behavior (str): Behavior for handling inconsistencies (&apos;sync&apos; or &apos;recreate&apos;).
        &quot;&quot;&quot;
        if &quot;deepfloyd&quot; in StateTracker.get_args().model_type:
            return
        if vae_cache_behavior not in [&quot;sync&quot;, &quot;recreate&quot;]:
            raise ValueError(&quot;Invalid VAE cache behavior specified.&quot;)
        logger.info(&quot;Scanning VAE cache for inconsistencies with aspect buckets...&quot;)
        try:
            for cache_file, cache_content in vae_cache.scan_cache_contents():
                if cache_content is None:
                    continue
                if vae_cache_behavior == &quot;sync&quot;:
                    # Sync aspect buckets with the cache
                    expected_bucket = str(
                        self._get_aspect_ratio_from_tensor(cache_content)
                    )
                    self._modify_cache_entry_bucket(cache_file, expected_bucket)
                elif vae_cache_behavior == &quot;recreate&quot;:
                    # Delete the cache file if it doesn&apos;t match the aspect bucket indices
                    if self.is_cache_inconsistent(vae_cache, cache_file, cache_content):
                        threading.Thread(
                            target=self.data_backend.delete,
                            args=(cache_file,),
                            daemon=True,
                        ).start()
        except Exception as e:
            logger.debug(f&quot;Error running VAE cache scan: {e}&quot;)
            return
        # Update any state or metadata post-processing
        self.save_cache()
    def _recalculate_target_resolution(self, original_aspect_ratio: float) -&gt; tuple:
        &quot;&quot;&quot;Given the original resolution, use our backend config to properly recalculate the size.&quot;&quot;&quot;
        resolution_type = StateTracker.get_data_backend_config(self.id)[
            &quot;resolution_type&quot;
        ]
        resolution = StateTracker.get_data_backend_config(self.id)[&quot;resolution&quot;]
        if resolution_type == &quot;pixel&quot;:
            return MultiaspectImage.calculate_new_size_by_pixel_edge(
                original_aspect_ratio, int(resolution)
            )
        elif resolution_type == &quot;area&quot;:
            if original_aspect_ratio is None:
                raise ValueError(
                    &quot;Original aspect ratio must be provided for area-based resolution.&quot;
                )
            return MultiaspectImage.calculate_new_size_by_pixel_area(
                original_aspect_ratio, resolution
            )
    def is_cache_inconsistent(self, vae_cache, cache_file, cache_content):
        &quot;&quot;&quot;
        Check if a cache file&apos;s content is inconsistent with the aspect ratio bucket indices.
        Args:
            cache_file (str): The cache file path.
            cache_content: The content of the cache file (PyTorch Tensor).
        Returns:
            bool: True if the cache file is inconsistent, False otherwise.
        &quot;&quot;&quot;
        # Get tensor shape and multiply by self.scaling_factor or 8
        if cache_content is None:
            return True
        # is it a tensor with nan or inf values?
        if torch.isnan(cache_content).any() or torch.isinf(cache_content).any():
            logger.warning(f&quot;Cache file {cache_file} contains NaN or Inf values.&quot;)
            return True
        image_filename = vae_cache._image_filename_from_vaecache_filename(cache_file)
        logger.debug(
            f&quot;Checking cache file {cache_file} for inconsistencies. Image filename: {image_filename}&quot;
        )
        actual_resolution = self._get_image_size_from_tensor(cache_content)
        original_resolution = self.get_metadata_attribute_by_filepath(
            image_filename, &quot;original_size&quot;
        )
        metadata_target_size = self.get_metadata_attribute_by_filepath(
            image_filename, &quot;target_size&quot;
        )
        if metadata_target_size is None:
            logger.error(
                f&quot;Received sample with no metadata: {self.get_metadata_by_filepath(image_filename)}&quot;
            )
            return True
        target_resolution = tuple(metadata_target_size)
        recalculated_target_resolution, intermediary_size, recalculated_aspect_ratio = (
            self._recalculate_target_resolution(
                original_aspect_ratio=MultiaspectImage.calculate_image_aspect_ratio(
                    original_resolution
                )
            )
        )
        logger.debug(
            f&quot;Original resolution: {original_resolution}, Target resolution: {target_resolution}, Recalculated target resolution: {recalculated_target_resolution}&quot;
        )
        if (
            original_resolution is not None
            and target_resolution is not None
            and (
                actual_resolution != target_resolution
                or actual_resolution != recalculated_target_resolution
            )
        ):
            logger.debug(
                f&quot;Actual resolution {actual_resolution} does not match target resolution {target_resolution}, recalculated as {recalculated_target_resolution}.&quot;
            )
            return True
        else:
            logger.debug(
                f&quot;Actual resolution {actual_resolution} matches target resolution {target_resolution}.&quot;
            )
        actual_aspect_ratio = self._get_aspect_ratio_from_tensor(cache_content)
        expected_bucket = str(recalculated_aspect_ratio)
        logger.debug(
            f&quot;Expected bucket for {cache_file}: {expected_bucket} vs actual {actual_aspect_ratio}&quot;
        )
        # Extract the base filename without the extension
        base_filename = os.path.splitext(os.path.basename(cache_file))[0]
        base_filename_png = os.path.join(self.instance_data_dir, f&quot;{base_filename}.png&quot;)
        base_filename_jpg = os.path.join(self.instance_data_dir, f&quot;{base_filename}.jpg&quot;)
        # Check if the base filename is in the correct bucket
        if any(
            base_filename_png in self.aspect_ratio_bucket_indices.get(bucket, set())
            for bucket in [expected_bucket, str(expected_bucket)]
        ):
            logger.debug(f&quot;File {base_filename} is in the correct bucket.&quot;)
            return False
        if any(
            base_filename_jpg in self.aspect_ratio_bucket_indices.get(bucket, set())
            for bucket in [expected_bucket, str(expected_bucket)]
        ):
            logger.debug(f&quot;File {base_filename} is in the correct bucket.&quot;)
            return False
        logger.debug(f&quot;File {base_filename} was not found in the correct place.&quot;)
        return True
    def _get_aspect_ratio_from_tensor(self, tensor):
        &quot;&quot;&quot;
        Calculate the aspect ratio from a PyTorch Tensor.
        Args:
            tensor (torch.Tensor): The tensor representing the image.
        Returns:
            float: The aspect ratio of the image.
        &quot;&quot;&quot;
        if tensor.dim() &lt; 3:
            raise ValueError(
                &quot;Tensor does not have enough dimensions to determine aspect ratio.&quot;
            )
        # Assuming tensor is in CHW format (channel, height, width)
        _, height, width = tensor.size()
        return width / height
    def _get_image_size_from_tensor(self, tensor):
        &quot;&quot;&quot;
        Calculate the image size from a PyTorch Tensor.
        Args:
            tensor (torch.Tensor): The tensor representing the image.
        Returns:
            tuple[width, height]: The resolution of the image just before it was encoded.
        &quot;&quot;&quot;
        if tensor.dim() &lt; 3:
            raise ValueError(
                f&quot;Tensor does not have enough dimensions to determine an image resolution. Its shape is: {tensor.size}&quot;
            )
        # Assuming tensor is in CHW format (channel, height, width)
        _, height, width = tensor.size()
        return (
            width * self.vae_output_scaling_factor,
            height * self.vae_output_scaling_factor,
        )
    def _modify_cache_entry_bucket(self, cache_file, expected_bucket):
        &quot;&quot;&quot;
        Update the bucket indices based on the cache file&apos;s actual aspect ratio.
        Args:
            cache_file (str): The cache file path.
            expected_bucket (str): The bucket that the cache file should belong to.
        &quot;&quot;&quot;
        for bucket, files in self.aspect_ratio_bucket_indices.items():
            if cache_file in files and str(bucket) != str(expected_bucket):
                files.remove(cache_file)
                self.aspect_ratio_bucket_indices[expected_bucket].append(cache_file)
                break</file><file path="helpers/metadata/backends/discovery.py">from helpers.training.state_tracker import StateTracker
from helpers.data_backend.base import BaseDataBackend
from helpers.metadata.backends.base import MetadataBackend
from helpers.image_manipulation.training_sample import TrainingSample
from helpers.image_manipulation.load import load_image, load_video
from helpers.training.multi_process import should_log
import json
import logging
import os
import traceback
from io import BytesIO
from helpers.image_manipulation.brightness import calculate_luminance
from helpers.training import image_file_extensions, video_file_extensions
logger = logging.getLogger(&quot;DiscoveryMetadataBackend&quot;)
if should_log():
    target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
else:
    target_level = &quot;ERROR&quot;
logger.setLevel(target_level)
class DiscoveryMetadataBackend(MetadataBackend):
    def __init__(
        self,
        id: str,
        instance_data_dir: str,
        cache_file: str,
        metadata_file: str,
        data_backend: BaseDataBackend,
        accelerator,
        batch_size: int,
        resolution: float,
        resolution_type: str,
        delete_problematic_images: bool = False,
        delete_unwanted_images: bool = False,
        metadata_update_interval: int = 3600,
        minimum_image_size: int = None,
        minimum_aspect_ratio: int = None,
        maximum_aspect_ratio: int = None,
        num_frames: int = None,
        minimum_num_frames: int = None,
        maximum_num_frames: int = None,
        cache_file_suffix: str = None,
        repeats: int = 0,
    ):
        super().__init__(
            id=id,
            instance_data_dir=instance_data_dir,
            cache_file=cache_file,
            metadata_file=metadata_file,
            data_backend=data_backend,
            accelerator=accelerator,
            batch_size=batch_size,
            resolution=resolution,
            resolution_type=resolution_type,
            delete_problematic_images=delete_problematic_images,
            delete_unwanted_images=delete_unwanted_images,
            metadata_update_interval=metadata_update_interval,
            minimum_image_size=minimum_image_size,
            minimum_aspect_ratio=minimum_aspect_ratio,
            maximum_aspect_ratio=maximum_aspect_ratio,
            maximum_num_frames=maximum_num_frames,
            minimum_num_frames=minimum_num_frames,
            num_frames=num_frames,
            cache_file_suffix=cache_file_suffix,
            repeats=repeats,
        )
    def _discover_new_files(
        self, for_metadata: bool = False, ignore_existing_cache: bool = False
    ):
        &quot;&quot;&quot;
        Discover new files that have not been processed yet.
        Returns:
            list: A list of new files.
        &quot;&quot;&quot;
        all_image_files = StateTracker.get_image_files(
            data_backend_id=self.data_backend.id
        )
        if ignore_existing_cache:
            # Return all files and remove the existing buckets.
            logger.debug(
                &quot;Resetting the entire aspect bucket cache as we&apos;ve received the signal to ignore existing cache.&quot;
            )
            self.aspect_ratio_bucket_indices = {}
            return list(all_image_files.keys())
        if all_image_files is None:
            logger.debug(&quot;No image file cache available, retrieving fresh&quot;)
            all_image_files = self.data_backend.list_files(
                instance_data_dir=self.instance_data_dir,
                file_extensions=image_file_extensions,
            )
            all_image_files = StateTracker.set_image_files(
                all_image_files, data_backend_id=self.data_backend.id
            )
        else:
            logger.debug(&quot;Using cached image file list&quot;)
        # Flatten the list if it contains nested lists
        if any(isinstance(i, list) for i in all_image_files):
            all_image_files = [item for sublist in all_image_files for item in sublist]
        # logger.debug(f&quot;All image files: {json.dumps(all_image_files, indent=4)}&quot;)
        all_image_files_set = set(all_image_files)
        if for_metadata:
            result = [
                file
                for file in all_image_files
                if self.get_metadata_by_filepath(file) is None
            ]
        else:
            processed_files = set(
                path
                for paths in self.aspect_ratio_bucket_indices.values()
                for path in paths
            )
            result = [
                file for file in all_image_files_set if file not in processed_files
            ]
        return result
    def reload_cache(self, set_config: bool = True):
        &quot;&quot;&quot;
        Load cache data from a JSON file.
        Returns:
            dict: The cache data.
        &quot;&quot;&quot;
        # Query our DataBackend to see whether the cache file exists.
        logger.debug(f&quot;Checking for cache file: {self.cache_file}&quot;)
        if self.data_backend.exists(self.cache_file):
            try:
                # Use our DataBackend to actually read the cache file.
                logger.debug(&quot;Pulling cache file from storage&quot;)
                cache_data_raw = self.data_backend.read(self.cache_file)
                cache_data = json.loads(cache_data_raw)
            except Exception as e:
                logger.warning(
                    f&quot;Error loading aspect bucket cache, creating new one: {e}&quot;
                )
                cache_data = {}
            self.aspect_ratio_bucket_indices = cache_data.get(
                &quot;aspect_ratio_bucket_indices&quot;, {}
            )
            if set_config:
                self.config = cache_data.get(&quot;config&quot;, {})
                if self.config != {}:
                    logger.debug(f&quot;Setting config to {self.config}&quot;)
                    logger.debug(f&quot;Loaded previous data backend config: {self.config}&quot;)
                    StateTracker.set_data_backend_config(
                        data_backend_id=self.id,
                        config=self.config,
                    )
            logger.debug(
                f&quot;(id={self.id}) Loaded {len(self.aspect_ratio_bucket_indices)} aspect ratio buckets&quot;
            )
        else:
            logger.warning(&quot;No cache file found, creating new one.&quot;)
    def save_cache(self, enforce_constraints: bool = False):
        &quot;&quot;&quot;
        Save cache data to file.
        &quot;&quot;&quot;
        # Prune any buckets that have fewer samples than batch_size
        if enforce_constraints:
            self._enforce_min_bucket_size()
        self._enforce_min_aspect_ratio()
        self._enforce_max_aspect_ratio()
        if self.read_only:
            logger.debug(&quot;Skipping cache update on storage backend, read-only mode.&quot;)
            return
        # Convert any non-strings into strings as we save the index.
        aspect_ratio_bucket_indices_str = {
            key: [str(path) for path in value]
            for key, value in self.aspect_ratio_bucket_indices.items()
        }
        # Encode the cache as JSON.
        cache_data = {
            &quot;config&quot;: StateTracker.get_data_backend_config(
                data_backend_id=self.data_backend.id
            ),
            &quot;aspect_ratio_bucket_indices&quot;: aspect_ratio_bucket_indices_str,
        }
        logger.debug(f&quot;save_cache has config to write: {cache_data[&apos;config&apos;]}&quot;)
        cache_data_str = json.dumps(cache_data)
        # Use our DataBackend to write the cache file.
        self.data_backend.write(self.cache_file, cache_data_str)
    def load_image_metadata(self):
        &quot;&quot;&quot;Load image metadata from a JSON file.&quot;&quot;&quot;
        self.image_metadata = {}
        self.image_metadata_loaded = False
        if self.data_backend.exists(self.metadata_file):
            cache_data_raw = self.data_backend.read(self.metadata_file)
            self.image_metadata = json.loads(cache_data_raw)
            self.image_metadata_loaded = True
    def save_image_metadata(self):
        &quot;&quot;&quot;Save image metadata to a JSON file.&quot;&quot;&quot;
        self.data_backend.write(self.metadata_file, json.dumps(self.image_metadata))
    def _process_for_bucket(
        self,
        image_path_str,
        aspect_ratio_bucket_indices,
        aspect_ratio_rounding: int = 3,
        metadata_updates=None,
        delete_problematic_images: bool = False,
        statistics: dict = {},
    ):
        try:
            image_metadata = {}
            image_data = self.data_backend.read(image_path_str)
            if image_data is None:
                logger.debug(
                    f&quot;Image {image_path_str} was not found on the backend. Skipping image.&quot;
                )
                statistics.setdefault(&quot;skipped&quot;, {}).setdefault(&quot;not_found&quot;, 0)
                statistics[&quot;skipped&quot;][&quot;not_found&quot;] += 1
                return aspect_ratio_bucket_indices
            file_extension = os.path.splitext(image_path_str)[1].lower()
            file_loader = load_image
            if file_extension.strip(&quot;.&quot;) in video_file_extensions:
                file_loader = load_video
            image = file_loader(BytesIO(image_data))
            if not self.meets_resolution_requirements(image=image):
                if not self.delete_unwanted_images:
                    logger.debug(
                        f&quot;Image {image_path_str} does not meet minimum size requirements. Skipping image.&quot;
                    )
                else:
                    logger.debug(
                        f&quot;Image {image_path_str} does not meet minimum size requirements. Deleting image.&quot;
                    )
                    self.data_backend.delete(image_path_str)
                statistics.setdefault(&quot;skipped&quot;, {}).setdefault(&quot;too_small&quot;, 0)
                statistics[&quot;skipped&quot;][&quot;too_small&quot;] += 1
                return aspect_ratio_bucket_indices
            if hasattr(image, &quot;shape&quot;):
                image_metadata[&quot;original_size&quot;] = (image.shape[2], image.shape[1])
                image_metadata[&quot;num_frames&quot;] = image.shape[0]
            elif hasattr(image, &quot;resize&quot;):
                image_metadata[&quot;original_size&quot;] = image.size
            training_sample = TrainingSample(
                image=image,
                data_backend_id=self.id,
                image_metadata=image_metadata,
                image_path=image_path_str,
            )
            prepared_sample = training_sample.prepare()
            image_metadata.update(
                {
                    &quot;crop_coordinates&quot;: prepared_sample.crop_coordinates,
                    &quot;target_size&quot;: prepared_sample.target_size,
                    &quot;intermediary_size&quot;: prepared_sample.intermediary_size,
                    &quot;aspect_ratio&quot;: prepared_sample.aspect_ratio,
                    &quot;luminance&quot;: calculate_luminance(image),
                }
            )
            logger.debug(
                f&quot;Image {image_path_str} has aspect ratio {prepared_sample.aspect_ratio} and size {image.size}.&quot;
            )
            aspect_ratio_key = str(prepared_sample.aspect_ratio)
            if aspect_ratio_key not in aspect_ratio_bucket_indices:
                aspect_ratio_bucket_indices[aspect_ratio_key] = []
            aspect_ratio_bucket_indices[aspect_ratio_key].append(image_path_str)
            if metadata_updates is not None:
                metadata_updates[image_path_str] = image_metadata
        except Exception as e:
            logger.error(f&quot;Error processing image: {e}&quot;)
            logger.error(f&quot;Error traceback: {traceback.format_exc()}&quot;)
            if delete_problematic_images:
                logger.error(f&quot;Deleting image {image_path_str}.&quot;)
                self.data_backend.delete(image_path_str)
        return aspect_ratio_bucket_indices
    def __len__(self):
        &quot;&quot;&quot;
        Returns:
            int: The number of batches in the dataset, accounting for images that can&apos;t form a complete batch and are discarded.
        &quot;&quot;&quot;
        def repeat_len(bucket):
            return len(bucket) * (self.repeats + 1)
        return sum(
            (repeat_len(bucket) + (self.batch_size - 1)) // self.batch_size
            for bucket in self.aspect_ratio_bucket_indices.values()
            if repeat_len(bucket) &gt;= self.batch_size
        )</file><file path="helpers/metadata/backends/parquet.py">from helpers.training.state_tracker import StateTracker
from helpers.training import image_file_extensions, video_file_extensions
from helpers.multiaspect.image import MultiaspectImage
from helpers.data_backend.base import BaseDataBackend
from helpers.image_manipulation.training_sample import TrainingSample
from helpers.metadata.backends.base import MetadataBackend
from tqdm import tqdm
import json
import logging
import os
import time
import traceback
import numpy
logger = logging.getLogger(&quot;ParquetMetadataBackend&quot;)
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
try:
    import pandas as pd
except ImportError:
    raise ImportError(&quot;Pandas is required for the ParquetMetadataBackend.&quot;)
class ParquetMetadataBackend(MetadataBackend):
    def __init__(
        self,
        id: str,
        instance_data_dir: str,
        cache_file: str,
        metadata_file: str,
        data_backend: BaseDataBackend,
        accelerator,
        batch_size: int,
        resolution: float,
        resolution_type: str,
        parquet_config: dict,
        delete_problematic_images: bool = False,
        delete_unwanted_images: bool = False,
        metadata_update_interval: int = 3600,
        minimum_image_size: int = None,
        minimum_aspect_ratio: int = None,
        maximum_aspect_ratio: int = None,
        num_frames: int = None,
        minimum_num_frames: int = None,
        maximum_num_frames: int = None,
        cache_file_suffix: str = None,
        repeats: int = 0,
    ):
        self.parquet_config = parquet_config
        self.parquet_path = parquet_config.get(&quot;path&quot;, None)
        self.is_json_lines = self.parquet_path.endswith(&quot;.jsonl&quot;)
        self.is_json_file = self.parquet_path.endswith(&quot;.json&quot;)
        self.num_frames_column = parquet_config.get(&quot;num_frames_column&quot;, None)
        self.fps_column = parquet_config.get(&quot;fps_column&quot;, None)
        super().__init__(
            id=id,
            instance_data_dir=instance_data_dir,
            cache_file=cache_file,
            metadata_file=metadata_file,
            data_backend=data_backend,
            accelerator=accelerator,
            batch_size=batch_size,
            resolution=resolution,
            resolution_type=resolution_type,
            delete_problematic_images=delete_problematic_images,
            delete_unwanted_images=delete_unwanted_images,
            metadata_update_interval=metadata_update_interval,
            minimum_image_size=minimum_image_size,
            minimum_aspect_ratio=minimum_aspect_ratio,
            maximum_aspect_ratio=maximum_aspect_ratio,
            maximum_num_frames=maximum_num_frames,
            minimum_num_frames=minimum_num_frames,
            num_frames=num_frames,
            cache_file_suffix=cache_file_suffix,
            repeats=repeats,
        )
        self.load_parquet_database()
        self.caption_cache = self._extract_captions_to_fast_list()
        self.missing_captions = self._locate_missing_caption_from_fast_list()
        if self.missing_captions:
            logger.warning(
                f&quot;Missing captions for {len(self.missing_captions)} images: {self.missing_captions}&quot;
            )
            self._remove_images_with_missing_captions()
    def _remove_images_with_missing_captions(self):
        &quot;&quot;&quot;
        Remove images/videos from the aspect ratio bucket indices that have missing captions.
        &quot;&quot;&quot;
        for key in self.aspect_ratio_bucket_indices.keys():
            len_before = len(self.aspect_ratio_bucket_indices[key])
            self.aspect_ratio_bucket_indices[key] = [
                path
                for path in self.aspect_ratio_bucket_indices[key]
                if path not in self.missing_captions
            ]
            len_after = len(self.aspect_ratio_bucket_indices[key])
            if len_before != len_after:
                logger.warning(
                    f&quot;Removed {len_before - len_after} items from aspect ratio bucket {key} due to missing captions.&quot;
                )
        self.save_cache(enforce_constraints=True)
        self.missing_captions = []
    def load_parquet_database(self):
        &quot;&quot;&quot;
        Load the parquet database from file, either as JSON/JSONL or Parquet.
        &quot;&quot;&quot;
        if self.data_backend.exists(self.parquet_path):
            try:
                bytes_string = self.data_backend.read(self.parquet_path)
                import io
                pq = io.BytesIO(bytes_string)
            except Exception as e:
                raise e
            if self.is_json_lines or self.is_json_file:
                self.parquet_database = pd.read_json(pq, lines=self.is_json_lines)
            else:
                self.parquet_database = pd.read_parquet(pq, engine=&quot;pyarrow&quot;)
            self.parquet_database.set_index(
                self.parquet_config.get(&quot;filename_column&quot;), inplace=True
            )
        else:
            raise FileNotFoundError(
                f&quot;Parquet could not be loaded from {self.parquet_path}: file does not exist.&quot;
            )
    def _locate_missing_caption_from_fast_list(self):
        &quot;&quot;&quot;
        Compare the fast list keys vs the filenames in our aspect ratio bucket indices.
        Return a list of items that have no caption present in the parquet.
        &quot;&quot;&quot;
        missing_captions = []
        identifier_includes_extension = self.parquet_config.get(
            &quot;identifier_includes_extension&quot;, False
        )
        # currently we do not do path-based identifiers
        identifier_includes_path = False
        for key in self.aspect_ratio_bucket_indices.keys():
            for filename in self.aspect_ratio_bucket_indices[key]:
                # Chop extension if parquet expects no extension
                if not identifier_includes_extension:
                    filename = os.path.splitext(filename)[0]
                if not identifier_includes_path:
                    filename = filename.replace(self.instance_data_dir, &quot;&quot;)
                    if filename.startswith(&quot;/&quot;):
                        filename = filename[1:]
                if filename not in self.caption_cache:
                    missing_captions.append(filename)
        return missing_captions
    def _extract_captions_to_fast_list(self):
        &quot;&quot;&quot;
        Pull the captions from the parquet table into a dict {filename: caption}.
        This helps because parquet is columnar, so repeated lookups can be slow. We want a quick dict.
        &quot;&quot;&quot;
        if self.parquet_database is None:
            raise ValueError(&quot;Parquet database is not loaded.&quot;)
        filename_column = self.parquet_config.get(&quot;filename_column&quot;)
        caption_column = self.parquet_config.get(&quot;caption_column&quot;)
        fallback_caption_column = self.parquet_config.get(&quot;fallback_caption_column&quot;)
        identifier_includes_extension = self.parquet_config.get(
            &quot;identifier_includes_extension&quot;, False
        )
        captions = {}
        for index, row in self.parquet_database.iterrows():
            if filename_column in row:
                file_id = str(row[filename_column])
            else:
                file_id = str(index)
            if not identifier_includes_extension:
                file_id = os.path.splitext(file_id)[0]
            # Could be list or single str
            if isinstance(caption_column, list):
                if len(caption_column) &gt; 0:
                    # Gather all columns in a list
                    cvals = [row.get(c, &quot;&quot;) for c in caption_column]
                    cvals = [c for c in cvals if c]  # remove empties
                    caption = cvals if cvals else None
                else:
                    caption = None
            else:
                caption = row.get(caption_column, None)
            if caption is None and fallback_caption_column:
                caption = row.get(fallback_caption_column, None)
            if isinstance(caption, (numpy.ndarray, pd.Series)):
                caption = [str(item) for item in caption if item is not None]
            elif isinstance(caption, bytes):
                caption = caption.decode(&quot;utf-8&quot;)
            elif isinstance(caption, str):
                caption = caption.strip()
            # If it remains None or empty, we can&apos;t do anything
            if not caption:
                continue
            captions[file_id] = caption
        return captions
    def caption_cache_entry(self, index: str):
        result = self.caption_cache.get(str(index), None)
        logger.debug(f&quot;Caption cache entry for {str(index)}: {result}&quot;)
        return result
    def _discover_new_files(
        self, for_metadata: bool = False, ignore_existing_cache: bool = False
    ):
        &quot;&quot;&quot;
        Discover new files that have not been processed yet, from the state tracker or local data_backend.
        &quot;&quot;&quot;
        all_image_files = StateTracker.get_image_files(
            data_backend_id=self.data_backend.id
        )
        if all_image_files is None:
            logger.debug(&quot;No image file cache available, retrieving fresh&quot;)
            all_image_files = self.data_backend.list_files(
                instance_data_dir=self.instance_data_dir,
                file_extensions=image_file_extensions,
            )
            # Flatten nested lists
            if any(isinstance(i, list) for i in all_image_files):
                all_image_files = [
                    item for sublist in all_image_files for item in sublist
                ]
            all_image_files = StateTracker.set_image_files(
                all_image_files, data_backend_id=self.data_backend.id
            )
        else:
            logger.debug(&quot;Using cached image file list&quot;)
            # Flatten if necessary
            if any(isinstance(i, list) for i in all_image_files):
                all_image_files = [
                    item for sublist in all_image_files for item in sublist
                ]
        if ignore_existing_cache:
            logger.debug(
                &quot;Resetting entire aspect ratio bucket cache due to ignore_existing_cache=True.&quot;
            )
            self.aspect_ratio_bucket_indices = {}
            return list(all_image_files)
        all_image_files_set = set(all_image_files)
        if for_metadata:
            result = [
                file
                for file in all_image_files
                if self.get_metadata_by_filepath(file) is None
            ]
        else:
            processed_files = set(
                path
                for paths in self.aspect_ratio_bucket_indices.values()
                for path in paths
            )
            result = [
                file for file in all_image_files_set if file not in processed_files
            ]
        return result
    def reload_cache(self, set_config: bool = True):
        &quot;&quot;&quot;
        Load cache data from a JSON file on the data_backend.
        &quot;&quot;&quot;
        if self.data_backend.exists(self.cache_file):
            try:
                cache_data_raw = self.data_backend.read(self.cache_file)
                cache_data = json.loads(cache_data_raw)
                logger.debug(&quot;Loaded existing aspect ratio cache.&quot;)
            except Exception as e:
                logger.warning(
                    f&quot;Error loading aspect ratio bucket cache, creating new one: {e}&quot;
                )
                cache_data = {}
            self.aspect_ratio_bucket_indices = cache_data.get(
                &quot;aspect_ratio_bucket_indices&quot;, {}
            )
            if set_config:
                self.config = cache_data.get(&quot;config&quot;, {})
                if self.config != {}:
                    logger.debug(f&quot;Setting config to {self.config}&quot;)
                    StateTracker.set_data_backend_config(
                        data_backend_id=self.id,
                        config=self.config,
                    )
        else:
            logger.warning(&quot;No cache file found, starting a fresh one.&quot;)
    def save_cache(self, enforce_constraints: bool = False):
        &quot;&quot;&quot;
        Save cache data as JSON to the data_backend.
        &quot;&quot;&quot;
        if enforce_constraints:
            self._enforce_min_bucket_size()
        self._enforce_min_aspect_ratio()
        self._enforce_max_aspect_ratio()
        if self.read_only:
            logger.debug(&quot;Metadata backend is read-only. Skipping save.&quot;)
            return
        aspect_ratio_bucket_indices_str = {
            key: [str(path) for path in value]
            for key, value in self.aspect_ratio_bucket_indices.items()
        }
        cache_data = {
            &quot;config&quot;: StateTracker.get_data_backend_config(
                data_backend_id=self.data_backend.id
            ),
            &quot;aspect_ratio_bucket_indices&quot;: aspect_ratio_bucket_indices_str,
        }
        cache_data_str = json.dumps(cache_data)
        self.data_backend.write(self.cache_file, cache_data_str)
        logger.debug(&quot;Aspect ratio cache saved.&quot;)
    def load_image_metadata(self):
        logger.debug(f&quot;Loading metadata from {self.metadata_file}&quot;)
        self.image_metadata = {}
        self.image_metadata_loaded = False
        if self.data_backend.exists(self.metadata_file):
            raw = self.data_backend.read(self.metadata_file)
            self.image_metadata = json.loads(raw)
            self.image_metadata_loaded = True
        logger.debug(&quot;Metadata loaded.&quot;)
    def save_image_metadata(self):
        self.data_backend.write(self.metadata_file, json.dumps(self.image_metadata))
        logger.debug(&quot;Metadata file saved.&quot;)
    def compute_aspect_ratio_bucket_indices(self, ignore_existing_cache: bool = False):
        &quot;&quot;&quot;
        Build the aspect ratio buckets from the parquet data. We do not physically load images or videos.
        &quot;&quot;&quot;
        logger.info(&quot;Discovering new files for parquet-based metadata.&quot;)
        new_files = self._discover_new_files(
            ignore_existing_cache=ignore_existing_cache
        )
        existing_files_set = set().union(*self.aspect_ratio_bucket_indices.values())
        statistics = {
            &quot;total_processed&quot;: 0,
            &quot;skipped&quot;: {
                &quot;already_exists&quot;: len(existing_files_set),
                &quot;metadata_missing&quot;: 0,
                &quot;not_found&quot;: 0,
                &quot;too_small&quot;: 0,
                &quot;other&quot;: 0,
            },
        }
        if not new_files:
            logger.debug(&quot;No new files discovered. Stopping.&quot;)
            return
        try:
            self.load_image_metadata()
        except Exception as e:
            if ignore_existing_cache:
                logger.warning(
                    f&quot;Error loading image metadata, ignoring existing metadata: {e}&quot;
                )
                self.image_metadata = {}
            else:
                raise Exception(
                    f&quot;Error loading image metadata. Consider removing the metadata file manually: {e}&quot;
                )
        last_write_time = time.time()
        aspect_ratio_bucket_updates = {}
        for file in tqdm(
            new_files,
            desc=&quot;Generating aspect bucket cache&quot;,
            total=len(new_files),
            leave=False,
            ncols=100,
            miniters=max(1, len(new_files) // 100),
        ):
            current_time = time.time()
            if file not in existing_files_set:
                metadata_updates = {}
                if self.should_abort:
                    logger.info(&quot;Aborting aspect bucket update.&quot;)
                    return
                aspect_ratio_bucket_updates = self._process_for_bucket(
                    file,
                    aspect_ratio_bucket_updates,
                    metadata_updates=metadata_updates,
                    delete_problematic_images=self.delete_problematic_images,
                    statistics=statistics,
                )
                statistics[&quot;total_processed&quot;] += 1
            else:
                statistics[&quot;skipped&quot;][&quot;already_exists&quot;] += 1
                continue
            # If we have metadata updates, store them
            if metadata_updates and file in metadata_updates:
                self.set_metadata_by_filepath(
                    filepath=file, metadata=metadata_updates[file], update_json=False
                )
            # Periodically write partial data
            if (current_time - last_write_time) &gt;= self.metadata_update_interval:
                logger.debug(
                    f&quot;In-flight metadata update after {current_time - last_write_time:.2f} sec. &quot;
                    f&quot;Saving {len(self.image_metadata)} metadata entries &amp; {len(self.aspect_ratio_bucket_indices)} buckets.&quot;
                )
                self.save_cache(enforce_constraints=False)
                self.save_image_metadata()
                last_write_time = current_time
        # Extend final results
        for key, value in aspect_ratio_bucket_updates.items():
            self.aspect_ratio_bucket_indices.setdefault(key, []).extend(value)
        logger.info(f&quot;Image processing statistics: {statistics}&quot;)
        self.save_image_metadata()
        self.save_cache(enforce_constraints=True)
        logger.info(&quot;Completed aspect bucket update (parquet).&quot;)
    def _get_first_value(self, series_or_scalar):
        &quot;&quot;&quot;
        Extract the first value if the input is a Series, else return it.
        &quot;&quot;&quot;
        import pandas as pd
        import numpy as np
        if isinstance(series_or_scalar, pd.Series):
            return int(series_or_scalar.iloc[0])
        elif isinstance(series_or_scalar, str):
            return int(series_or_scalar)
        elif isinstance(series_or_scalar, (int, float, np.int64)):
            return series_or_scalar
        else:
            raise ValueError(f&quot;Unsupported data type: {type(series_or_scalar)}&quot;)
    def _process_for_bucket(
        self,
        image_path_str,
        aspect_ratio_bucket_indices,
        aspect_ratio_rounding: int = 3,
        metadata_updates=None,
        delete_problematic_images: bool = False,
        statistics: dict = {},
    ):
        &quot;&quot;&quot;
        For each file, we look up its row in the Parquet, get width/height (or frames).
        We skip physically loading the data for videos. We only trust columns in parquet.
        &quot;&quot;&quot;
        try:
            # 1. Identify the row in parquet
            image_path_filtered = image_path_str
            if not self.parquet_config.get(&quot;identifier_includes_extension&quot;, False):
                image_path_filtered = os.path.splitext(
                    os.path.split(image_path_str)[-1]
                )[0]
            if self.instance_data_dir in image_path_filtered:
                image_path_filtered = image_path_filtered.replace(
                    self.instance_data_dir, &quot;&quot;
                )
                if image_path_filtered.startswith(&quot;/&quot;):
                    image_path_filtered = image_path_filtered[1:]
            try:
                database_row = self.parquet_database.loc[image_path_filtered]
            except KeyError:
                database_row = None
            if database_row is None:
                logger.debug(f&quot;Metadata missing in parquet for {image_path_str}.&quot;)
                statistics.setdefault(&quot;skipped&quot;, {}).setdefault(&quot;metadata_missing&quot;, 0)
                statistics[&quot;skipped&quot;][&quot;metadata_missing&quot;] += 1
                return aspect_ratio_bucket_indices
            # 2. Check if it&apos;s image or video by extension
            extension = os.path.splitext(image_path_str)[1].lower().strip(&quot;.&quot;)
            is_video = extension in video_file_extensions
            # We&apos;ll store image/video metadata in the same place
            image_metadata = {}
            # 3. If it&apos;s an image, get width/height from parquet columns
            width_column = self.parquet_config.get(&quot;width_column&quot;, &quot;width&quot;)
            height_column = self.parquet_config.get(&quot;height_column&quot;, &quot;height&quot;)
            if is_video:
                if self.num_frames_column:
                    num_frames_val = database_row.get(self.num_frames_column, None)
                    if num_frames_val is not None:
                        num_frames_val = self._get_first_value(num_frames_val)
                        image_metadata[&quot;num_frames&quot;] = num_frames_val
                        # Check if it meets min/max frames
                        if (
                            self.minimum_num_frames
                            and num_frames_val &lt; self.minimum_num_frames
                        ):
                            logger.debug(
                                f&quot;Video {image_path_str} has {num_frames_val} frames, below min {self.minimum_num_frames}. Skipping.&quot;
                            )
                            statistics.setdefault(&quot;skipped&quot;, {}).setdefault(
                                &quot;too_small&quot;, 0
                            )
                            statistics[&quot;skipped&quot;][&quot;too_small&quot;] += 1
                            return aspect_ratio_bucket_indices
                        if (
                            self.maximum_num_frames
                            and num_frames_val &gt; self.maximum_num_frames
                        ):
                            logger.debug(
                                f&quot;Video {image_path_str} has {num_frames_val} frames, above max {self.maximum_num_frames}. Deleting or skipping.&quot;
                            )
                            if self.delete_unwanted_images:
                                try:
                                    self.data_backend.delete(image_path_str)
                                except:
                                    pass
                            statistics.setdefault(&quot;skipped&quot;, {}).setdefault(
                                &quot;too_small&quot;, 0
                            )
                            statistics[&quot;skipped&quot;][&quot;too_small&quot;] += 1
                            return aspect_ratio_bucket_indices
                else:
                    # if no num_frames_column is provided, do a fallback or skip
                    logger.warning(
                        f&quot;Video {image_path_str} found but no num_frames_column in parquet_config. Skipping.&quot;
                    )
                    return aspect_ratio_bucket_indices
                # For videos, also store fallback width/height if provided in parquet
                w = database_row.get(width_column, None)
                h = database_row.get(height_column, None)
                if w is not None and h is not None:
                    w = self._get_first_value(w)
                    h = self._get_first_value(h)
                    image_metadata[&quot;original_size&quot;] = (w, h)
                else:
                    logger.warning(
                        f&quot;Video {image_path_str} found but no width/height columns in parquet_config. Skipping.&quot;
                    )
                    return aspect_ratio_bucket_indices
            else:
                # It&apos;s an image, use width/height for min size checks
                if width_column is None or height_column is None:
                    raise ValueError(
                        &quot;ParquetMetadataBackend requires width and height columns for images.&quot;
                    )
                w = self._get_first_value(database_row[width_column])
                h = self._get_first_value(database_row[height_column])
                image_metadata[&quot;original_size&quot;] = (w, h)
            # 4. Check if the item meets size constraints
            if not self.meets_resolution_requirements(image_metadata=image_metadata):
                if self.delete_unwanted_images:
                    logger.debug(
                        f&quot;{image_path_str} does not meet resolution requirements. Deleting.&quot;
                    )
                    try:
                        self.data_backend.delete(image_path_str)
                    except:
                        pass
                statistics.setdefault(&quot;skipped&quot;, {}).setdefault(&quot;too_small&quot;, 0)
                statistics[&quot;skipped&quot;][&quot;too_small&quot;] += 1
                return aspect_ratio_bucket_indices
            # 5. Create a TrainingSample with minimal metadata
            training_sample = TrainingSample(
                image=None,  # no actual image data
                data_backend_id=self.id,
                image_metadata={&quot;original_size&quot;: image_metadata.get(&quot;original_size&quot;)},
                image_path=image_path_str,
            )
            prepared_sample = training_sample.prepare()
            # We&apos;ll store final aspect ratio from the TrainingSample or parquet
            aspect_ratio_column = self.parquet_config.get(&quot;aspect_ratio_column&quot;)
            if aspect_ratio_column and aspect_ratio_column in database_row:
                raw_ar = database_row[aspect_ratio_column]
                raw_ar = float(self._get_first_value(raw_ar))
                aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(raw_ar)
            else:
                aspect_ratio = float(prepared_sample.aspect_ratio)
            # Build the final metadata
            image_metadata.update(
                {
                    &quot;intermediary_size&quot;: prepared_sample.intermediary_size,
                    &quot;crop_coordinates&quot;: prepared_sample.crop_coordinates,
                    &quot;target_size&quot;: prepared_sample.target_size,
                    &quot;aspect_ratio&quot;: aspect_ratio,
                }
            )
            # if is_video and you want to store e.g. fps
            if is_video and self.fps_column:
                fps_val = database_row.get(self.fps_column, None)
                if fps_val is not None:
                    fps_val = self._get_first_value(fps_val)
                    image_metadata[&quot;fps&quot;] = fps_val
            # Insert into bucket
            aspect_ratio_key = str(aspect_ratio)
            if aspect_ratio_key not in aspect_ratio_bucket_indices:
                aspect_ratio_bucket_indices[aspect_ratio_key] = []
            aspect_ratio_bucket_indices[aspect_ratio_key].append(image_path_str)
            # If we are accumulating metadata updates
            if metadata_updates is not None:
                metadata_updates[image_path_str] = image_metadata
        except Exception as e:
            logger.error(f&quot;Error processing file {image_path_str}: {e}&quot;)
            logger.error(traceback.format_exc())
            if delete_problematic_images:
                logger.error(f&quot;Deleting file {image_path_str} after error.&quot;)
                self.data_backend.delete(image_path_str)
        return aspect_ratio_bucket_indices
    def __len__(self):
        &quot;&quot;&quot;
        Count how many full batches we can form with the aspect_ratio_bucket_indices.
        &quot;&quot;&quot;
        def repeat_len(bucket):
            return len(bucket) * (self.repeats + 1)
        return sum(
            (repeat_len(bucket) + (self.batch_size - 1)) // self.batch_size
            for bucket in self.aspect_ratio_bucket_indices.values()
            if repeat_len(bucket) &gt;= self.batch_size
        )</file><file path="helpers/models/flux/__init__.py">import torch
import random
import math
from helpers.models.flux.pipeline import FluxPipeline
from helpers.training import steps_remaining_in_epoch
from diffusers.pipelines.flux.pipeline_flux import (
    calculate_shift as calculate_shift_flux,
)
def apply_flow_schedule_shift(args, noise_scheduler, sigmas, noise):
    # Resolution-dependent shifting of timestep schedules as per section 5.3.2 of SD3 paper
    shift = None
    if args.flow_schedule_shift is not None and args.flow_schedule_shift &gt; 0:
        # Static shift value for every resolution
        shift = args.flow_schedule_shift
    elif args.flow_schedule_auto_shift:
        # Resolution-dependent shift value calculation used by official Flux inference implementation
        image_seq_len = (noise.shape[-1] * noise.shape[-2]) // 4
        mu = calculate_shift_flux(
            (noise.shape[-1] * noise.shape[-2]) // 4,
            noise_scheduler.config.base_image_seq_len,
            noise_scheduler.config.max_image_seq_len,
            noise_scheduler.config.base_shift,
            noise_scheduler.config.max_shift,
        )
        shift = math.exp(mu)
    if shift is not None:
        sigmas = (sigmas * shift) / (1 + (shift - 1) * sigmas)
    return sigmas
def get_mobius_guidance(args, global_step, steps_per_epoch, batch_size, device):
    &quot;&quot;&quot;
    state of the art
    &quot;&quot;&quot;
    steps_remaining = steps_remaining_in_epoch(global_step, steps_per_epoch)
    # Start with a linear mapping from remaining steps to a scale between 0 and 1
    scale_factor = steps_remaining / steps_per_epoch
    # we want the last 10% of the epoch to have a guidance of 1.0
    threshold_step_count = max(1, int(steps_per_epoch * 0.1))
    if (
        steps_remaining &lt;= threshold_step_count
    ):  # Last few steps in the epoch, set guidance to 1.0
        guidance_values = [1.0 for _ in range(batch_size)]
    else:
        # Sample between flux_guidance_min and flux_guidance_max with bias towards 1.0
        guidance_values = [
            random.uniform(args.flux_guidance_min, args.flux_guidance_max)
            * scale_factor
            + (1.0 - scale_factor)
            for _ in range(batch_size)
        ]
    return guidance_values
def update_flux_schedule_to_fast(args, noise_scheduler_to_copy):
    if args.flux_fast_schedule and args.model_family.lower() == &quot;flux&quot;:
        # 4-step noise schedule [0.7, 0.1, 0.1, 0.1] from SD3-Turbo paper
        for i in range(0, 250):
            noise_scheduler_to_copy.sigmas[i] = 1.0
        for i in range(250, 500):
            noise_scheduler_to_copy.sigmas[i] = 0.3
        for i in range(500, 750):
            noise_scheduler_to_copy.sigmas[i] = 0.2
        for i in range(750, 1000):
            noise_scheduler_to_copy.sigmas[i] = 0.1
    return noise_scheduler_to_copy
def pack_latents(latents, batch_size, num_channels_latents, height, width):
    latents = latents.view(
        batch_size, num_channels_latents, height // 2, 2, width // 2, 2
    )
    latents = latents.permute(0, 2, 4, 1, 3, 5)
    latents = latents.reshape(
        batch_size, (height // 2) * (width // 2), num_channels_latents * 4
    )
    return latents
def unpack_latents(latents, height, width, vae_scale_factor):
    batch_size, num_patches, channels = latents.shape
    height = height // vae_scale_factor
    width = width // vae_scale_factor
    latents = latents.view(batch_size, height, width, channels // 4, 2, 2)
    latents = latents.permute(0, 3, 1, 4, 2, 5)
    latents = latents.reshape(batch_size, channels // (2 * 2), height * 2, width * 2)
    return latents
def prepare_latent_image_ids(batch_size, height, width, device, dtype):
    latent_image_ids = torch.zeros(height // 2, width // 2, 3)
    latent_image_ids[..., 1] = (
        latent_image_ids[..., 1] + torch.arange(height // 2)[:, None]
    )
    latent_image_ids[..., 2] = (
        latent_image_ids[..., 2] + torch.arange(width // 2)[None, :]
    )
    latent_image_id_height, latent_image_id_width, latent_image_id_channels = (
        latent_image_ids.shape
    )
    latent_image_ids = latent_image_ids[None, :].repeat(batch_size, 1, 1, 1)
    latent_image_ids = latent_image_ids.reshape(
        batch_size,
        latent_image_id_height * latent_image_id_width,
        latent_image_id_channels,
    )
    return latent_image_ids.to(device=device, dtype=dtype)[0]</file><file path="helpers/models/flux/attention.py">from torch import Tensor, FloatTensor
from torch.nn import functional as F
from einops import rearrange
from diffusers.models.attention_processor import Attention
from diffusers.models.embeddings import apply_rotary_emb
try:
    from flash_attn_interface import flash_attn_func
except:
    pass
def fa3_sdpa(
    q,
    k,
    v,
):
    # flash attention 3 sdpa drop-in replacement
    q, k, v = [x.permute(0, 2, 1, 3) for x in [q, k, v]]
    out = flash_attn_func(q, k, v)[0]
    return out.permute(0, 2, 1, 3)
class FluxSingleAttnProcessor3_0:
    r&quot;&quot;&quot;
    Processor for implementing scaled dot-product attention (enabled by default if you&apos;re using PyTorch 2.0).
    &quot;&quot;&quot;
    def __init__(self):
        if not hasattr(F, &quot;scaled_dot_product_attention&quot;):
            raise ImportError(
                &quot;AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.&quot;
            )
    def __call__(
        self,
        attn,
        hidden_states: Tensor,
        encoder_hidden_states: Tensor = None,
        attention_mask: FloatTensor = None,
        image_rotary_emb: Tensor = None,
    ) -&gt; Tensor:
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)
        batch_size, _, _ = (
            hidden_states.shape
            if encoder_hidden_states is None
            else encoder_hidden_states.shape
        )
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        # Apply RoPE if needed
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            key = apply_rotary_emb(key, image_rotary_emb)
        # the output of sdp = (batch, num_heads, seq_len, head_dim)
        # TODO: add support for attn.scale when we move to Torch 2.1
        # hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = fa3_sdpa(query, key, value)
        hidden_states = rearrange(hidden_states, &quot;B H L D -&gt; B L (H D)&quot;)
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, attn.heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        return hidden_states
class FluxAttnProcessor3_0:
    &quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;
    def __init__(self):
        if not hasattr(F, &quot;scaled_dot_product_attention&quot;):
            raise ImportError(
                &quot;FluxAttnProcessor3_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.&quot;
            )
    def __call__(
        self,
        attn,
        hidden_states: FloatTensor,
        encoder_hidden_states: FloatTensor = None,
        attention_mask: FloatTensor = None,
        image_rotary_emb: Tensor = None,
    ) -&gt; FloatTensor:
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)
        context_input_ndim = encoder_hidden_states.ndim
        if context_input_ndim == 4:
            batch_size, channel, height, width = encoder_hidden_states.shape
            encoder_hidden_states = encoder_hidden_states.view(
                batch_size, channel, height * width
            ).transpose(1, 2)
        batch_size = encoder_hidden_states.shape[0]
        # `sample` projections.
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        # `context` projections.
        encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(
            batch_size, -1, attn.heads, head_dim
        ).transpose(1, 2)
        encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(
            batch_size, -1, attn.heads, head_dim
        ).transpose(1, 2)
        encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(
            batch_size, -1, attn.heads, head_dim
        ).transpose(1, 2)
        if attn.norm_added_q is not None:
            encoder_hidden_states_query_proj = attn.norm_added_q(
                encoder_hidden_states_query_proj
            )
        if attn.norm_added_k is not None:
            encoder_hidden_states_key_proj = attn.norm_added_k(
                encoder_hidden_states_key_proj
            )
        # attention
        query = torch.cat([encoder_hidden_states_query_proj, query], dim=2)
        key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
        value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            key = apply_rotary_emb(key, image_rotary_emb)
        # hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = fa3_sdpa(query, key, value)
        hidden_states = rearrange(hidden_states, &quot;B H L D -&gt; B L (H D)&quot;)
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, attn.heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        encoder_hidden_states, hidden_states = (
            hidden_states[:, : encoder_hidden_states.shape[1]],
            hidden_states[:, encoder_hidden_states.shape[1] :],
        )
        # linear proj
        hidden_states = attn.to_out[0](hidden_states)
        # dropout
        hidden_states = attn.to_out[1](hidden_states)
        encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        if context_input_ndim == 4:
            encoder_hidden_states = encoder_hidden_states.transpose(-1, -2).reshape(
                batch_size, channel, height, width
            )
        return hidden_states, encoder_hidden_states</file><file path="helpers/models/flux/pipeline.py"># Copyright 2024 Black Forest Labs and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from typing import Any, Callable, Dict, List, Optional, Union
import numpy as np
import torch
from transformers import (
    CLIPTextModel,
    CLIPTokenizer,
    T5EncoderModel,
    T5TokenizerFast,
)
from diffusers.image_processor import VaeImageProcessor
from diffusers.loaders import FluxLoraLoaderMixin
from diffusers.models.autoencoders import AutoencoderKL
from diffusers.models.transformers import FluxTransformer2DModel
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_xla_available,
    logging,
    replace_example_docstring,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm
    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import FluxPipeline
        &gt;&gt;&gt; pipe = FluxPipeline.from_pretrained(&quot;black-forest-labs/FLUX.1-schnell&quot;, torch_dtype=torch.bfloat16)
        &gt;&gt;&gt; pipe.to(&quot;cuda&quot;)
        &gt;&gt;&gt; prompt = &quot;A cat holding a sign that says hello world&quot;
        &gt;&gt;&gt; # Depending on the variant being used, the pipeline call will slightly vary.
        &gt;&gt;&gt; # Refer to the pipeline documentation for more details.
        &gt;&gt;&gt; image = pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]
        &gt;&gt;&gt; image.save(&quot;flux.png&quot;)
        ```
&quot;&quot;&quot;
def calculate_shift(
    image_seq_len,
    base_seq_len: int = 256,
    max_seq_len: int = 4096,
    base_shift: float = 0.5,
    max_shift: float = 1.16,
):
    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)
    b = base_shift - m * base_seq_len
    mu = image_seq_len * m + b
    return mu
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class FluxPipeline(DiffusionPipeline, FluxLoraLoaderMixin):
    r&quot;&quot;&quot;
    The Flux pipeline for text-to-image generation.
    Reference: https://blackforestlabs.ai/announcing-black-forest-labs/
    Args:
        transformer ([`FluxTransformer2DModel`]):
            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.
        scheduler ([`FlowMatchEulerDiscreteScheduler`]):
            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,
            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`
            as its dimension.
        text_encoder_2 ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the
            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
            variant.
        tokenizer (`CLIPTokenizer`):
            Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_2 (`CLIPTokenizer`):
            Second Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;text_encoder_2-&gt;transformer-&gt;vae&quot;
    _optional_components = []
    _callback_tensor_inputs = [&quot;latents&quot;, &quot;prompt_embeds&quot;]
    def __init__(
        self,
        scheduler: FlowMatchEulerDiscreteScheduler,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModel,
        tokenizer: CLIPTokenizer,
        text_encoder_2: T5EncoderModel,
        tokenizer_2: T5TokenizerFast,
        transformer: FluxTransformer2DModel,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            text_encoder_2=text_encoder_2,
            tokenizer=tokenizer,
            tokenizer_2=tokenizer_2,
            transformer=transformer,
            scheduler=scheduler,
        )
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels))
            if hasattr(self, &quot;vae&quot;) and self.vae is not None
            else 16
        )
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.tokenizer_max_length = (
            self.tokenizer.model_max_length
            if hasattr(self, &quot;tokenizer&quot;) and self.tokenizer is not None
            else 77
        )
        self.default_sample_size = 64
    def _get_t5_prompt_embeds(
        self,
        prompt: Union[str, List[str]] = None,
        num_images_per_prompt: int = 1,
        max_sequence_length: int = 512,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        device = device or self._execution_device
        dtype = dtype or self.text_encoder.dtype
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        text_inputs = self.tokenizer_2(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=max_sequence_length,
            truncation=True,
            return_length=False,
            return_overflowing_tokens=False,
            return_tensors=&quot;pt&quot;,
        )
        prompt_attention_mask = text_inputs.attention_mask
        text_input_ids = text_inputs.input_ids
        untruncated_ids = self.tokenizer_2(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = self.tokenizer_2.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            # logger.warning(
            #     &quot;The following part of your input was truncated because `max_sequence_length` is set to &quot;
            #     f&quot; {max_sequence_length} tokens: {removed_text}&quot;
            # )
        prompt_embeds = self.text_encoder_2(
            text_input_ids.to(device), output_hidden_states=False
        )[0]
        dtype = self.text_encoder_2.dtype
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
        _, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )
        return prompt_embeds, prompt_attention_mask
    def _get_clip_prompt_embeds(
        self,
        prompt: Union[str, List[str]],
        num_images_per_prompt: int = 1,
        device: Optional[torch.device] = None,
    ):
        device = device or self._execution_device
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        text_inputs = self.tokenizer(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=self.tokenizer_max_length,
            truncation=True,
            return_overflowing_tokens=False,
            return_length=False,
            return_tensors=&quot;pt&quot;,
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = self.tokenizer(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = self.tokenizer.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            # logger.warning(
            #     &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
            #     f&quot; {self.tokenizer_max_length} tokens: {removed_text}&quot;
            # )
        prompt_embeds = self.text_encoder(
            text_input_ids.to(device), output_hidden_states=False
        )
        # Use pooled output of CLIPTextModel
        prompt_embeds = prompt_embeds.pooler_output
        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, -1)
        return prompt_embeds
    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        prompt_2: Union[str, List[str]],
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        max_sequence_length: int = 512,
        lora_scale: Optional[float] = None,
    ):
        r&quot;&quot;&quot;
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in all text-encoders
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            lora_scale (`float`, *optional*):
                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
        &quot;&quot;&quot;
        device = device or self._execution_device
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(self, FluxLoraLoaderMixin):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if self.text_encoder is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder, lora_scale)
            if self.text_encoder_2 is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder_2, lora_scale)
        prompt = [prompt] if isinstance(prompt, str) else prompt
        if prompt is not None:
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        prompt_attention_mask = None
        if prompt_embeds is None:
            prompt_2 = prompt_2 or prompt
            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2
            # We only use the pooled prompt output from the CLIPTextModel
            pooled_prompt_embeds = self._get_clip_prompt_embeds(
                prompt=prompt,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
            )
            prompt_embeds, prompt_attention_mask = self._get_t5_prompt_embeds(
                prompt=prompt_2,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                device=device,
            )
        if self.text_encoder is not None:
            if isinstance(self, FluxLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder, lora_scale)
        if self.text_encoder_2 is not None:
            if isinstance(self, FluxLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder_2, lora_scale)
        text_ids = torch.zeros(batch_size, prompt_embeds.shape[1], 3).to(
            device=device, dtype=prompt_embeds.dtype
        )
        return prompt_embeds, pooled_prompt_embeds, text_ids, prompt_attention_mask
    def check_inputs(
        self,
        prompt,
        prompt_2,
        height,
        width,
        prompt_embeds=None,
        pooled_prompt_embeds=None,
        callback_on_step_end_tensor_inputs=None,
        max_sequence_length=None,
    ):
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_2 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        elif prompt_2 is not None and (
            not isinstance(prompt_2, str) and not isinstance(prompt_2, list)
        ):
            raise ValueError(
                f&quot;`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}&quot;
            )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if max_sequence_length is not None and max_sequence_length &gt; 512:
            raise ValueError(
                f&quot;`max_sequence_length` cannot be greater than 512 but is {max_sequence_length}&quot;
            )
    @staticmethod
    def _prepare_latent_image_ids(batch_size, height, width, device, dtype):
        latent_image_ids = torch.zeros(height // 2, width // 2, 3)
        latent_image_ids[..., 1] = (
            latent_image_ids[..., 1] + torch.arange(height // 2)[:, None]
        )
        latent_image_ids[..., 2] = (
            latent_image_ids[..., 2] + torch.arange(width // 2)[None, :]
        )
        latent_image_id_height, latent_image_id_width, latent_image_id_channels = (
            latent_image_ids.shape
        )
        latent_image_ids = latent_image_ids[None, :].repeat(batch_size, 1, 1, 1)
        latent_image_ids = latent_image_ids.reshape(
            batch_size,
            latent_image_id_height * latent_image_id_width,
            latent_image_id_channels,
        )
        return latent_image_ids.to(device=device, dtype=dtype)
    @staticmethod
    def _pack_latents(latents, batch_size, num_channels_latents, height, width):
        latents = latents.view(
            batch_size, num_channels_latents, height // 2, 2, width // 2, 2
        )
        latents = latents.permute(0, 2, 4, 1, 3, 5)
        latents = latents.reshape(
            batch_size, (height // 2) * (width // 2), num_channels_latents * 4
        )
        return latents
    @staticmethod
    def _unpack_latents(latents, height, width, vae_scale_factor):
        batch_size, num_patches, channels = latents.shape
        height = height // vae_scale_factor
        width = width // vae_scale_factor
        latents = latents.view(batch_size, height, width, channels // 4, 2, 2)
        latents = latents.permute(0, 3, 1, 4, 2, 5)
        latents = latents.reshape(
            batch_size, channels // (2 * 2), height * 2, width * 2
        )
        return latents
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        height = 2 * (int(height) // self.vae_scale_factor)
        width = 2 * (int(width) // self.vae_scale_factor)
        shape = (batch_size, num_channels_latents, height, width)
        if latents is not None:
            latent_image_ids = self._prepare_latent_image_ids(
                batch_size, height, width, device, dtype
            )
            return latents.to(device=device, dtype=dtype), latent_image_ids
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        latents = self._pack_latents(
            latents, batch_size, num_channels_latents, height, width
        )
        latent_image_ids = self._prepare_latent_image_ids(
            batch_size, height, width, device, dtype
        )
        return latents, latent_image_ids
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def joint_attention_kwargs(self):
        return self._joint_attention_kwargs
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        prompt_mask: Optional[Union[torch.FloatTensor, List[torch.FloatTensor]]] = None,
        negative_mask: Optional[
            Union[torch.FloatTensor, List[torch.FloatTensor]]
        ] = None,
        prompt_2: Optional[Union[str, List[str]]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 28,
        timesteps: List[int] = None,
        guidance_scale: float = 3.5,
        num_images_per_prompt: Optional[int] = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        max_sequence_length: int = 512,
        guidance_scale_real: float = 1.0,
        negative_prompt: Union[str, List[str]] = &quot;&quot;,
        negative_prompt_2: Union[str, List[str]] = &quot;&quot;,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        no_cfg_until_timestep: int = 2,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            prompt_mask (`str` or `List[str]`, *optional*):
                The prompt or prompts to be used as a mask for the image generation. If not defined, `prompt` is used
                instead.
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                will be used instead
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            guidance_scale (`float`, *optional*, defaults to 7.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.flux.FluxPipelineOutput`] instead of a plain tuple.
            joint_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.
        Examples:
        Returns:
            [`~pipelines.flux.FluxPipelineOutput`] or `tuple`: [`~pipelines.flux.FluxPipelineOutput`] if `return_dict`
            is True, otherwise a `tuple`. When returning a tuple, the first element is a list with the generated
            images.
        &quot;&quot;&quot;
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            prompt_2,
            height,
            width,
            prompt_embeds=prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,
            max_sequence_length=max_sequence_length,
        )
        self._guidance_scale = guidance_scale
        self._guidance_scale_real = guidance_scale_real
        self._joint_attention_kwargs = joint_attention_kwargs
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        lora_scale = (
            self.joint_attention_kwargs.get(&quot;scale&quot;, None)
            if self.joint_attention_kwargs is not None
            else None
        )
        (
            prompt_embeds,
            pooled_prompt_embeds,
            text_ids,
            _,
        ) = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt_2,
            prompt_embeds=prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            max_sequence_length=max_sequence_length,
            lora_scale=lora_scale,
        )
        if negative_prompt_2 == &quot;&quot; and negative_prompt != &quot;&quot;:
            negative_prompt_2 = negative_prompt
        negative_text_ids = text_ids
        if guidance_scale_real &gt; 1.0 and (
            negative_prompt_embeds is None or negative_pooled_prompt_embeds is None
        ):
            (
                negative_prompt_embeds,
                negative_pooled_prompt_embeds,
                negative_text_ids,
                _,
            ) = self.encode_prompt(
                prompt=negative_prompt,
                prompt_2=negative_prompt_2,
                prompt_embeds=None,
                pooled_prompt_embeds=None,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                lora_scale=lora_scale,
            )
        # 4. Prepare latent variables
        num_channels_latents = self.transformer.config.in_channels // 4
        latents, latent_image_ids = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 5. Prepare timesteps
        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)
        image_seq_len = latents.shape[1]
        mu = calculate_shift(
            image_seq_len,
            self.scheduler.config.base_image_seq_len,
            self.scheduler.config.max_image_seq_len,
            self.scheduler.config.base_shift,
            self.scheduler.config.max_shift,
        )
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler,
            num_inference_steps,
            device,
            timesteps,
            sigmas,
            mu=mu,
        )
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        self._num_timesteps = len(timesteps)
        latents = latents.to(self.transformer.device)
        latent_image_ids = latent_image_ids.to(self.transformer.device)[0]
        timesteps = timesteps.to(self.transformer.device)
        text_ids = text_ids.to(self.transformer.device)[0]
        # 6. Denoising loop
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # broadcast to batch dimension in a way that&apos;s compatible with ONNX/Core ML
                timestep = t.expand(latents.shape[0]).to(latents.dtype)
                # handle guidance
                if self.transformer.config.guidance_embeds:
                    guidance = torch.tensor(
                        [guidance_scale], device=self.transformer.device
                    )
                    guidance = guidance.expand(latents.shape[0])
                else:
                    guidance = None
                extra_transformer_args = {}
                if prompt_mask is not None:
                    extra_transformer_args[&quot;attention_mask&quot;] = prompt_mask.to(
                        device=self.transformer.device
                    )
                noise_pred = self.transformer(
                    hidden_states=latents.to(
                        device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                    ),
                    # YiYi notes: divide it by 1000 for now because we scale it by 1000 in the transforme rmodel (we should not keep it but I want to keep the inputs same for the model for testing)
                    timestep=timestep / 1000,
                    guidance=guidance,
                    pooled_projections=pooled_prompt_embeds.to(
                        device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                    ),
                    encoder_hidden_states=prompt_embeds.to(
                        device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                    ),
                    txt_ids=text_ids,
                    img_ids=latent_image_ids,
                    joint_attention_kwargs=self.joint_attention_kwargs,
                    return_dict=False,
                    **extra_transformer_args,
                )[0]
                # TODO optionally use batch prediction to speed this up.
                if guidance_scale_real &gt; 1.0 and i &gt;= no_cfg_until_timestep:
                    noise_pred_uncond = self.transformer(
                        hidden_states=latents.to(
                            device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                        ),
                        # YiYi notes: divide it by 1000 for now because we scale it by 1000 in the transforme rmodel (we should not keep it but I want to keep the inputs same for the model for testing)
                        timestep=timestep / 1000,
                        guidance=guidance,
                        pooled_projections=negative_pooled_prompt_embeds.to(
                            device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                        ),
                        encoder_hidden_states=negative_prompt_embeds.to(
                            device=self.transformer.device  # , dtype=self.transformer.dtype     # can&apos;t cast dtype like this because of NF4
                        ),
                        txt_ids=negative_text_ids.to(device=self.transformer.device),
                        img_ids=latent_image_ids.to(device=self.transformer.device),
                        joint_attention_kwargs=self.joint_attention_kwargs,
                        return_dict=False,
                    )[0]
                    noise_pred = noise_pred_uncond + guidance_scale_real * (
                        noise_pred - noise_pred_uncond
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                if XLA_AVAILABLE:
                    xm.mark_step()
        if output_type == &quot;latent&quot;:
            image = latents
        else:
            latents = self._unpack_latents(
                latents, height, width, self.vae_scale_factor
            )
            latents = (
                latents / self.vae.config.scaling_factor
            ) + self.vae.config.shift_factor
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype), return_dict=False
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return FluxPipelineOutput(images=image)
from dataclasses import dataclass
from typing import List, Union
import PIL.Image
from diffusers.utils import BaseOutput
@dataclass
class FluxPipelineOutput(BaseOutput):
    &quot;&quot;&quot;
    Output class for Stable Diffusion pipelines.
    Args:
        images (`List[PIL.Image.Image]` or `np.ndarray`)
            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,
            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.
    &quot;&quot;&quot;
    images: Union[List[PIL.Image.Image], np.ndarray]</file><file path="helpers/models/flux/transformer.py"># Copyright 2024 Stability AI, The HuggingFace Team, The InstantX Team, and Terminus Research Group. All rights reserved.
#
# Originally licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# Updated to &quot;Affero GENERAL PUBLIC LICENSE Version 3, 19 November 2007&quot; via extensive updates to attn_mask usage.
from typing import Any, Dict, List, Optional, Tuple, Union
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.loaders import FromOriginalModelMixin, PeftAdapterMixin
from diffusers.models.attention import FeedForward
from diffusers.models.attention_processor import (
    Attention,
)
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import (
    AdaLayerNormContinuous,
    AdaLayerNormZero,
    AdaLayerNormZeroSingle,
)
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_version,
    logging,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import maybe_allow_in_graph
from diffusers.models.embeddings import (
    CombinedTimestepGuidanceTextProjEmbeddings,
    CombinedTimestepTextProjEmbeddings,
    FluxPosEmbed,
)
from diffusers.models.modeling_outputs import Transformer2DModelOutput
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
is_flash_attn_available = False
try:
    from flash_attn_interface import flash_attn_func
    is_flash_attn_available = True
except:
    pass
from helpers.models.flux.attention import (
    FluxSingleAttnProcessor3_0,
    FluxAttnProcessor3_0,
)
class FluxAttnProcessor2_0:
    &quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;
    def __init__(self):
        if not hasattr(F, &quot;scaled_dot_product_attention&quot;):
            raise ImportError(
                &quot;FluxAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.&quot;
            )
    def __call__(
        self,
        attn: Attention,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        image_rotary_emb: Optional[torch.Tensor] = None,
    ) -&gt; torch.FloatTensor:
        batch_size, _, _ = (
            hidden_states.shape
            if encoder_hidden_states is None
            else encoder_hidden_states.shape
        )
        # `sample` projections.
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        # the attention in FluxSingleTransformerBlock does not use `encoder_hidden_states`
        if encoder_hidden_states is not None:
            # `context` projections.
            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(
                batch_size, -1, attn.heads, head_dim
            ).transpose(1, 2)
            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(
                batch_size, -1, attn.heads, head_dim
            ).transpose(1, 2)
            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(
                batch_size, -1, attn.heads, head_dim
            ).transpose(1, 2)
            if attn.norm_added_q is not None:
                encoder_hidden_states_query_proj = attn.norm_added_q(
                    encoder_hidden_states_query_proj
                )
            if attn.norm_added_k is not None:
                encoder_hidden_states_key_proj = attn.norm_added_k(
                    encoder_hidden_states_key_proj
                )
            # attention
            query = torch.cat([encoder_hidden_states_query_proj, query], dim=2)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        if image_rotary_emb is not None:
            from diffusers.models.embeddings import apply_rotary_emb
            query = apply_rotary_emb(query, image_rotary_emb)
            key = apply_rotary_emb(key, image_rotary_emb)
        if attention_mask is not None:
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            attention_mask = (attention_mask &gt; 0).bool()
            attention_mask = attention_mask.to(
                device=hidden_states.device, dtype=hidden_states.dtype
            )
        hidden_states = F.scaled_dot_product_attention(
            query,
            key,
            value,
            dropout_p=0.0,
            is_causal=False,
            attn_mask=attention_mask,
        )
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, attn.heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        if encoder_hidden_states is not None:
            encoder_hidden_states, hidden_states = (
                hidden_states[:, : encoder_hidden_states.shape[1]],
                hidden_states[:, encoder_hidden_states.shape[1] :],
            )
            # linear proj
            hidden_states = attn.to_out[0](hidden_states)
            # dropout
            hidden_states = attn.to_out[1](hidden_states)
            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
            return hidden_states, encoder_hidden_states
        return hidden_states
def expand_flux_attention_mask(
    hidden_states: torch.Tensor,
    attn_mask: torch.Tensor,
) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Expand a mask so that the image is included.
    &quot;&quot;&quot;
    bsz = attn_mask.shape[0]
    assert bsz == hidden_states.shape[0]
    residual_seq_len = hidden_states.shape[1]
    mask_seq_len = attn_mask.shape[1]
    expanded_mask = torch.ones(bsz, residual_seq_len)
    expanded_mask[:, :mask_seq_len] = attn_mask
    return expanded_mask
@maybe_allow_in_graph
class FluxSingleTransformerBlock(nn.Module):
    r&quot;&quot;&quot;
    A Transformer block following the MMDiT architecture, introduced in Stable Diffusion 3.
    Reference: https://arxiv.org/abs/2403.03206
    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        context_pre_only (`bool`): Boolean to determine if we should add some blocks associated with the
            processing of `context` conditions.
    &quot;&quot;&quot;
    def __init__(self, dim, num_attention_heads, attention_head_dim, mlp_ratio=4.0):
        super().__init__()
        self.mlp_hidden_dim = int(dim * mlp_ratio)
        self.norm = AdaLayerNormZeroSingle(dim)
        self.proj_mlp = nn.Linear(dim, self.mlp_hidden_dim)
        self.act_mlp = nn.GELU(approximate=&quot;tanh&quot;)
        self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)
        processor = FluxAttnProcessor2_0()
        if torch.cuda.is_available():
            # let&apos;s assume that the box only ever has H100s.
            rank = 0
            primary_device = torch.cuda.get_device_properties(rank)
            if primary_device.major == 9 and primary_device.minor == 0:
                if is_flash_attn_available:
                    if rank == 0:
                        print(&quot;Using FlashAttention3_0 for H100 GPU (Single block)&quot;)
                    processor = FluxSingleAttnProcessor3_0()
                else:
                    if rank == 0:
                        print(
                            &quot;FlashAttention3_0 is not available, using FlashAttention2_0 for H100 GPU (Single block). Install flash_attn to make use of it.&quot;
                        )
        self.attn = Attention(
            query_dim=dim,
            cross_attention_dim=None,
            dim_head=attention_head_dim,
            heads=num_attention_heads,
            out_dim=dim,
            bias=True,
            processor=processor,
            qk_norm=&quot;rms_norm&quot;,
            eps=1e-6,
            pre_only=True,
        )
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        temb: torch.FloatTensor,
        image_rotary_emb=None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        residual = hidden_states
        norm_hidden_states, gate = self.norm(hidden_states, emb=temb)
        mlp_hidden_states = self.act_mlp(self.proj_mlp(norm_hidden_states))
        if attention_mask is not None:
            attention_mask = expand_flux_attention_mask(
                hidden_states,
                attention_mask,
            )
        attn_output = self.attn(
            hidden_states=norm_hidden_states,
            image_rotary_emb=image_rotary_emb,
            attention_mask=attention_mask,
        )
        hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=2)
        gate = gate.unsqueeze(1)
        hidden_states = gate * self.proj_out(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states
@maybe_allow_in_graph
class FluxTransformerBlock(nn.Module):
    r&quot;&quot;&quot;
    A Transformer block following the MMDiT architecture, introduced in Stable Diffusion 3.
    Reference: https://arxiv.org/abs/2403.03206
    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        context_pre_only (`bool`): Boolean to determine if we should add some blocks associated with the
            processing of `context` conditions.
    &quot;&quot;&quot;
    def __init__(
        self, dim, num_attention_heads, attention_head_dim, qk_norm=&quot;rms_norm&quot;, eps=1e-6
    ):
        super().__init__()
        self.norm1 = AdaLayerNormZero(dim)
        self.norm1_context = AdaLayerNormZero(dim)
        if hasattr(F, &quot;scaled_dot_product_attention&quot;):
            processor = FluxAttnProcessor2_0()
            if torch.cuda.is_available():
                rank = (
                    torch.distributed.get_rank()
                    if torch.distributed.is_initialized()
                    else 0
                )
                primary_device = torch.cuda.get_device_properties(rank)
                if primary_device.major == 9 and primary_device.minor == 0:
                    if is_flash_attn_available:
                        if rank == 0:
                            print(&quot;Using FlashAttention3_0 for H100 GPU (Double block)&quot;)
                        processor = FluxAttnProcessor3_0()
        else:
            raise ValueError(
                &quot;The current PyTorch version does not support the `scaled_dot_product_attention` function.&quot;
            )
        self.attn = Attention(
            query_dim=dim,
            cross_attention_dim=None,
            added_kv_proj_dim=dim,
            dim_head=attention_head_dim,
            heads=num_attention_heads,
            out_dim=dim,
            context_pre_only=False,
            bias=True,
            processor=processor,
            qk_norm=qk_norm,
            eps=eps,
        )
        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)
        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=&quot;gelu-approximate&quot;)
        self.norm2_context = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)
        self.ff_context = FeedForward(
            dim=dim, dim_out=dim, activation_fn=&quot;gelu-approximate&quot;
        )
        # let chunk size default to None
        self._chunk_size = None
        self._chunk_dim = 0
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor,
        temb: torch.FloatTensor,
        image_rotary_emb=None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(
            hidden_states, emb=temb
        )
        norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = (
            self.norm1_context(encoder_hidden_states, emb=temb)
        )
        if attention_mask is not None:
            attention_mask = expand_flux_attention_mask(
                torch.cat([encoder_hidden_states, hidden_states], dim=1),
                attention_mask,
            )
        # Attention.
        attn_output, context_attn_output = self.attn(
            hidden_states=norm_hidden_states,
            encoder_hidden_states=norm_encoder_hidden_states,
            image_rotary_emb=image_rotary_emb,
            attention_mask=attention_mask,
        )
        # Process attention outputs for the `hidden_states`.
        attn_output = gate_msa.unsqueeze(1) * attn_output
        hidden_states = hidden_states + attn_output
        norm_hidden_states = self.norm2(hidden_states)
        norm_hidden_states = (
            norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]
        )
        ff_output = self.ff(norm_hidden_states)
        ff_output = gate_mlp.unsqueeze(1) * ff_output
        hidden_states = hidden_states + ff_output
        # Process attention outputs for the `encoder_hidden_states`.
        context_attn_output = c_gate_msa.unsqueeze(1) * context_attn_output
        encoder_hidden_states = encoder_hidden_states + context_attn_output
        norm_encoder_hidden_states = self.norm2_context(encoder_hidden_states)
        norm_encoder_hidden_states = (
            norm_encoder_hidden_states * (1 + c_scale_mlp[:, None])
            + c_shift_mlp[:, None]
        )
        context_ff_output = self.ff_context(norm_encoder_hidden_states)
        encoder_hidden_states = (
            encoder_hidden_states + c_gate_mlp.unsqueeze(1) * context_ff_output
        )
        return encoder_hidden_states, hidden_states
class FluxTransformer2DModelWithMasking(
    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin
):
    &quot;&quot;&quot;
    The Transformer model introduced in Flux.
    Reference: https://blackforestlabs.ai/announcing-black-forest-labs/
    Parameters:
        patch_size (`int`): Patch size to turn the input data into small patches.
        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.
        num_layers (`int`, *optional*, defaults to 18): The number of layers of MMDiT blocks to use.
        num_single_layers (`int`, *optional*, defaults to 18): The number of layers of single DiT blocks to use.
        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.
        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.
        joint_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.
        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.
        guidance_embeds (`bool`, defaults to False): Whether to use guidance embeddings.
    &quot;&quot;&quot;
    _supports_gradient_checkpointing = True
    @register_to_config
    def __init__(
        self,
        patch_size: int = 1,
        in_channels: int = 64,
        num_layers: int = 19,
        num_single_layers: int = 38,
        attention_head_dim: int = 128,
        num_attention_heads: int = 24,
        joint_attention_dim: int = 4096,
        pooled_projection_dim: int = 768,
        guidance_embeds: bool = False,
        axes_dims_rope: Tuple[int] = (16, 56, 56),
    ):
        super().__init__()
        self.out_channels = in_channels
        self.inner_dim = (
            self.config.num_attention_heads * self.config.attention_head_dim
        )
        self.pos_embed = FluxPosEmbed(theta=10000, axes_dim=axes_dims_rope)
        text_time_guidance_cls = (
            CombinedTimestepGuidanceTextProjEmbeddings
            if guidance_embeds
            else CombinedTimestepTextProjEmbeddings
        )
        self.time_text_embed = text_time_guidance_cls(
            embedding_dim=self.inner_dim,
            pooled_projection_dim=self.config.pooled_projection_dim,
        )
        self.context_embedder = nn.Linear(
            self.config.joint_attention_dim, self.inner_dim
        )
        self.x_embedder = torch.nn.Linear(self.config.in_channels, self.inner_dim)
        self.transformer_blocks = nn.ModuleList(
            [
                FluxTransformerBlock(
                    dim=self.inner_dim,
                    num_attention_heads=self.config.num_attention_heads,
                    attention_head_dim=self.config.attention_head_dim,
                )
                for i in range(self.config.num_layers)
            ]
        )
        self.single_transformer_blocks = nn.ModuleList(
            [
                FluxSingleTransformerBlock(
                    dim=self.inner_dim,
                    num_attention_heads=self.config.num_attention_heads,
                    attention_head_dim=self.config.attention_head_dim,
                )
                for i in range(self.config.num_single_layers)
            ]
        )
        self.norm_out = AdaLayerNormContinuous(
            self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6
        )
        self.proj_out = nn.Linear(
            self.inner_dim, patch_size * patch_size * self.out_channels, bias=True
        )
        self.gradient_checkpointing = False
        # added for users to disable checkpointing every nth step
        self.gradient_checkpointing_interval = None
    def _set_gradient_checkpointing(self, module, value=False):
        if hasattr(module, &quot;gradient_checkpointing&quot;):
            module.gradient_checkpointing = value
    def set_gradient_checkpointing_interval(self, value: int):
        self.gradient_checkpointing_interval = value
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor = None,
        pooled_projections: torch.Tensor = None,
        timestep: torch.LongTensor = None,
        img_ids: torch.Tensor = None,
        txt_ids: torch.Tensor = None,
        guidance: torch.Tensor = None,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        return_dict: bool = True,
        attention_mask: Optional[torch.Tensor] = None,
    ) -&gt; Union[torch.FloatTensor, Transformer2DModelOutput]:
        &quot;&quot;&quot;
        The [`FluxTransformer2DModelWithMasking`] forward method.
        Args:
            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):
                Input `hidden_states`.
            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):
                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.
            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected
                from the embeddings of input conditions.
            timestep ( `torch.LongTensor`):
                Used to indicate denoising step.
            block_controlnet_hidden_states: (`list` of `torch.Tensor`):
                A list of tensors that if specified are added to the residuals of transformer blocks.
            joint_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain
                tuple.
        Returns:
            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a
            `tuple` where the first element is the sample tensor.
        &quot;&quot;&quot;
        if joint_attention_kwargs is not None:
            joint_attention_kwargs = joint_attention_kwargs.copy()
            lora_scale = joint_attention_kwargs.pop(&quot;scale&quot;, 1.0)
        else:
            lora_scale = 1.0
        if USE_PEFT_BACKEND:
            # weight the lora layers by setting `lora_scale` for each PEFT layer
            scale_lora_layers(self, lora_scale)
        else:
            if (
                joint_attention_kwargs is not None
                and joint_attention_kwargs.get(&quot;scale&quot;, None) is not None
            ):
                logger.warning(
                    &quot;Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.&quot;
                )
        hidden_states = self.x_embedder(hidden_states)
        timestep = timestep.to(hidden_states.dtype) * 1000
        if guidance is not None:
            guidance = guidance.to(hidden_states.dtype) * 1000
        else:
            guidance = None
        temb = (
            self.time_text_embed(timestep, pooled_projections)
            if guidance is None
            else self.time_text_embed(timestep, guidance, pooled_projections)
        )
        encoder_hidden_states = self.context_embedder(encoder_hidden_states)
        if txt_ids.ndim == 3:
            txt_ids = txt_ids[0]
        if img_ids.ndim == 3:
            img_ids = img_ids[0]
        ids = torch.cat((txt_ids, img_ids), dim=0)
        image_rotary_emb = self.pos_embed(ids)
        for index_block, block in enumerate(self.transformer_blocks):
            if (
                self.training
                and self.gradient_checkpointing
                and (
                    self.gradient_checkpointing_interval is None
                    or index_block % self.gradient_checkpointing_interval == 0
                )
            ):
                def create_custom_forward(module, return_dict=None):
                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: Dict[str, Any] = (
                    {&quot;use_reentrant&quot;: False} if is_torch_version(&quot;&gt;=&quot;, &quot;1.11.0&quot;) else {}
                )
                encoder_hidden_states, hidden_states = (
                    torch.utils.checkpoint.checkpoint(
                        create_custom_forward(block),
                        hidden_states,
                        encoder_hidden_states,
                        temb,
                        image_rotary_emb,
                        attention_mask,
                        **ckpt_kwargs,
                    )
                )
            else:
                encoder_hidden_states, hidden_states = block(
                    hidden_states=hidden_states,
                    encoder_hidden_states=encoder_hidden_states,
                    temb=temb,
                    image_rotary_emb=image_rotary_emb,
                    attention_mask=attention_mask,
                )
        # Flux places the text tokens in front of the image tokens in the
        # sequence.
        hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)
        for index_block, block in enumerate(self.single_transformer_blocks):
            if (
                self.training
                and self.gradient_checkpointing
                or (
                    self.gradient_checkpointing_interval is not None
                    and index_block % self.gradient_checkpointing_interval == 0
                )
            ):
                def create_custom_forward(module, return_dict=None):
                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: Dict[str, Any] = (
                    {&quot;use_reentrant&quot;: False} if is_torch_version(&quot;&gt;=&quot;, &quot;1.11.0&quot;) else {}
                )
                hidden_states = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    temb,
                    image_rotary_emb,
                    attention_mask,
                    **ckpt_kwargs,
                )
            else:
                hidden_states = block(
                    hidden_states=hidden_states,
                    temb=temb,
                    image_rotary_emb=image_rotary_emb,
                    attention_mask=attention_mask,
                )
        hidden_states = hidden_states[:, encoder_hidden_states.shape[1] :, ...]
        hidden_states = self.norm_out(hidden_states, temb)
        output = self.proj_out(hidden_states)
        if USE_PEFT_BACKEND:
            # remove `lora_scale` from each PEFT layer
            unscale_lora_layers(self, lora_scale)
        if not return_dict:
            return (output,)
        return Transformer2DModelOutput(sample=output)
if __name__ == &quot;__main__&quot;:
    dtype = torch.bfloat16
    bsz = 2
    img = torch.rand((bsz, 16, 64, 64)).to(&quot;cuda&quot;, dtype=dtype)
    timestep = torch.tensor([0.5, 0.5]).to(&quot;cuda&quot;, dtype=torch.float32)
    pooled = torch.rand(bsz, 768).to(&quot;cuda&quot;, dtype=dtype)
    text = torch.rand((bsz, 512, 4096)).to(&quot;cuda&quot;, dtype=dtype)
    attn_mask = torch.tensor([[1.0] * 384 + [0.0] * 128] * bsz).to(
        &quot;cuda&quot;, dtype=dtype
    )  # Last 128 positions are masked
    def _pack_latents(latents, batch_size, num_channels_latents, height, width):
        latents = latents.view(
            batch_size, num_channels_latents, height // 2, 2, width // 2, 2
        )
        latents = latents.permute(0, 2, 4, 1, 3, 5)
        latents = latents.reshape(
            batch_size, (height // 2) * (width // 2), num_channels_latents * 4
        )
        return latents
    def _prepare_latent_image_ids(
        batch_size, height, width, device=&quot;cuda&quot;, dtype=dtype
    ):
        latent_image_ids = torch.zeros(height // 2, width // 2, 3)
        latent_image_ids[..., 1] = (
            latent_image_ids[..., 1] + torch.arange(height // 2)[:, None]
        )
        latent_image_ids[..., 2] = (
            latent_image_ids[..., 2] + torch.arange(width // 2)[None, :]
        )
        latent_image_id_height, latent_image_id_width, latent_image_id_channels = (
            latent_image_ids.shape
        )
        latent_image_ids = latent_image_ids[None, :].repeat(batch_size, 1, 1, 1)
        latent_image_ids = latent_image_ids.reshape(
            batch_size,
            latent_image_id_height * latent_image_id_width,
            latent_image_id_channels,
        )
        return latent_image_ids.to(device=device, dtype=dtype)
    txt_ids = torch.zeros(bsz, text.shape[1], 3).to(device=&quot;cuda&quot;, dtype=dtype)
    vae_scale_factor = 16
    height = 2 * (int(512) // vae_scale_factor)
    width = 2 * (int(512) // vae_scale_factor)
    img_ids = _prepare_latent_image_ids(bsz, height, width)
    img = _pack_latents(img, img.shape[0], 16, height, width)
    # Gotta go fast
    transformer = FluxTransformer2DModelWithMasking.from_config(
        {
            &quot;attention_head_dim&quot;: 128,
            &quot;guidance_embeds&quot;: True,
            &quot;in_channels&quot;: 64,
            &quot;joint_attention_dim&quot;: 4096,
            &quot;num_attention_heads&quot;: 24,
            &quot;num_layers&quot;: 4,
            &quot;num_single_layers&quot;: 8,
            &quot;patch_size&quot;: 1,
            &quot;pooled_projection_dim&quot;: 768,
        }
    ).to(&quot;cuda&quot;, dtype=dtype)
    guidance = torch.tensor([2.0], device=&quot;cuda&quot;)
    guidance = guidance.expand(bsz)
    with torch.no_grad():
        no_mask = transformer(
            img,
            encoder_hidden_states=text,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids,
            txt_ids=txt_ids,
            guidance=guidance,
        )
        mask = transformer(
            img,
            encoder_hidden_states=text,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids,
            txt_ids=txt_ids,
            guidance=guidance,
            attention_mask=attn_mask,
        )
    assert torch.allclose(no_mask.sample, mask.sample) is False
    print(&quot;Attention masking test ran OK. Differences in output were detected.&quot;)</file><file path="helpers/models/ltxvideo/__init__.py">import torch
import random
from torch import randn as randn_tensor
from typing import Tuple, Optional
def normalize_ltx_latents(
    latents: torch.Tensor,
    latents_mean: torch.Tensor,
    latents_std: torch.Tensor,
    scaling_factor: float = 1.0,
    reverse=False,
) -&gt; torch.Tensor:
    # Normalize latents across the channel dimension [B, C, F, H, W]
    latents_mean = latents_mean.view(1, -1, 1, 1, 1).to(latents.device, latents.dtype)
    latents_std = latents_std.view(1, -1, 1, 1, 1).to(latents.device, latents.dtype)
    if not reverse:
        latents = (latents - latents_mean) * scaling_factor / latents_std
    else:
        latents = latents * latents_std / scaling_factor + latents_mean
    return latents
def unpack_ltx_latents(
    latents: torch.Tensor,
    num_frames: int,
    height: int,
    width: int,
    patch_size: int = 1,
    patch_size_t: int = 1,
) -&gt; torch.Tensor:
    # Packed latents of shape [B, S, D] (S is the effective video sequence length, D is the effective feature dimensions)
    # are unpacked and reshaped into a video tensor of shape [B, C, F, H, W]. This is the inverse operation of
    # what happens in the `_pack_latents` method.
    batch_size = latents.size(0)
    latents = latents.reshape(
        batch_size, num_frames, height, width, -1, patch_size_t, patch_size, patch_size
    )
    latents = (
        latents.permute(0, 4, 1, 5, 2, 6, 3, 7)
        .flatten(6, 7)
        .flatten(4, 5)
        .flatten(2, 3)
    )
    return latents
def pack_ltx_latents(
    latents: torch.Tensor, patch_size: int = 1, patch_size_t: int = 1
) -&gt; torch.Tensor:
    # Unpacked latents of shape are [B, C, F, H, W] are patched into tokens of shape [B, C, F // p_t, p_t, H // p, p, W // p, p].
    # The patch dimensions are then permuted and collapsed into the channel dimension of shape:
    # [B, F // p_t * H // p * W // p, C * p_t * p * p] (an ndim=3 tensor).
    # dim=0 is the batch size, dim=1 is the effective video sequence length, dim=2 is the effective number of input features
    batch_size, num_channels, num_frames, height, width = latents.shape
    post_patch_num_frames = num_frames // patch_size_t
    post_patch_height = height // patch_size
    post_patch_width = width // patch_size
    latents = latents.reshape(
        batch_size,
        -1,
        post_patch_num_frames,
        patch_size_t,
        post_patch_height,
        patch_size,
        post_patch_width,
        patch_size,
    )
    latents = latents.permute(0, 2, 4, 6, 1, 3, 5, 7).flatten(4, 7).flatten(1, 3)
    return latents
def apply_first_frame_protection(
    latents: torch.Tensor,
    timesteps: torch.Tensor,
    noise: torch.Tensor,
    i2v_conditioning_mask: torch.Tensor,
    protect_first_frame: bool = False,
    first_frame_probability: float = 0.0,
    partial_noise_fraction: float = 0.05,
    return_sigmas: bool = True,
) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    &quot;&quot;&quot;
    Optionally protect the first frame in a video-like latent tensor, either completely
    or probabilistically.
    Args:
        latents (torch.Tensor):
            The clean latents of shape [B, C, T, H, W].
        timesteps (torch.Tensor):
            The “time” or step-level for each frame, typically derived from sigmas
            via: timesteps = (sigmas * 1000).long() or float.
            Expected shape could be [B], [B, T], or broadcasted [B, 1, T, H, W].
        noise (torch.Tensor):
            The random noise tensor, same shape as latents ([B, C, T, H, W]).
        i2v_conditioning_mask (torch.Tensor):
            A 5D mask [B, 1, T, H, W] that indicates which frames/pixels to protect (1.0)
            vs. not protect (0.0). Typically, i2v logic sets the first frame to 1.0 if
            you want to “preserve” it.
        protect_first_frame (bool):
            If True, always protect the first frame (set timesteps=0 there).
            If False, do not guarantee full protection (see probability).
        first_frame_probability (float):
            A probability in [0, 1] for applying partial protection.
            If random.random() &lt; first_frame_probability, we do a partial noise
            mix on the first frame.
        partial_noise_fraction (float):
            The maximum fraction of noise to introduce in the first frame when
            partial protection is applied. (e.g. 0.05 =&gt; up to 5% noise)
        return_sigmas (bool):
            If True, we also return the sigmas after converting timesteps / 1000.
            Otherwise, return None for that slot.
    Returns:
        updated_timesteps (torch.Tensor):
            Possibly masked or partially zeroed timesteps. Same shape as input `timesteps` (after broadcasting).
        updated_noise (torch.Tensor):
            If partial protection is triggered, the first frame in `noise` might get scaled down.
            (If complete protection, you could leave it as-is or zero it for the protected frame.)
        sigmas (Optional[torch.Tensor]):
            If `return_sigmas=True`, this is timesteps.float()/1000.0;
            otherwise, None. Shape matches updated_timesteps.
    Example:
        updated_t, updated_n, s = apply_first_frame_protection(
            latents=latents,
            timesteps=timesteps,
            noise=noise,
            i2v_conditioning_mask=i2v_mask,
            protect_first_frame=True,
            first_frame_probability=0.1,
            partial_noise_fraction=0.05,
        )
    &quot;&quot;&quot;
    # Make sure timesteps is at least 5D if we want to broadcast it with [B,1,T,H,W].
    # For example, if timesteps is [B], reshape to [B, 1, 1, 1, 1].
    # If timesteps is [B, T], reshape to [B, 1, T, 1, 1], etc.
    # Below is an example if we assume [B] =&gt; [B,1,1,1,1].
    if timesteps.ndim == 1:
        bsz = timesteps.shape[0]
        # shape: [B, 1, 1, 1, 1]
        timesteps = timesteps.view(bsz, 1, 1, 1, 1)
    # If you have [B, T], do a different reshape:
    # elif timesteps.ndim == 2:
    #     bsz, t = timesteps.shape
    #     timesteps = timesteps.view(bsz, 1, t, 1, 1)
    # etc.
    # We&apos;ll copy noise so we can modify the first frame if partial protection is triggered
    updated_noise = noise.clone()
    # 1. Decide if partial protection triggers
    do_partial = (not protect_first_frame) and (
        random.random() &lt; first_frame_probability
    )
    if protect_first_frame:
        # Completely zero out timesteps where i2v_conditioning_mask=1
        updated_timesteps = timesteps * (1 - i2v_conditioning_mask)
        # Optionally also zero out the noise in the protected frames:
        # updated_noise = updated_noise * (1 - i2v_conditioning_mask)
    elif do_partial:
        # PARTIAL PROTECTION =&gt; only add partial_noise_fraction * random() to the first frame
        # e.g., if i2v_mask for the first frame is 1.0, let&apos;s reduce timesteps or noise for that frame
        # Usually, partial approach might be done at the noise level rather than timesteps,
        # so that we don&apos;t disrupt the entire schedule.
        # We&apos;ll do: noise_first_frame = alpha * noise + (1-alpha) * latents
        # or timesteps_first_frame = timesteps * something.
        # One approach:
        rand_noise_ff = random.random() * partial_noise_fraction  # e.g. up to 5%
        alpha_mask = 1.0 - (i2v_conditioning_mask * rand_noise_ff)
        # Multiply timesteps by alpha_mask =&gt; reduces timesteps in the protected region
        updated_timesteps = timesteps * alpha_mask
        # Or scale the noise in that region
        updated_noise = updated_noise * alpha_mask
    else:
        # No protection at all =&gt; leave timesteps as is
        updated_timesteps = timesteps
    # Convert timesteps back to sigmas if requested
    # If timesteps was an integer approximation, you lose decimal precision
    sigmas = None
    if return_sigmas:
        sigmas = updated_timesteps.float() / 1000.0
    return updated_timesteps, updated_noise, sigmas
def make_i2v_conditioning_mask(
    latents: torch.Tensor, protect_frame_index: int = 0
) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Create a mask that is 1.0 at the given &apos;protect_frame_index&apos; frame (e.g., the first frame),
    and 0.0 elsewhere.
    Args:
        latents (torch.Tensor): The latents of shape [B, C, T, H, W].
        protect_frame_index (int): Which frame to protect (default=0 =&gt; the very first frame).
    Returns:
        torch.Tensor: An i2v conditioning mask of shape [B, 1, T, H, W].
                      The selected frame is set to 1.0, all others 0.0.
    &quot;&quot;&quot;
    bsz, _, num_frames, height, width = latents.shape
    mask = torch.zeros(
        (bsz, 1, num_frames, height, width), dtype=latents.dtype, device=latents.device
    )
    if protect_frame_index &lt; num_frames:
        mask[:, :, protect_frame_index, :, :] = 1.0
    return mask</file><file path="helpers/models/omnigen/pipeline.py">import os
import inspect
from typing import Any, Callable, Dict, List, Optional, Union
import gc
from PIL import Image
import numpy as np
import torch
from huggingface_hub import snapshot_download
from peft import LoraConfig, PeftModel
from diffusers.models import AutoencoderKL
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_xla_available,
    logging,
    replace_example_docstring,
    scale_lora_layers,
    unscale_lora_layers,
)
from safetensors.torch import load_file
from OmniGen import OmniGen, OmniGenProcessor, OmniGenScheduler
logger = logging.get_logger(__name__)
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; from OmniGen import OmniGenPipeline
        &gt;&gt;&gt; pipe = FluxControlNetPipeline.from_pretrained(
        ...     base_model
        ... )
        &gt;&gt;&gt; prompt = &quot;A woman holds a bouquet of flowers and faces the camera&quot;
        &gt;&gt;&gt; image = pipe(
        ...     prompt,
        ...     guidance_scale=2.5,
        ...     num_inference_steps=50,
        ... ).images[0]
        &gt;&gt;&gt; image.save(&quot;t2i.png&quot;)
        ```
&quot;&quot;&quot;
90
class OmniGenPipeline:
    def __init__(
        self,
        vae: AutoencoderKL,
        model: OmniGen,
        processor: OmniGenProcessor,
        device: Union[str, torch.device],
    ):
        self.vae = vae
        self.model = model
        self.processor = processor
        self.device = device
        self.model.to(torch.bfloat16)
        self.model.eval()
        self.vae.eval()
        self.model_cpu_offload = False
    @classmethod
    def from_pretrained(
        cls, pretrained_model_name_or_path, vae_path: str = None, **kwargs
    ):
        if not os.path.exists(pretrained_model_name_or_path) or (
            not os.path.exists(
                os.path.join(pretrained_model_name_or_path, &quot;model.safetensors&quot;)
            )
            and pretrained_model_name_or_path == &quot;Shitao/OmniGen-v1&quot;
        ):
            logger.info(&quot;Model not found, downloading...&quot;)
            cache_folder = os.getenv(&quot;HF_HUB_CACHE&quot;)
            pretrained_model_name_or_path = snapshot_download(
                repo_id=pretrained_model_name_or_path,
                cache_dir=cache_folder,
                ignore_patterns=[
                    &quot;flax_model.msgpack&quot;,
                    &quot;rust_model.ot&quot;,
                    &quot;tf_model.h5&quot;,
                    &quot;model.pt&quot;,
                ],
            )
            logger.info(f&quot;Downloaded model to {pretrained_model_name_or_path}&quot;)
        model = OmniGen.from_pretrained(pretrained_model_name_or_path)
        processor = OmniGenProcessor.from_pretrained(pretrained_model_name_or_path)
        if os.path.exists(os.path.join(pretrained_model_name_or_path, &quot;vae&quot;)):
            vae = AutoencoderKL.from_pretrained(
                os.path.join(pretrained_model_name_or_path, &quot;vae&quot;)
            )
        elif vae_path is not None:
            vae = AutoencoderKL.from_pretrained(vae_path).to(
                StateTracker.get_accelerator().device
            )
        else:
            logger.info(
                f&quot;No VAE found in {pretrained_model_name_or_path}, downloading stabilityai/sdxl-vae from HF&quot;
            )
            vae = AutoencoderKL.from_pretrained(&quot;stabilityai/sdxl-vae&quot;).to(
                StateTracker.get_accelerator().device
            )
        print(f&quot;OmniGenPipeline received unexpected arguments: {kwargs.keys()}&quot;)
        return cls(vae, model, processor)
    def merge_lora(self, lora_path: str):
        model = PeftModel.from_pretrained(self.model, lora_path)
        model.merge_and_unload()
        self.model = model
    def to(self, device: Union[str, torch.device]):
        if isinstance(device, str):
            device = torch.device(device)
        self.model.to(device)
        self.vae.to(device)
        self.device = device
    def vae_encode(self, x, dtype):
        if self.vae.config.shift_factor is not None:
            x = self.vae.encode(x).latent_dist.sample()
            x = (x - self.vae.config.shift_factor) * self.vae.config.scaling_factor
        else:
            x = (
                self.vae.encode(x)
                .latent_dist.sample()
                .mul_(self.vae.config.scaling_factor)
            )
        x = x.to(dtype)
        return x
    def move_to_device(self, data):
        if isinstance(data, list):
            return [x.to(self.device) for x in data]
        return data.to(self.device)
    def enable_model_cpu_offload(self):
        self.model_cpu_offload = True
        self.model.to(&quot;cpu&quot;)
        self.vae.to(&quot;cpu&quot;)
        torch.cuda.empty_cache()  # Clear VRAM
        gc.collect()  # Run garbage collection to free system RAM
    def disable_model_cpu_offload(self):
        self.model_cpu_offload = False
        self.model.to(self.device)
        self.vae.to(self.device)
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]],
        input_images: Union[List[str], List[List[str]]] = None,
        height: int = 1024,
        width: int = 1024,
        num_inference_steps: int = 50,
        guidance_scale: float = 3,
        use_img_guidance: bool = True,
        img_guidance_scale: float = 1.6,
        max_input_image_size: int = 1024,
        separate_cfg_infer: bool = True,
        offload_model: bool = False,
        use_kv_cache: bool = True,
        offload_kv_cache: bool = True,
        use_input_image_size_as_output: bool = False,
        dtype: torch.dtype = torch.bfloat16,
        seed: int = None,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`):
                The prompt or prompts to guide the image generation.
            input_images (`List[str]` or `List[List[str]]`, *optional*):
                The list of input images. We will replace the &quot;&lt;|image_i|&gt;&quot; in prompt with the 1-th image in list.
            height (`int`, *optional*, defaults to 1024):
                The height in pixels of the generated image. The number must be a multiple of 16.
            width (`int`, *optional*, defaults to 1024):
                The width in pixels of the generated image. The number must be a multiple of 16.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.
            guidance_scale (`float`, *optional*, defaults to 4.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            use_img_guidance (`bool`, *optional*, defaults to True):
                Defined as equation 3 in [Instrucpix2pix](https://arxiv.org/pdf/2211.09800).
            img_guidance_scale (`float`, *optional*, defaults to 1.6):
                Defined as equation 3 in [Instrucpix2pix](https://arxiv.org/pdf/2211.09800).
            max_input_image_size (`int`, *optional*, defaults to 1024): the maximum size of input image, which will be used to crop the input image to the maximum size
            separate_cfg_infer (`bool`, *optional*, defaults to False):
                Perform inference on images with different guidance separately; this can save memory when generating images of large size at the expense of slower inference.
            use_kv_cache (`bool`, *optional*, defaults to True): enable kv cache to speed up the inference
            offload_kv_cache (`bool`, *optional*, defaults to True): offload the cached key and value to cpu, which can save memory but slow down the generation silightly
            offload_model (`bool`, *optional*, defaults to False): offload the model to cpu, which can save memory but slow down the generation
            use_input_image_size_as_output (bool, defaults to False): whether to use the input image size as the output image size, which can be used for single-image input, e.g., image editing task
            seed (`int`, *optional*):
                A random seed for generating output.
            dtype (`torch.dtype`, *optional*, defaults to `torch.bfloat16`):
                data type for the model
        Examples:
        Returns:
            A list with the generated images.
        &quot;&quot;&quot;
        # check inputs:
        if use_input_image_size_as_output:
            assert (
                isinstance(prompt, str) and len(input_images) == 1
            ), &quot;if you want to make sure the output image have the same size as the input image, please only input one image instead of multiple input images&quot;
        else:
            assert (
                height % 16 == 0 and width % 16 == 0
            ), &quot;The height and width must be a multiple of 16.&quot;
        if input_images is None:
            use_img_guidance = False
        if isinstance(prompt, str):
            prompt = [prompt]
            input_images = [input_images] if input_images is not None else None
        # set model and processor
        if max_input_image_size != self.processor.max_image_size:
            self.processor = OmniGenProcessor(
                self.processor.text_tokenizer, max_image_size=max_input_image_size
            )
        if offload_model:
            self.enable_model_cpu_offload()
        else:
            self.disable_model_cpu_offload()
        input_data = self.processor(
            prompt,
            input_images,
            height=height,
            width=width,
            use_img_cfg=use_img_guidance,
            separate_cfg_input=separate_cfg_infer,
            use_input_image_size_as_output=use_input_image_size_as_output,
        )
        print(f&quot;Input shapes: {input_data[&apos;attention_mask&apos;][0].shape}&quot;)
        num_prompt = len(prompt)
        num_cfg = 2 if use_img_guidance else 1
        if use_input_image_size_as_output:
            if separate_cfg_infer:
                height, width = input_data[&quot;input_pixel_values&quot;][0][0].shape[-2:]
            else:
                height, width = input_data[&quot;input_pixel_values&quot;][0].shape[-2:]
        latent_size_h, latent_size_w = height // 8, width // 8
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
        else:
            generator = None
        latents = torch.randn(
            num_prompt,
            4,
            latent_size_h,
            latent_size_w,
            device=self.device,
            generator=generator,
        )
        latents = torch.cat([latents] * (1 + num_cfg), 0).to(dtype)
        if input_images is not None and self.model_cpu_offload:
            self.vae.to(self.device)
        input_img_latents = []
        if separate_cfg_infer:
            for temp_pixel_values in input_data[&quot;input_pixel_values&quot;]:
                temp_input_latents = []
                for img in temp_pixel_values:
                    img = self.vae_encode(img.to(self.device), dtype)
                    temp_input_latents.append(img)
                input_img_latents.append(temp_input_latents)
        else:
            for img in input_data[&quot;input_pixel_values&quot;]:
                img = self.vae_encode(img.to(self.device), dtype)
                input_img_latents.append(img)
        if input_images is not None and self.model_cpu_offload:
            self.vae.to(&quot;cpu&quot;)
            torch.cuda.empty_cache()  # Clear VRAM
            gc.collect()  # Run garbage collection to free system RAM
        model_kwargs = dict(
            input_ids=self.move_to_device(input_data[&quot;input_ids&quot;]),
            input_img_latents=input_img_latents,
            input_image_sizes=input_data[&quot;input_image_sizes&quot;],
            attention_mask=self.move_to_device(input_data[&quot;attention_mask&quot;]),
            position_ids=self.move_to_device(input_data[&quot;position_ids&quot;]),
            cfg_scale=guidance_scale,
            img_cfg_scale=img_guidance_scale,
            use_img_cfg=use_img_guidance,
            use_kv_cache=use_kv_cache,
            offload_model=offload_model,
        )
        if separate_cfg_infer:
            func = self.model.forward_with_separate_cfg
        else:
            func = self.model.forward_with_cfg
        self.model.to(dtype)
        if self.model_cpu_offload:
            for name, param in self.model.named_parameters():
                if &quot;layers&quot; in name and &quot;layers.0&quot; not in name:
                    param.data = param.data.cpu()
                else:
                    param.data = param.data.to(self.device)
            for buffer_name, buffer in self.model.named_buffers():
                setattr(self.model, buffer_name, buffer.to(self.device))
        # else:
        #     self.model.to(self.device)
        scheduler = OmniGenScheduler(num_steps=num_inference_steps)
        samples = scheduler(
            latents,
            func,
            model_kwargs,
            use_kv_cache=use_kv_cache,
            offload_kv_cache=offload_kv_cache,
        )
        samples = samples.chunk((1 + num_cfg), dim=0)[0]
        if self.model_cpu_offload:
            self.model.to(&quot;cpu&quot;)
            torch.cuda.empty_cache()
            gc.collect()
        self.vae.to(self.device)
        samples = samples.to(torch.float32)
        if self.vae.config.shift_factor is not None:
            samples = (
                samples / self.vae.config.scaling_factor + self.vae.config.shift_factor
            )
        else:
            samples = samples / self.vae.config.scaling_factor
        if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
            # we have SageAttention loaded. fallback to SDPA for decode.
            torch.nn.functional.scaled_dot_product_attention = (
                torch.nn.functional.scaled_dot_product_attention_sdpa
            )
        image = self.vae.decode(latents.to(dtype=self.vae.dtype), return_dict=False)[0]
        if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
            # reenable SageAttention for training.
            torch.nn.functional.scaled_dot_product_attention = (
                torch.nn.functional.scaled_dot_product_attention_sage
            )
        if self.model_cpu_offload:
            self.vae.to(&quot;cpu&quot;)
            torch.cuda.empty_cache()
            gc.collect()
        output_samples = (samples * 0.5 + 0.5).clamp(0, 1) * 255
        output_samples = (
            output_samples.permute(0, 2, 3, 1).to(&quot;cpu&quot;, dtype=torch.uint8).numpy()
        )
        output_images = []
        for i, sample in enumerate(output_samples):
            output_images.append(Image.fromarray(sample))
        torch.cuda.empty_cache()  # Clear VRAM
        gc.collect()  # Run garbage collection to free system RAM
        return output_images</file><file path="helpers/models/pixart/pipeline.py"># Copyright 2024 PixArt-Sigma Authors and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import html
import inspect
import re
import urllib.parse as ul
from typing import Callable, List, Optional, Tuple, Union
import torch
from transformers import T5EncoderModel, T5Tokenizer
from diffusers.image_processor import PixArtImageProcessor, PipelineImageInput
from diffusers.models import AutoencoderKL, PixArtTransformer2DModel
from diffusers.schedulers import KarrasDiffusionSchedulers
from diffusers.utils import (
    BACKENDS_MAPPING,
    deprecate,
    is_bs4_available,
    is_ftfy_available,
    logging,
    replace_example_docstring,
)
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput
from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha import (
    ASPECT_RATIO_256_BIN,
    ASPECT_RATIO_512_BIN,
    ASPECT_RATIO_1024_BIN,
)
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents
def retrieve_latents(
    encoder_output: torch.Tensor,
    generator: Optional[torch.Generator] = None,
    sample_mode: str = &quot;sample&quot;,
):
    if hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;sample&quot;:
        return encoder_output.latent_dist.sample(generator)
    elif hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;argmax&quot;:
        return encoder_output.latent_dist.mode()
    elif hasattr(encoder_output, &quot;latents&quot;):
        return encoder_output.latents
    else:
        raise AttributeError(&quot;Could not access latents of provided encoder_output&quot;)
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
if is_bs4_available():
    from bs4 import BeautifulSoup
if is_ftfy_available():
    import ftfy
ASPECT_RATIO_2048_BIN = {
    &quot;0.25&quot;: [1024.0, 4096.0],
    &quot;0.26&quot;: [1024.0, 3968.0],
    &quot;0.27&quot;: [1024.0, 3840.0],
    &quot;0.28&quot;: [1024.0, 3712.0],
    &quot;0.32&quot;: [1152.0, 3584.0],
    &quot;0.33&quot;: [1152.0, 3456.0],
    &quot;0.35&quot;: [1152.0, 3328.0],
    &quot;0.4&quot;: [1280.0, 3200.0],
    &quot;0.42&quot;: [1280.0, 3072.0],
    &quot;0.48&quot;: [1408.0, 2944.0],
    &quot;0.5&quot;: [1408.0, 2816.0],
    &quot;0.52&quot;: [1408.0, 2688.0],
    &quot;0.57&quot;: [1536.0, 2688.0],
    &quot;0.6&quot;: [1536.0, 2560.0],
    &quot;0.68&quot;: [1664.0, 2432.0],
    &quot;0.72&quot;: [1664.0, 2304.0],
    &quot;0.78&quot;: [1792.0, 2304.0],
    &quot;0.82&quot;: [1792.0, 2176.0],
    &quot;0.88&quot;: [1920.0, 2176.0],
    &quot;0.94&quot;: [1920.0, 2048.0],
    &quot;1.0&quot;: [2048.0, 2048.0],
    &quot;1.07&quot;: [2048.0, 1920.0],
    &quot;1.13&quot;: [2176.0, 1920.0],
    &quot;1.21&quot;: [2176.0, 1792.0],
    &quot;1.29&quot;: [2304.0, 1792.0],
    &quot;1.38&quot;: [2304.0, 1664.0],
    &quot;1.46&quot;: [2432.0, 1664.0],
    &quot;1.67&quot;: [2560.0, 1536.0],
    &quot;1.75&quot;: [2688.0, 1536.0],
    &quot;2.0&quot;: [2816.0, 1408.0],
    &quot;2.09&quot;: [2944.0, 1408.0],
    &quot;2.4&quot;: [3072.0, 1280.0],
    &quot;2.5&quot;: [3200.0, 1280.0],
    &quot;2.89&quot;: [3328.0, 1152.0],
    &quot;3.0&quot;: [3456.0, 1152.0],
    &quot;3.11&quot;: [3584.0, 1152.0],
    &quot;3.62&quot;: [3712.0, 1024.0],
    &quot;3.75&quot;: [3840.0, 1024.0],
    &quot;3.88&quot;: [3968.0, 1024.0],
    &quot;4.0&quot;: [4096.0, 1024.0],
}
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import PixArtSigmaPipeline
        &gt;&gt;&gt; # You can replace the checkpoint id with &quot;PixArt-alpha/PixArt-Sigma-XL-2-512-MS&quot; too.
        &gt;&gt;&gt; pipe = PixArtSigmaPipeline.from_pretrained(
        ...     &quot;PixArt-alpha/PixArt-Sigma-XL-2-1024-MS&quot;, torch_dtype=torch.float16
        ... )
        &gt;&gt;&gt; # Enable memory optimizations.
        &gt;&gt;&gt; # pipe.enable_model_cpu_offload()
        &gt;&gt;&gt; prompt = &quot;A small cactus with a happy face in the Sahara desert.&quot;
        &gt;&gt;&gt; image = pipe(prompt).images[0]
        ```
&quot;&quot;&quot;
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class PixArtSigmaPipeline(DiffusionPipeline):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using PixArt-Sigma.
    &quot;&quot;&quot;
    bad_punct_regex = re.compile(
        r&quot;[&quot;
        + &quot;#®•©™&amp;@·º½¾¿¡§~&quot;
        + r&quot;\)&quot;
        + r&quot;\(&quot;
        + r&quot;\]&quot;
        + r&quot;\[&quot;
        + r&quot;\}&quot;
        + r&quot;\{&quot;
        + r&quot;\|&quot;
        + &quot;\\&quot;
        + r&quot;\/&quot;
        + r&quot;\*&quot;
        + r&quot;]{1,}&quot;
    )  # noqa
    _optional_components = [&quot;tokenizer&quot;, &quot;text_encoder&quot;]
    model_cpu_offload_seq = &quot;text_encoder-&gt;transformer-&gt;vae&quot;
    def __init__(
        self,
        tokenizer: T5Tokenizer,
        text_encoder: T5EncoderModel,
        vae: AutoencoderKL,
        transformer: PixArtTransformer2DModel,
        scheduler: KarrasDiffusionSchedulers,
    ):
        super().__init__()
        self.register_modules(
            tokenizer=tokenizer,
            text_encoder=text_encoder,
            vae=vae,
            transformer=transformer,
            scheduler=scheduler,
        )
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = PixArtImageProcessor(
            vae_scale_factor=self.vae_scale_factor
        )
    def get_timesteps(
        self, num_inference_steps, strength, device, denoising_start=None
    ):
        # get the original timestep using init_timestep
        if denoising_start is not None:
            init_timestep = min(
                int(num_inference_steps * denoising_start), num_inference_steps
            )
            t_start = max(num_inference_steps - init_timestep, 0)
        else:
            t_start = 0
        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]
        # Strength is irrelevant if we directly request a timestep to start at;
        # that is, strength is determined by the denoising_start instead.
        if denoising_start is not None:
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (denoising_start * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = (timesteps &lt; discrete_timestep_cutoff).sum().item()
            if self.scheduler.order == 2 and num_inference_steps % 2 == 0:
                # if the scheduler is a 2nd order scheduler we might have to do +1
                # because `num_inference_steps` might be even given that every timestep
                # (except the highest one) is duplicated. If `num_inference_steps` is even it would
                # mean that we cut the timesteps in the middle of the denoising step
                # (between 1st and 2nd derivative) which leads to incorrect results. By adding 1
                # we ensure that the denoising process always ends after the 2nd derivate step of the scheduler
                num_inference_steps = num_inference_steps + 1
            # because t_n+1 &gt;= t_n, we slice the timesteps starting from the end
            timesteps = timesteps[-num_inference_steps:]
            return timesteps, num_inference_steps
        return timesteps, num_inference_steps - t_start
    # Copied from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha.PixArtAlphaPipeline.encode_prompt with 120-&gt;300
    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        do_classifier_free_guidance: bool = True,
        negative_prompt: str = &quot;&quot;,
        num_images_per_prompt: int = 1,
        device: Optional[torch.device] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        prompt_attention_mask: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        clean_caption: bool = False,
        max_sequence_length: int = 300,
        **kwargs,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
                instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`). For
                PixArt-Alpha, this should be &quot;&quot;.
            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):
                whether to use classifier free guidance or not
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                number of images that should be generated per prompt
            device: (`torch.device`, *optional*):
                torch device to place the resulting embeddings on
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. For PixArt-Alpha, it&apos;s should be the embeddings of the &quot;&quot;
                string.
            clean_caption (`bool`, defaults to `False`):
                If `True`, the function will preprocess and clean the provided caption before encoding.
            max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.
        &quot;&quot;&quot;
        if &quot;mask_feature&quot; in kwargs:
            deprecation_message = &quot;The use of `mask_feature` is deprecated. It is no longer used in any computation and that doesn&apos;t affect the end results. It will be removed in a future version.&quot;
            deprecate(&quot;mask_feature&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
        if device is None:
            device = self._execution_device
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        # See Section 3.1. of the paper.
        max_length = max_sequence_length
        if prompt_embeds is None:
            prompt = self._text_preprocessing(prompt, clean_caption=clean_caption)
            text_inputs = self.tokenizer(
                prompt,
                padding=&quot;max_length&quot;,
                max_length=max_length,
                truncation=True,
                add_special_tokens=True,
                return_tensors=&quot;pt&quot;,
            )
            text_input_ids = text_inputs.input_ids
            untruncated_ids = self.tokenizer(
                prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
            ).input_ids
            if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[
                -1
            ] and not torch.equal(text_input_ids, untruncated_ids):
                removed_text = self.tokenizer.batch_decode(
                    untruncated_ids[:, max_length - 1 : -1]
                )
                logger.warning(
                    &quot;The following part of your input was truncated because T5 can only handle sequences up to&quot;
                    f&quot; {max_length} tokens: {removed_text}&quot;
                )
            prompt_attention_mask = text_inputs.attention_mask
            prompt_attention_mask = prompt_attention_mask.to(device)
            prompt_embeds = self.text_encoder(
                text_input_ids.to(device), attention_mask=prompt_attention_mask
            )
            prompt_embeds = prompt_embeds[0]
        if self.text_encoder is not None:
            dtype = self.text_encoder.dtype
        elif self.transformer is not None:
            dtype = self.transformer.dtype
        else:
            dtype = None
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )
        prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)
        prompt_attention_mask = prompt_attention_mask.repeat(num_images_per_prompt, 1)
        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance and negative_prompt_embeds is None:
            uncond_tokens = (
                [negative_prompt] * batch_size
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            uncond_tokens = self._text_preprocessing(
                uncond_tokens, clean_caption=clean_caption
            )
            max_length = prompt_embeds.shape[1]
            uncond_input = self.tokenizer(
                uncond_tokens,
                padding=&quot;max_length&quot;,
                max_length=max_length,
                truncation=True,
                return_attention_mask=True,
                add_special_tokens=True,
                return_tensors=&quot;pt&quot;,
            )
            negative_prompt_attention_mask = uncond_input.attention_mask
            negative_prompt_attention_mask = negative_prompt_attention_mask.to(device)
            negative_prompt_embeds = self.text_encoder(
                uncond_input.input_ids.to(device),
                attention_mask=negative_prompt_attention_mask,
            )
            negative_prompt_embeds = negative_prompt_embeds[0]
        if do_classifier_free_guidance:
            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = negative_prompt_embeds.shape[1]
            negative_prompt_embeds = negative_prompt_embeds.to(
                dtype=dtype, device=device
            )
            negative_prompt_embeds = negative_prompt_embeds.repeat(
                1, num_images_per_prompt, 1
            )
            negative_prompt_embeds = negative_prompt_embeds.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )
            negative_prompt_attention_mask = negative_prompt_attention_mask.view(
                bs_embed, -1
            )
            negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(
                num_images_per_prompt, 1
            )
        else:
            negative_prompt_embeds = None
            negative_prompt_attention_mask = None
        return (
            prompt_embeds,
            prompt_attention_mask,
            negative_prompt_embeds,
            negative_prompt_attention_mask,
        )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    # Copied from diffusers.pipelines.pixart_alpha.pipeline_pixart_alpha.PixArtAlphaPipeline.check_inputs
    def check_inputs(
        self,
        prompt,
        height,
        width,
        strength,
        num_inference_steps,
        negative_prompt,
        callback_steps,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        prompt_attention_mask=None,
        negative_prompt_attention_mask=None,
    ):
        if strength is None:
            if height % 8 != 0 or width % 8 != 0:
                raise ValueError(
                    f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
                )
        else:
            if strength &lt; 0 or strength &gt; 1:
                raise ValueError(
                    f&quot;The value of strength should in [0.0, 1.0] but is {strength}&quot;
                )
            if num_inference_steps is None:
                raise ValueError(&quot;`num_inference_steps` cannot be None.&quot;)
            elif not isinstance(num_inference_steps, int) or num_inference_steps &lt;= 0:
                raise ValueError(
                    f&quot;`num_inference_steps` has to be a positive integer but is {num_inference_steps} of type&quot;
                    f&quot; {type(num_inference_steps)}.&quot;
                )
        if (callback_steps is None) or (
            callback_steps is not None
            and (not isinstance(callback_steps, int) or callback_steps &lt;= 0)
        ):
            raise ValueError(
                f&quot;`callback_steps` has to be a positive integer but is {callback_steps} of type&quot;
                f&quot; {type(callback_steps)}.&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            prompt = None
        if prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        if prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            negative_prompt = None
        if prompt_embeds is not None and prompt_attention_mask is None:
            raise ValueError(
                &quot;Must provide `prompt_attention_mask` when specifying `prompt_embeds`.&quot;
            )
        if (
            negative_prompt_embeds is not None
            and negative_prompt_attention_mask is None
        ):
            raise ValueError(
                &quot;Must provide `negative_prompt_attention_mask` when specifying `negative_prompt_embeds`.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
            if prompt_attention_mask.shape != negative_prompt_attention_mask.shape:
                raise ValueError(
                    &quot;`prompt_attention_mask` and `negative_prompt_attention_mask` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_attention_mask` {prompt_attention_mask.shape} != `negative_prompt_attention_mask`&quot;
                    f&quot; {negative_prompt_attention_mask.shape}.&quot;
                )
    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._text_preprocessing
    def _text_preprocessing(self, text, clean_caption=False):
        if clean_caption and not is_bs4_available():
            logger.warning(
                BACKENDS_MAPPING[&quot;bs4&quot;][-1].format(&quot;Setting `clean_caption=True`&quot;)
            )
            logger.warning(&quot;Setting `clean_caption` to False...&quot;)
            clean_caption = False
        if clean_caption and not is_ftfy_available():
            logger.warning(
                BACKENDS_MAPPING[&quot;ftfy&quot;][-1].format(&quot;Setting `clean_caption=True`&quot;)
            )
            logger.warning(&quot;Setting `clean_caption` to False...&quot;)
            clean_caption = False
        if not isinstance(text, (tuple, list)):
            text = [text]
        def process(text: str):
            if clean_caption:
                text = self._clean_caption(text)
                text = self._clean_caption(text)
            else:
                text = text.lower().strip()
            return text
        return [process(t) for t in text]
    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._clean_caption
    def _clean_caption(self, caption):
        caption = str(caption)
        caption = ul.unquote_plus(caption)
        caption = caption.strip().lower()
        caption = re.sub(&quot;&lt;person&gt;&quot;, &quot;person&quot;, caption)
        # urls:
        caption = re.sub(
            r&quot;\b((?:https?:(?:\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\w/-]*\b\/?(?!@)))&quot;,  # noqa
            &quot;&quot;,
            caption,
        )  # regex for urls
        caption = re.sub(
            r&quot;\b((?:www:(?:\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\w/-]*\b\/?(?!@)))&quot;,  # noqa
            &quot;&quot;,
            caption,
        )  # regex for urls
        # html:
        caption = BeautifulSoup(caption, features=&quot;html.parser&quot;).text
        # @&lt;nickname&gt;
        caption = re.sub(r&quot;@[\w\d]+\b&quot;, &quot;&quot;, caption)
        # 31C0—31EF CJK Strokes
        # 31F0—31FF Katakana Phonetic Extensions
        # 3200—32FF Enclosed CJK Letters and Months
        # 3300—33FF CJK Compatibility
        # 3400—4DBF CJK Unified Ideographs Extension A
        # 4DC0—4DFF Yijing Hexagram Symbols
        # 4E00—9FFF CJK Unified Ideographs
        caption = re.sub(r&quot;[\u31c0-\u31ef]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u31f0-\u31ff]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u3200-\u32ff]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u3300-\u33ff]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u3400-\u4dbf]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u4dc0-\u4dff]+&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;[\u4e00-\u9fff]+&quot;, &quot;&quot;, caption)
        #######################################################
        # все виды тире / all types of dash --&gt; &quot;-&quot;
        caption = re.sub(
            r&quot;[\u002D\u058A\u05BE\u1400\u1806\u2010-\u2015\u2E17\u2E1A\u2E3A\u2E3B\u2E40\u301C\u3030\u30A0\uFE31\uFE32\uFE58\uFE63\uFF0D]+&quot;,  # noqa
            &quot;-&quot;,
            caption,
        )
        # кавычки к одному стандарту
        caption = re.sub(r&quot;[`´«»“”¨]&quot;, &apos;&quot;&apos;, caption)
        caption = re.sub(r&quot;[‘’]&quot;, &quot;&apos;&quot;, caption)
        # &amp;quot;
        caption = re.sub(r&quot;&amp;quot;?&quot;, &quot;&quot;, caption)
        # &amp;amp
        caption = re.sub(r&quot;&amp;amp&quot;, &quot;&quot;, caption)
        # ip adresses:
        caption = re.sub(r&quot;\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}&quot;, &quot; &quot;, caption)
        # article ids:
        caption = re.sub(r&quot;\d:\d\d\s+$&quot;, &quot;&quot;, caption)
        # \n
        caption = re.sub(r&quot;\\n&quot;, &quot; &quot;, caption)
        # &quot;#123&quot;
        caption = re.sub(r&quot;#\d{1,3}\b&quot;, &quot;&quot;, caption)
        # &quot;#12345..&quot;
        caption = re.sub(r&quot;#\d{5,}\b&quot;, &quot;&quot;, caption)
        # &quot;123456..&quot;
        caption = re.sub(r&quot;\b\d{6,}\b&quot;, &quot;&quot;, caption)
        # filenames:
        caption = re.sub(
            r&quot;[\S]+\.(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)&quot;, &quot;&quot;, caption
        )
        #
        caption = re.sub(r&quot;[\&quot;\&apos;]{2,}&quot;, r&apos;&quot;&apos;, caption)  # &quot;&quot;&quot;AUSVERKAUFT&quot;&quot;&quot;
        caption = re.sub(r&quot;[\.]{2,}&quot;, r&quot; &quot;, caption)  # &quot;&quot;&quot;AUSVERKAUFT&quot;&quot;&quot;
        caption = re.sub(
            self.bad_punct_regex, r&quot; &quot;, caption
        )  # ***AUSVERKAUFT***, #AUSVERKAUFT
        caption = re.sub(r&quot;\s+\.\s+&quot;, r&quot; &quot;, caption)  # &quot; . &quot;
        # this-is-my-cute-cat / this_is_my_cute_cat
        regex2 = re.compile(r&quot;(?:\-|\_)&quot;)
        if len(re.findall(regex2, caption)) &gt; 3:
            caption = re.sub(regex2, &quot; &quot;, caption)
        caption = ftfy.fix_text(caption)
        caption = html.unescape(html.unescape(caption))
        caption = re.sub(r&quot;\b[a-zA-Z]{1,3}\d{3,15}\b&quot;, &quot;&quot;, caption)  # jc6640
        caption = re.sub(r&quot;\b[a-zA-Z]+\d+[a-zA-Z]+\b&quot;, &quot;&quot;, caption)  # jc6640vc
        caption = re.sub(r&quot;\b\d+[a-zA-Z]+\d+\b&quot;, &quot;&quot;, caption)  # 6640vc231
        caption = re.sub(r&quot;(worldwide\s+)?(free\s+)?shipping&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;(free\s)?download(\sfree)?&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;\bclick\b\s(?:for|on)\s\w+&quot;, &quot;&quot;, caption)
        caption = re.sub(
            r&quot;\b(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)(\simage[s]?)?&quot;, &quot;&quot;, caption
        )
        caption = re.sub(r&quot;\bpage\s+\d+\b&quot;, &quot;&quot;, caption)
        caption = re.sub(
            r&quot;\b\d*[a-zA-Z]+\d+[a-zA-Z]+\d+[a-zA-Z\d]*\b&quot;, r&quot; &quot;, caption
        )  # j2d1a2a...
        caption = re.sub(r&quot;\b\d+\.?\d*[xх×]\d+\.?\d*\b&quot;, &quot;&quot;, caption)
        caption = re.sub(r&quot;\b\s+\:\s+&quot;, r&quot;: &quot;, caption)
        caption = re.sub(r&quot;(\D[,\./])\b&quot;, r&quot;\1 &quot;, caption)
        caption = re.sub(r&quot;\s+&quot;, &quot; &quot;, caption)
        caption.strip()
        caption = re.sub(r&quot;^[\&quot;\&apos;]([\w\W]+)[\&quot;\&apos;]$&quot;, r&quot;\1&quot;, caption)
        caption = re.sub(r&quot;^[\&apos;\_,\-\:;]&quot;, r&quot;&quot;, caption)
        caption = re.sub(r&quot;[\&apos;\_,\-\:\-\+]$&quot;, r&quot;&quot;, caption)
        caption = re.sub(r&quot;^\.\S+$&quot;, &quot;&quot;, caption)
        return caption.strip()
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
        timestep=None,
        add_noise=False,
        image=None,
    ):
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        if latents is None:
            latents = randn_tensor(
                shape, generator=generator, device=device, dtype=dtype
            )
        else:
            latents = latents.to(device)
            if add_noise and timestep is not None:
                shape = latents.shape
                noise = randn_tensor(
                    shape, generator=generator, device=device, dtype=dtype
                )
                # get latents
                latents = self.scheduler.add_noise(latents, noise, timestep)
        # scale the initial noise by the standard deviation required by the scheduler
        init_latents = latents * self.scheduler.init_noise_sigma
        if image is not None:
            if image.shape[1] == 4:
                init_latents = image
            else:
                # make sure the VAE is in float32 mode, as it overflows in float16
                if self.vae.config.force_upcast:
                    image = image.float()
                    self.vae.to(dtype=torch.float32)
                if isinstance(generator, list) and len(generator) != batch_size:
                    raise ValueError(
                        f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                        f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
                    )
                elif isinstance(generator, list):
                    init_latents = [
                        retrieve_latents(
                            self.vae.encode(image[i : i + 1]), generator=generator[i]
                        )
                        for i in range(batch_size)
                    ]
                    init_latents = torch.cat(init_latents, dim=0)
                else:
                    init_latents = retrieve_latents(
                        self.vae.encode(image), generator=generator
                    )
                if self.vae.config.force_upcast:
                    self.vae.to(dtype)
                init_latents = init_latents.to(dtype)
                if latents_mean is not None and latents_std is not None:
                    latents_mean = latents_mean.to(device=device, dtype=dtype)
                    latents_std = latents_std.to(device=device, dtype=dtype)
                    init_latents = (
                        (init_latents - latents_mean)
                        * self.vae.config.scaling_factor
                        / latents_std
                    )
                else:
                    init_latents = self.vae.config.scaling_factor * init_latents
            if (
                batch_size &gt; init_latents.shape[0]
                and batch_size % init_latents.shape[0] == 0
            ):
                # expand init_latents for batch_size
                additional_image_per_prompt = batch_size // init_latents.shape[0]
                init_latents = torch.cat(
                    [init_latents] * additional_image_per_ompt, dim=0
                )
            elif (
                batch_size &gt; init_latents.shape[0]
                and batch_size % init_latents.shape[0] != 0
            ):
                raise ValueError(
                    f&quot;Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.&quot;
                )
            else:
                init_latents = torch.cat([init_latents], dim=0)
        return init_latents
    @property
    def denoising_start(self):
        return self._denoising_start
    @property
    def denoising_end(self):
        return self._denoising_end
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        negative_prompt: str = &quot;&quot;,
        strength: float = None,
        num_inference_steps: int = 20,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        denoising_start: Optional[float] = None,
        denoising_end: Optional[float] = None,
        guidance_scale: float = 4.5,
        num_images_per_prompt: Optional[int] = 1,
        height: Optional[int] = None,
        width: Optional[int] = None,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        image: Optional[PipelineImageInput] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        prompt_attention_mask: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
        callback_steps: int = 1,
        clean_caption: bool = True,
        use_resolution_binning: bool = True,
        max_sequence_length: int = 300,
        **kwargs,
    ) -&gt; Union[ImagePipelineOutput, Tuple]:
        &quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            strength (`float`, *optional*, defaults to 0.3):
                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1. `image`
                will be used as a starting point, adding more noise to it the larger the `strength`. The number of
                denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will
                be maximum and the denoising process will run for the full number of iterations specified in
                `num_inference_steps`. A value of 1, therefore, essentially ignores `image`. Note that in the case of
                `denoising_start` being declared as an integer, the value of `strength` will be ignored.
            num_inference_steps (`int`, *optional*, defaults to 100):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            denoising_start (`float`, *optional*):
                When specified, indicates the fraction (between 0.0 and 1.0) of the total denoising process to be
                bypassed before it is initiated. Consequently, the initial part of the denoising process is skipped and
                it is assumed that the passed `image` is a partly denoised image. Note that when this is specified,
                strength will be ignored. The `denoising_start` parameter is particularly beneficial when this pipeline
                is integrated into a &quot;Mixture of Denoisers&quot; multi-pipeline setup, as detailed in [**Refine Image
                Quality**](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality).
            denoising_end (`float`, *optional*):
                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be
                completed before it is intentionally prematurely terminated. As a result, the returned sample will
                still retain a substantial amount of noise as determined by the discrete timesteps selected by the
                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a
                &quot;Mixture of Denoisers&quot; multi-pipeline setup, as elaborated in [**Refining the Image
                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            sigmas (`List[float]`, *optional*):
                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
                will be used.
            guidance_scale (`float`, *optional*, defaults to 4.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            height (`int`, *optional*, defaults to self.unet.config.sample_size):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to self.unet.config.sample_size):
                The width in pixels of the generated image.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            prompt_attention_mask (`torch.Tensor`, *optional*): Pre-generated attention mask for text embeddings.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. For PixArt-Sigma this negative prompt should be &quot;&quot;. If not
                provided, negative_prompt_embeds will be generated from `negative_prompt` input argument.
            negative_prompt_attention_mask (`torch.Tensor`, *optional*):
                Pre-generated attention mask for negative text embeddings.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.IFPipelineOutput`] instead of a plain tuple.
            callback (`Callable`, *optional*):
                A function that will be called every `callback_steps` steps during inference. The function will be
                called with the following arguments: `callback(step: int, timestep: int, latents: torch.Tensor)`.
            callback_steps (`int`, *optional*, defaults to 1):
                The frequency at which the `callback` function will be called. If not specified, the callback will be
                called at every step.
            clean_caption (`bool`, *optional*, defaults to `True`):
                Whether or not to clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy` to
                be installed. If the dependencies are not installed, the embeddings will be created from the raw
                prompt.
            use_resolution_binning (`bool` defaults to `True`):
                If set to `True`, the requested height and width are first mapped to the closest resolutions using
                `ASPECT_RATIO_1024_BIN`. After the produced latents are decoded into images, they are resized back to
                the requested resolution. Useful for generating non-square images.
            max_sequence_length (`int` defaults to 300): Maximum sequence length to use with the `prompt`.
        Examples:
        Returns:
            [`~pipelines.ImagePipelineOutput`] or `tuple`:
                If `return_dict` is `True`, [`~pipelines.ImagePipelineOutput`] is returned, otherwise a `tuple` is
                returned where the first element is a list with the generated images
        &quot;&quot;&quot;
        # 1. Check inputs. Raise error if not correct
        height = height or self.transformer.config.sample_size * self.vae_scale_factor
        width = width or self.transformer.config.sample_size * self.vae_scale_factor
        if use_resolution_binning:
            if self.transformer.config.sample_size == 256:
                aspect_ratio_bin = ASPECT_RATIO_2048_BIN
            elif self.transformer.config.sample_size == 128:
                aspect_ratio_bin = ASPECT_RATIO_1024_BIN
            elif self.transformer.config.sample_size == 64:
                aspect_ratio_bin = ASPECT_RATIO_512_BIN
            elif self.transformer.config.sample_size == 32:
                aspect_ratio_bin = ASPECT_RATIO_256_BIN
            else:
                raise ValueError(&quot;Invalid sample size&quot;)
            orig_height, orig_width = height, width
            height, width = self.image_processor.classify_height_width_bin(
                height, width, ratios=aspect_ratio_bin
            )
        self.check_inputs(
            prompt,
            height,
            width,
            strength,
            num_inference_steps,
            negative_prompt,
            callback_steps,
            prompt_embeds,
            negative_prompt_embeds,
            prompt_attention_mask,
            negative_prompt_attention_mask,
        )
        # 2. Default height and width to transformer
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        self._denoising_start = denoising_start
        self._num_timesteps = num_inference_steps
        self._denoising_end = denoising_end
        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale &gt; 1.0
        # 3. Encode input prompt
        (
            prompt_embeds,
            prompt_attention_mask,
            negative_prompt_embeds,
            negative_prompt_attention_mask,
        ) = self.encode_prompt(
            prompt,
            do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_images_per_prompt,
            device=device,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            prompt_attention_mask=prompt_attention_mask,
            negative_prompt_attention_mask=negative_prompt_attention_mask,
            clean_caption=clean_caption,
            max_sequence_length=max_sequence_length,
        )
        if do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            prompt_attention_mask = torch.cat(
                [negative_prompt_attention_mask, prompt_attention_mask], dim=0
            )
        # 4. Prepare timesteps
        def denoising_value_valid(dnv):
            return isinstance(dnv, float) and 0 &lt; dnv &lt; 1
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps, sigmas
        )
        # 5. Prepare latents.
        if image is not None:
            image = self.image_processor.preprocess(image)
            image = image.to(device=device, dtype=dtype)
        latent_channels = self.transformer.config.in_channels
        latent_timestep = None
        if denoising_end is not None or denoising_start is not None:
            timesteps, num_inference_steps = self.get_timesteps(
                num_inference_steps,
                strength,
                device,
                denoising_start=(
                    self.denoising_start
                    if denoising_value_valid(self.denoising_start)
                    else None
                ),
            )
            latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)
            if latents is not None:
                height, width = latents.shape[-2:]
                height = height * self.vae_scale_factor
                width = width * self.vae_scale_factor
        add_noise = True if self.denoising_start is None else False
        if latents is None:
            latents = self.prepare_latents(
                batch_size * num_images_per_prompt,
                latent_channels,
                height,
                width,
                prompt_embeds.dtype,
                device,
                generator,
                latents,
                timestep=latent_timestep,
                add_noise=add_noise,
                image=image,
            )
        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        # 6.1 Prepare micro-conditions.
        added_cond_kwargs = {&quot;resolution&quot;: None, &quot;aspect_ratio&quot;: None}
        # 7. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        if (
            self.denoising_end is not None
            and self.denoising_start is not None
            and denoising_value_valid(self.denoising_end)
            and denoising_value_valid(self.denoising_start)
            and self.denoising_start &gt;= self.denoising_end
        ):
            raise ValueError(
                f&quot;`denoising_start`: {self.denoising_start} cannot be larger than or equal to `denoising_end`: &quot;
                + f&quot; {self.denoising_end} when using type float.&quot;
            )
        if self.denoising_start is not None:
            if denoising_value_valid(self.denoising_start):
                discrete_timestep_cutoff = int(
                    round(
                        self.scheduler.config.num_train_timesteps
                        - (denoising_start * self.scheduler.config.num_train_timesteps)
                    )
                )
                num_inference_steps = (
                    (timesteps &lt; discrete_timestep_cutoff).sum().item()
                )
                print(
                    f&quot;Beginning inference for stage2 with {num_inference_steps} steps.&quot;
                )
            else:
                raise ValueError(
                    f&quot;`denoising_start` must be a float between 0 and 1: {denoising_start}&quot;
                )
        if self.denoising_end is not None:
            if denoising_value_valid(self.denoising_end):
                discrete_timestep_cutoff = int(
                    round(
                        self.scheduler.config.num_train_timesteps
                        - (
                            self.denoising_end
                            * self.scheduler.config.num_train_timesteps
                        )
                    )
                )
                num_inference_steps = len(
                    list(filter(lambda ts: ts &gt;= discrete_timestep_cutoff, timesteps))
                )
                print(
                    f&quot;Beginning inference for stage1 with {num_inference_steps} steps.&quot;
                )
                timesteps = timesteps[:num_inference_steps]
            else:
                raise ValueError(
                    f&quot;`denoising_end` must be a float between 0 and 1: {denoising_end}&quot;
                )
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                latent_model_input = (
                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                current_timestep = t
                if not torch.is_tensor(current_timestep):
                    # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can
                    # This would be a good case for the `match` statement (Python 3.10+)
                    is_mps = latent_model_input.device.type == &quot;mps&quot;
                    if isinstance(current_timestep, float):
                        dtype = torch.float32 if is_mps else torch.float64
                    else:
                        dtype = torch.int32 if is_mps else torch.int64
                    current_timestep = torch.tensor(
                        [current_timestep],
                        dtype=dtype,
                        device=latent_model_input.device,
                    )
                elif len(current_timestep.shape) == 0:
                    current_timestep = current_timestep[None].to(
                        latent_model_input.device
                    )
                # broadcast to batch dimension in a way that&apos;s compatible with ONNX/Core ML
                current_timestep = current_timestep.expand(latent_model_input.shape[0])
                # predict noise model_output
                noise_pred = self.transformer(
                    latent_model_input.to(
                        device=self.transformer.device, dtype=self.transformer.dtype
                    ),
                    encoder_hidden_states=prompt_embeds,
                    encoder_attention_mask=prompt_attention_mask,
                    timestep=current_timestep,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                # learned sigma
                if self.transformer.config.out_channels // 2 == latent_channels:
                    noise_pred = noise_pred.chunk(2, dim=1)[0]
                else:
                    noise_pred = noise_pred
                # compute previous image: x_t -&gt; x_t-1
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, &quot;order&quot;, 1)
                        callback(step_idx, t, latents)
        if not output_type == &quot;latent&quot;:
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype) / self.vae.config.scaling_factor,
                return_dict=False,
                generator=generator,
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            if use_resolution_binning:
                image = self.image_processor.resize_and_crop_tensor(
                    image, orig_width, orig_height
                )
        else:
            image = latents
        if not output_type == &quot;latent&quot;:
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return ImagePipelineOutput(images=image)</file><file path="helpers/models/sana/transformer.py"># Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Dict, Optional, Tuple, Union
import torch
from torch import nn
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_version,
    logging,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.models.attention_processor import (
    Attention,
    AttentionProcessor,
    AttnProcessor2_0,
    SanaLinearAttnProcessor2_0,
)
from diffusers.models.embeddings import PatchEmbed, PixArtAlphaTextProjection
from diffusers.models.modeling_outputs import Transformer2DModelOutput
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import AdaLayerNormSingle, RMSNorm
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
class GLUMBConv(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        expand_ratio: float = 4,
        norm_type: Optional[str] = None,
        residual_connection: bool = True,
    ) -&gt; None:
        super().__init__()
        hidden_channels = int(expand_ratio * in_channels)
        self.norm_type = norm_type
        self.residual_connection = residual_connection
        self.nonlinearity = nn.SiLU()
        self.conv_inverted = nn.Conv2d(in_channels, hidden_channels * 2, 1, 1, 0)
        self.conv_depth = nn.Conv2d(
            hidden_channels * 2,
            hidden_channels * 2,
            3,
            1,
            1,
            groups=hidden_channels * 2,
        )
        self.conv_point = nn.Conv2d(hidden_channels, out_channels, 1, 1, 0, bias=False)
        self.norm = None
        if norm_type == &quot;rms_norm&quot;:
            self.norm = RMSNorm(
                out_channels, eps=1e-5, elementwise_affine=True, bias=True
            )
    def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
        if self.residual_connection:
            residual = hidden_states
        hidden_states = self.conv_inverted(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.conv_depth(hidden_states)
        hidden_states, gate = torch.chunk(hidden_states, 2, dim=1)
        hidden_states = hidden_states * self.nonlinearity(gate)
        hidden_states = self.conv_point(hidden_states)
        if self.norm_type == &quot;rms_norm&quot;:
            # move channel to the last dimension so we apply RMSnorm across channel dimension
            hidden_states = self.norm(hidden_states.movedim(1, -1)).movedim(-1, 1)
        if self.residual_connection:
            hidden_states = hidden_states + residual
        return hidden_states
class SanaTransformerBlock(nn.Module):
    r&quot;&quot;&quot;
    Transformer block introduced in [Sana](https://huggingface.co/papers/2410.10629).
    &quot;&quot;&quot;
    def __init__(
        self,
        dim: int = 2240,
        num_attention_heads: int = 70,
        attention_head_dim: int = 32,
        dropout: float = 0.0,
        num_cross_attention_heads: Optional[int] = 20,
        cross_attention_head_dim: Optional[int] = 112,
        cross_attention_dim: Optional[int] = 2240,
        attention_bias: bool = True,
        norm_elementwise_affine: bool = False,
        norm_eps: float = 1e-6,
        attention_out_bias: bool = True,
        mlp_ratio: float = 2.5,
    ) -&gt; None:
        super().__init__()
        # 1. Self Attention
        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False, eps=norm_eps)
        self.attn1 = Attention(
            query_dim=dim,
            heads=num_attention_heads,
            dim_head=attention_head_dim,
            dropout=dropout,
            bias=attention_bias,
            cross_attention_dim=None,
            processor=SanaLinearAttnProcessor2_0(),
        )
        # 2. Cross Attention
        if cross_attention_dim is not None:
            self.norm2 = nn.LayerNorm(
                dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps
            )
            self.attn2 = Attention(
                query_dim=dim,
                cross_attention_dim=cross_attention_dim,
                heads=num_cross_attention_heads,
                dim_head=cross_attention_head_dim,
                dropout=dropout,
                bias=True,
                out_bias=attention_out_bias,
                processor=AttnProcessor2_0(),
            )
        # 3. Feed-forward
        self.ff = GLUMBConv(
            dim, dim, mlp_ratio, norm_type=None, residual_connection=False
        )
        self.scale_shift_table = nn.Parameter(torch.randn(6, dim) / dim**0.5)
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        timestep: Optional[torch.LongTensor] = None,
        height: int = None,
        width: int = None,
    ) -&gt; torch.Tensor:
        batch_size = hidden_states.shape[0]
        # 1. Modulation
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (
            self.scale_shift_table[None] + timestep.reshape(batch_size, 6, -1)
        ).chunk(6, dim=1)
        # 2. Self Attention
        norm_hidden_states = self.norm1(hidden_states)
        norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa
        norm_hidden_states = norm_hidden_states.to(hidden_states.dtype)
        attn_output = self.attn1(norm_hidden_states)
        hidden_states = hidden_states + gate_msa * attn_output
        # 3. Cross Attention
        if self.attn2 is not None:
            attn_output = self.attn2(
                hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                attention_mask=encoder_attention_mask,
            )
            hidden_states = attn_output + hidden_states
        # 4. Feed-forward
        norm_hidden_states = self.norm2(hidden_states)
        norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp
        norm_hidden_states = norm_hidden_states.unflatten(1, (height, width)).permute(
            0, 3, 1, 2
        )
        ff_output = self.ff(norm_hidden_states)
        ff_output = ff_output.flatten(2, 3).permute(0, 2, 1)
        hidden_states = hidden_states + gate_mlp * ff_output
        return hidden_states
class SanaTransformer2DModel(ModelMixin, ConfigMixin):
    r&quot;&quot;&quot;
    A 2D Transformer model introduced in [Sana](https://huggingface.co/papers/2410.10629) family of models.
    Args:
        in_channels (`int`, defaults to `32`):
            The number of channels in the input.
        out_channels (`int`, *optional*, defaults to `32`):
            The number of channels in the output.
        num_attention_heads (`int`, defaults to `70`):
            The number of heads to use for multi-head attention.
        attention_head_dim (`int`, defaults to `32`):
            The number of channels in each head.
        num_layers (`int`, defaults to `20`):
            The number of layers of Transformer blocks to use.
        num_cross_attention_heads (`int`, *optional*, defaults to `20`):
            The number of heads to use for cross-attention.
        cross_attention_head_dim (`int`, *optional*, defaults to `112`):
            The number of channels in each head for cross-attention.
        cross_attention_dim (`int`, *optional*, defaults to `2240`):
            The number of channels in the cross-attention output.
        caption_channels (`int`, defaults to `2304`):
            The number of channels in the caption embeddings.
        mlp_ratio (`float`, defaults to `2.5`):
            The expansion ratio to use in the GLUMBConv layer.
        dropout (`float`, defaults to `0.0`):
            The dropout probability.
        attention_bias (`bool`, defaults to `False`):
            Whether to use bias in the attention layer.
        sample_size (`int`, defaults to `32`):
            The base size of the input latent.
        patch_size (`int`, defaults to `1`):
            The size of the patches to use in the patch embedding layer.
        norm_elementwise_affine (`bool`, defaults to `False`):
            Whether to use elementwise affinity in the normalization layer.
        norm_eps (`float`, defaults to `1e-6`):
            The epsilon value for the normalization layer.
    &quot;&quot;&quot;
    _supports_gradient_checkpointing = True
    _no_split_modules = [&quot;SanaTransformerBlock&quot;, &quot;PatchEmbed&quot;]
    @register_to_config
    def __init__(
        self,
        in_channels: int = 32,
        out_channels: Optional[int] = 32,
        num_attention_heads: int = 70,
        attention_head_dim: int = 32,
        num_layers: int = 20,
        num_cross_attention_heads: Optional[int] = 20,
        cross_attention_head_dim: Optional[int] = 112,
        cross_attention_dim: Optional[int] = 2240,
        caption_channels: int = 2304,
        mlp_ratio: float = 2.5,
        dropout: float = 0.0,
        attention_bias: bool = False,
        sample_size: int = 32,
        patch_size: int = 1,
        norm_elementwise_affine: bool = False,
        norm_eps: float = 1e-6,
        interpolation_scale: Optional[int] = None,
    ) -&gt; None:
        super().__init__()
        out_channels = out_channels or in_channels
        inner_dim = num_attention_heads * attention_head_dim
        # 1. Patch Embedding
        self.patch_embed = PatchEmbed(
            height=sample_size,
            width=sample_size,
            patch_size=patch_size,
            in_channels=in_channels,
            embed_dim=inner_dim,
            interpolation_scale=None,
            pos_embed_type=&quot;sincos&quot; if interpolation_scale is not None else None,
        )
        # 2. Additional condition embeddings
        self.time_embed = AdaLayerNormSingle(inner_dim)
        self.caption_projection = PixArtAlphaTextProjection(
            in_features=caption_channels, hidden_size=inner_dim
        )
        self.caption_norm = RMSNorm(inner_dim, eps=1e-5, elementwise_affine=True)
        # 3. Transformer blocks
        self.transformer_blocks = nn.ModuleList(
            [
                SanaTransformerBlock(
                    inner_dim,
                    num_attention_heads,
                    attention_head_dim,
                    dropout=dropout,
                    num_cross_attention_heads=num_cross_attention_heads,
                    cross_attention_head_dim=cross_attention_head_dim,
                    cross_attention_dim=cross_attention_dim,
                    attention_bias=attention_bias,
                    norm_elementwise_affine=norm_elementwise_affine,
                    norm_eps=norm_eps,
                    mlp_ratio=mlp_ratio,
                )
                for _ in range(num_layers)
            ]
        )
        # 4. Output blocks
        self.scale_shift_table = nn.Parameter(
            torch.randn(2, inner_dim) / inner_dim**0.5
        )
        self.norm_out = nn.LayerNorm(inner_dim, elementwise_affine=False, eps=1e-6)
        self.proj_out = nn.Linear(inner_dim, patch_size * patch_size * out_channels)
        self.gradient_checkpointing = False
        self.gradient_checkpointing_interval = None
    def set_gradient_checkpointing_interval(self, interval: int):
        r&quot;&quot;&quot;
        Sets the gradient checkpointing interval for the model.
        Parameters:
            interval (`int`):
                The interval at which to checkpoint the gradients.
        &quot;&quot;&quot;
        self.gradient_checkpointing_interval = interval
    def _set_gradient_checkpointing(self, module, value=False):
        if hasattr(module, &quot;gradient_checkpointing&quot;):
            module.gradient_checkpointing = value
    @property
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors
    def attn_processors(self) -&gt; Dict[str, AttentionProcessor]:
        r&quot;&quot;&quot;
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        &quot;&quot;&quot;
        # set recursively
        processors = {}
        def fn_recursive_add_processors(
            name: str,
            module: torch.nn.Module,
            processors: Dict[str, AttentionProcessor],
        ):
            if hasattr(module, &quot;get_processor&quot;):
                processors[f&quot;{name}.processor&quot;] = module.get_processor()
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f&quot;{name}.{sub_name}&quot;, child, processors)
            return processors
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
        return processors
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor
    def set_attn_processor(
        self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]
    ):
        r&quot;&quot;&quot;
        Sets the attention processor to use to compute attention.
        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.
                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.
        &quot;&quot;&quot;
        count = len(self.attn_processors.keys())
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f&quot;A dict of processors was passed, but the number of processors {len(processor)} does not match the&quot;
                f&quot; number of attention layers: {count}. Please make sure to pass {count} processor classes.&quot;
            )
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            if hasattr(module, &quot;set_processor&quot;):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f&quot;{name}.processor&quot;))
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f&quot;{name}.{sub_name}&quot;, child, processor)
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        timestep: torch.LongTensor,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        attention_kwargs: Optional[dict] = None,
        return_dict: bool = True,
    ) -&gt; Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:
        if attention_kwargs is not None:
            attention_kwargs = attention_kwargs.copy()
            lora_scale = attention_kwargs.pop(&quot;scale&quot;, 1.0)
        else:
            lora_scale = 1.0
        if USE_PEFT_BACKEND:
            # weight the lora layers by setting `lora_scale` for each PEFT layer
            scale_lora_layers(self, lora_scale)
        else:
            if (
                attention_kwargs is not None
                and attention_kwargs.get(&quot;scale&quot;, None) is not None
            ):
                logger.warning(
                    &quot;Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.&quot;
                )
        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.
        #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.
        #   we can tell by counting dims; if ndim == 2: it&apos;s a mask rather than a bias.
        # expects mask of shape:
        #   [batch, key_tokens]
        # adds singleton query_tokens dimension:
        #   [batch,                    1, key_tokens]
        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:
        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)
        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)
        if attention_mask is not None and attention_mask.ndim == 2:
            # assume that mask is expressed as:
            #   (1 = keep,      0 = discard)
            # convert mask into a bias that can be added to attention scores:
            #       (keep = +0,     discard = -10000.0)
            attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0
            attention_mask = attention_mask.unsqueeze(1)
        # convert encoder_attention_mask to a bias the same way we do for attention_mask
        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:
            encoder_attention_mask = (
                1 - encoder_attention_mask.to(hidden_states.dtype)
            ) * -10000.0
            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)
        # 1. Input
        batch_size, num_channels, height, width = hidden_states.shape
        p = self.config.patch_size
        post_patch_height, post_patch_width = height // p, width // p
        hidden_states = self.patch_embed(hidden_states)
        timestep, embedded_timestep = self.time_embed(
            timestep, batch_size=batch_size, hidden_dtype=hidden_states.dtype
        )
        encoder_hidden_states = self.caption_projection(encoder_hidden_states)
        encoder_hidden_states = encoder_hidden_states.view(
            batch_size, -1, hidden_states.shape[-1]
        )
        encoder_hidden_states = self.caption_norm(encoder_hidden_states)
        # 2. Transformer blocks
        use_reentrant = is_torch_version(&quot;&lt;=&quot;, &quot;1.11.0&quot;)
        def create_block_forward(block):
            if (
                self.gradient_checkpointing_interval is not None
                and self.gradient_checkpointing_interval &gt; 0
                and self.gradient_checkpointing
            ):
                self._set_gradient_checkpointing(
                    block, timestep % self.gradient_checkpointing_interval == 0
                )
            if torch.is_grad_enabled() and self.gradient_checkpointing:
                return lambda *inputs: torch.utils.checkpoint.checkpoint(
                    lambda *x: block(*x), *inputs, use_reentrant=use_reentrant
                )
            else:
                return block
        for block in self.transformer_blocks:
            hidden_states = create_block_forward(block)(
                hidden_states,
                attention_mask,
                encoder_hidden_states,
                encoder_attention_mask,
                timestep,
                post_patch_height,
                post_patch_width,
            )
        # 3. Normalization
        shift, scale = (
            self.scale_shift_table[None]
            + embedded_timestep[:, None].to(self.scale_shift_table.device)
        ).chunk(2, dim=1)
        hidden_states = self.norm_out(hidden_states)
        # 4. Modulation
        hidden_states = hidden_states * (1 + scale) + shift
        hidden_states = self.proj_out(hidden_states)
        # 5. Unpatchify
        hidden_states = hidden_states.reshape(
            batch_size,
            post_patch_height,
            post_patch_width,
            self.config.patch_size,
            self.config.patch_size,
            -1,
        )
        hidden_states = hidden_states.permute(0, 5, 1, 3, 2, 4)
        output = hidden_states.reshape(
            batch_size, -1, post_patch_height * p, post_patch_width * p
        )
        if USE_PEFT_BACKEND:
            # remove `lora_scale` from each PEFT layer
            unscale_lora_layers(self, lora_scale)
        if not return_dict:
            return (output,)
        return Transformer2DModelOutput(sample=output)</file><file path="helpers/models/sd3/expanded.py">import argparse
import gc
import operator
import os
from typing import Any, Dict, Optional, Union
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.loaders import FromOriginalModelMixin, PeftAdapterMixin
from diffusers.models.attention import FeedForward, _chunked_feed_forward
from diffusers.models.attention_processor import (
    Attention,
    AttentionProcessor,
    JointAttnProcessor2_0,
)
from diffusers.models.embeddings import CombinedTimestepTextProjEmbeddings, PatchEmbed
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import AdaLayerNormContinuous, AdaLayerNormZero
from diffusers.models.transformers.transformer_2d import Transformer2DModelOutput
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_version,
    logging,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import maybe_allow_in_graph
ORIG_DEPTH = 24
FINAL_DEPTH = 36
M_VALUE = 6
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
@maybe_allow_in_graph
class JointTransformerBlock(nn.Module):
    r&quot;&quot;&quot;
    A Transformer block following the MMDiT architecture, introduced in Stable Diffusion 3.
    Reference: https://arxiv.org/abs/2403.03206
    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        context_pre_only (`bool`): Boolean to determine if we should add some blocks associated with the
            processing of `context` conditions.
    &quot;&quot;&quot;
    def __init__(
        self,
        dim,
        num_attention_heads,
        attention_head_dim,
        context_pre_only=False,
        qk_norm=&quot;layer_norm&quot;,
    ):
        super().__init__()
        self.context_pre_only = context_pre_only
        context_norm_type = (
            &quot;ada_norm_continous&quot; if context_pre_only else &quot;ada_norm_zero&quot;
        )
        self.norm1 = AdaLayerNormZero(dim)
        if context_norm_type == &quot;ada_norm_continous&quot;:
            self.norm1_context = AdaLayerNormContinuous(
                dim,
                dim,
                elementwise_affine=False,
                eps=1e-6,
                bias=True,
                norm_type=&quot;layer_norm&quot;,
            )
        elif context_norm_type == &quot;ada_norm_zero&quot;:
            self.norm1_context = AdaLayerNormZero(dim)
        else:
            raise ValueError(
                f&quot;Unknown context_norm_type: {context_norm_type}, currently only support `ada_norm_continous`, `ada_norm_zero`&quot;
            )
        if hasattr(F, &quot;scaled_dot_product_attention&quot;):
            processor = JointAttnProcessor2_0()
        else:
            raise ValueError(
                &quot;The current PyTorch version does not support the `scaled_dot_product_attention` function.&quot;
            )
        self.attn = Attention(
            query_dim=dim,
            cross_attention_dim=None,
            added_kv_proj_dim=dim,
            qk_norm=qk_norm,
            dim_head=attention_head_dim // num_attention_heads,
            heads=num_attention_heads,
            out_dim=attention_head_dim,
            context_pre_only=context_pre_only,
            bias=True,
            processor=processor,
        )
        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)
        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=&quot;gelu-approximate&quot;)
        if not context_pre_only:
            self.norm2_context = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)
            self.ff_context = FeedForward(
                dim=dim, dim_out=dim, activation_fn=&quot;gelu-approximate&quot;
            )
        else:
            self.norm2_context = None
            self.ff_context = None
        # let chunk size default to None
        self._chunk_size = None
        self._chunk_dim = 0
    # Copied from diffusers.models.attention.BasicTransformerBlock.set_chunk_feed_forward
    def set_chunk_feed_forward(self, chunk_size: Optional[int], dim: int = 0):
        # Sets chunk feed-forward
        self._chunk_size = chunk_size
        self._chunk_dim = dim
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor,
        temb: torch.FloatTensor,
    ):
        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(
            hidden_states, emb=temb
        )
        if self.context_pre_only:
            norm_encoder_hidden_states = self.norm1_context(encoder_hidden_states, temb)
        else:
            (
                norm_encoder_hidden_states,
                c_gate_msa,
                c_shift_mlp,
                c_scale_mlp,
                c_gate_mlp,
            ) = self.norm1_context(encoder_hidden_states, emb=temb)
        # Attention.
        attn_output, context_attn_output = self.attn(
            hidden_states=norm_hidden_states,
            encoder_hidden_states=norm_encoder_hidden_states,
        )
        # Process attention outputs for the `hidden_states`.
        attn_output = gate_msa.unsqueeze(1) * attn_output
        hidden_states = hidden_states + attn_output
        norm_hidden_states = self.norm2(hidden_states)
        norm_hidden_states = (
            norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]
        )
        if self._chunk_size is not None:
            # &quot;feed_forward_chunk_size&quot; can be used to save memory
            ff_output = _chunked_feed_forward(
                self.ff, norm_hidden_states, self._chunk_dim, self._chunk_size
            )
        else:
            ff_output = self.ff(norm_hidden_states)
        ff_output = gate_mlp.unsqueeze(1) * ff_output
        hidden_states = hidden_states + ff_output
        # Process attention outputs for the `encoder_hidden_states`.
        if self.context_pre_only:
            encoder_hidden_states = None
        else:
            context_attn_output = c_gate_msa.unsqueeze(1) * context_attn_output
            encoder_hidden_states = encoder_hidden_states + context_attn_output
            norm_encoder_hidden_states = self.norm2_context(encoder_hidden_states)
            norm_encoder_hidden_states = (
                norm_encoder_hidden_states * (1 + c_scale_mlp[:, None])
                + c_shift_mlp[:, None]
            )
            if self._chunk_size is not None:
                # &quot;feed_forward_chunk_size&quot; can be used to save memory
                context_ff_output = _chunked_feed_forward(
                    self.ff_context,
                    norm_encoder_hidden_states,
                    self._chunk_dim,
                    self._chunk_size,
                )
            else:
                context_ff_output = self.ff_context(norm_encoder_hidden_states)
            encoder_hidden_states = (
                encoder_hidden_states + c_gate_mlp.unsqueeze(1) * context_ff_output
            )
        return encoder_hidden_states, hidden_states
class SD3TransformerQKNorm2DModel(
    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin
):
    &quot;&quot;&quot;
    The Transformer model introduced in Stable Diffusion 3.
    Reference: https://arxiv.org/abs/2403.03206
    Parameters:
        sample_size (`int`): The width of the latent images. This is fixed during training since
            it is used to learn a number of position embeddings.
        patch_size (`int`): Patch size to turn the input data into small patches.
        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.
        num_layers (`int`, *optional*, defaults to 18): The number of layers of Transformer blocks to use.
        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.
        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.
        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.
        caption_projection_dim (`int`): Number of dimensions to use when projecting the `encoder_hidden_states`.
        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.
        out_channels (`int`, defaults to 16): Number of output channels.
        qk_norm (`str`, defaults to &quot;layer_norm&quot;): The type of qk_norm to use.
        TODO The SD3 paper uses RMSNorm instead of LayerNorm but it is unlikely
             that there is much difference betweens RMSNorm being faster.
    &quot;&quot;&quot;
    _supports_gradient_checkpointing = True
    @register_to_config
    def __init__(
        self,
        sample_size: int = 128,
        patch_size: int = 2,
        in_channels: int = 16,
        num_layers: int = 18,
        attention_head_dim: int = 64,
        num_attention_heads: int = 18,
        joint_attention_dim: int = 4096,
        caption_projection_dim: int = 1152,
        pooled_projection_dim: int = 2048,
        out_channels: int = 16,
        pos_embed_max_size: int = 96,
        qk_norm: str | None = &quot;layer_norm&quot;,
    ):
        super().__init__()
        default_out_channels = in_channels
        self.out_channels = (
            out_channels if out_channels is not None else default_out_channels
        )
        self.inner_dim = (
            self.config.num_attention_heads * self.config.attention_head_dim
        )
        self.pos_embed = PatchEmbed(
            height=self.config.sample_size,
            width=self.config.sample_size,
            patch_size=self.config.patch_size,
            in_channels=self.config.in_channels,
            embed_dim=self.inner_dim,
            pos_embed_max_size=pos_embed_max_size,  # hard-code for now.
        )
        self.time_text_embed = CombinedTimestepTextProjEmbeddings(
            embedding_dim=self.inner_dim,
            pooled_projection_dim=self.config.pooled_projection_dim,
        )
        self.context_embedder = nn.Linear(
            self.config.joint_attention_dim, self.config.caption_projection_dim
        )
        # `attention_head_dim` is doubled to account for the mixing.
        # It needs to crafted when we get the actual checkpoints.
        self.transformer_blocks = nn.ModuleList(
            [
                JointTransformerBlock(
                    dim=self.inner_dim,
                    num_attention_heads=self.config.num_attention_heads,
                    attention_head_dim=self.inner_dim,
                    context_pre_only=i == num_layers - 1,
                    qk_norm=qk_norm,
                )
                for i in range(self.config.num_layers)
            ]
        )
        self.norm_out = AdaLayerNormContinuous(
            self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6
        )
        self.proj_out = nn.Linear(
            self.inner_dim, patch_size * patch_size * self.out_channels, bias=True
        )
        self.gradient_checkpointing = False
    # Copied from diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.enable_forward_chunking
    def enable_forward_chunking(
        self, chunk_size: Optional[int] = None, dim: int = 0
    ) -&gt; None:
        &quot;&quot;&quot;
        Sets the attention processor to use [feed forward
        chunking](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers).
        Parameters:
            chunk_size (`int`, *optional*):
                The chunk size of the feed-forward layers. If not specified, will run feed-forward layer individually
                over each tensor of dim=`dim`.
            dim (`int`, *optional*, defaults to `0`):
                The dimension over which the feed-forward computation should be chunked. Choose between dim=0 (batch)
                or dim=1 (sequence length).
        &quot;&quot;&quot;
        if dim not in [0, 1]:
            raise ValueError(f&quot;Make sure to set `dim` to either 0 or 1, not {dim}&quot;)
        # By default chunk size is 1
        chunk_size = chunk_size or 1
        def fn_recursive_feed_forward(
            module: torch.nn.Module, chunk_size: int, dim: int
        ):
            if hasattr(module, &quot;set_chunk_feed_forward&quot;):
                module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
            for child in module.children():
                fn_recursive_feed_forward(child, chunk_size, dim)
        for module in self.children():
            fn_recursive_feed_forward(module, chunk_size, dim)
    @property
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors
    def attn_processors(self) -&gt; Dict[str, AttentionProcessor]:
        r&quot;&quot;&quot;
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        &quot;&quot;&quot;
        # set recursively
        processors = {}
        def fn_recursive_add_processors(
            name: str,
            module: torch.nn.Module,
            processors: Dict[str, AttentionProcessor],
        ):
            if hasattr(module, &quot;get_processor&quot;):
                processors[f&quot;{name}.processor&quot;] = module.get_processor(
                    return_deprecated_lora=True
                )
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f&quot;{name}.{sub_name}&quot;, child, processors)
            return processors
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
        return processors
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor
    def set_attn_processor(
        self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]
    ):
        r&quot;&quot;&quot;
        Sets the attention processor to use to compute attention.
        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.
                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.
        &quot;&quot;&quot;
        count = len(self.attn_processors.keys())
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f&quot;A dict of processors was passed, but the number of processors {len(processor)} does not match the&quot;
                f&quot; number of attention layers: {count}. Please make sure to pass {count} processor classes.&quot;
            )
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            if hasattr(module, &quot;set_processor&quot;):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f&quot;{name}.processor&quot;))
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f&quot;{name}.{sub_name}&quot;, child, processor)
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections
    def fuse_qkv_projections(self):
        &quot;&quot;&quot;
        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)
        are fused. For cross-attention modules, key and value projection matrices are fused.
        &lt;Tip warning={true}&gt;
        This API is 🧪 experimental.
        &lt;/Tip&gt;
        &quot;&quot;&quot;
        self.original_attn_processors = None
        for _, attn_processor in self.attn_processors.items():
            if &quot;Added&quot; in str(attn_processor.__class__.__name__):
                raise ValueError(
                    &quot;`fuse_qkv_projections()` is not supported for models having added KV projections.&quot;
                )
        self.original_attn_processors = self.attn_processors
        for module in self.modules():
            if isinstance(module, Attention):
                module.fuse_projections(fuse=True)
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
    def unfuse_qkv_projections(self):
        &quot;&quot;&quot;Disables the fused QKV projection if enabled.
        &lt;Tip warning={true}&gt;
        This API is 🧪 experimental.
        &lt;/Tip&gt;
        &quot;&quot;&quot;
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)
    def _set_gradient_checkpointing(self, module, value=False):
        if hasattr(module, &quot;gradient_checkpointing&quot;):
            module.gradient_checkpointing = value
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor = None,
        pooled_projections: torch.FloatTensor = None,
        timestep: torch.LongTensor = None,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        return_dict: bool = True,
    ) -&gt; Union[torch.FloatTensor, Transformer2DModelOutput]:
        &quot;&quot;&quot;
        The [`SD3Transformer2DModel`] forward method.
        Args:
            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):
                Input `hidden_states`.
            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):
                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.
            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected
                from the embeddings of input conditions.
            timestep ( `torch.LongTensor`):
                Used to indicate denoising step.
            joint_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain
                tuple.
        Returns:
            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a
            `tuple` where the first element is the sample tensor.
        &quot;&quot;&quot;
        if joint_attention_kwargs is not None:
            joint_attention_kwargs = joint_attention_kwargs.copy()
            lora_scale = joint_attention_kwargs.pop(&quot;scale&quot;, 1.0)
        else:
            lora_scale = 1.0
        if USE_PEFT_BACKEND:
            # weight the lora layers by setting `lora_scale` for each PEFT layer
            scale_lora_layers(self, lora_scale)
        else:
            if (
                joint_attention_kwargs is not None
                and joint_attention_kwargs.get(&quot;scale&quot;, None) is not None
            ):
                logger.warning(
                    &quot;Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.&quot;
                )
        height, width = hidden_states.shape[-2:]
        hidden_states = self.pos_embed(
            hidden_states
        )  # takes care of adding positional embeddings too.
        temb = self.time_text_embed(timestep, pooled_projections)
        encoder_hidden_states = self.context_embedder(encoder_hidden_states)
        for block in self.transformer_blocks:
            if self.training and self.gradient_checkpointing:
                def create_custom_forward(module, return_dict=None):
                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: Dict[str, Any] = (
                    {&quot;use_reentrant&quot;: False} if is_torch_version(&quot;&gt;=&quot;, &quot;1.11.0&quot;) else {}
                )
                hidden_states = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    encoder_hidden_states,
                    temb,
                    **ckpt_kwargs,
                )
            else:
                encoder_hidden_states, hidden_states = block(
                    hidden_states=hidden_states,
                    encoder_hidden_states=encoder_hidden_states,
                    temb=temb,
                )
        hidden_states = self.norm_out(hidden_states, temb)
        hidden_states = self.proj_out(hidden_states)
        # unpatchify
        patch_size = self.config.patch_size
        height = height // patch_size
        width = width // patch_size
        hidden_states = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                height,
                width,
                patch_size,
                patch_size,
                self.out_channels,
            )
        )
        hidden_states = torch.einsum(&quot;nhwpqc-&gt;nchpwq&quot;, hidden_states)
        output = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                self.out_channels,
                height * patch_size,
                width * patch_size,
            )
        )
        if USE_PEFT_BACKEND:
            # remove `lora_scale` from each PEFT layer
            unscale_lora_layers(self, lora_scale)
        if not return_dict:
            return (output,)
        return Transformer2DModelOutput(sample=output)
def verify_all_parameters_offset_copy(
    model_old,
    model_new,
    layer_name_prefix,
    source_start_idx,
    dest_start_idx,
    num_layers_to_check,
):
    &quot;&quot;&quot;
    Verifies that all parameters from a specified range in the old model are correctly copied to a new range in the scaled model.
    Parameters:
    - model_old: The original PyTorch model.
    - model_new: The depth-scaled PyTorch model.
    - layer_name_prefix: The prefix of the layer names to check, e.g., &apos;transformer_blocks&apos;.
    - source_start_idx: The starting index of the layers in the old model from which parameters are copied.
    - dest_start_idx: The starting index of the layers in the new model where parameters are copied into.
    - num_layers_to_check: The number of layers to check from the source_start_idx.
    &quot;&quot;&quot;
    for offset in range(num_layers_to_check):
        source_idx = source_start_idx + offset
        dest_idx = dest_start_idx + offset
        source_layer = getattr(model_old, layer_name_prefix)[source_idx]
        dest_layer = getattr(model_new, layer_name_prefix)[dest_idx]
        for param_name, source_param in source_layer.named_parameters():
            # Retrieve the corresponding parameter from the destination layer
            if isinstance(operator.attrgetter(param_name)(dest_layer), torch.Tensor):
                dest_param = operator.attrgetter(param_name)(dest_layer)
                # Check if the parameters are close enough (considering floating-point arithmetic)
                if not torch.allclose(source_param, dest_param, atol=1e-6):
                    raise AssertionError(
                        f&quot;Parameter mismatch for {layer_name_prefix}.{source_idx}.{param_name} (original) -&gt; {layer_name_prefix}.{dest_idx}.{param_name} (new).&quot;
                    )
            else:
                raise AssertionError(
                    f&quot;Missing parameter {layer_name_prefix}.{dest_idx}.{param_name} in the new model.&quot;
                )
    print(
        f&quot;All parameters from {source_start_idx} to {source_start_idx + num_layers_to_check - 1} ({num_layers_to_check} layers) in {layer_name_prefix} have been verified to be correctly copied to {dest_start_idx} to {dest_start_idx + num_layers_to_check - 1}.&quot;
    )
def expand_existing_sd3_model(model_old):
    # This model is 36 layers deep, versus 24 layers deep from the original model.
    # We will prune 12 layers off from the end and the start of the merged weights.
    model_new = SD3TransformerQKNorm2DModel.from_config(
        {
            &quot;_class_name&quot;: &quot;SD3Transformer2DModel&quot;,
            &quot;_diffusers_version&quot;: &quot;0.30.0.dev0&quot;,
            &quot;_name_or_path&quot;: &quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;,
            &quot;attention_head_dim&quot;: 64,
            &quot;caption_projection_dim&quot;: 1536,
            &quot;in_channels&quot;: 16,
            &quot;joint_attention_dim&quot;: 4096,
            &quot;num_attention_heads&quot;: 24,
            &quot;num_layers&quot;: FINAL_DEPTH,
            &quot;out_channels&quot;: 16,
            &quot;patch_size&quot;: 2,
            &quot;pooled_projection_dim&quot;: 2048,
            &quot;pos_embed_max_size&quot;: 192,
            &quot;qk_norm&quot;: &quot;layer_norm&quot;,
            &quot;sample_size&quot;: 128,
        }
    )
    # Copy in layers 0...23 and all other layers.
    with torch.no_grad():
        new_model_param_names = set(name for name, _ in model_new.named_parameters())
        # Iterate through parameters of the old model
        for name, param in model_old.named_parameters():
            if name in new_model_param_names:
                # Get the corresponding parameter from the new model and copy the old param in
                try:
                    model_new.state_dict()[name].copy_(param)
                except RuntimeError as e:
                    if (
                        &quot;The size of tensor a (9216) must match the size of tensor b (3072) at non-singleton dimension 0&quot;
                        in str(e)
                    ):
                        pass
                    else:
                        print(f&quot;Got {str(e)} on layer {name}&quot;)
                        raise
    # We now need to deal with [18:] for both transformer_blocks.
    # We do this by copying in [6:] into [18:] for these blocks.
    with torch.no_grad():
        for layer_idx, injection_idx in zip(
            range(M_VALUE, FINAL_DEPTH),
            range(ORIG_DEPTH - M_VALUE, FINAL_DEPTH),
        ):
            for name, param in model_old.named_parameters():
                if &quot;transformer_blocks&quot; in name:
                    if f&quot;transformer_blocks.{layer_idx}.&quot; in name:
                        name_to_inject_into = name.replace(
                            f&quot;transformer_blocks.{layer_idx}.&quot;,
                            f&quot;transformer_blocks.{injection_idx}.&quot;,
                        )
                        model_new.state_dict()[name_to_inject_into].copy_(param)
    # Finally, transform all the newly added qk norm layers in passthroughs.
    # Setting the weights to 1 and the bias to zero means that initially they
    # should do nothing to the model.
    with torch.no_grad():
        for name, param in model_new.named_parameters():
            if &quot;transformer_blocks&quot; in name and (&quot;norm_q&quot; in name or &quot;norm_k&quot; in name):
                if &quot;norm_q.weight&quot; in name:
                    param.fill_(1)
                elif &quot;norm_q.bias&quot; in name:
                    param.fill_(0)
    verify_all_parameters_offset_copy(
        model_old, model_new, &quot;transformer_blocks&quot;, 0, 0, ORIG_DEPTH - M_VALUE
    )  # Adjust the index as needed
    verify_all_parameters_offset_copy(
        model_old, model_new, &quot;transformer_blocks&quot;, 6, 18, ORIG_DEPTH - M_VALUE
    )  # Adjust the last parameter as needed based on the number of layers you&apos;re checking
    orig_params = sum(p.numel() for p in model_old.parameters())
    expanded_params = sum(p.numel() for p in model_new.parameters())
    print(
        f&quot;Model has been successfully expanded from {orig_params / 1e6:.2f}M to {expanded_params / 1e6:.2f}M.&quot;
    )
    model_new.save_pretrained((os.path.join(args.output_model, &quot;transformer&quot;)))
    return model_new
if __name__ == &quot;__main__&quot;:
    from diffusers.models.transformers.transformer_sd3 import SD3Transformer2DModel
    parser = argparse.ArgumentParser(
        description=&quot;Make a 24 block deep SD3 2B into a 36 block deep version&quot;,
    )
    parser.add_argument(
        &quot;input_model&quot;,
        action=&quot;store&quot;,
        type=str,
        help=&quot;The input pretrained model&quot;,
    )
    parser.add_argument(
        &quot;output_model&quot;,
        action=&quot;store&quot;,
        type=str,
        help=&quot;The output pretrained model location&quot;,
    )
    args = parser.parse_args()
    model_old = SD3Transformer2DModel.from_pretrained(
        args.input_model,
        subfolder=&quot;transformer&quot;,
    )
    model_new = expand_existing_sd3_model(model_old)
    del model_old
    gc.collect()
    model_new = model_new.to(&quot;cuda&quot;, dtype=torch.bfloat16)
    with torch.no_grad(), torch.inference_mode():
        model_new(
            hidden_states=torch.rand((1, 16, 64, 64)).to(&quot;cuda&quot;, dtype=torch.bfloat16),
            encoder_hidden_states=torch.rand((1, 144, 4096)).to(
                &quot;cuda&quot;, dtype=torch.bfloat16
            ),
            pooled_projections=torch.rand((1, 2048)).to(&quot;cuda&quot;, dtype=torch.bfloat16),
            timestep=torch.tensor([500]).to(&quot;cuda&quot;, dtype=torch.bfloat16),
        )
    print(&quot;Successfully expanded and tested model.&quot;)</file><file path="helpers/models/sd3/pipeline.py"># Copyright 2024 Stability AI and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from typing import Any, Callable, Dict, List, Optional, Union
import torch
from transformers import (
    CLIPTextModelWithProjection,
    CLIPTokenizer,
    T5EncoderModel,
    T5TokenizerFast,
)
from diffusers.image_processor import VaeImageProcessor
from diffusers.loaders import FromSingleFileMixin, SD3LoraLoaderMixin
from diffusers.models.autoencoders import AutoencoderKL
from diffusers.models.transformers import SD3Transformer2DModel
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_xla_available,
    logging,
    replace_example_docstring,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline
from diffusers.pipelines.stable_diffusion_3.pipeline_output import (
    StableDiffusion3PipelineOutput,
)
from diffusers.image_processor import PipelineImageInput
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm
    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import StableDiffusion3Pipeline
        &gt;&gt;&gt; pipe = StableDiffusion3Pipeline.from_pretrained(
        ...     &quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;, torch_dtype=torch.float16
        ... )
        &gt;&gt;&gt; pipe.to(&quot;cuda&quot;)
        &gt;&gt;&gt; prompt = &quot;A cat holding a sign that says hello world&quot;
        &gt;&gt;&gt; image = pipe(prompt).images[0]
        &gt;&gt;&gt; image.save(&quot;sd3.png&quot;)
        ```
&quot;&quot;&quot;
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    r&quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class StableDiffusion3Pipeline(
    DiffusionPipeline, SD3LoraLoaderMixin, FromSingleFileMixin
):
    r&quot;&quot;&quot;
    Args:
        transformer ([`SD3Transformer2DModel`]):
            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.
        scheduler ([`FlowMatchEulerDiscreteScheduler`]):
            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,
            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`
            as its dimension.
        text_encoder_2 ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the
            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
            variant.
        text_encoder_3 ([`T5EncoderModel`]):
            Frozen text-encoder. Stable Diffusion 3 uses
            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the
            [t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl) variant.
        tokenizer (`CLIPTokenizer`):
            Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_2 (`CLIPTokenizer`):
            Second Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_3 (`T5TokenizerFast`):
            Tokenizer of class
            [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).
    &quot;&quot;&quot;
    model_cpu_offload_seq = (
        &quot;text_encoder-&gt;text_encoder_2-&gt;text_encoder_3-&gt;transformer-&gt;vae&quot;
    )
    _optional_components = []
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
    ]
    def __init__(
        self,
        transformer: SD3Transformer2DModel,
        scheduler: FlowMatchEulerDiscreteScheduler,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModelWithProjection,
        tokenizer: CLIPTokenizer,
        text_encoder_2: CLIPTextModelWithProjection,
        tokenizer_2: CLIPTokenizer,
        text_encoder_3: T5EncoderModel,
        tokenizer_3: T5TokenizerFast,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            text_encoder_2=text_encoder_2,
            text_encoder_3=text_encoder_3,
            tokenizer=tokenizer,
            tokenizer_2=tokenizer_2,
            tokenizer_3=tokenizer_3,
            transformer=transformer,
            scheduler=scheduler,
        )
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels) - 1)
            if hasattr(self, &quot;vae&quot;) and self.vae is not None
            else 8
        )
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.tokenizer_max_length = (
            self.tokenizer.model_max_length
            if hasattr(self, &quot;tokenizer&quot;) and self.tokenizer is not None
            else 77
        )
        self.default_sample_size = (
            self.transformer.config.sample_size
            if hasattr(self, &quot;transformer&quot;) and self.transformer is not None
            else 128
        )
        self.patch_size = (
            self.transformer.config.patch_size
            if hasattr(self, &quot;transformer&quot;) and self.transformer is not None
            else 2
        )
    def _get_t5_prompt_embeds(
        self,
        prompt: Union[str, List[str]] = None,
        num_images_per_prompt: int = 1,
        max_sequence_length: int = 256,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        device = device or self._execution_device
        dtype = dtype or self.text_encoder.dtype
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        if self.text_encoder_3 is None:
            return torch.zeros(
                (
                    batch_size * num_images_per_prompt,
                    self.tokenizer_max_length,
                    self.transformer.config.joint_attention_dim,
                ),
                device=device,
                dtype=dtype,
            )
        text_inputs = self.tokenizer_3(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=max_sequence_length,
            truncation=True,
            add_special_tokens=True,
            return_tensors=&quot;pt&quot;,
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = self.tokenizer_3(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = self.tokenizer_3.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            logger.warning(
                &quot;The following part of your input was truncated because `max_sequence_length` is set to &quot;
                f&quot; {max_sequence_length} tokens: {removed_text}&quot;
            )
        prompt_embeds = self.text_encoder_3(text_input_ids.to(device))[0]
        dtype = self.text_encoder_3.dtype
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
        _, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )
        return prompt_embeds
    def _get_clip_prompt_embeds(
        self,
        prompt: Union[str, List[str]],
        num_images_per_prompt: int = 1,
        device: Optional[torch.device] = None,
        clip_skip: Optional[int] = None,
        clip_model_index: int = 0,
    ):
        device = device or self._execution_device
        clip_tokenizers = [self.tokenizer, self.tokenizer_2]
        clip_text_encoders = [self.text_encoder, self.text_encoder_2]
        tokenizer = clip_tokenizers[clip_model_index]
        text_encoder = clip_text_encoders[clip_model_index]
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        text_inputs = tokenizer(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=self.tokenizer_max_length,
            truncation=True,
            return_tensors=&quot;pt&quot;,
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = tokenizer(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = tokenizer.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            logger.warning(
                &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
                f&quot; {self.tokenizer_max_length} tokens: {removed_text}&quot;
            )
        prompt_embeds = text_encoder(
            text_input_ids.to(device), output_hidden_states=True
        )
        pooled_prompt_embeds = prompt_embeds[0]
        if clip_skip is None:
            prompt_embeds = prompt_embeds.hidden_states[-2]
        else:
            prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]
        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)
        _, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(1, num_images_per_prompt, 1)
        pooled_prompt_embeds = pooled_prompt_embeds.view(
            batch_size * num_images_per_prompt, -1
        )
        return prompt_embeds, pooled_prompt_embeds
    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        prompt_2: Union[str, List[str]],
        prompt_3: Union[str, List[str]],
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        negative_prompt_3: Optional[Union[str, List[str]]] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        clip_skip: Optional[int] = None,
        max_sequence_length: int = 256,
        lora_scale: Optional[float] = None,
    ):
        r&quot;&quot;&quot;
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in all text-encoders
            prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is
                used in all text-encoders
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in all the text-encoders.
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and
                `text_encoder_3`. If not defined, `negative_prompt` is used in both text-encoders
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            lora_scale (`float`, *optional*):
                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
        &quot;&quot;&quot;
        device = device or self._execution_device
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(self, SD3LoraLoaderMixin):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if self.text_encoder is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder, lora_scale)
            if self.text_encoder_2 is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder_2, lora_scale)
        prompt = [prompt] if isinstance(prompt, str) else prompt
        if prompt is not None:
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        if prompt_embeds is None:
            prompt_2 = prompt_2 or prompt
            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2
            prompt_3 = prompt_3 or prompt
            prompt_3 = [prompt_3] if isinstance(prompt_3, str) else prompt_3
            prompt_embed, pooled_prompt_embed = self._get_clip_prompt_embeds(
                prompt=prompt,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
                clip_skip=clip_skip,
                clip_model_index=0,
            )
            prompt_2_embed, pooled_prompt_2_embed = self._get_clip_prompt_embeds(
                prompt=prompt_2,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
                clip_skip=clip_skip,
                clip_model_index=1,
            )
            clip_prompt_embeds = torch.cat([prompt_embed, prompt_2_embed], dim=-1)
            t5_prompt_embed = self._get_t5_prompt_embeds(
                prompt=prompt_3,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                device=device,
            )
            clip_prompt_embeds = torch.nn.functional.pad(
                clip_prompt_embeds,
                (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1]),
            )
            prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)
            pooled_prompt_embeds = torch.cat(
                [pooled_prompt_embed, pooled_prompt_2_embed], dim=-1
            )
        if do_classifier_free_guidance and negative_prompt_embeds is None:
            negative_prompt = negative_prompt or &quot;&quot;
            negative_prompt_2 = negative_prompt_2 or negative_prompt
            negative_prompt_3 = negative_prompt_3 or negative_prompt
            # normalize str to list
            negative_prompt = (
                batch_size * [negative_prompt]
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            negative_prompt_2 = (
                batch_size * [negative_prompt_2]
                if isinstance(negative_prompt_2, str)
                else negative_prompt_2
            )
            negative_prompt_3 = (
                batch_size * [negative_prompt_3]
                if isinstance(negative_prompt_3, str)
                else negative_prompt_3
            )
            if prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            negative_prompt_embed, negative_pooled_prompt_embed = (
                self._get_clip_prompt_embeds(
                    negative_prompt,
                    device=device,
                    num_images_per_prompt=num_images_per_prompt,
                    clip_skip=None,
                    clip_model_index=0,
                )
            )
            negative_prompt_2_embed, negative_pooled_prompt_2_embed = (
                self._get_clip_prompt_embeds(
                    negative_prompt_2,
                    device=device,
                    num_images_per_prompt=num_images_per_prompt,
                    clip_skip=None,
                    clip_model_index=1,
                )
            )
            negative_clip_prompt_embeds = torch.cat(
                [negative_prompt_embed, negative_prompt_2_embed], dim=-1
            )
            t5_negative_prompt_embed = self._get_t5_prompt_embeds(
                prompt=negative_prompt_3,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                device=device,
            )
            negative_clip_prompt_embeds = torch.nn.functional.pad(
                negative_clip_prompt_embeds,
                (
                    0,
                    t5_negative_prompt_embed.shape[-1]
                    - negative_clip_prompt_embeds.shape[-1],
                ),
            )
            negative_prompt_embeds = torch.cat(
                [negative_clip_prompt_embeds, t5_negative_prompt_embed], dim=-2
            )
            negative_pooled_prompt_embeds = torch.cat(
                [negative_pooled_prompt_embed, negative_pooled_prompt_2_embed], dim=-1
            )
        if self.text_encoder is not None:
            if isinstance(self, SD3LoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder, lora_scale)
        if self.text_encoder_2 is not None:
            if isinstance(self, SD3LoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder_2, lora_scale)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    def check_inputs(
        self,
        prompt,
        prompt_2,
        prompt_3,
        height,
        width,
        negative_prompt=None,
        negative_prompt_2=None,
        negative_prompt_3=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        pooled_prompt_embeds=None,
        negative_pooled_prompt_embeds=None,
        callback_on_step_end_tensor_inputs=None,
        max_sequence_length=None,
    ):
        if (
            height % (self.vae_scale_factor * self.patch_size) != 0
            or width % (self.vae_scale_factor * self.patch_size) != 0
        ):
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by {self.vae_scale_factor * self.patch_size} but are {height} and {width}.&quot;
                f&quot;You can use height {height - height % (self.vae_scale_factor * self.patch_size)} and width {width - width % (self.vae_scale_factor * self.patch_size)}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_2 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_3 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_3`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        elif prompt_2 is not None and (
            not isinstance(prompt_2, str) and not isinstance(prompt_2, list)
        ):
            raise ValueError(
                f&quot;`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}&quot;
            )
        elif prompt_3 is not None and (
            not isinstance(prompt_3, str) and not isinstance(prompt_3, list)
        ):
            raise ValueError(
                f&quot;`prompt_3` has to be of type `str` or `list` but is {type(prompt_3)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_3 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_3`: {negative_prompt_3} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.&quot;
            )
        if max_sequence_length is not None and max_sequence_length &gt; 512:
            raise ValueError(
                f&quot;`max_sequence_length` cannot be greater than 512 but is {max_sequence_length}&quot;
            )
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        if latents is not None:
            return latents.to(device=device, dtype=dtype)
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        return latents
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def clip_skip(self):
        return self._clip_skip
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1
    @property
    def joint_attention_kwargs(self):
        return self._joint_attention_kwargs
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        prompt_2: Optional[Union[str, List[str]]] = None,
        prompt_3: Optional[Union[str, List[str]]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 28,
        timesteps: List[int] = None,
        guidance_scale: float = 7.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        negative_prompt_3: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        clip_skip: Optional[int] = None,
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        max_sequence_length: int = 256,
        skip_guidance_layers: List[int] = None,
        skip_layer_guidance_scale: int = 2.8,
        skip_layer_guidance_stop: int = 0.2,
        skip_layer_guidance_start: int = 0.01,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                will be used instead
            prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is
                will be used instead
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            guidance_scale (`float`, *optional*, defaults to 7.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used instead
            negative_prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and
                `text_encoder_3`. If not defined, `negative_prompt` is used instead
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] instead of
                a plain tuple.
            joint_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
            skip_guidance_layers (`List[int]`, *optional*): A list of integers that specify layers to skip during guidance.
                If not provided, all layers will be used for guidance. If provided, the guidance will only be applied
                to the layers specified in the list. Recommended value by StabiltyAI for Stable Diffusion 3.5 Medium is
                [7, 8, 9].
            skip_layer_guidance_scale (`int`, *optional*): The scale of the guidance for the layers specified in
                `skip_guidance_layers`. The guidance will be applied to the layers specified in `skip_guidance_layers`
                with a scale of `skip_layer_guidance_scale`. The guidance will be applied to the rest of the layers with
                a scale of `1`.
            skip_layer_guidance_stop (`int`, *optional*): The step at which the guidance for the layers specified in
                `skip_guidance_layers` will stop. The guidance will be applied to the layers specified in
                `skip_guidance_layers` until the fraction specified in `skip_layer_guidance_stop`. Recommended value by
                StabiltyAI for Stable Diffusion 3.5 Medium is 0.2.
            skip_layer_guidance_start (`int`, *optional*): The step at which the guidance for the layers specified in
                `skip_guidance_layers` will start. The guidance will be applied to the layers specified in
                `skip_guidance_layers` from the fraction specified in `skip_layer_guidance_start`. Recommended value by
                StabiltyAI for Stable Diffusion 3.5 Medium is 0.01.
        Examples:
        Returns:
            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] if `return_dict` is True, otherwise a
            `tuple`. When returning a tuple, the first element is a list with the generated images.
        &quot;&quot;&quot;
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            prompt_2,
            prompt_3,
            height,
            width,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            negative_prompt_3=negative_prompt_3,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,
            max_sequence_length=max_sequence_length,
        )
        self._guidance_scale = guidance_scale
        self._skip_layer_guidance_scale = skip_layer_guidance_scale
        self._clip_skip = clip_skip
        self._joint_attention_kwargs = joint_attention_kwargs
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        lora_scale = (
            self.joint_attention_kwargs.get(&quot;scale&quot;, None)
            if self.joint_attention_kwargs is not None
            else None
        )
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt_2,
            prompt_3=prompt_3,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            negative_prompt_3=negative_prompt_3,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            device=device,
            clip_skip=self.clip_skip,
            num_images_per_prompt=num_images_per_prompt,
            max_sequence_length=max_sequence_length,
            lora_scale=lora_scale,
        )
        if self.do_classifier_free_guidance:
            if skip_guidance_layers is not None:
                original_prompt_embeds = prompt_embeds
                original_pooled_prompt_embeds = pooled_prompt_embeds
            # we do not combine the inference if we skip guidance layers.
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            pooled_prompt_embeds = torch.cat(
                [negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0
            )
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps
        )
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        self._num_timesteps = len(timesteps)
        # 5. Prepare latent variables
        num_channels_latents = self.transformer.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 6. Denoising loop
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                # added fix from: https://github.com/huggingface/diffusers/pull/10086/files
                # to allow for num_images_per_prompt &gt; 1
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                # broadcast to batch dimension in a way that&apos;s compatible with ONNX/Core ML
                timestep = t.expand(latent_model_input.shape[0])
                noise_pred = self.transformer(
                    hidden_states=latent_model_input.to(device=self.transformer.device),
                    timestep=timestep,
                    encoder_hidden_states=prompt_embeds.to(
                        device=self.transformer.device
                    ),
                    pooled_projections=pooled_prompt_embeds.to(
                        device=self.transformer.device
                    ),
                    joint_attention_kwargs=self.joint_attention_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                    should_skip_layers = (
                        True
                        if i &gt; num_inference_steps * skip_layer_guidance_start
                        and i &lt; num_inference_steps * skip_layer_guidance_stop
                        else False
                    )
                    if skip_guidance_layers is not None and should_skip_layers:
                        timestep = t.expand(latents.shape[0])
                        latent_model_input = latents
                        noise_pred_skip_layers = self.transformer(
                            hidden_states=latent_model_input.to(
                                device=self.transformer.device,
                            ),
                            timestep=timestep,
                            encoder_hidden_states=original_prompt_embeds.to(
                                device=self.transformer.device,
                            ),
                            pooled_projections=original_pooled_prompt_embeds.to(
                                device=self.transformer.device,
                            ),
                            joint_attention_kwargs=self.joint_attention_kwargs,
                            return_dict=False,
                            skip_layers=skip_guidance_layers,
                        )[0]
                        noise_pred = (
                            noise_pred
                            + (noise_pred_text - noise_pred_skip_layers)
                            * self._skip_layer_guidance_scale
                        )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                if XLA_AVAILABLE:
                    xm.mark_step()
        if output_type == &quot;latent&quot;:
            image = latents
        else:
            latents = (
                latents / self.vae.config.scaling_factor
            ) + self.vae.config.shift_factor
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype), return_dict=False
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return StableDiffusion3PipelineOutput(images=image)
class StableDiffusion3Img2ImgPipeline(
    DiffusionPipeline, SD3LoraLoaderMixin, FromSingleFileMixin
):
    r&quot;&quot;&quot;
    Args:
        transformer ([`SD3Transformer2DModel`]):
            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.
        scheduler ([`FlowMatchEulerDiscreteScheduler`]):
            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,
            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`
            as its dimension.
        text_encoder_2 ([`CLIPTextModelWithProjection`]):
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the
            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
            variant.
        text_encoder_3 ([`T5EncoderModel`]):
            Frozen text-encoder. Stable Diffusion 3 uses
            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the
            [t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl) variant.
        tokenizer (`CLIPTokenizer`):
            Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_2 (`CLIPTokenizer`):
            Second Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_3 (`T5TokenizerFast`):
            Tokenizer of class
            [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).
    &quot;&quot;&quot;
    model_cpu_offload_seq = (
        &quot;text_encoder-&gt;text_encoder_2-&gt;text_encoder_3-&gt;transformer-&gt;vae&quot;
    )
    _optional_components = []
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
    ]
    def __init__(
        self,
        transformer: SD3Transformer2DModel,
        scheduler: FlowMatchEulerDiscreteScheduler,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModelWithProjection,
        tokenizer: CLIPTokenizer,
        text_encoder_2: CLIPTextModelWithProjection,
        tokenizer_2: CLIPTokenizer,
        text_encoder_3: T5EncoderModel,
        tokenizer_3: T5TokenizerFast,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            text_encoder_2=text_encoder_2,
            text_encoder_3=text_encoder_3,
            tokenizer=tokenizer,
            tokenizer_2=tokenizer_2,
            tokenizer_3=tokenizer_3,
            transformer=transformer,
            scheduler=scheduler,
        )
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = VaeImageProcessor(
            vae_scale_factor=self.vae_scale_factor,
            vae_latent_channels=self.vae.config.latent_channels,
        )
        self.tokenizer_max_length = self.tokenizer.model_max_length
        self.default_sample_size = self.transformer.config.sample_size
    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3.StableDiffusion3Pipeline._get_t5_prompt_embeds
    def _get_t5_prompt_embeds(
        self,
        prompt: Union[str, List[str]] = None,
        num_images_per_prompt: int = 1,
        max_sequence_length: int = 256,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ):
        device = device or self._execution_device
        dtype = dtype or self.text_encoder.dtype
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        if self.text_encoder_3 is None:
            return torch.zeros(
                (
                    batch_size * num_images_per_prompt,
                    self.tokenizer_max_length,
                    self.transformer.config.joint_attention_dim,
                ),
                device=device,
                dtype=dtype,
            )
        text_inputs = self.tokenizer_3(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=max_sequence_length,
            truncation=True,
            add_special_tokens=True,
            return_tensors=&quot;pt&quot;,
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = self.tokenizer_3(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = self.tokenizer_3.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            logger.warning(
                &quot;The following part of your input was truncated because `max_sequence_length` is set to &quot;
                f&quot; {max_sequence_length} tokens: {removed_text}&quot;
            )
        prompt_embeds = self.text_encoder_3(text_input_ids.to(device))[0]
        dtype = self.text_encoder_3.dtype
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
        _, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )
        return prompt_embeds
    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3.StableDiffusion3Pipeline._get_clip_prompt_embeds
    def _get_clip_prompt_embeds(
        self,
        prompt: Union[str, List[str]],
        num_images_per_prompt: int = 1,
        device: Optional[torch.device] = None,
        clip_skip: Optional[int] = None,
        clip_model_index: int = 0,
    ):
        device = device or self._execution_device
        clip_tokenizers = [self.tokenizer, self.tokenizer_2]
        clip_text_encoders = [self.text_encoder, self.text_encoder_2]
        tokenizer = clip_tokenizers[clip_model_index]
        text_encoder = clip_text_encoders[clip_model_index]
        prompt = [prompt] if isinstance(prompt, str) else prompt
        batch_size = len(prompt)
        text_inputs = tokenizer(
            prompt,
            padding=&quot;max_length&quot;,
            max_length=self.tokenizer_max_length,
            truncation=True,
            return_tensors=&quot;pt&quot;,
        )
        text_input_ids = text_inputs.input_ids
        untruncated_ids = tokenizer(
            prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
        ).input_ids
        if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[-1] and not torch.equal(
            text_input_ids, untruncated_ids
        ):
            removed_text = tokenizer.batch_decode(
                untruncated_ids[:, self.tokenizer_max_length - 1 : -1]
            )
            logger.warning(
                &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
                f&quot; {self.tokenizer_max_length} tokens: {removed_text}&quot;
            )
        prompt_embeds = text_encoder(
            text_input_ids.to(device), output_hidden_states=True
        )
        pooled_prompt_embeds = prompt_embeds[0]
        if clip_skip is None:
            prompt_embeds = prompt_embeds.hidden_states[-2]
        else:
            prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]
        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)
        _, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            batch_size * num_images_per_prompt, seq_len, -1
        )
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(1, num_images_per_prompt, 1)
        pooled_prompt_embeds = pooled_prompt_embeds.view(
            batch_size * num_images_per_prompt, -1
        )
        return prompt_embeds, pooled_prompt_embeds
    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3.StableDiffusion3Pipeline.encode_prompt
    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        prompt_2: Union[str, List[str]],
        prompt_3: Union[str, List[str]],
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        negative_prompt_3: Optional[Union[str, List[str]]] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        clip_skip: Optional[int] = None,
        max_sequence_length: int = 256,
        lora_scale: Optional[float] = None,
    ):
        r&quot;&quot;&quot;
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in all text-encoders
            prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is
                used in all text-encoders
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in all the text-encoders.
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and
                `text_encoder_3`. If not defined, `negative_prompt` is used in both text-encoders
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            lora_scale (`float`, *optional*):
                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
        &quot;&quot;&quot;
        device = device or self._execution_device
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(self, SD3LoraLoaderMixin):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if self.text_encoder is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder, lora_scale)
            if self.text_encoder_2 is not None and USE_PEFT_BACKEND:
                scale_lora_layers(self.text_encoder_2, lora_scale)
        prompt = [prompt] if isinstance(prompt, str) else prompt
        if prompt is not None:
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        if prompt_embeds is None:
            prompt_2 = prompt_2 or prompt
            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2
            prompt_3 = prompt_3 or prompt
            prompt_3 = [prompt_3] if isinstance(prompt_3, str) else prompt_3
            prompt_embed, pooled_prompt_embed = self._get_clip_prompt_embeds(
                prompt=prompt,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
                clip_skip=clip_skip,
                clip_model_index=0,
            )
            prompt_2_embed, pooled_prompt_2_embed = self._get_clip_prompt_embeds(
                prompt=prompt_2,
                device=device,
                num_images_per_prompt=num_images_per_prompt,
                clip_skip=clip_skip,
                clip_model_index=1,
            )
            clip_prompt_embeds = torch.cat([prompt_embed, prompt_2_embed], dim=-1)
            t5_prompt_embed = self._get_t5_prompt_embeds(
                prompt=prompt_3,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                device=device,
            )
            clip_prompt_embeds = torch.nn.functional.pad(
                clip_prompt_embeds,
                (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1]),
            )
            prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)
            pooled_prompt_embeds = torch.cat(
                [pooled_prompt_embed, pooled_prompt_2_embed], dim=-1
            )
        if do_classifier_free_guidance and negative_prompt_embeds is None:
            negative_prompt = negative_prompt or &quot;&quot;
            negative_prompt_2 = negative_prompt_2 or negative_prompt
            negative_prompt_3 = negative_prompt_3 or negative_prompt
            # normalize str to list
            negative_prompt = (
                batch_size * [negative_prompt]
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            negative_prompt_2 = (
                batch_size * [negative_prompt_2]
                if isinstance(negative_prompt_2, str)
                else negative_prompt_2
            )
            negative_prompt_3 = (
                batch_size * [negative_prompt_3]
                if isinstance(negative_prompt_3, str)
                else negative_prompt_3
            )
            if prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            negative_prompt_embed, negative_pooled_prompt_embed = (
                self._get_clip_prompt_embeds(
                    negative_prompt,
                    device=device,
                    num_images_per_prompt=num_images_per_prompt,
                    clip_skip=None,
                    clip_model_index=0,
                )
            )
            negative_prompt_2_embed, negative_pooled_prompt_2_embed = (
                self._get_clip_prompt_embeds(
                    negative_prompt_2,
                    device=device,
                    num_images_per_prompt=num_images_per_prompt,
                    clip_skip=None,
                    clip_model_index=1,
                )
            )
            negative_clip_prompt_embeds = torch.cat(
                [negative_prompt_embed, negative_prompt_2_embed], dim=-1
            )
            t5_negative_prompt_embed = self._get_t5_prompt_embeds(
                prompt=negative_prompt_3,
                num_images_per_prompt=num_images_per_prompt,
                max_sequence_length=max_sequence_length,
                device=device,
            )
            negative_clip_prompt_embeds = torch.nn.functional.pad(
                negative_clip_prompt_embeds,
                (
                    0,
                    t5_negative_prompt_embed.shape[-1]
                    - negative_clip_prompt_embeds.shape[-1],
                ),
            )
            negative_prompt_embeds = torch.cat(
                [negative_clip_prompt_embeds, t5_negative_prompt_embed], dim=-2
            )
            negative_pooled_prompt_embeds = torch.cat(
                [negative_pooled_prompt_embed, negative_pooled_prompt_2_embed], dim=-1
            )
        if self.text_encoder is not None:
            if isinstance(self, SD3LoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder, lora_scale)
        if self.text_encoder_2 is not None:
            if isinstance(self, SD3LoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder_2, lora_scale)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    def check_inputs(
        self,
        prompt,
        prompt_2,
        prompt_3,
        strength,
        negative_prompt=None,
        negative_prompt_2=None,
        negative_prompt_3=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        pooled_prompt_embeds=None,
        negative_pooled_prompt_embeds=None,
        callback_on_step_end_tensor_inputs=None,
        max_sequence_length=None,
    ):
        if strength &lt; 0 or strength &gt; 1:
            raise ValueError(
                f&quot;The value of strength should in [0.0, 1.0] but is {strength}&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_2 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_3 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_3`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        elif prompt_2 is not None and (
            not isinstance(prompt_2, str) and not isinstance(prompt_2, list)
        ):
            raise ValueError(
                f&quot;`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}&quot;
            )
        elif prompt_3 is not None and (
            not isinstance(prompt_3, str) and not isinstance(prompt_3, list)
        ):
            raise ValueError(
                f&quot;`prompt_3` has to be of type `str` or `list` but is {type(prompt_3)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_3 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_3`: {negative_prompt_3} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.&quot;
            )
        if max_sequence_length is not None and max_sequence_length &gt; 512:
            raise ValueError(
                f&quot;`max_sequence_length` cannot be greater than 512 but is {max_sequence_length}&quot;
            )
    def get_timesteps(self, num_inference_steps, strength, device):
        # get the original timestep using init_timestep
        init_timestep = min(num_inference_steps * strength, num_inference_steps)
        t_start = int(max(num_inference_steps - init_timestep, 0))
        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]
        if hasattr(self.scheduler, &quot;set_begin_index&quot;):
            self.scheduler.set_begin_index(t_start * self.scheduler.order)
        return timesteps, num_inference_steps - t_start
    def prepare_latents(
        self,
        image,
        timestep,
        batch_size,
        num_images_per_prompt,
        dtype,
        device,
        generator=None,
    ):
        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):
            raise ValueError(
                f&quot;`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}&quot;
            )
        image = image.to(device=device, dtype=dtype)
        batch_size = batch_size * num_images_per_prompt
        if image.shape[1] == self.vae.config.latent_channels:
            init_latents = image
        else:
            if isinstance(generator, list) and len(generator) != batch_size:
                raise ValueError(
                    f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                    f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
                )
            elif isinstance(generator, list):
                init_latents = [
                    retrieve_latents(
                        self.vae.encode(image[i : i + 1]), generator=generator[i]
                    )
                    for i in range(batch_size)
                ]
                init_latents = torch.cat(init_latents, dim=0)
            else:
                init_latents = retrieve_latents(
                    self.vae.encode(image), generator=generator
                )
            init_latents = (
                init_latents - self.vae.config.shift_factor
            ) * self.vae.config.scaling_factor
        if (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] == 0
        ):
            # expand init_latents for batch_size
            additional_image_per_prompt = batch_size // init_latents.shape[0]
            init_latents = torch.cat(
                [init_latents] * additional_image_per_prompt, dim=0
            )
        elif (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] != 0
        ):
            raise ValueError(
                f&quot;Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.&quot;
            )
        else:
            init_latents = torch.cat([init_latents], dim=0)
        shape = init_latents.shape
        noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        # get latents
        init_latents = self.scheduler.scale_noise(init_latents, timestep, noise)
        latents = init_latents.to(device=device, dtype=dtype)
        return latents
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def joint_attention_kwargs(self):
        return self._joint_attention_kwargs
    @property
    def clip_skip(self):
        return self._clip_skip
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        prompt_2: Optional[Union[str, List[str]]] = None,
        prompt_3: Optional[Union[str, List[str]]] = None,
        image: PipelineImageInput = None,
        strength: float = 0.6,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        guidance_scale: float = 7.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        negative_prompt_3: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        clip_skip: Optional[int] = None,
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        max_sequence_length: int = 256,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                will be used instead
            prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to `tokenizer_3` and `text_encoder_3`. If not defined, `prompt` is
                will be used instead
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            guidance_scale (`float`, *optional*, defaults to 7.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used instead
            negative_prompt_3 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_3` and
                `text_encoder_3`. If not defined, `negative_prompt` is used instead
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] instead of
                a plain tuple.
            joint_attention_kwargs (`dict`, *optional*):
               A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
            max_sequence_length (`int` defaults to 256): Maximum sequence length to use with the `prompt`.
        Examples:
        Returns:
            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion_3.StableDiffusion3PipelineOutput`] if `return_dict` is True, otherwise a
            `tuple`. When returning a tuple, the first element is a list with the generated images.
        &quot;&quot;&quot;
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            prompt_2,
            prompt_3,
            strength,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            negative_prompt_3=negative_prompt_3,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,
            max_sequence_length=max_sequence_length,
        )
        self._guidance_scale = guidance_scale
        self._clip_skip = clip_skip
        self._joint_attention_kwargs = joint_attention_kwargs
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        lora_scale = (
            self.joint_attention_kwargs.get(&quot;scale&quot;, None)
            if self.joint_attention_kwargs is not None
            else None
        )
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt_2,
            prompt_3=prompt_3,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            negative_prompt_3=negative_prompt_3,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            device=device,
            clip_skip=self.clip_skip,
            num_images_per_prompt=num_images_per_prompt,
            max_sequence_length=max_sequence_length,
            lora_scale=lora_scale,
        )
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            pooled_prompt_embeds = torch.cat(
                [negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0
            )
        # 3. Preprocess image
        image = self.image_processor.preprocess(image)
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps
        )
        timesteps, num_inference_steps = self.get_timesteps(
            num_inference_steps, strength, device
        )
        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)
        # 5. Prepare latent variables
        if latents is None:
            latents = self.prepare_latents(
                image,
                latent_timestep,
                batch_size,
                num_images_per_prompt,
                prompt_embeds.dtype,
                device,
                generator,
            )
        # 6. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                # broadcast to batch dimension in a way that&apos;s compatible with ONNX/Core ML
                timestep = t.expand(latent_model_input.shape[0])
                noise_pred = self.transformer(
                    hidden_states=latent_model_input,
                    timestep=timestep,
                    encoder_hidden_states=prompt_embeds,
                    pooled_projections=pooled_prompt_embeds,
                    joint_attention_kwargs=self.joint_attention_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                if XLA_AVAILABLE:
                    xm.mark_step()
        if output_type == &quot;latent&quot;:
            image = latents
        else:
            latents = (
                latents / self.vae.config.scaling_factor
            ) + self.vae.config.shift_factor
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype), return_dict=False
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return StableDiffusion3PipelineOutput(images=image)</file><file path="helpers/models/sd3/transformer.py"># Copyright 2024 Stability AI, The HuggingFace Team and The InstantX Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Any, Dict, List, Optional, Tuple, Union
import torch
import torch.nn as nn
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.loaders import FromOriginalModelMixin, PeftAdapterMixin
from diffusers.models.attention import JointTransformerBlock
from diffusers.models.attention_processor import (
    Attention,
    AttentionProcessor,
    FusedJointAttnProcessor2_0,
)
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import AdaLayerNormContinuous
from diffusers.utils import (
    USE_PEFT_BACKEND,
    is_torch_version,
    logging,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.models.embeddings import (
    CombinedTimestepTextProjEmbeddings,
    PatchEmbed,
)
from diffusers.models.modeling_outputs import (
    Transformer2DModelOutput,
)
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
class SD3Transformer2DModel(
    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin
):
    &quot;&quot;&quot;
    The Transformer model introduced in Stable Diffusion 3.
    Reference: https://arxiv.org/abs/2403.03206
    Parameters:
        sample_size (`int`): The width of the latent images. This is fixed during training since
            it is used to learn a number of position embeddings.
        patch_size (`int`): Patch size to turn the input data into small patches.
        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.
        num_layers (`int`, *optional*, defaults to 18): The number of layers of Transformer blocks to use.
        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.
        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.
        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.
        caption_projection_dim (`int`): Number of dimensions to use when projecting the `encoder_hidden_states`.
        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.
        out_channels (`int`, defaults to 16): Number of output channels.
    &quot;&quot;&quot;
    _supports_gradient_checkpointing = True
    @register_to_config
    def __init__(
        self,
        sample_size: int = 128,
        patch_size: int = 2,
        in_channels: int = 16,
        num_layers: int = 18,
        attention_head_dim: int = 64,
        num_attention_heads: int = 18,
        joint_attention_dim: int = 4096,
        caption_projection_dim: int = 1152,
        pooled_projection_dim: int = 2048,
        out_channels: int = 16,
        pos_embed_max_size: int = 96,
        dual_attention_layers: Tuple[
            int, ...
        ] = (),  # () for sd3.0; (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) for sd3.5
        qk_norm: Optional[str] = None,
    ):
        super().__init__()
        default_out_channels = in_channels
        self.out_channels = (
            out_channels if out_channels is not None else default_out_channels
        )
        self.inner_dim = (
            self.config.num_attention_heads * self.config.attention_head_dim
        )
        self.pos_embed = PatchEmbed(
            height=self.config.sample_size,
            width=self.config.sample_size,
            patch_size=self.config.patch_size,
            in_channels=self.config.in_channels,
            embed_dim=self.inner_dim,
            pos_embed_max_size=pos_embed_max_size,  # hard-code for now.
        )
        self.time_text_embed = CombinedTimestepTextProjEmbeddings(
            embedding_dim=self.inner_dim,
            pooled_projection_dim=self.config.pooled_projection_dim,
        )
        self.context_embedder = nn.Linear(
            self.config.joint_attention_dim, self.config.caption_projection_dim
        )
        # `attention_head_dim` is doubled to account for the mixing.
        # It needs to crafted when we get the actual checkpoints.
        self.transformer_blocks = nn.ModuleList(
            [
                JointTransformerBlock(
                    dim=self.inner_dim,
                    num_attention_heads=self.config.num_attention_heads,
                    attention_head_dim=self.config.attention_head_dim,
                    context_pre_only=i == num_layers - 1,
                    qk_norm=qk_norm,
                    use_dual_attention=True if i in dual_attention_layers else False,
                )
                for i in range(self.config.num_layers)
            ]
        )
        self.norm_out = AdaLayerNormContinuous(
            self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6
        )
        self.proj_out = nn.Linear(
            self.inner_dim, patch_size * patch_size * self.out_channels, bias=True
        )
        self.gradient_checkpointing = False
        self.gradient_checkpointing_interval = None
    def set_gradient_checkpointing_interval(self, interval: int):
        &quot;&quot;&quot;
        Sets the interval for gradient checkpointing.
        Parameters:
            interval (`int`): The interval for gradient checkpointing.
        &quot;&quot;&quot;
        self.gradient_checkpointing_interval = interval
    # Copied from diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.enable_forward_chunking
    def enable_forward_chunking(
        self, chunk_size: Optional[int] = None, dim: int = 0
    ) -&gt; None:
        &quot;&quot;&quot;
        Sets the attention processor to use [feed forward
        chunking](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers).
        Parameters:
            chunk_size (`int`, *optional*):
                The chunk size of the feed-forward layers. If not specified, will run feed-forward layer individually
                over each tensor of dim=`dim`.
            dim (`int`, *optional*, defaults to `0`):
                The dimension over which the feed-forward computation should be chunked. Choose between dim=0 (batch)
                or dim=1 (sequence length).
        &quot;&quot;&quot;
        if dim not in [0, 1]:
            raise ValueError(f&quot;Make sure to set `dim` to either 0 or 1, not {dim}&quot;)
        # By default chunk size is 1
        chunk_size = chunk_size or 1
        def fn_recursive_feed_forward(
            module: torch.nn.Module, chunk_size: int, dim: int
        ):
            if hasattr(module, &quot;set_chunk_feed_forward&quot;):
                module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
            for child in module.children():
                fn_recursive_feed_forward(child, chunk_size, dim)
        for module in self.children():
            fn_recursive_feed_forward(module, chunk_size, dim)
    # Copied from diffusers.models.unets.unet_3d_condition.UNet3DConditionModel.disable_forward_chunking
    def disable_forward_chunking(self):
        def fn_recursive_feed_forward(
            module: torch.nn.Module, chunk_size: int, dim: int
        ):
            if hasattr(module, &quot;set_chunk_feed_forward&quot;):
                module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)
            for child in module.children():
                fn_recursive_feed_forward(child, chunk_size, dim)
        for module in self.children():
            fn_recursive_feed_forward(module, None, 0)
    @property
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors
    def attn_processors(self) -&gt; Dict[str, AttentionProcessor]:
        r&quot;&quot;&quot;
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        &quot;&quot;&quot;
        # set recursively
        processors = {}
        def fn_recursive_add_processors(
            name: str,
            module: torch.nn.Module,
            processors: Dict[str, AttentionProcessor],
        ):
            if hasattr(module, &quot;get_processor&quot;):
                processors[f&quot;{name}.processor&quot;] = module.get_processor()
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f&quot;{name}.{sub_name}&quot;, child, processors)
            return processors
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
        return processors
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor
    def set_attn_processor(
        self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]]
    ):
        r&quot;&quot;&quot;
        Sets the attention processor to use to compute attention.
        Parameters:
            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                for **all** `Attention` layers.
                If `processor` is a dict, the key needs to define the path to the corresponding cross attention
                processor. This is strongly recommended when setting trainable attention processors.
        &quot;&quot;&quot;
        count = len(self.attn_processors.keys())
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(
                f&quot;A dict of processors was passed, but the number of processors {len(processor)} does not match the&quot;
                f&quot; number of attention layers: {count}. Please make sure to pass {count} processor classes.&quot;
            )
        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
            if hasattr(module, &quot;set_processor&quot;):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f&quot;{name}.processor&quot;))
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f&quot;{name}.{sub_name}&quot;, child, processor)
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections with FusedAttnProcessor2_0-&gt;FusedJointAttnProcessor2_0
    def fuse_qkv_projections(self):
        &quot;&quot;&quot;
        Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)
        are fused. For cross-attention modules, key and value projection matrices are fused.
        &lt;Tip warning={true}&gt;
        This API is 🧪 experimental.
        &lt;/Tip&gt;
        &quot;&quot;&quot;
        self.original_attn_processors = None
        for _, attn_processor in self.attn_processors.items():
            if &quot;Added&quot; in str(attn_processor.__class__.__name__):
                raise ValueError(
                    &quot;`fuse_qkv_projections()` is not supported for models having added KV projections.&quot;
                )
        self.original_attn_processors = self.attn_processors
        for module in self.modules():
            if isinstance(module, Attention):
                module.fuse_projections(fuse=True)
        self.set_attn_processor(FusedJointAttnProcessor2_0())
    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
    def unfuse_qkv_projections(self):
        &quot;&quot;&quot;Disables the fused QKV projection if enabled.
        &lt;Tip warning={true}&gt;
        This API is 🧪 experimental.
        &lt;/Tip&gt;
        &quot;&quot;&quot;
        if self.original_attn_processors is not None:
            self.set_attn_processor(self.original_attn_processors)
    def _set_gradient_checkpointing(self, module, value=False):
        if hasattr(module, &quot;gradient_checkpointing&quot;):
            module.gradient_checkpointing = value
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: torch.FloatTensor = None,
        pooled_projections: torch.FloatTensor = None,
        timestep: torch.LongTensor = None,
        block_controlnet_hidden_states: List = None,
        joint_attention_kwargs: Optional[Dict[str, Any]] = None,
        return_dict: bool = True,
        skip_layers: Optional[List[int]] = None,
    ) -&gt; Union[torch.FloatTensor, Transformer2DModelOutput]:
        &quot;&quot;&quot;
        The [`SD3Transformer2DModel`] forward method.
        Args:
            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):
                Input `hidden_states`.
            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):
                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.
            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected
                from the embeddings of input conditions.
            timestep (`torch.LongTensor`):
                Used to indicate denoising step.
            block_controlnet_hidden_states (`list` of `torch.Tensor`):
                A list of tensors that if specified are added to the residuals of transformer blocks.
            joint_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain
                tuple.
            skip_layers (`list` of `int`, *optional*):
                A list of layer indices to skip during the forward pass.
        Returns:
            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a
            `tuple` where the first element is the sample tensor.
        &quot;&quot;&quot;
        if joint_attention_kwargs is not None:
            joint_attention_kwargs = joint_attention_kwargs.copy()
            lora_scale = joint_attention_kwargs.pop(&quot;scale&quot;, 1.0)
        else:
            lora_scale = 1.0
        if USE_PEFT_BACKEND:
            # weight the lora layers by setting `lora_scale` for each PEFT layer
            scale_lora_layers(self, lora_scale)
        else:
            if (
                joint_attention_kwargs is not None
                and joint_attention_kwargs.get(&quot;scale&quot;, None) is not None
            ):
                logger.warning(
                    &quot;Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.&quot;
                )
        height, width = hidden_states.shape[-2:]
        hidden_states = self.pos_embed(
            hidden_states
        )  # takes care of adding positional embeddings too.
        temb = self.time_text_embed(timestep, pooled_projections)
        encoder_hidden_states = self.context_embedder(encoder_hidden_states)
        for index_block, block in enumerate(self.transformer_blocks):
            # Skip specified layers
            if skip_layers is not None and index_block in skip_layers:
                if (
                    block_controlnet_hidden_states is not None
                    and block.context_pre_only is False
                ):
                    interval_control = len(self.transformer_blocks) // len(
                        block_controlnet_hidden_states
                    )
                    hidden_states = (
                        hidden_states
                        + block_controlnet_hidden_states[
                            index_block // interval_control
                        ]
                    )
                continue
            if (
                self.training
                and self.gradient_checkpointing
                and (
                    self.gradient_checkpointing_interval is None
                    or index_block % self.gradient_checkpointing_interval == 0
                )
            ):
                def create_custom_forward(module, return_dict=None):
                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: Dict[str, Any] = (
                    {&quot;use_reentrant&quot;: False} if is_torch_version(&quot;&gt;=&quot;, &quot;1.11.0&quot;) else {}
                )
                encoder_hidden_states, hidden_states = (
                    torch.utils.checkpoint.checkpoint(
                        create_custom_forward(block),
                        hidden_states,
                        encoder_hidden_states,
                        temb,
                        **ckpt_kwargs,
                    )
                )
            else:
                encoder_hidden_states, hidden_states = block(
                    hidden_states=hidden_states,
                    encoder_hidden_states=encoder_hidden_states,
                    temb=temb,
                )
            # controlnet residual
            if (
                block_controlnet_hidden_states is not None
                and block.context_pre_only is False
            ):
                interval_control = len(self.transformer_blocks) // len(
                    block_controlnet_hidden_states
                )
                hidden_states = (
                    hidden_states
                    + block_controlnet_hidden_states[index_block // interval_control]
                )
        hidden_states = self.norm_out(hidden_states, temb)
        hidden_states = self.proj_out(hidden_states)
        # unpatchify
        patch_size = self.config.patch_size
        height = height // patch_size
        width = width // patch_size
        hidden_states = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                height,
                width,
                patch_size,
                patch_size,
                self.out_channels,
            )
        )
        hidden_states = torch.einsum(&quot;nhwpqc-&gt;nchpwq&quot;, hidden_states)
        output = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                self.out_channels,
                height * patch_size,
                width * patch_size,
            )
        )
        if USE_PEFT_BACKEND:
            # remove `lora_scale` from each PEFT layer
            unscale_lora_layers(self, lora_scale)
        if not return_dict:
            return (output,)
        return Transformer2DModelOutput(sample=output)</file><file path="helpers/models/sdxl/pipeline.py"># Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import PIL
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
from helpers.training.state_tracker import StateTracker
from diffusers.callbacks import PipelineCallback, MultiPipelineCallbacks
import torch
from transformers import (
    CLIPImageProcessor,
    CLIPTextModel,
    CLIPTextModelWithProjection,
    CLIPTokenizer,
    CLIPVisionModelWithProjection,
)
from diffusers.image_processor import PipelineImageInput, VaeImageProcessor
from diffusers.loaders import (
    FromSingleFileMixin,
    IPAdapterMixin,
    StableDiffusionXLLoraLoaderMixin,
    TextualInversionLoaderMixin,
)
from diffusers.models import AutoencoderKL, ImageProjection, UNet2DConditionModel
from diffusers.models.attention_processor import (
    AttnProcessor2_0,
    FusedAttnProcessor2_0,
    XFormersAttnProcessor,
)
from diffusers.models.lora import adjust_lora_scale_text_encoder
from diffusers.schedulers import KarrasDiffusionSchedulers
from diffusers.utils import (
    USE_PEFT_BACKEND,
    deprecate,
    is_invisible_watermark_available,
    is_torch_xla_available,
    logging,
    replace_example_docstring,
    scale_lora_layers,
    unscale_lora_layers,
)
from diffusers.utils.torch_utils import randn_tensor
from diffusers.pipelines.pipeline_utils import DiffusionPipeline, StableDiffusionMixin
from diffusers.pipelines.stable_diffusion_xl.pipeline_output import (
    StableDiffusionXLPipelineOutput,
)
if is_invisible_watermark_available():
    from diffusers.pipelines.stable_diffusion_xl.watermark import (
        StableDiffusionXLWatermarker,
    )
if is_torch_xla_available():
    import torch_xla.core.xla_model as xm
    XLA_AVAILABLE = True
else:
    XLA_AVAILABLE = False
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
EXAMPLE_DOC_STRING = &quot;&quot;&quot;
    Examples:
        ```py
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from diffusers import StableDiffusionXLPipeline
        &gt;&gt;&gt; pipe = StableDiffusionXLPipeline.from_pretrained(
        ...     &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.float16
        ... )
        &gt;&gt;&gt; pipe = pipe.to(&quot;cuda&quot;)
        &gt;&gt;&gt; prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
        &gt;&gt;&gt; image = pipe(prompt).images[0]
        ```
&quot;&quot;&quot;
def retrieve_latents(
    encoder_output: torch.Tensor,
    generator: Optional[torch.Generator] = None,
    sample_mode: str = &quot;sample&quot;,
):
    if hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;sample&quot;:
        return encoder_output.latent_dist.sample(generator)
    elif hasattr(encoder_output, &quot;latent_dist&quot;) and sample_mode == &quot;argmax&quot;:
        return encoder_output.latent_dist.mode()
    elif hasattr(encoder_output, &quot;latents&quot;):
        return encoder_output.latents
    else:
        raise AttributeError(&quot;Could not access latents of provided encoder_output&quot;)
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.rescale_noise_cfg
def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):
    &quot;&quot;&quot;
    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and
    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4
    &quot;&quot;&quot;
    std_text = noise_pred_text.std(
        dim=list(range(1, noise_pred_text.ndim)), keepdim=True
    )
    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)
    # rescale the results from guidance (fixes overexposure)
    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)
    # mix with the original results from guidance by factor guidance_rescale to avoid &quot;plain looking&quot; images
    noise_cfg = (
        guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg
    )
    return noise_cfg
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class StableDiffusionXLPipeline(
    DiffusionPipeline,
    StableDiffusionMixin,
    FromSingleFileMixin,
    StableDiffusionXLLoraLoaderMixin,
    TextualInversionLoaderMixin,
    IPAdapterMixin,
):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using Stable Diffusion XL.
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)
    The pipeline also inherits the following loading methods:
        - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings
        - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights
        - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters
    Args:
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`CLIPTextModel`]):
            Frozen text-encoder. Stable Diffusion XL uses the text portion of
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically
            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.
        text_encoder_2 ([` CLIPTextModelWithProjection`]):
            Second frozen text-encoder. Stable Diffusion XL uses the text and pool portion of
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the
            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
            variant.
        tokenizer (`CLIPTokenizer`):
            Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_2 (`CLIPTokenizer`):
            Second Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.
        scheduler ([`SchedulerMixin`]):
            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of
            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].
        force_zeros_for_empty_prompt (`bool`, *optional*, defaults to `&quot;True&quot;`):
            Whether the negative prompt embeddings shall be forced to always be set to 0. Also see the config of
            `stabilityai/stable-diffusion-xl-base-1-0`.
        add_watermarker (`bool`, *optional*):
            Whether to use the [invisible_watermark library](https://github.com/ShieldMnt/invisible-watermark/) to
            watermark output images. If not defined, it will default to True if the package is installed, otherwise no
            watermarker will be used.
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;text_encoder_2-&gt;image_encoder-&gt;unet-&gt;vae&quot;
    _optional_components = [
        &quot;tokenizer&quot;,
        &quot;tokenizer_2&quot;,
        &quot;text_encoder&quot;,
        &quot;text_encoder_2&quot;,
        &quot;image_encoder&quot;,
        &quot;feature_extractor&quot;,
    ]
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;add_text_embeds&quot;,
        &quot;add_time_ids&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
        &quot;negative_add_time_ids&quot;,
    ]
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModel,
        text_encoder_2: CLIPTextModelWithProjection,
        tokenizer: CLIPTokenizer,
        tokenizer_2: CLIPTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        image_encoder: CLIPVisionModelWithProjection = None,
        feature_extractor: CLIPImageProcessor = None,
        force_zeros_for_empty_prompt: bool = True,
        add_watermarker: Optional[bool] = None,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            text_encoder_2=text_encoder_2,
            tokenizer=tokenizer,
            tokenizer_2=tokenizer_2,
            unet=unet,
            scheduler=scheduler,
            image_encoder=image_encoder,
            feature_extractor=feature_extractor,
        )
        self.register_to_config(
            force_zeros_for_empty_prompt=force_zeros_for_empty_prompt
        )
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        self.default_sample_size = self.unet.config.sample_size
        add_watermarker = (
            add_watermarker
            if add_watermarker is not None
            else is_invisible_watermark_available()
        )
        if add_watermarker:
            self.watermark = StableDiffusionXLWatermarker()
        else:
            self.watermark = None
    def encode_prompt(
        self,
        prompt: str,
        prompt_2: Optional[str] = None,
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt: Optional[str] = None,
        negative_prompt_2: Optional[str] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        lora_scale: Optional[float] = None,
        clip_skip: Optional[int] = None,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in both text-encoders
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            lora_scale (`float`, *optional*):
                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
        &quot;&quot;&quot;
        device = device or self._execution_device
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(
            self, StableDiffusionXLLoraLoaderMixin
        ):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if self.text_encoder is not None:
                if not USE_PEFT_BACKEND:
                    adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)
                else:
                    scale_lora_layers(self.text_encoder, lora_scale)
            if self.text_encoder_2 is not None:
                if not USE_PEFT_BACKEND:
                    adjust_lora_scale_text_encoder(self.text_encoder_2, lora_scale)
                else:
                    scale_lora_layers(self.text_encoder_2, lora_scale)
        prompt = [prompt] if isinstance(prompt, str) else prompt
        if prompt is not None:
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        # Define tokenizers and text encoders
        tokenizers = (
            [self.tokenizer, self.tokenizer_2]
            if self.tokenizer is not None
            else [self.tokenizer_2]
        )
        text_encoders = (
            [self.text_encoder, self.text_encoder_2]
            if self.text_encoder is not None
            else [self.text_encoder_2]
        )
        if prompt_embeds is None:
            prompt_2 = prompt_2 or prompt
            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2
            # textual inversion: process multi-vector tokens if necessary
            prompt_embeds_list = []
            prompts = [prompt, prompt_2]
            for prompt, tokenizer, text_encoder in zip(
                prompts, tokenizers, text_encoders
            ):
                if isinstance(self, TextualInversionLoaderMixin):
                    prompt = self.maybe_convert_prompt(prompt, tokenizer)
                text_inputs = tokenizer(
                    prompt,
                    padding=&quot;max_length&quot;,
                    max_length=tokenizer.model_max_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                )
                text_input_ids = text_inputs.input_ids
                untruncated_ids = tokenizer(
                    prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
                ).input_ids
                if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[
                    -1
                ] and not torch.equal(text_input_ids, untruncated_ids):
                    removed_text = tokenizer.batch_decode(
                        untruncated_ids[:, tokenizer.model_max_length - 1 : -1]
                    )
                    logger.warning(
                        &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
                        f&quot; {tokenizer.model_max_length} tokens: {removed_text}&quot;
                    )
                prompt_embeds = text_encoder(
                    text_input_ids.to(device), output_hidden_states=True
                )
                # We are only ALWAYS interested in the pooled output of the final text encoder
                pooled_prompt_embeds = (
                    prompt_embeds[0]
                    if pooled_prompt_embeds is None
                    else pooled_prompt_embeds
                )
                if clip_skip is None:
                    prompt_embeds = prompt_embeds.hidden_states[-2]
                else:
                    # &quot;2&quot; because SDXL always indexes from the penultimate layer.
                    prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]
                prompt_embeds_list.append(prompt_embeds)
            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)
        # get unconditional embeddings for classifier free guidance
        zero_out_negative_prompt = (
            negative_prompt is None and self.config.force_zeros_for_empty_prompt
        )
        if (
            do_classifier_free_guidance
            and negative_prompt_embeds is None
            and zero_out_negative_prompt
        ):
            negative_prompt_embeds = torch.zeros_like(prompt_embeds)
            negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)
        elif do_classifier_free_guidance and negative_prompt_embeds is None:
            negative_prompt = negative_prompt or &quot;&quot;
            negative_prompt_2 = negative_prompt_2 or negative_prompt
            # normalize str to list
            negative_prompt = (
                batch_size * [negative_prompt]
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            negative_prompt_2 = (
                batch_size * [negative_prompt_2]
                if isinstance(negative_prompt_2, str)
                else negative_prompt_2
            )
            uncond_tokens: List[str]
            if prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            else:
                uncond_tokens = [negative_prompt, negative_prompt_2]
            negative_prompt_embeds_list = []
            for negative_prompt, tokenizer, text_encoder in zip(
                uncond_tokens, tokenizers, text_encoders
            ):
                if isinstance(self, TextualInversionLoaderMixin):
                    negative_prompt = self.maybe_convert_prompt(
                        negative_prompt, tokenizer
                    )
                max_length = prompt_embeds.shape[1]
                uncond_input = tokenizer(
                    negative_prompt,
                    padding=&quot;max_length&quot;,
                    max_length=max_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                )
                negative_prompt_embeds = text_encoder(
                    uncond_input.input_ids.to(device),
                    output_hidden_states=True,
                )
                # We are only ALWAYS interested in the pooled output of the final text encoder
                negative_pooled_prompt_embeds = negative_prompt_embeds[0]
                negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]
                negative_prompt_embeds_list.append(negative_prompt_embeds)
            negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)
        if self.text_encoder_2 is not None:
            prompt_embeds = prompt_embeds.to(
                dtype=self.text_encoder_2.dtype, device=device
            )
        else:
            prompt_embeds = prompt_embeds.to(dtype=self.unet.dtype, device=device)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )
        if do_classifier_free_guidance:
            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = negative_prompt_embeds.shape[1]
            if self.text_encoder_2 is not None:
                negative_prompt_embeds = negative_prompt_embeds.to(
                    dtype=self.text_encoder_2.dtype, device=device
                )
            else:
                negative_prompt_embeds = negative_prompt_embeds.to(
                    dtype=self.unet.dtype, device=device
                )
            negative_prompt_embeds = negative_prompt_embeds.repeat(
                1, num_images_per_prompt, 1
            )
            negative_prompt_embeds = negative_prompt_embeds.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)
        if do_classifier_free_guidance:
            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(
                1, num_images_per_prompt
            ).view(bs_embed * num_images_per_prompt, -1)
        if self.text_encoder is not None:
            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder, lora_scale)
        if self.text_encoder_2 is not None:
            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder_2, lora_scale)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.encode_image
    def encode_image(
        self, image, device, num_images_per_prompt, output_hidden_states=None
    ):
        dtype = next(self.image_encoder.parameters()).dtype
        if not isinstance(image, torch.Tensor):
            image = self.feature_extractor(image, return_tensors=&quot;pt&quot;).pixel_values
        image = image.to(device=device, dtype=dtype)
        if output_hidden_states:
            image_enc_hidden_states = self.image_encoder(
                image, output_hidden_states=True
            ).hidden_states[-2]
            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(
                num_images_per_prompt, dim=0
            )
            uncond_image_enc_hidden_states = self.image_encoder(
                torch.zeros_like(image), output_hidden_states=True
            ).hidden_states[-2]
            uncond_image_enc_hidden_states = (
                uncond_image_enc_hidden_states.repeat_interleave(
                    num_images_per_prompt, dim=0
                )
            )
            return image_enc_hidden_states, uncond_image_enc_hidden_states
        else:
            image_embeds = self.image_encoder(image).image_embeds
            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)
            uncond_image_embeds = torch.zeros_like(image_embeds)
            return image_embeds, uncond_image_embeds
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_ip_adapter_image_embeds
    def prepare_ip_adapter_image_embeds(
        self,
        ip_adapter_image,
        ip_adapter_image_embeds,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
    ):
        if ip_adapter_image_embeds is None:
            if not isinstance(ip_adapter_image, list):
                ip_adapter_image = [ip_adapter_image]
            if len(ip_adapter_image) != len(
                self.unet.encoder_hid_proj.image_projection_layers
            ):
                raise ValueError(
                    f&quot;`ip_adapter_image` must have same length as the number of IP Adapters. Got {len(ip_adapter_image)} images and {len(self.unet.encoder_hid_proj.image_projection_layers)} IP Adapters.&quot;
                )
            image_embeds = []
            for single_ip_adapter_image, image_proj_layer in zip(
                ip_adapter_image, self.unet.encoder_hid_proj.image_projection_layers
            ):
                output_hidden_state = not isinstance(image_proj_layer, ImageProjection)
                single_image_embeds, single_negative_image_embeds = self.encode_image(
                    single_ip_adapter_image, device, 1, output_hidden_state
                )
                single_image_embeds = torch.stack(
                    [single_image_embeds] * num_images_per_prompt, dim=0
                )
                single_negative_image_embeds = torch.stack(
                    [single_negative_image_embeds] * num_images_per_prompt, dim=0
                )
                if do_classifier_free_guidance:
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                    single_image_embeds = single_image_embeds.to(device)
                image_embeds.append(single_image_embeds)
        else:
            repeat_dims = [1]
            image_embeds = []
            for single_image_embeds in ip_adapter_image_embeds:
                if do_classifier_free_guidance:
                    single_negative_image_embeds, single_image_embeds = (
                        single_image_embeds.chunk(2)
                    )
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                    single_negative_image_embeds = single_negative_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_negative_image_embeds.shape[1:])),
                    )
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                else:
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                image_embeds.append(single_image_embeds)
        return image_embeds
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        prompt_2,
        height,
        width,
        callback_steps,
        negative_prompt=None,
        negative_prompt_2=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        pooled_prompt_embeds=None,
        negative_pooled_prompt_embeds=None,
        ip_adapter_image=None,
        ip_adapter_image_embeds=None,
        callback_on_step_end_tensor_inputs=None,
    ):
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if callback_steps is not None and (
            not isinstance(callback_steps, int) or callback_steps &lt;= 0
        ):
            raise ValueError(
                f&quot;`callback_steps` has to be a positive integer but is {callback_steps} of type&quot;
                f&quot; {type(callback_steps)}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_2 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        elif prompt_2 is not None and (
            not isinstance(prompt_2, str) and not isinstance(prompt_2, list)
        ):
            raise ValueError(
                f&quot;`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if prompt_embeds is not None and pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.&quot;
            )
        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:
            raise ValueError(
                &quot;If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.&quot;
            )
        if ip_adapter_image is not None and ip_adapter_image_embeds is not None:
            raise ValueError(
                &quot;Provide either `ip_adapter_image` or `ip_adapter_image_embeds`. Cannot leave both `ip_adapter_image` and `ip_adapter_image_embeds` defined.&quot;
            )
        if ip_adapter_image_embeds is not None:
            if not isinstance(ip_adapter_image_embeds, list):
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be of type `list` but is {type(ip_adapter_image_embeds)}&quot;
                )
            elif ip_adapter_image_embeds[0].ndim not in [3, 4]:
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be a list of 3D or 4D tensors but is {ip_adapter_image_embeds[0].ndim}D&quot;
                )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        shape = (
            batch_size,
            num_channels_latents,
            height // self.vae_scale_factor,
            width // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        if latents is None:
            latents = randn_tensor(
                shape, generator=generator, device=device, dtype=dtype
            )
        else:
            latents = latents.to(device)
        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents
    def _get_add_time_ids(
        self,
        original_size,
        crops_coords_top_left,
        target_size,
        dtype,
        text_encoder_projection_dim=None,
    ):
        if StateTracker.is_sdxl_refiner():
            add_time_ids = list(
                original_size
                + crops_coords_top_left
                + (StateTracker.get_args().data_aesthetic_score,)
            )
        else:
            add_time_ids = list(original_size + crops_coords_top_left + target_size)
        passed_add_embed_dim = (
            self.unet.config.addition_time_embed_dim * len(add_time_ids)
            + text_encoder_projection_dim
        )
        expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features
        if expected_add_embed_dim != passed_add_embed_dim:
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.&quot;
            )
        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
        return add_time_ids
    def upcast_vae(self):
        dtype = self.vae.dtype
        self.vae.to(dtype=torch.float32)
        use_torch_2_0_or_xformers = isinstance(
            self.vae.decoder.mid_block.attentions[0].processor,
            (
                AttnProcessor2_0,
                XFormersAttnProcessor,
                FusedAttnProcessor2_0,
            ),
        )
        # if xformers or torch_2_0 is used attention block does not need
        # to be in float32 which can save lots of memory
        if use_torch_2_0_or_xformers:
            self.vae.post_quant_conv.to(dtype)
            self.vae.decoder.conv_in.to(dtype)
            self.vae.decoder.mid_block.to(dtype)
    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
    def get_guidance_scale_embedding(
        self,
        w: torch.Tensor,
        embedding_dim: int = 512,
        dtype: torch.dtype = torch.float32,
    ) -&gt; torch.FloatTensor:
        &quot;&quot;&quot;
        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
        Args:
            w (`torch.Tensor`):
                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
            embedding_dim (`int`, *optional*, defaults to 512):
                Dimension of the embeddings to generate.
            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
                Data type of the generated embeddings.
        Returns:
            `torch.FloatTensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
        &quot;&quot;&quot;
        assert len(w.shape) == 1
        w = w * 1000.0
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
        emb = w.to(dtype)[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1))
        assert emb.shape == (w.shape[0], embedding_dim)
        return emb
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def guidance_rescale(self):
        return self._guidance_rescale
    @property
    def clip_skip(self):
        return self._clip_skip
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1 and self.unet.config.time_cond_proj_dim is None
    @property
    def cross_attention_kwargs(self):
        return self._cross_attention_kwargs
    @property
    def denoising_end(self):
        return self._denoising_end
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        prompt_2: Optional[Union[str, List[str]]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        denoising_end: Optional[float] = None,
        guidance_scale: float = 5.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,
        ip_adapter_image: Optional[PipelineImageInput] = None,
        ip_adapter_image_embeds: Optional[List[torch.FloatTensor]] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        guidance_rescale: float = 0.0,
        original_size: Optional[Tuple[int, int]] = None,
        crops_coords_top_left: Tuple[int, int] = (0, 0),
        target_size: Optional[Tuple[int, int]] = None,
        negative_original_size: Optional[Tuple[int, int]] = None,
        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),
        negative_target_size: Optional[Tuple[int, int]] = None,
        clip_skip: Optional[int] = None,
        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        **kwargs,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in both text-encoders
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
                and checkpoints that are not specifically fine-tuned on low resolutions.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image. This is set to 1024 by default for the best results.
                Anything below 512 pixels won&apos;t work well for
                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
                and checkpoints that are not specifically fine-tuned on low resolutions.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            denoising_end (`float`, *optional*):
                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be
                completed before it is intentionally prematurely terminated. As a result, the returned sample will
                still retain a substantial amount of noise as determined by the discrete timesteps selected by the
                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a
                &quot;Mixture of Denoisers&quot; multi-pipeline setup, as elaborated in [**Refining the Image
                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)
            guidance_scale (`float`, *optional*, defaults to 5.0):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            ip_adapter_image: (`PipelineImageInput`, *optional*): Optional image input to work with IP Adapters.
            ip_adapter_image_embeds (`List[torch.FloatTensor]`, *optional*):
                Pre-generated image embeddings for IP-Adapter. It should be a list of length same as number of IP-adapters.
                Each element should be a tensor of shape `(batch_size, num_images, emb_dim)`. It should contain the negative image embedding
                if `do_classifier_free_guidance` is set to `True`.
                If not provided, embeddings are computed from the `ip_adapter_image` input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead
                of a plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            guidance_rescale (`float`, *optional*, defaults to 0.0):
                Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are
                Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `φ` in equation 16. of
                [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
                Guidance rescale factor should fix overexposure when using zero terminal SNR.
            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.
                `original_size` defaults to `(height, width)` if not specified. Part of SDXL&apos;s micro-conditioning as
                explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                `crops_coords_top_left` can be used to generate an image that appears to be &quot;cropped&quot; from the position
                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting
                `crops_coords_top_left` to (0, 0). Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                For most cases, `target_size` should be set to the desired height and width of the generated image. If
                not specified it will default to `(height, width)`. Part of SDXL&apos;s micro-conditioning as explained in
                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a specific image resolution. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a target image resolution. It should be as same
                as the `target_size` for most cases. Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            callback_on_step_end (`Callable`, *optional*):
                A function that calls at the end of each denoising steps during the inference. The function is called
                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,
                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by
                `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
        Examples:
        Returns:
            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] if `return_dict` is True, otherwise a
            `tuple`. When returning a tuple, the first element is a list with the generated images.
        &quot;&quot;&quot;
        callback = kwargs.pop(&quot;callback&quot;, None)
        callback_steps = kwargs.pop(&quot;callback_steps&quot;, None)
        if callback is not None:
            deprecate(
                &quot;callback&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`&quot;,
            )
        if callback_steps is not None:
            deprecate(
                &quot;callback_steps&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`&quot;,
            )
        # 0. Default height and width to unet
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        original_size = original_size or (height, width)
        target_size = target_size or (height, width)
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            prompt_2,
            height,
            width,
            callback_steps,
            negative_prompt,
            negative_prompt_2,
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
            ip_adapter_image,
            ip_adapter_image_embeds,
            callback_on_step_end_tensor_inputs,
        )
        self._guidance_scale = guidance_scale
        self._guidance_rescale = guidance_rescale
        self._clip_skip = clip_skip
        self._cross_attention_kwargs = cross_attention_kwargs
        self._denoising_end = denoising_end
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self.unet.device
        # 3. Encode input prompt
        lora_scale = (
            self.cross_attention_kwargs.get(&quot;scale&quot;, None)
            if self.cross_attention_kwargs is not None
            else None
        )
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt_2,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            lora_scale=lora_scale,
            clip_skip=self.clip_skip,
        )
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps
        )
        # 5. Prepare latent variables
        num_channels_latents = self.unet.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        # 7. Prepare added time ids &amp; embeddings
        add_text_embeds = pooled_prompt_embeds
        if self.text_encoder_2 is None:
            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])
        else:
            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim
        add_time_ids = self._get_add_time_ids(
            original_size,
            crops_coords_top_left,
            target_size,
            dtype=prompt_embeds.dtype,
            text_encoder_projection_dim=text_encoder_projection_dim,
        )
        if negative_original_size is not None and negative_target_size is not None:
            negative_add_time_ids = self._get_add_time_ids(
                negative_original_size,
                negative_crops_coords_top_left,
                negative_target_size,
                dtype=prompt_embeds.dtype,
                text_encoder_projection_dim=text_encoder_projection_dim,
            )
        else:
            negative_add_time_ids = add_time_ids
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            add_text_embeds = torch.cat(
                [negative_pooled_prompt_embeds, add_text_embeds], dim=0
            )
            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)
        prompt_embeds = prompt_embeds.to(device)
        add_text_embeds = add_text_embeds.to(device)
        add_time_ids = add_time_ids.to(device).repeat(
            batch_size * num_images_per_prompt, 1
        )
        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:
            image_embeds = self.prepare_ip_adapter_image_embeds(
                ip_adapter_image,
                ip_adapter_image_embeds,
                device,
                batch_size * num_images_per_prompt,
                self.do_classifier_free_guidance,
            )
        # 8. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        # 8.1 Apply denoising_end
        if (
            self.denoising_end is not None
            and isinstance(self.denoising_end, float)
            and self.denoising_end &gt; 0
            and self.denoising_end &lt; 1
        ):
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = len(
                list(filter(lambda ts: ts &gt;= discrete_timestep_cutoff, timesteps))
            )
            timesteps = timesteps[:num_inference_steps]
        # 9. Optionally get Guidance Scale Embedding
        timestep_cond = None
        if self.unet.config.time_cond_proj_dim is not None:
            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(
                batch_size * num_images_per_prompt
            )
            timestep_cond = self.get_guidance_scale_embedding(
                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim
            ).to(device=device, dtype=latents.dtype)
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                # predict the noise residual
                added_cond_kwargs = {
                    &quot;text_embeds&quot;: add_text_embeds,
                    &quot;time_ids&quot;: add_time_ids,
                }
                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:
                    added_cond_kwargs[&quot;image_embeds&quot;] = image_embeds
                noise_pred = self.unet(
                    latent_model_input.to(self.unet.device),
                    t,
                    encoder_hidden_states=prompt_embeds.to(self.unet.device),
                    timestep_cond=timestep_cond,
                    cross_attention_kwargs=self.cross_attention_kwargs,
                    added_cond_kwargs={
                        k: v.to(self.unet.device) for k, v in added_cond_kwargs.items()
                    },
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                if self.do_classifier_free_guidance and self.guidance_rescale &gt; 0.0:
                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                    noise_pred = rescale_noise_cfg(
                        noise_pred,
                        noise_pred_text,
                        guidance_rescale=self.guidance_rescale,
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    add_text_embeds = callback_outputs.pop(
                        &quot;add_text_embeds&quot;, add_text_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                    add_time_ids = callback_outputs.pop(&quot;add_time_ids&quot;, add_time_ids)
                    negative_add_time_ids = callback_outputs.pop(
                        &quot;negative_add_time_ids&quot;, negative_add_time_ids
                    )
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, &quot;order&quot;, 1)
                        callback(step_idx, t, latents)
                if XLA_AVAILABLE:
                    xm.mark_step()
        if not output_type == &quot;latent&quot;:
            # make sure the VAE is in float32 mode, as it overflows in float16
            needs_upcasting = (
                self.vae.dtype == torch.float16 and self.vae.config.force_upcast
            )
            if needs_upcasting:
                self.upcast_vae()
                latents = latents.to(
                    next(iter(self.vae.post_quant_conv.parameters())).dtype
                )
            elif latents.dtype != self.vae.dtype:
                if torch.backends.mps.is_available():
                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                    self.vae = self.vae.to(latents.dtype)
            # unscale/denormalize the latents
            # denormalize with the mean and std if available and not None
            has_latents_mean = (
                hasattr(self.vae.config, &quot;latents_mean&quot;)
                and self.vae.config.latents_mean is not None
            )
            has_latents_std = (
                hasattr(self.vae.config, &quot;latents_std&quot;)
                and self.vae.config.latents_std is not None
            )
            if has_latents_mean and has_latents_std:
                latents_mean = (
                    torch.tensor(self.vae.config.latents_mean)
                    .view(1, 4, 1, 1)
                    .to(latents.device, latents.dtype)
                )
                latents_std = (
                    torch.tensor(self.vae.config.latents_std)
                    .view(1, 4, 1, 1)
                    .to(latents.device, latents.dtype)
                )
                latents = (
                    latents * latents_std / self.vae.config.scaling_factor
                    + latents_mean
                )
            else:
                latents = latents / self.vae.config.scaling_factor
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # we have SageAttention loaded. fallback to SDPA for decode.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sdpa
                )
            image = self.vae.decode(
                latents.to(dtype=self.vae.dtype), return_dict=False
            )[0]
            if hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                # reenable SageAttention for training.
                torch.nn.functional.scaled_dot_product_attention = (
                    torch.nn.functional.scaled_dot_product_attention_sage
                )
            # cast back to fp16 if needed
            if needs_upcasting:
                self.vae.to(dtype=torch.float16)
        else:
            image = latents
        if not output_type == &quot;latent&quot;:
            # apply watermark if available
            if self.watermark is not None:
                image = self.watermark.apply_watermark(image)
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return StableDiffusionXLPipelineOutput(images=image)
class StableDiffusionXLImg2ImgPipeline(
    DiffusionPipeline,
    StableDiffusionMixin,
    TextualInversionLoaderMixin,
    FromSingleFileMixin,
    StableDiffusionXLLoraLoaderMixin,
    IPAdapterMixin,
):
    r&quot;&quot;&quot;
    Pipeline for text-to-image generation using Stable Diffusion XL.
    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the
    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)
    The pipeline also inherits the following loading methods:
        - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings
        - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights`] for loading LoRA weights
        - [`~loaders.StableDiffusionXLLoraLoaderMixin.save_lora_weights`] for saving LoRA weights
        - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters
    Args:
        vae ([`AutoencoderKL`]):
            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.
        text_encoder ([`CLIPTextModel`]):
            Frozen text-encoder. Stable Diffusion XL uses the text portion of
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically
            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.
        text_encoder_2 ([` CLIPTextModelWithProjection`]):
            Second frozen text-encoder. Stable Diffusion XL uses the text and pool portion of
            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),
            specifically the
            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)
            variant.
        tokenizer (`CLIPTokenizer`):
            Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        tokenizer_2 (`CLIPTokenizer`):
            Second Tokenizer of class
            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).
        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.
        scheduler ([`SchedulerMixin`]):
            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of
            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].
        requires_aesthetics_score (`bool`, *optional*, defaults to `&quot;False&quot;`):
            Whether the `unet` requires an `aesthetic_score` condition to be passed during inference. Also see the
            config of `stabilityai/stable-diffusion-xl-refiner-1-0`.
        force_zeros_for_empty_prompt (`bool`, *optional*, defaults to `&quot;True&quot;`):
            Whether the negative prompt embeddings shall be forced to always be set to 0. Also see the config of
            `stabilityai/stable-diffusion-xl-base-1-0`.
        add_watermarker (`bool`, *optional*):
            Whether to use the [invisible_watermark library](https://github.com/ShieldMnt/invisible-watermark/) to
            watermark output images. If not defined, it will default to True if the package is installed, otherwise no
            watermarker will be used.
    &quot;&quot;&quot;
    model_cpu_offload_seq = &quot;text_encoder-&gt;text_encoder_2-&gt;image_encoder-&gt;unet-&gt;vae&quot;
    _optional_components = [
        &quot;tokenizer&quot;,
        &quot;tokenizer_2&quot;,
        &quot;text_encoder&quot;,
        &quot;text_encoder_2&quot;,
        &quot;image_encoder&quot;,
        &quot;feature_extractor&quot;,
    ]
    _callback_tensor_inputs = [
        &quot;latents&quot;,
        &quot;prompt_embeds&quot;,
        &quot;negative_prompt_embeds&quot;,
        &quot;add_text_embeds&quot;,
        &quot;add_time_ids&quot;,
        &quot;negative_pooled_prompt_embeds&quot;,
        &quot;add_neg_time_ids&quot;,
    ]
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: CLIPTextModel,
        text_encoder_2: CLIPTextModelWithProjection,
        tokenizer: CLIPTokenizer,
        tokenizer_2: CLIPTokenizer,
        unet: UNet2DConditionModel,
        scheduler: KarrasDiffusionSchedulers,
        image_encoder: CLIPVisionModelWithProjection = None,
        feature_extractor: CLIPImageProcessor = None,
        requires_aesthetics_score: bool = False,
        force_zeros_for_empty_prompt: bool = True,
        add_watermarker: Optional[bool] = None,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            text_encoder_2=text_encoder_2,
            tokenizer=tokenizer,
            tokenizer_2=tokenizer_2,
            unet=unet,
            image_encoder=image_encoder,
            feature_extractor=feature_extractor,
            scheduler=scheduler,
        )
        self.register_to_config(
            force_zeros_for_empty_prompt=force_zeros_for_empty_prompt
        )
        self.register_to_config(requires_aesthetics_score=requires_aesthetics_score)
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
        add_watermarker = (
            add_watermarker
            if add_watermarker is not None
            else is_invisible_watermark_available()
        )
        if add_watermarker:
            self.watermark = StableDiffusionXLWatermarker()
        else:
            self.watermark = None
    # Copied from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl.StableDiffusionXLPipeline.encode_prompt
    def encode_prompt(
        self,
        prompt: str,
        prompt_2: Optional[str] = None,
        device: Optional[torch.device] = None,
        num_images_per_prompt: int = 1,
        do_classifier_free_guidance: bool = True,
        negative_prompt: Optional[str] = None,
        negative_prompt_2: Optional[str] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        lora_scale: Optional[float] = None,
        clip_skip: Optional[int] = None,
    ):
        r&quot;&quot;&quot;
        Encodes the prompt into text encoder hidden states.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                prompt to be encoded
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in both text-encoders
            device: (`torch.device`):
                torch device
            num_images_per_prompt (`int`):
                number of images that should be generated per prompt
            do_classifier_free_guidance (`bool`):
                whether to use classifier free guidance or not
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            lora_scale (`float`, *optional*):
                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
        &quot;&quot;&quot;
        device = device or self._execution_device
        # set lora scale so that monkey patched LoRA
        # function of text encoder can correctly access it
        if lora_scale is not None and isinstance(
            self, StableDiffusionXLLoraLoaderMixin
        ):
            self._lora_scale = lora_scale
            # dynamically adjust the LoRA scale
            if self.text_encoder is not None:
                if not USE_PEFT_BACKEND:
                    adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)
                else:
                    scale_lora_layers(self.text_encoder, lora_scale)
            if self.text_encoder_2 is not None:
                if not USE_PEFT_BACKEND:
                    adjust_lora_scale_text_encoder(self.text_encoder_2, lora_scale)
                else:
                    scale_lora_layers(self.text_encoder_2, lora_scale)
        prompt = [prompt] if isinstance(prompt, str) else prompt
        if prompt is not None:
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        # Define tokenizers and text encoders
        tokenizers = (
            [self.tokenizer, self.tokenizer_2]
            if self.tokenizer is not None
            else [self.tokenizer_2]
        )
        text_encoders = (
            [self.text_encoder, self.text_encoder_2]
            if self.text_encoder is not None
            else [self.text_encoder_2]
        )
        if prompt_embeds is None:
            prompt_2 = prompt_2 or prompt
            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2
            # textual inversion: process multi-vector tokens if necessary
            prompt_embeds_list = []
            prompts = [prompt, prompt_2]
            for prompt, tokenizer, text_encoder in zip(
                prompts, tokenizers, text_encoders
            ):
                if isinstance(self, TextualInversionLoaderMixin):
                    prompt = self.maybe_convert_prompt(prompt, tokenizer)
                text_inputs = tokenizer(
                    prompt,
                    padding=&quot;max_length&quot;,
                    max_length=tokenizer.model_max_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                )
                text_input_ids = text_inputs.input_ids
                untruncated_ids = tokenizer(
                    prompt, padding=&quot;longest&quot;, return_tensors=&quot;pt&quot;
                ).input_ids
                if untruncated_ids.shape[-1] &gt;= text_input_ids.shape[
                    -1
                ] and not torch.equal(text_input_ids, untruncated_ids):
                    removed_text = tokenizer.batch_decode(
                        untruncated_ids[:, tokenizer.model_max_length - 1 : -1]
                    )
                    logger.warning(
                        &quot;The following part of your input was truncated because CLIP can only handle sequences up to&quot;
                        f&quot; {tokenizer.model_max_length} tokens: {removed_text}&quot;
                    )
                prompt_embeds = text_encoder(
                    text_input_ids.to(device), output_hidden_states=True
                )
                # We are only ALWAYS interested in the pooled output of the final text encoder
                pooled_prompt_embeds = prompt_embeds[0]
                if clip_skip is None:
                    prompt_embeds = prompt_embeds.hidden_states[-2]
                else:
                    # &quot;2&quot; because SDXL always indexes from the penultimate layer.
                    prompt_embeds = prompt_embeds.hidden_states[-(clip_skip + 2)]
                prompt_embeds_list.append(prompt_embeds)
            prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)
        # get unconditional embeddings for classifier free guidance
        zero_out_negative_prompt = (
            negative_prompt is None and self.config.force_zeros_for_empty_prompt
        )
        if (
            do_classifier_free_guidance
            and negative_prompt_embeds is None
            and zero_out_negative_prompt
        ):
            negative_prompt_embeds = torch.zeros_like(prompt_embeds)
            negative_pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)
        elif do_classifier_free_guidance and negative_prompt_embeds is None:
            negative_prompt = negative_prompt or &quot;&quot;
            negative_prompt_2 = negative_prompt_2 or negative_prompt
            # normalize str to list
            negative_prompt = (
                batch_size * [negative_prompt]
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            negative_prompt_2 = (
                batch_size * [negative_prompt_2]
                if isinstance(negative_prompt_2, str)
                else negative_prompt_2
            )
            uncond_tokens: List[str]
            if prompt is not None and type(prompt) is not type(negative_prompt):
                raise TypeError(
                    f&quot;`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=&quot;
                    f&quot; {type(prompt)}.&quot;
                )
            elif batch_size != len(negative_prompt):
                raise ValueError(
                    f&quot;`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:&quot;
                    f&quot; {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches&quot;
                    &quot; the batch size of `prompt`.&quot;
                )
            else:
                uncond_tokens = [negative_prompt, negative_prompt_2]
            negative_prompt_embeds_list = []
            for negative_prompt, tokenizer, text_encoder in zip(
                uncond_tokens, tokenizers, text_encoders
            ):
                if isinstance(self, TextualInversionLoaderMixin):
                    negative_prompt = self.maybe_convert_prompt(
                        negative_prompt, tokenizer
                    )
                max_length = prompt_embeds.shape[1]
                uncond_input = tokenizer(
                    negative_prompt,
                    padding=&quot;max_length&quot;,
                    max_length=max_length,
                    truncation=True,
                    return_tensors=&quot;pt&quot;,
                )
                negative_prompt_embeds = text_encoder(
                    uncond_input.input_ids.to(device),
                    output_hidden_states=True,
                )
                # We are only ALWAYS interested in the pooled output of the final text encoder
                negative_pooled_prompt_embeds = negative_prompt_embeds[0]
                negative_prompt_embeds = negative_prompt_embeds.hidden_states[-2]
                negative_prompt_embeds_list.append(negative_prompt_embeds)
            negative_prompt_embeds = torch.concat(negative_prompt_embeds_list, dim=-1)
        if self.text_encoder_2 is not None:
            prompt_embeds = prompt_embeds.to(
                dtype=self.text_encoder_2.dtype, device=device
            )
        else:
            prompt_embeds = prompt_embeds.to(dtype=self.unet.dtype, device=device)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )
        if do_classifier_free_guidance:
            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = negative_prompt_embeds.shape[1]
            if self.text_encoder_2 is not None:
                negative_prompt_embeds = negative_prompt_embeds.to(
                    dtype=self.text_encoder_2.dtype, device=device
                )
            else:
                negative_prompt_embeds = negative_prompt_embeds.to(
                    dtype=self.unet.dtype, device=device
                )
            negative_prompt_embeds = negative_prompt_embeds.repeat(
                1, num_images_per_prompt, 1
            )
            negative_prompt_embeds = negative_prompt_embeds.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )
        pooled_prompt_embeds = pooled_prompt_embeds.repeat(
            1, num_images_per_prompt
        ).view(bs_embed * num_images_per_prompt, -1)
        if do_classifier_free_guidance:
            negative_pooled_prompt_embeds = negative_pooled_prompt_embeds.repeat(
                1, num_images_per_prompt
            ).view(bs_embed * num_images_per_prompt, -1)
        if self.text_encoder is not None:
            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder, lora_scale)
        if self.text_encoder_2 is not None:
            if isinstance(self, StableDiffusionXLLoraLoaderMixin) and USE_PEFT_BACKEND:
                # Retrieve the original scale by scaling back the LoRA layers
                unscale_lora_layers(self.text_encoder_2, lora_scale)
        return (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        )
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        prompt_2,
        strength,
        num_inference_steps,
        callback_steps,
        negative_prompt=None,
        negative_prompt_2=None,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        ip_adapter_image=None,
        ip_adapter_image_embeds=None,
        callback_on_step_end_tensor_inputs=None,
    ):
        if strength &lt; 0 or strength &gt; 1:
            raise ValueError(
                f&quot;The value of strength should in [0.0, 1.0] but is {strength}&quot;
            )
        if num_inference_steps is None:
            raise ValueError(&quot;`num_inference_steps` cannot be None.&quot;)
        elif not isinstance(num_inference_steps, int) or num_inference_steps &lt;= 0:
            raise ValueError(
                f&quot;`num_inference_steps` has to be a positive integer but is {num_inference_steps} of type&quot;
                f&quot; {type(num_inference_steps)}.&quot;
            )
        if callback_steps is not None and (
            not isinstance(callback_steps, int) or callback_steps &lt;= 0
        ):
            raise ValueError(
                f&quot;`callback_steps` has to be a positive integer but is {callback_steps} of type&quot;
                f&quot; {type(callback_steps)}.&quot;
            )
        if callback_on_step_end_tensor_inputs is not None and not all(
            k in self._callback_tensor_inputs
            for k in callback_on_step_end_tensor_inputs
        ):
            raise ValueError(
                f&quot;`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt_2 is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        elif prompt_2 is not None and (
            not isinstance(prompt_2, str) and not isinstance(prompt_2, list)
        ):
            raise ValueError(
                f&quot;`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
        if ip_adapter_image is not None and ip_adapter_image_embeds is not None:
            raise ValueError(
                &quot;Provide either `ip_adapter_image` or `ip_adapter_image_embeds`. Cannot leave both `ip_adapter_image` and `ip_adapter_image_embeds` defined.&quot;
            )
        if ip_adapter_image_embeds is not None:
            if not isinstance(ip_adapter_image_embeds, list):
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be of type `list` but is {type(ip_adapter_image_embeds)}&quot;
                )
            elif ip_adapter_image_embeds[0].ndim not in [3, 4]:
                raise ValueError(
                    f&quot;`ip_adapter_image_embeds` has to be a list of 3D or 4D tensors but is {ip_adapter_image_embeds[0].ndim}D&quot;
                )
    def get_timesteps(
        self, num_inference_steps, strength, device, denoising_start=None
    ):
        # get the original timestep using init_timestep
        if denoising_start is None:
            init_timestep = min(
                int(num_inference_steps * strength), num_inference_steps
            )
            t_start = max(num_inference_steps - init_timestep, 0)
        else:
            t_start = 0
        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]
        # Strength is irrelevant if we directly request a timestep to start at;
        # that is, strength is determined by the denoising_start instead.
        if denoising_start is not None:
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (denoising_start * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = (timesteps &lt; discrete_timestep_cutoff).sum().item()
            if self.scheduler.order == 2 and num_inference_steps % 2 == 0:
                # if the scheduler is a 2nd order scheduler we might have to do +1
                # because `num_inference_steps` might be even given that every timestep
                # (except the highest one) is duplicated. If `num_inference_steps` is even it would
                # mean that we cut the timesteps in the middle of the denoising step
                # (between 1st and 2nd derivative) which leads to incorrect results. By adding 1
                # we ensure that the denoising process always ends after the 2nd derivate step of the scheduler
                num_inference_steps = num_inference_steps + 1
            # because t_n+1 &gt;= t_n, we slice the timesteps starting from the end
            timesteps = timesteps[-num_inference_steps:]
            return timesteps, num_inference_steps
        return timesteps, num_inference_steps - t_start
    def prepare_latents(
        self,
        image,
        timestep,
        batch_size,
        num_images_per_prompt,
        dtype,
        device,
        generator=None,
        add_noise=True,
    ):
        if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):
            raise ValueError(
                f&quot;`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}&quot;
            )
        latents_mean = latents_std = None
        if (
            hasattr(self.vae.config, &quot;latents_mean&quot;)
            and self.vae.config.latents_mean is not None
        ):
            latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1)
        if (
            hasattr(self.vae.config, &quot;latents_std&quot;)
            and self.vae.config.latents_std is not None
        ):
            latents_std = torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1)
        # Offload text encoder if `enable_model_cpu_offload` was enabled
        if hasattr(self, &quot;final_offload_hook&quot;) and self.final_offload_hook is not None:
            self.text_encoder_2.to(&quot;cpu&quot;)
            torch.cuda.empty_cache()
        image = image.to(device=device, dtype=dtype)
        batch_size = batch_size * num_images_per_prompt
        if image.shape[1] == 4:
            init_latents = image
        else:
            # make sure the VAE is in float32 mode, as it overflows in float16
            if self.vae.config.force_upcast:
                image = image.float()
                self.vae.to(dtype=torch.float32)
            if isinstance(generator, list) and len(generator) != batch_size:
                raise ValueError(
                    f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                    f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
                )
            elif isinstance(generator, list):
                init_latents = [
                    retrieve_latents(
                        self.vae.encode(image[i : i + 1]), generator=generator[i]
                    )
                    for i in range(batch_size)
                ]
                init_latents = torch.cat(init_latents, dim=0)
            else:
                init_latents = retrieve_latents(
                    self.vae.encode(image), generator=generator
                )
            if self.vae.config.force_upcast:
                self.vae.to(dtype)
            init_latents = init_latents.to(dtype)
            if latents_mean is not None and latents_std is not None:
                latents_mean = latents_mean.to(device=self.device, dtype=dtype)
                latents_std = latents_std.to(device=self.device, dtype=dtype)
                init_latents = (
                    (init_latents - latents_mean)
                    * self.vae.config.scaling_factor
                    / latents_std
                )
            else:
                init_latents = self.vae.config.scaling_factor * init_latents
        if (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] == 0
        ):
            # expand init_latents for batch_size
            additional_image_per_prompt = batch_size // init_latents.shape[0]
            init_latents = torch.cat(
                [init_latents] * additional_image_per_prompt, dim=0
            )
        elif (
            batch_size &gt; init_latents.shape[0]
            and batch_size % init_latents.shape[0] != 0
        ):
            raise ValueError(
                f&quot;Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.&quot;
            )
        else:
            init_latents = torch.cat([init_latents], dim=0)
        if add_noise:
            shape = init_latents.shape
            noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
            # get latents
            init_latents = self.scheduler.add_noise(init_latents, noise, timestep)
        latents = init_latents
        return latents
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.encode_image
    def encode_image(
        self, image, device, num_images_per_prompt, output_hidden_states=None
    ):
        dtype = next(self.image_encoder.parameters()).dtype
        if not isinstance(image, torch.Tensor):
            image = self.feature_extractor(image, return_tensors=&quot;pt&quot;).pixel_values
        image = image.to(device=device, dtype=dtype)
        if output_hidden_states:
            image_enc_hidden_states = self.image_encoder(
                image, output_hidden_states=True
            ).hidden_states[-2]
            image_enc_hidden_states = image_enc_hidden_states.repeat_interleave(
                num_images_per_prompt, dim=0
            )
            uncond_image_enc_hidden_states = self.image_encoder(
                torch.zeros_like(image), output_hidden_states=True
            ).hidden_states[-2]
            uncond_image_enc_hidden_states = (
                uncond_image_enc_hidden_states.repeat_interleave(
                    num_images_per_prompt, dim=0
                )
            )
            return image_enc_hidden_states, uncond_image_enc_hidden_states
        else:
            image_embeds = self.image_encoder(image).image_embeds
            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)
            uncond_image_embeds = torch.zeros_like(image_embeds)
            return image_embeds, uncond_image_embeds
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_ip_adapter_image_embeds
    def prepare_ip_adapter_image_embeds(
        self,
        ip_adapter_image,
        ip_adapter_image_embeds,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
    ):
        if ip_adapter_image_embeds is None:
            if not isinstance(ip_adapter_image, list):
                ip_adapter_image = [ip_adapter_image]
            if len(ip_adapter_image) != len(
                self.unet.encoder_hid_proj.image_projection_layers
            ):
                raise ValueError(
                    f&quot;`ip_adapter_image` must have same length as the number of IP Adapters. Got {len(ip_adapter_image)} images and {len(self.unet.encoder_hid_proj.image_projection_layers)} IP Adapters.&quot;
                )
            image_embeds = []
            for single_ip_adapter_image, image_proj_layer in zip(
                ip_adapter_image, self.unet.encoder_hid_proj.image_projection_layers
            ):
                output_hidden_state = not isinstance(image_proj_layer, ImageProjection)
                single_image_embeds, single_negative_image_embeds = self.encode_image(
                    single_ip_adapter_image, device, 1, output_hidden_state
                )
                single_image_embeds = torch.stack(
                    [single_image_embeds] * num_images_per_prompt, dim=0
                )
                single_negative_image_embeds = torch.stack(
                    [single_negative_image_embeds] * num_images_per_prompt, dim=0
                )
                if do_classifier_free_guidance:
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                    single_image_embeds = single_image_embeds.to(device)
                image_embeds.append(single_image_embeds)
        else:
            repeat_dims = [1]
            image_embeds = []
            for single_image_embeds in ip_adapter_image_embeds:
                if do_classifier_free_guidance:
                    single_negative_image_embeds, single_image_embeds = (
                        single_image_embeds.chunk(2)
                    )
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                    single_negative_image_embeds = single_negative_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_negative_image_embeds.shape[1:])),
                    )
                    single_image_embeds = torch.cat(
                        [single_negative_image_embeds, single_image_embeds]
                    )
                else:
                    single_image_embeds = single_image_embeds.repeat(
                        num_images_per_prompt,
                        *(repeat_dims * len(single_image_embeds.shape[1:])),
                    )
                image_embeds.append(single_image_embeds)
        return image_embeds
    def _get_add_time_ids(
        self,
        original_size,
        crops_coords_top_left,
        target_size,
        aesthetic_score,
        negative_aesthetic_score,
        negative_original_size,
        negative_crops_coords_top_left,
        negative_target_size,
        dtype,
        text_encoder_projection_dim=None,
    ):
        if self.config.requires_aesthetics_score:
            add_time_ids = list(
                original_size + crops_coords_top_left + (aesthetic_score,)
            )
            add_neg_time_ids = list(
                negative_original_size
                + negative_crops_coords_top_left
                + (negative_aesthetic_score,)
            )
        else:
            add_time_ids = list(original_size + crops_coords_top_left + target_size)
            add_neg_time_ids = list(
                negative_original_size + crops_coords_top_left + negative_target_size
            )
        passed_add_embed_dim = (
            self.unet.config.addition_time_embed_dim * len(add_time_ids)
            + text_encoder_projection_dim
        )
        expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features
        if (
            expected_add_embed_dim &gt; passed_add_embed_dim
            and (expected_add_embed_dim - passed_add_embed_dim)
            == self.unet.config.addition_time_embed_dim
        ):
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. Please make sure to enable `requires_aesthetics_score` with `pipe.register_to_config(requires_aesthetics_score=True)` to make sure `aesthetic_score` {aesthetic_score} and `negative_aesthetic_score` {negative_aesthetic_score} is correctly used by the model.&quot;
            )
        elif (
            expected_add_embed_dim &lt; passed_add_embed_dim
            and (passed_add_embed_dim - expected_add_embed_dim)
            == self.unet.config.addition_time_embed_dim
        ):
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. Please make sure to disable `requires_aesthetics_score` with `pipe.register_to_config(requires_aesthetics_score=False)` to make sure `target_size` {target_size} is correctly used by the model.&quot;
            )
        elif expected_add_embed_dim != passed_add_embed_dim:
            raise ValueError(
                f&quot;Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.&quot;
            )
        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
        add_neg_time_ids = torch.tensor([add_neg_time_ids], dtype=dtype)
        return add_time_ids, add_neg_time_ids
    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_upscale.StableDiffusionUpscalePipeline.upcast_vae
    def upcast_vae(self):
        dtype = self.vae.dtype
        self.vae.to(dtype=torch.float32)
        use_torch_2_0_or_xformers = isinstance(
            self.vae.decoder.mid_block.attentions[0].processor,
            (
                AttnProcessor2_0,
                XFormersAttnProcessor,
            ),
        )
        # if xformers or torch_2_0 is used attention block does not need
        # to be in float32 which can save lots of memory
        if use_torch_2_0_or_xformers:
            self.vae.post_quant_conv.to(dtype)
            self.vae.decoder.conv_in.to(dtype)
            self.vae.decoder.mid_block.to(dtype)
    # Copied from diffusers.pipelines.latent_consistency_models.pipeline_latent_consistency_text2img.LatentConsistencyModelPipeline.get_guidance_scale_embedding
    def get_guidance_scale_embedding(
        self,
        w: torch.Tensor,
        embedding_dim: int = 512,
        dtype: torch.dtype = torch.float32,
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298
        Args:
            w (`torch.Tensor`):
                Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.
            embedding_dim (`int`, *optional*, defaults to 512):
                Dimension of the embeddings to generate.
            dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):
                Data type of the generated embeddings.
        Returns:
            `torch.Tensor`: Embedding vectors with shape `(len(w), embedding_dim)`.
        &quot;&quot;&quot;
        assert len(w.shape) == 1
        w = w * 1000.0
        half_dim = embedding_dim // 2
        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
        emb = w.to(dtype)[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1))
        assert emb.shape == (w.shape[0], embedding_dim)
        return emb
    @property
    def guidance_scale(self):
        return self._guidance_scale
    @property
    def guidance_rescale(self):
        return self._guidance_rescale
    @property
    def clip_skip(self):
        return self._clip_skip
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    @property
    def do_classifier_free_guidance(self):
        return self._guidance_scale &gt; 1 and self.unet.config.time_cond_proj_dim is None
    @property
    def cross_attention_kwargs(self):
        return self._cross_attention_kwargs
    @property
    def denoising_end(self):
        return self._denoising_end
    @property
    def denoising_start(self):
        return self._denoising_start
    @property
    def num_timesteps(self):
        return self._num_timesteps
    @property
    def interrupt(self):
        return self._interrupt
    @torch.no_grad()
    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        prompt_2: Optional[Union[str, List[str]]] = None,
        image: PipelineImageInput = None,
        strength: float = 0.3,
        num_inference_steps: int = 50,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        denoising_start: Optional[float] = None,
        denoising_end: Optional[float] = None,
        guidance_scale: float = 5.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        negative_prompt_2: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        pooled_prompt_embeds: Optional[torch.Tensor] = None,
        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,
        ip_adapter_image: Optional[PipelineImageInput] = None,
        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        guidance_rescale: float = 0.0,
        original_size: Tuple[int, int] = None,
        crops_coords_top_left: Tuple[int, int] = (0, 0),
        target_size: Tuple[int, int] = None,
        negative_original_size: Optional[Tuple[int, int]] = None,
        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),
        negative_target_size: Optional[Tuple[int, int]] = None,
        aesthetic_score: float = 6.0,
        negative_aesthetic_score: float = 2.5,
        clip_skip: Optional[int] = None,
        callback_on_step_end: Optional[
            Union[
                Callable[[int, int, Dict], None],
                PipelineCallback,
                MultiPipelineCallbacks,
            ]
        ] = None,
        callback_on_step_end_tensor_inputs: List[str] = [&quot;latents&quot;],
        **kwargs,
    ):
        r&quot;&quot;&quot;
        Function invoked when calling the pipeline for generation.
        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is
                used in both text-encoders
            image (`torch.Tensor` or `PIL.Image.Image` or `np.ndarray` or `List[torch.Tensor]` or `List[PIL.Image.Image]` or `List[np.ndarray]`):
                The image(s) to modify with the pipeline.
            strength (`float`, *optional*, defaults to 0.3):
                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1. `image`
                will be used as a starting point, adding more noise to it the larger the `strength`. The number of
                denoising steps depends on the amount of noise initially added. When `strength` is 1, added noise will
                be maximum and the denoising process will run for the full number of iterations specified in
                `num_inference_steps`. A value of 1, therefore, essentially ignores `image`. Note that in the case of
                `denoising_start` being declared as an integer, the value of `strength` will be ignored.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            sigmas (`List[float]`, *optional*):
                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
                will be used.
            denoising_start (`float`, *optional*):
                When specified, indicates the fraction (between 0.0 and 1.0) of the total denoising process to be
                bypassed before it is initiated. Consequently, the initial part of the denoising process is skipped and
                it is assumed that the passed `image` is a partly denoised image. Note that when this is specified,
                strength will be ignored. The `denoising_start` parameter is particularly beneficial when this pipeline
                is integrated into a &quot;Mixture of Denoisers&quot; multi-pipeline setup, as detailed in [**Refine Image
                Quality**](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality).
            denoising_end (`float`, *optional*):
                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be
                completed before it is intentionally prematurely terminated. As a result, the returned sample will
                still retain a substantial amount of noise (ca. final 20% of timesteps still needed) and should be
                denoised by a successor pipeline that has `denoising_start` set to 0.8 so that it only denoises the
                final 20% of the scheduler. The denoising_end parameter should ideally be utilized when this pipeline
                forms a part of a &quot;Mixture of Denoisers&quot; multi-pipeline setup, as elaborated in [**Refine Image
                Quality**](https://huggingface.co/docs/diffusers/using-diffusers/sdxl#refine-image-quality).
            guidance_scale (`float`, *optional*, defaults to 7.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale &gt;
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
                less than `1`).
            negative_prompt_2 (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and
                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
                If not provided, pooled text embeddings will be generated from `prompt` input argument.
            negative_pooled_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`
                input argument.
            ip_adapter_image: (`PipelineImageInput`, *optional*): Optional image input to work with IP Adapters.
            ip_adapter_image_embeds (`List[torch.Tensor]`, *optional*):
                Pre-generated image embeddings for IP-Adapter. It should be a list of length same as number of
                IP-adapters. Each element should be a tensor of shape `(batch_size, num_images, emb_dim)`. It should
                contain the negative image embedding if `do_classifier_free_guidance` is set to `True`. If not
                provided, embeddings are computed from the `ip_adapter_image` input argument.
            output_type (`str`, *optional*, defaults to `&quot;pil&quot;`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`] instead of a
                plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            guidance_rescale (`float`, *optional*, defaults to 0.0):
                Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are
                Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `φ` in equation 16. of
                [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
                Guidance rescale factor should fix overexposure when using zero terminal SNR.
            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.
                `original_size` defaults to `(height, width)` if not specified. Part of SDXL&apos;s micro-conditioning as
                explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                `crops_coords_top_left` can be used to generate an image that appears to be &quot;cropped&quot; from the position
                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting
                `crops_coords_top_left` to (0, 0). Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                For most cases, `target_size` should be set to the desired height and width of the generated image. If
                not specified it will default to `(height, width)`. Part of SDXL&apos;s micro-conditioning as explained in
                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a specific image resolution. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):
                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL&apos;s
                micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):
                To negatively condition the generation process based on a target image resolution. It should be as same
                as the `target_size` for most cases. Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more
                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.
            aesthetic_score (`float`, *optional*, defaults to 6.0):
                Used to simulate an aesthetic score of the generated image by influencing the positive text condition.
                Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
            negative_aesthetic_score (`float`, *optional*, defaults to 2.5):
                Part of SDXL&apos;s micro-conditioning as explained in section 2.2 of
                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). Can be used to
                simulate an aesthetic score of the generated image by influencing the negative text condition.
            clip_skip (`int`, *optional*):
                Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that
                the output of the pre-final layer will be used for computing the prompt embeddings.
            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):
                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of
                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:
                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.
            callback_on_step_end_tensor_inputs (`List`, *optional*):
                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list
                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the
                `._callback_tensor_inputs` attribute of your pipeline class.
        Examples:
        Returns:
            [`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput`] if `return_dict` is True, otherwise a
            `tuple. When returning a tuple, the first element is a list with the generated images.
        &quot;&quot;&quot;
        callback = kwargs.pop(&quot;callback&quot;, None)
        callback_steps = kwargs.pop(&quot;callback_steps&quot;, None)
        if callback is not None:
            deprecate(
                &quot;callback&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`&quot;,
            )
        if callback_steps is not None:
            deprecate(
                &quot;callback_steps&quot;,
                &quot;1.0.0&quot;,
                &quot;Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`&quot;,
            )
        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):
            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs
        # 1. Check inputs. Raise error if not correct
        self.check_inputs(
            prompt,
            prompt_2,
            strength,
            num_inference_steps,
            callback_steps,
            negative_prompt,
            negative_prompt_2,
            prompt_embeds,
            negative_prompt_embeds,
            ip_adapter_image,
            ip_adapter_image_embeds,
            callback_on_step_end_tensor_inputs,
        )
        self._guidance_scale = guidance_scale
        self._guidance_rescale = guidance_rescale
        self._clip_skip = clip_skip
        self._cross_attention_kwargs = cross_attention_kwargs
        self._denoising_end = denoising_end
        self._denoising_start = denoising_start
        self._interrupt = False
        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        # 3. Encode input prompt
        text_encoder_lora_scale = (
            self.cross_attention_kwargs.get(&quot;scale&quot;, None)
            if self.cross_attention_kwargs is not None
            else None
        )
        (
            prompt_embeds,
            negative_prompt_embeds,
            pooled_prompt_embeds,
            negative_pooled_prompt_embeds,
        ) = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt_2,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            do_classifier_free_guidance=self.do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            negative_prompt_2=negative_prompt_2,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            pooled_prompt_embeds=pooled_prompt_embeds,
            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,
            lora_scale=text_encoder_lora_scale,
            clip_skip=self.clip_skip,
        )
        # 4. Preprocess image
        image = self.image_processor.preprocess(image)
        # 5. Prepare timesteps
        def denoising_value_valid(dnv):
            return isinstance(dnv, float) and 0 &lt; dnv &lt; 1
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler,
            num_inference_steps,
            device,
            timesteps=timesteps,
            sigmas=sigmas,
        )
        timesteps, num_inference_steps = self.get_timesteps(
            num_inference_steps,
            strength,
            device,
            denoising_start=(
                self.denoising_start
                if denoising_value_valid(self.denoising_start)
                else None
            ),
        )
        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)
        add_noise = True if self.denoising_start is None else False
        # 6. Prepare latent variables
        if latents is None:
            latents = self.prepare_latents(
                image,
                latent_timestep,
                batch_size,
                num_images_per_prompt,
                prompt_embeds.dtype,
                device,
                generator,
                add_noise,
            )
        # 7. Prepare extra step kwargs.
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        height, width = latents.shape[-2:]
        height = height * self.vae_scale_factor
        width = width * self.vae_scale_factor
        original_size = original_size or (height, width)
        target_size = target_size or (height, width)
        # 8. Prepare added time ids &amp; embeddings
        if negative_original_size is None:
            negative_original_size = original_size
        if negative_target_size is None:
            negative_target_size = target_size
        add_text_embeds = pooled_prompt_embeds
        if self.text_encoder_2 is None:
            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])
        else:
            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim
        add_time_ids, add_neg_time_ids = self._get_add_time_ids(
            original_size,
            crops_coords_top_left,
            target_size,
            aesthetic_score,
            negative_aesthetic_score,
            negative_original_size,
            negative_crops_coords_top_left,
            negative_target_size,
            dtype=prompt_embeds.dtype,
            text_encoder_projection_dim=text_encoder_projection_dim,
        )
        add_time_ids = add_time_ids.repeat(batch_size * num_images_per_prompt, 1)
        if self.do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            add_text_embeds = torch.cat(
                [negative_pooled_prompt_embeds, add_text_embeds], dim=0
            )
            add_neg_time_ids = add_neg_time_ids.repeat(
                batch_size * num_images_per_prompt, 1
            )
            add_time_ids = torch.cat([add_neg_time_ids, add_time_ids], dim=0)
        prompt_embeds = prompt_embeds.to(device)
        add_text_embeds = add_text_embeds.to(device)
        add_time_ids = add_time_ids.to(device)
        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:
            image_embeds = self.prepare_ip_adapter_image_embeds(
                ip_adapter_image,
                ip_adapter_image_embeds,
                device,
                batch_size * num_images_per_prompt,
                self.do_classifier_free_guidance,
            )
        # 9. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        # 9.1 Apply denoising_end
        if (
            self.denoising_end is not None
            and self.denoising_start is not None
            and denoising_value_valid(self.denoising_end)
            and denoising_value_valid(self.denoising_start)
            and self.denoising_start &gt;= self.denoising_end
        ):
            raise ValueError(
                f&quot;`denoising_start`: {self.denoising_start} cannot be larger than or equal to `denoising_end`: &quot;
                + f&quot; {self.denoising_end} when using type float.&quot;
            )
        elif self.denoising_end is not None and denoising_value_valid(
            self.denoising_end
        ):
            discrete_timestep_cutoff = int(
                round(
                    self.scheduler.config.num_train_timesteps
                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)
                )
            )
            num_inference_steps = len(
                list(filter(lambda ts: ts &gt;= discrete_timestep_cutoff, timesteps))
            )
            timesteps = timesteps[:num_inference_steps]
        # 9.2 Optionally get Guidance Scale Embedding
        timestep_cond = None
        if self.unet.config.time_cond_proj_dim is not None:
            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(
                batch_size * num_images_per_prompt
            )
            timestep_cond = self.get_guidance_scale_embedding(
                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim
            ).to(device=device, dtype=latents.dtype)
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                if self.interrupt:
                    continue
                # expand the latents if we are doing classifier free guidance
                latent_model_input = (
                    torch.cat([latents] * 2)
                    if self.do_classifier_free_guidance
                    else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                # predict the noise residual
                added_cond_kwargs = {
                    &quot;text_embeds&quot;: add_text_embeds,
                    &quot;time_ids&quot;: add_time_ids,
                }
                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:
                    added_cond_kwargs[&quot;image_embeds&quot;] = image_embeds
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=prompt_embeds,
                    timestep_cond=timestep_cond,
                    cross_attention_kwargs=self.cross_attention_kwargs,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                # perform guidance
                if self.do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + self.guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                if self.do_classifier_free_guidance and self.guidance_rescale &gt; 0.0:
                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                    noise_pred = rescale_noise_cfg(
                        noise_pred,
                        noise_pred_text,
                        guidance_rescale=self.guidance_rescale,
                    )
                # compute the previous noisy sample x_t -&gt; x_t-1
                latents_dtype = latents.dtype
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                if latents.dtype != latents_dtype:
                    if torch.backends.mps.is_available():
                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                        latents = latents.to(latents_dtype)
                if callback_on_step_end is not None:
                    callback_kwargs = {}
                    for k in callback_on_step_end_tensor_inputs:
                        callback_kwargs[k] = locals()[k]
                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)
                    latents = callback_outputs.pop(&quot;latents&quot;, latents)
                    prompt_embeds = callback_outputs.pop(&quot;prompt_embeds&quot;, prompt_embeds)
                    negative_prompt_embeds = callback_outputs.pop(
                        &quot;negative_prompt_embeds&quot;, negative_prompt_embeds
                    )
                    add_text_embeds = callback_outputs.pop(
                        &quot;add_text_embeds&quot;, add_text_embeds
                    )
                    negative_pooled_prompt_embeds = callback_outputs.pop(
                        &quot;negative_pooled_prompt_embeds&quot;, negative_pooled_prompt_embeds
                    )
                    add_time_ids = callback_outputs.pop(&quot;add_time_ids&quot;, add_time_ids)
                    add_neg_time_ids = callback_outputs.pop(
                        &quot;add_neg_time_ids&quot;, add_neg_time_ids
                    )
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, &quot;order&quot;, 1)
                        callback(step_idx, t, latents)
                if XLA_AVAILABLE:
                    xm.mark_step()
        if not output_type == &quot;latent&quot;:
            # make sure the VAE is in float32 mode, as it overflows in float16
            needs_upcasting = (
                self.vae.dtype == torch.float16 and self.vae.config.force_upcast
            )
            if needs_upcasting:
                self.upcast_vae()
                latents = latents.to(
                    next(iter(self.vae.post_quant_conv.parameters())).dtype
                )
            elif latents.dtype != self.vae.dtype:
                if torch.backends.mps.is_available():
                    # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272
                    self.vae = self.vae.to(latents.dtype)
            # unscale/denormalize the latents
            # denormalize with the mean and std if available and not None
            has_latents_mean = (
                hasattr(self.vae.config, &quot;latents_mean&quot;)
                and self.vae.config.latents_mean is not None
            )
            has_latents_std = (
                hasattr(self.vae.config, &quot;latents_std&quot;)
                and self.vae.config.latents_std is not None
            )
            if has_latents_mean and has_latents_std:
                latents_mean = (
                    torch.tensor(self.vae.config.latents_mean)
                    .view(1, 4, 1, 1)
                    .to(latents.device, latents.dtype)
                )
                latents_std = (
                    torch.tensor(self.vae.config.latents_std)
                    .view(1, 4, 1, 1)
                    .to(latents.device, latents.dtype)
                )
                latents = (
                    latents * latents_std / self.vae.config.scaling_factor
                    + latents_mean
                )
            else:
                latents = latents / self.vae.config.scaling_factor
            image = self.vae.decode(latents.to(self.vae.dtype), return_dict=False)[0]
            # cast back to fp16 if needed
            if needs_upcasting:
                self.vae.to(dtype=torch.float16)
        else:
            image = latents
        # apply watermark if available
        if self.watermark is not None:
            image = self.watermark.apply_watermark(image)
        image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return StableDiffusionXLPipelineOutput(images=image)</file><file path="helpers/models/smoldit/__init__.py">from helpers.models.smoldit.transformer import SmolDiT2DModel
from helpers.models.smoldit.pipeline import SmolDiTPipeline
SmolDiTConfigurations = {
    &quot;smoldit-small&quot;: {
        &quot;sample_size&quot;: 64,
        &quot;num_layers&quot;: 18,
        &quot;patch_size&quot;: 2,
        &quot;attention_head_dim&quot;: 64,
        &quot;num_attention_heads&quot;: 16,
        &quot;num_kv_heads&quot;: 4,
        &quot;in_channels&quot;: 4,
        &quot;cross_attention_dim&quot;: 768,
        &quot;out_channels&quot;: 4,
        &quot;activation_fn&quot;: &quot;gelu-approximate&quot;,
    },
    &quot;smoldit-swiglu&quot;: {
        &quot;sample_size&quot;: 64,
        &quot;num_layers&quot;: 24,
        &quot;patch_size&quot;: 2,
        &quot;attention_head_dim&quot;: 72,
        &quot;num_attention_heads&quot;: 16,
        &quot;num_kv_heads&quot;: 4,
        &quot;in_channels&quot;: 4,
        &quot;cross_attention_dim&quot;: 768,
        &quot;out_channels&quot;: 4,
        &quot;activation_fn&quot;: &quot;swiglu&quot;,
    },
    &quot;smoldit-base&quot;: {
        &quot;sample_size&quot;: 64,
        &quot;num_layers&quot;: 24,
        &quot;patch_size&quot;: 2,
        &quot;attention_head_dim&quot;: 72,
        &quot;num_attention_heads&quot;: 16,
        &quot;num_kv_heads&quot;: 4,
        &quot;in_channels&quot;: 4,
        &quot;cross_attention_dim&quot;: 768,
        &quot;out_channels&quot;: 4,
        &quot;activation_fn&quot;: &quot;gelu-approximate&quot;,
    },
    &quot;smoldit-large&quot;: {
        &quot;sample_size&quot;: 64,
        &quot;num_layers&quot;: 30,
        &quot;patch_size&quot;: 2,
        &quot;attention_head_dim&quot;: 72,
        &quot;num_attention_heads&quot;: 32,
        &quot;num_kv_heads&quot;: 8,
        &quot;in_channels&quot;: 4,
        &quot;cross_attention_dim&quot;: 768,
        &quot;out_channels&quot;: 4,
        &quot;activation_fn&quot;: &quot;gelu-approximate&quot;,
    },
    &quot;smoldit-huge&quot;: {
        &quot;sample_size&quot;: 64,
        &quot;num_layers&quot;: 36,
        &quot;patch_size&quot;: 2,
        &quot;attention_head_dim&quot;: 96,
        &quot;num_attention_heads&quot;: 64,
        &quot;num_kv_heads&quot;: 16,
        &quot;in_channels&quot;: 4,
        &quot;cross_attention_dim&quot;: 768,
        &quot;out_channels&quot;: 4,
        &quot;activation_fn&quot;: &quot;gelu-approximate&quot;,
    },
}
SmolDiTConfigurationNames = list(SmolDiTConfigurations.keys())
def get_resize_crop_region_for_grid(src, tgt_size):
    th = tw = tgt_size
    h, w = src
    r = h / w
    # resize
    if r &gt; 1:
        resize_height = th
        resize_width = int(round(th / h * w))
    else:
        resize_width = tw
        resize_height = int(round(tw / w * h))
    crop_top = int(round((th - resize_height) / 2.0))
    crop_left = int(round((tw - resize_width) / 2.0))
    return (crop_top, crop_left), (crop_top + resize_height, crop_left + resize_width)</file><file path="helpers/models/smoldit/pipeline.py"># Copyright 2024 PixArt and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
from typing import Callable, List, Optional, Union
import torch
from transformers import T5EncoderModel, T5Tokenizer
from diffusers.models.embeddings import get_2d_rotary_pos_embed
from diffusers.pipelines.hunyuandit.pipeline_hunyuandit import (
    get_resize_crop_region_for_grid,
)
from diffusers.image_processor import VaeImageProcessor
from diffusers.models import AutoencoderKL
from diffusers.pipelines.pipeline_utils import DiffusionPipeline, ImagePipelineOutput
from diffusers.schedulers import KarrasDiffusionSchedulers
from diffusers.utils import logging
from diffusers.utils.torch_utils import randn_tensor
from helpers.models.smoldit import SmolDiT2DModel
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.rescale_noise_cfg
def rescale_noise_cfg(noise_cfg, noise_pred_text, guidance_rescale=0.0):
    &quot;&quot;&quot;
    Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and
    Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4
    &quot;&quot;&quot;
    std_text = noise_pred_text.std(
        dim=list(range(1, noise_pred_text.ndim)), keepdim=True
    )
    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)
    # rescale the results from guidance (fixes overexposure)
    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)
    # mix with the original results from guidance by factor guidance_rescale to avoid &quot;plain looking&quot; images
    noise_cfg = (
        guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg
    )
    return noise_cfg
def retrieve_timesteps(
    scheduler,
    num_inference_steps: Optional[int] = None,
    device: Optional[Union[str, torch.device]] = None,
    timesteps: Optional[List[int]] = None,
    sigmas: Optional[List[float]] = None,
    **kwargs,
):
    &quot;&quot;&quot;
    Calls the scheduler&apos;s `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles
    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.
    Args:
        scheduler (`SchedulerMixin`):
            The scheduler to get timesteps from.
        num_inference_steps (`int`):
            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`
            must be `None`.
        device (`str` or `torch.device`, *optional*):
            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.
        timesteps (`List[int]`, *optional*):
            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,
            `num_inference_steps` and `sigmas` must be `None`.
        sigmas (`List[float]`, *optional*):
            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,
            `num_inference_steps` and `timesteps` must be `None`.
    Returns:
        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the
        second element is the number of inference steps.
    &quot;&quot;&quot;
    if timesteps is not None and sigmas is not None:
        raise ValueError(
            &quot;Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values&quot;
        )
    if timesteps is not None:
        accepts_timesteps = &quot;timesteps&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accepts_timesteps:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; timestep schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = &quot;sigmas&quot; in set(
            inspect.signature(scheduler.set_timesteps).parameters.keys()
        )
        if not accept_sigmas:
            raise ValueError(
                f&quot;The current scheduler class {scheduler.__class__}&apos;s `set_timesteps` does not support custom&quot;
                f&quot; sigmas schedules. Please check whether you are using the correct scheduler.&quot;
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps
class SmolDiTPipeline(DiffusionPipeline):
    model_cpu_offload_seq = &quot;text_encoder-&gt;transformer-&gt;vae&quot;
    @property
    def guidance_rescale(self):
        return self._guidance_rescale
    def __init__(
        self,
        vae: AutoencoderKL,
        text_encoder: T5EncoderModel,
        tokenizer: T5Tokenizer,
        transformer: SmolDiT2DModel,
        scheduler: KarrasDiffusionSchedulers,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            transformer=transformer,
            scheduler=scheduler,
        )
        self.vae_scale_factor = (
            2 ** (len(self.vae.config.block_out_channels) - 1)
            if hasattr(self, &quot;vae&quot;) and self.vae is not None
            else 8
        )
        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)
    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        do_classifier_free_guidance: bool = True,
        negative_prompt: str = &quot;&quot;,
        num_images_per_prompt: int = 1,
        device: Optional[torch.device] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        prompt_attention_mask: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        max_sequence_length: int = 300,
    ):
        if device is None:
            device = self._execution_device
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        max_length = max_sequence_length
        if prompt_embeds is None:
            text_inputs = self.tokenizer(
                prompt,
                padding=&quot;max_length&quot;,
                max_length=max_length,
                truncation=True,
                add_special_tokens=True,
                return_tensors=&quot;pt&quot;,
            )
            text_input_ids = text_inputs.input_ids
            prompt_attention_mask = text_inputs.attention_mask
            prompt_attention_mask = prompt_attention_mask.to(device)
            prompt_embeds = self.text_encoder(
                text_input_ids.to(device), attention_mask=prompt_attention_mask
            )
            prompt_embeds = prompt_embeds[0]
        if self.text_encoder is not None:
            dtype = self.text_encoder.dtype
        elif self.transformer is not None:
            dtype = self.transformer.dtype
        else:
            dtype = None
        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)
        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(
            bs_embed * num_images_per_prompt, seq_len, -1
        )
        prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)
        prompt_attention_mask = prompt_attention_mask.repeat(num_images_per_prompt, 1)
        # get unconditional embeddings for classifier free guidance
        if do_classifier_free_guidance and negative_prompt_embeds is None:
            uncond_tokens = (
                [negative_prompt] * batch_size
                if isinstance(negative_prompt, str)
                else negative_prompt
            )
            max_length = prompt_embeds.shape[1]
            uncond_input = self.tokenizer(
                uncond_tokens,
                padding=&quot;max_length&quot;,
                max_length=max_length,
                truncation=True,
                add_special_tokens=True,
                return_tensors=&quot;pt&quot;,
            )
            negative_prompt_attention_mask = uncond_input.attention_mask
            negative_prompt_attention_mask = negative_prompt_attention_mask.to(device)
            negative_prompt_embeds = self.text_encoder(
                uncond_input.input_ids.to(device),
                attention_mask=negative_prompt_attention_mask,
            )
            negative_prompt_embeds = negative_prompt_embeds[0]
        if do_classifier_free_guidance:
            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
            seq_len = negative_prompt_embeds.shape[1]
            negative_prompt_embeds = negative_prompt_embeds.to(
                dtype=dtype, device=device
            )
            negative_prompt_embeds = negative_prompt_embeds.repeat(
                1, num_images_per_prompt, 1
            )
            negative_prompt_embeds = negative_prompt_embeds.view(
                batch_size * num_images_per_prompt, seq_len, -1
            )
            negative_prompt_attention_mask = negative_prompt_attention_mask.view(
                bs_embed, -1
            )
            negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(
                num_images_per_prompt, 1
            )
        else:
            negative_prompt_embeds = None
            negative_prompt_attention_mask = None
        return (
            prompt_embeds,
            prompt_attention_mask,
            negative_prompt_embeds,
            negative_prompt_attention_mask,
        )
    def prepare_extra_step_kwargs(self, generator, eta):
        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature
        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.
        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502
        # and should be between [0, 1]
        accepts_eta = &quot;eta&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        extra_step_kwargs = {}
        if accepts_eta:
            extra_step_kwargs[&quot;eta&quot;] = eta
        # check if the scheduler accepts generator
        accepts_generator = &quot;generator&quot; in set(
            inspect.signature(self.scheduler.step).parameters.keys()
        )
        if accepts_generator:
            extra_step_kwargs[&quot;generator&quot;] = generator
        return extra_step_kwargs
    def check_inputs(
        self,
        prompt,
        height,
        width,
        negative_prompt,
        callback_steps,
        prompt_embeds=None,
        negative_prompt_embeds=None,
        prompt_attention_mask=None,
        negative_prompt_attention_mask=None,
    ):
        if height % 8 != 0 or width % 8 != 0:
            raise ValueError(
                f&quot;`height` and `width` have to be divisible by 8 but are {height} and {width}.&quot;
            )
        if (callback_steps is None) or (
            callback_steps is not None
            and (not isinstance(callback_steps, int) or callback_steps &lt;= 0)
        ):
            raise ValueError(
                f&quot;`callback_steps` has to be a positive integer but is {callback_steps} of type&quot;
                f&quot; {type(callback_steps)}.&quot;
            )
        if prompt is not None and prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to&quot;
                &quot; only forward one of the two.&quot;
            )
        elif prompt is None and prompt_embeds is None:
            raise ValueError(
                &quot;Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.&quot;
            )
        elif prompt is not None and (
            not isinstance(prompt, str) and not isinstance(prompt, list)
        ):
            raise ValueError(
                f&quot;`prompt` has to be of type `str` or `list` but is {type(prompt)}&quot;
            )
        if prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `prompt`: {prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if negative_prompt is not None and negative_prompt_embeds is not None:
            raise ValueError(
                f&quot;Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:&quot;
                f&quot; {negative_prompt_embeds}. Please make sure to only forward one of the two.&quot;
            )
        if prompt_embeds is not None and prompt_attention_mask is None:
            raise ValueError(
                &quot;Must provide `prompt_attention_mask` when specifying `prompt_embeds`.&quot;
            )
        if (
            negative_prompt_embeds is not None
            and negative_prompt_attention_mask is None
        ):
            raise ValueError(
                &quot;Must provide `negative_prompt_attention_mask` when specifying `negative_prompt_embeds`.&quot;
            )
        if prompt_embeds is not None and negative_prompt_embeds is not None:
            if prompt_embeds.shape != negative_prompt_embeds.shape:
                raise ValueError(
                    &quot;`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`&quot;
                    f&quot; {negative_prompt_embeds.shape}.&quot;
                )
            if prompt_attention_mask.shape != negative_prompt_attention_mask.shape:
                raise ValueError(
                    &quot;`prompt_attention_mask` and `negative_prompt_attention_mask` must have the same shape when passed directly, but&quot;
                    f&quot; got: `prompt_attention_mask` {prompt_attention_mask.shape} != `negative_prompt_attention_mask`&quot;
                    f&quot; {negative_prompt_attention_mask.shape}.&quot;
                )
    def prepare_latents(
        self,
        batch_size,
        num_channels_latents,
        height,
        width,
        dtype,
        device,
        generator,
        latents=None,
    ):
        shape = (
            batch_size,
            num_channels_latents,
            int(height) // self.vae_scale_factor,
            int(width) // self.vae_scale_factor,
        )
        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f&quot;You have passed a list of generators of length {len(generator)}, but requested an effective batch&quot;
                f&quot; size of {batch_size}. Make sure the batch size matches the length of the generators.&quot;
            )
        if latents is None:
            latents = randn_tensor(
                shape, generator=generator, device=device, dtype=dtype
            )
        else:
            latents = latents.to(device)
        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents
    @torch.no_grad()
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        negative_prompt: str = &quot;&quot;,
        num_inference_steps: int = 20,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        guidance_scale: float = 4.5,
        num_images_per_prompt: Optional[int] = 1,
        height: Optional[int] = None,
        width: Optional[int] = None,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        prompt_attention_mask: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_attention_mask: Optional[torch.Tensor] = None,
        output_type: Optional[str] = &quot;pil&quot;,
        return_dict: bool = True,
        guidance_rescale: float = 0.0,
        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
        callback_steps: int = 1,
        max_sequence_length: int = 300,
    ):
        # 1. Check inputs. Raise error if not correct
        height = height or self.transformer.config.sample_size * self.vae_scale_factor
        width = width or self.transformer.config.sample_size * self.vae_scale_factor
        self.check_inputs(
            prompt,
            height,
            width,
            negative_prompt,
            callback_steps,
            prompt_embeds,
            negative_prompt_embeds,
            prompt_attention_mask,
            negative_prompt_attention_mask,
        )
        # 2. Default height and width to transformer
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]
        device = self._execution_device
        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale &gt; 1.0
        # 3. Encode input prompt
        (
            prompt_embeds,
            prompt_attention_mask,
            negative_prompt_embeds,
            negative_prompt_attention_mask,
        ) = self.encode_prompt(
            prompt,
            do_classifier_free_guidance,
            negative_prompt=negative_prompt,
            num_images_per_prompt=num_images_per_prompt,
            device=device,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            prompt_attention_mask=prompt_attention_mask,
            negative_prompt_attention_mask=negative_prompt_attention_mask,
            max_sequence_length=max_sequence_length,
        )
        if do_classifier_free_guidance:
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
            prompt_attention_mask = torch.cat(
                [negative_prompt_attention_mask, prompt_attention_mask], dim=0
            )
        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps, sigmas
        )
        # 5. Prepare latents.
        latent_channels = self.transformer.config.in_channels
        latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            latent_channels,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents,
        )
        # 6. Prepare rotary embeddings.
        grid_height = height // 8 // self.transformer.config.patch_size
        grid_width = width // 8 // self.transformer.config.patch_size
        base_size = 512 // 8 // self.transformer.config.patch_size
        grid_crops_coords = get_resize_crop_region_for_grid(
            (grid_height, grid_width), base_size
        )
        image_rotary_emb = get_2d_rotary_pos_embed(
            self.transformer.inner_dim // self.transformer.config.num_attention_heads,
            grid_crops_coords,
            (grid_height, grid_width),
        )
        # 7. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)
        # 8. Denoising loop
        num_warmup_steps = max(
            len(timesteps) - num_inference_steps * self.scheduler.order, 0
        )
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                latent_model_input = (
                    torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                )
                latent_model_input = self.scheduler.scale_model_input(
                    latent_model_input, t
                )
                current_timestep = t
                if not torch.is_tensor(current_timestep):
                    # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can
                    # This would be a good case for the `match` statement (Python 3.10+)
                    is_mps = latent_model_input.device.type == &quot;mps&quot;
                    if isinstance(current_timestep, float):
                        dtype = torch.float32 if is_mps else torch.float64
                    else:
                        dtype = torch.int32 if is_mps else torch.int64
                    current_timestep = torch.tensor(
                        [current_timestep],
                        dtype=dtype,
                        device=latent_model_input.device,
                    )
                elif len(current_timestep.shape) == 0:
                    current_timestep = current_timestep[None].to(
                        latent_model_input.device
                    )
                # broadcast to batch dimension in a way that&apos;s compatible with ONNX/Core ML
                current_timestep = current_timestep.expand(latent_model_input.shape[0])
                # predict noise model_output
                noise_pred = self.transformer(
                    latent_model_input,
                    encoder_hidden_states=prompt_embeds,
                    encoder_attention_mask=prompt_attention_mask,
                    timestep=current_timestep,
                    image_rotary_emb=image_rotary_emb,
                    return_dict=False,
                )[0]
                # perform guidance
                if do_classifier_free_guidance:
                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * (
                        noise_pred_text - noise_pred_uncond
                    )
                if do_classifier_free_guidance and guidance_rescale &gt; 0.0:
                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                    noise_pred = rescale_noise_cfg(
                        noise_pred, noise_pred_text, guidance_rescale=guidance_rescale
                    )
                # compute previous image: x_t -&gt; x_t-1
                latents = self.scheduler.step(
                    noise_pred, t, latents, **extra_step_kwargs, return_dict=False
                )[0]
                # call the callback, if provided
                if i == len(timesteps) - 1 or (
                    (i + 1) &gt; num_warmup_steps and (i + 1) % self.scheduler.order == 0
                ):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, &quot;order&quot;, 1)
                        callback(step_idx, t, latents)
        if not output_type == &quot;latent&quot;:
            image = self.vae.decode(
                latents.to(device=self.vae.device, dtype=self.vae.dtype)
                / self.vae.config.scaling_factor,
                return_dict=False,
            )[0]
        else:
            image = latents
        if not output_type == &quot;latent&quot;:
            image = self.image_processor.postprocess(image, output_type=output_type)
        # Offload all models
        self.maybe_free_model_hooks()
        if not return_dict:
            return (image,)
        return ImagePipelineOutput(images=image)</file><file path="helpers/models/smoldit/transformer.py"># Copyright 2024 Lumina, Hunyuan DiT, PixArt, The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Optional, Tuple
import torch
import torch.nn.functional as F
from torch import nn
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.models.attention import FeedForward
from diffusers.models.embeddings import (
    PatchEmbed,
    PixArtAlphaTextProjection,
    TimestepEmbedding,
    Timesteps,
    apply_rotary_emb,
)
from diffusers.models.modeling_outputs import Transformer2DModelOutput
from diffusers.models.modeling_utils import ModelMixin
from diffusers.models.normalization import AdaLayerNormContinuous, FP32LayerNorm
from diffusers.models.transformers.hunyuan_transformer_2d import AdaLayerNormShift
from diffusers.utils import logging
logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
class SmolDiTAttention(nn.Module):
    def __init__(
        self,
        query_dim,
        cross_attention_dim,
        dim_head,
        num_heads,
        kv_heads,
        sliding_window=None,
    ):
        super().__init__()
        self.inner_dim = dim_head * num_heads
        self.inner_kv_dim = self.inner_dim if kv_heads is None else dim_head * kv_heads
        self.query_dim = query_dim
        self.num_heads = num_heads
        self.is_cross_attention = cross_attention_dim is not None
        self.cross_attention_dim = (
            cross_attention_dim if cross_attention_dim is not None else query_dim
        )
        self.scale = dim_head**-0.5
        self.sliding_window = sliding_window
        self.to_q = nn.Linear(query_dim, self.inner_dim, bias=False)
        self.to_k = nn.Linear(self.cross_attention_dim, self.inner_kv_dim, bias=False)
        self.to_v = nn.Linear(self.cross_attention_dim, self.inner_kv_dim, bias=False)
        self.to_out = nn.Linear(self.inner_dim, query_dim, bias=False)
    # this mask processing utility is taken from the `prepare_attention_mask()`
    # function from diffusers. it is here for self-containment.
    def prepare_attention_mask(self, hidden_states, attention_mask):
        sequence_length = hidden_states.shape[1]
        current_length = attention_mask.shape[-1]
        batch_size = hidden_states.shape[0]
        if current_length != sequence_length:
            if attention_mask.device.type == &quot;mps&quot;:
                padding_shape = (
                    attention_mask.shape[0],
                    attention_mask.shape[1],
                    sequence_length,
                )
                padding = torch.zeros(
                    padding_shape,
                    dtype=attention_mask.dtype,
                    device=attention_mask.device,
                )
                attention_mask = torch.cat([attention_mask, padding], dim=2)
            else:
                attention_mask = F.pad(attention_mask, (0, sequence_length), value=0.0)
        if attention_mask.shape[0] &lt; batch_size * self.num_heads:
            attention_mask = attention_mask.repeat_interleave(self.num_heads, dim=0)
        return attention_mask
    def sliding_window_attention_mask(
        self,
        sequence_length: int,
        window_size: int,
        batch_size: int,
        num_heads: int,
        device,
    ) -&gt; torch.Tensor:
        mask = torch.zeros(
            (batch_size, num_heads, sequence_length, sequence_length), device=device
        )
        for i in range(sequence_length):
            start = max(0, i - window_size)
            end = min(sequence_length, i + window_size + 1)
            mask[:, :, i, start:end] = 1
        return mask
    def forward(
        self,
        hidden_states: torch.Tensor,
        encoder_hidden_states: torch.Tensor = None,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ):
        batch_size, _, _ = hidden_states.shape
        encoder_hidden_states = (
            hidden_states if encoder_hidden_states is None else encoder_hidden_states
        )
        # scaled_dot_product_attention expects attention_mask shape to be
        # (batch, heads, source_length, target_length)
        attention_mask = None
        if encoder_attention_mask is not None:
            encoder_attention_mask = self.prepare_attention_mask(
                encoder_hidden_states, encoder_attention_mask
            )
            encoder_attention_mask = encoder_attention_mask.view(
                batch_size, self.num_heads, -1, encoder_attention_mask.shape[-1]
            )
            attention_mask = encoder_attention_mask
        elif self.sliding_window:
            attention_mask = self.sliding_window_attention_mask(
                sequence_length=hidden_states.shape[1],
                window_size=self.sliding_window,
                batch_size=batch_size,
                num_heads=self.num_heads,
                device=hidden_states.device,
            )
        # Projections.
        query = self.to_q(hidden_states)
        key = self.to_k(encoder_hidden_states)
        value = self.to_v(encoder_hidden_states)
        query_dim = query.shape[-1]
        inner_dim = key.shape[-1]
        head_dim = query_dim // self.num_heads
        dtype = query.dtype
        # Get key-value heads
        kv_heads = inner_dim // head_dim
        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, kv_heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, kv_heads, head_dim).transpose(1, 2)
        # GQA
        if kv_heads != self.num_heads:
            # if GQA or MQA, repeat the key/value heads to reach the number of query heads.
            heads_per_kv_head = self.num_heads // kv_heads
            key = torch.repeat_interleave(key, heads_per_kv_head, dim=1)
            value = torch.repeat_interleave(value, heads_per_kv_head, dim=1)
        # Apply RoPE if needed
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            query = query.to(dtype)
            if not self.is_cross_attention:
                key = apply_rotary_emb(key, image_rotary_emb)
                key = query.to(dtype)
        # the output of sdpa = (batch, num_heads, seq_len, head_dim)
        hidden_states = F.scaled_dot_product_attention(
            query, key, value, attn_mask=attention_mask, scale=self.scale
        )
        # out
        hidden_states = hidden_states.transpose(1, 2).reshape(
            batch_size, -1, self.num_heads * head_dim
        )
        hidden_states = hidden_states.to(query.dtype)
        hidden_states = self.to_out(hidden_states)
        return hidden_states
class SmolDiTBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        num_attention_heads: int,
        num_kv_heads: int,
        ff_inner_dim: int,
        cross_attention_dim: int = 1024,
        activation_fn: str = &quot;gelu-approximate&quot;,
        layer_idx: int = None,
        sliding_window: int = None,
    ):
        super().__init__()
        # 1. Self-Attn
        self.norm1 = AdaLayerNormShift(dim, elementwise_affine=True, eps=1e-6)
        if layer_idx is not None and sliding_window is not None:
            sliding_window = sliding_window if not bool(layer_idx % 2) else None
        else:
            sliding_window = None
        self.attn1 = SmolDiTAttention(
            query_dim=dim,
            cross_attention_dim=None,
            dim_head=dim // num_attention_heads,
            num_heads=num_attention_heads,
            kv_heads=num_kv_heads,
            sliding_window=sliding_window,
        )
        # 2. Cross-Attn
        self.norm2 = FP32LayerNorm(dim, eps=1e-6, elementwise_affine=True)
        self.attn2 = SmolDiTAttention(
            query_dim=dim,
            cross_attention_dim=cross_attention_dim,
            dim_head=dim // num_attention_heads,
            num_heads=num_attention_heads,
            kv_heads=num_kv_heads,
        )
        # 3. Feed-forward
        self.ff = FeedForward(
            dim,
            activation_fn=activation_fn,
            inner_dim=ff_inner_dim,
            bias=False,
        )
    def forward(
        self,
        hidden_states: torch.Tensor,
        temb: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ) -&gt; torch.Tensor:
        # 1. Self-Attention
        norm_hidden_states = self.norm1(hidden_states, temb)
        attn_output = self.attn1(
            norm_hidden_states,
            image_rotary_emb=image_rotary_emb,
        )
        hidden_states = hidden_states + attn_output
        # 2. Cross-Attention
        hidden_states = hidden_states + self.attn2(
            self.norm2(hidden_states),
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            image_rotary_emb=image_rotary_emb,
        )
        # FFN Layer
        hidden_states = hidden_states + self.ff(hidden_states)
        return hidden_states
class SmolDiT2DModel(ModelMixin, ConfigMixin):
    @register_to_config
    def __init__(
        self,
        sample_size: int = 128,
        patch_size: int = 2,
        num_attention_heads: int = 16,
        num_kv_heads: int = 8,
        attention_head_dim: int = 88,
        in_channels: int = 4,
        out_channels: int = 4,
        activation_fn: str = &quot;gelu-approximate&quot;,
        num_layers: int = 28,
        mlp_ratio: float = 4.0,
        cross_attention_dim: int = 1024,
        sliding_window: int = None,
    ):
        super().__init__()
        self.inner_dim = num_attention_heads * attention_head_dim
        self.time_proj = Timesteps(
            num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0
        )
        self.timestep_embedder = TimestepEmbedding(
            in_channels=256, time_embed_dim=self.inner_dim
        )
        self.text_embedder = PixArtAlphaTextProjection(
            in_features=cross_attention_dim,
            hidden_size=cross_attention_dim * 4,
            out_features=cross_attention_dim,
            act_fn=&quot;silu_fp32&quot;,
        )
        self.pos_embed = PatchEmbed(
            height=sample_size,
            width=sample_size,
            in_channels=in_channels,
            embed_dim=self.inner_dim,
            patch_size=patch_size,
            pos_embed_type=None,
        )
        # SmolDiT Blocks
        self.blocks = nn.ModuleList(
            [
                SmolDiTBlock(
                    dim=self.inner_dim,
                    num_attention_heads=num_attention_heads,
                    num_kv_heads=num_kv_heads,
                    ff_inner_dim=int(self.inner_dim * mlp_ratio),
                    cross_attention_dim=cross_attention_dim,
                    activation_fn=activation_fn,
                    layer_idx=layer_idx,
                    sliding_window=(
                        sliding_window if sliding_window is not None else None
                    ),
                )
                for layer_idx in range(num_layers)
            ]
        )
        self.out_channels = out_channels
        self.norm_out = AdaLayerNormContinuous(
            self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6
        )
        self.proj_out = nn.Linear(
            self.inner_dim, patch_size * patch_size * out_channels
        )
    def forward(
        self,
        hidden_states: torch.Tensor,
        timestep: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_attention_mask: Optional[torch.Tensor] = None,
        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        return_dict=True,
    ):
        height, width = hidden_states.shape[-2:]
        hidden_dtype = hidden_states.dtype
        # convert encoder_attention_mask to a bias the same way we do for attention_mask
        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:
            encoder_attention_mask = (
                1 - encoder_attention_mask.to(hidden_states.dtype)
            ) * -10000.0
            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)
        # patch embed
        hidden_states = self.pos_embed(hidden_states)
        # timestep
        batch_size = hidden_states.shape[0]
        timesteps_proj = self.time_proj(timestep)
        temb = self.timestep_embedder(timesteps_proj.to(dtype=hidden_dtype))  # (N, 256)
        # text projection
        batch_size, sequence_length, _ = encoder_hidden_states.shape
        encoder_hidden_states = self.text_embedder(
            encoder_hidden_states.view(-1, encoder_hidden_states.shape[-1])
        )
        encoder_hidden_states = encoder_hidden_states.view(
            batch_size, sequence_length, -1
        )
        for _, block in enumerate(self.blocks):
            hidden_states = block(
                hidden_states=hidden_states,
                temb=temb,
                encoder_hidden_states=encoder_hidden_states,
                encoder_attention_mask=encoder_attention_mask,
                image_rotary_emb=image_rotary_emb,
            )  # (N, L, D)
        # final layer
        hidden_states = self.norm_out(hidden_states, temb.to(torch.float32))
        hidden_states = self.proj_out(hidden_states)
        # (N, L, patch_size ** 2 * out_channels)
        # unpatchify: (N, out_channels, H, W)
        patch_size = self.pos_embed.patch_size
        height = height // patch_size
        width = width // patch_size
        hidden_states = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                height,
                width,
                patch_size,
                patch_size,
                self.out_channels,
            )
        )
        hidden_states = torch.einsum(&quot;nhwpqc-&gt;nchpwq&quot;, hidden_states)
        output = hidden_states.reshape(
            shape=(
                hidden_states.shape[0],
                self.out_channels,
                height * patch_size,
                width * patch_size,
            )
        )
        if not return_dict:
            return (output,)
        return Transformer2DModelOutput(sample=output)</file><file path="helpers/models/__init__.py">upstream_config_sources = {
    &quot;sdxl&quot;: &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,
    &quot;kolors&quot;: &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,
    &quot;sd3&quot;: &quot;stabilityai/stable-diffusion-3-large&quot;,
    &quot;sana&quot;: &quot;terminusresearch/sana-1.6b-1024px&quot;,
    &quot;flux&quot;: &quot;black-forest-labs/flux.1-dev&quot;,
    &quot;legacy&quot;: &quot;stable-diffusion-v1-5/stable-diffusion-v1-5&quot;,
    &quot;ltxvideo&quot;: &quot;Lightricks/LTX-Video&quot;,
}
def get_model_config_path(model_family: str, model_path: str):
    if model_path.endswith(&quot;.safetensors&quot;):
        if model_family in upstream_config_sources:
            return upstream_config_sources[model_family]
        else:
            raise ValueError(
                &quot;Cannot find noise schedule config for .safetensors file in architecture {}&quot;.format(
                    model_family
                )
            )
    return model_path</file><file path="helpers/multiaspect/dataset.py">from torch.utils.data import Dataset
from helpers.training.state_tracker import StateTracker
from helpers.multiaspect.image import MultiaspectImage
from helpers.image_manipulation.training_sample import TrainingSample
import logging
import os
logger = logging.getLogger(&quot;MultiAspectDataset&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class MultiAspectDataset(Dataset):
    &quot;&quot;&quot;
    A multi-aspect dataset requires special consideration and handling.
    This class implements bucketed data loading for precomputed text embeddings.
    This class does not do any image transforms, as those are handled by VAECache.
    &quot;&quot;&quot;
    def __init__(
        self,
        id: str,
        datasets: list,
        print_names: bool = False,
        is_regularisation_data: bool = False,
        is_i2v_data: bool = False,
    ):
        self.id = id
        self.datasets = datasets
        self.print_names = print_names
        self.is_regularisation_data = is_regularisation_data
        self.is_i2v_data = is_i2v_data
    def __len__(self):
        # Sum the length of all data backends:
        return sum([len(dataset) for dataset in self.datasets])
    def __getitem__(self, image_tuple):
        output_data = {
            &quot;training_samples&quot;: [],
            &quot;conditioning_samples&quot;: [],
            &quot;is_regularisation_data&quot;: self.is_regularisation_data,
            &quot;is_i2v_data&quot;: self.is_i2v_data,
        }
        first_aspect_ratio = None
        for sample in image_tuple:
            if type(sample) is TrainingSample:
                image_metadata = sample.image_metadata
            else:
                image_metadata = sample
                if &quot;target_size&quot; in image_metadata:
                    calculated_aspect_ratio = (
                        MultiaspectImage.calculate_image_aspect_ratio(
                            image_metadata[&quot;target_size&quot;]
                        )
                    )
                    if first_aspect_ratio is None:
                        first_aspect_ratio = calculated_aspect_ratio
                    elif first_aspect_ratio != calculated_aspect_ratio:
                        raise ValueError(
                            f&quot;Aspect ratios must be the same for all images in a batch. Expected: {first_aspect_ratio}, got: {calculated_aspect_ratio}&quot;
                        )
                if &quot;deepfloyd&quot; not in StateTracker.get_args().model_type and (
                    image_metadata[&quot;original_size&quot;] is None
                    or image_metadata[&quot;target_size&quot;] is None
                ):
                    raise Exception(
                        f&quot;Metadata was unavailable for image: {image_metadata[&apos;image_path&apos;]}. Ensure --skip_file_discovery=metadata is not set.&quot;
                    )
                if self.print_names:
                    logger.info(
                        f&quot;Dataset is now using image: {image_metadata[&apos;image_path&apos;]}&quot;
                    )
            if type(sample) is TrainingSample:
                output_data[&quot;conditioning_samples&quot;].append(sample)
                continue
            else:
                output_data[&quot;training_samples&quot;].append(image_metadata)
            if &quot;instance_prompt_text&quot; not in image_metadata:
                raise ValueError(
                    f&quot;Instance prompt text must be provided in image metadata. Image metadata: {image_metadata}&quot;
                )
        return output_data</file><file path="helpers/multiaspect/image.py">from torchvision import transforms
from PIL import Image
import logging
import os
import numpy as np
from math import sqrt
from helpers.training.state_tracker import StateTracker
logger = logging.getLogger(&quot;MultiaspectImage&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_IMAGE_PREP_LOG_LEVEL&quot;, &quot;INFO&quot;))
import torch
from torchvision import transforms
from PIL import Image
import numpy as np
class VideoToTensor:
    def __call__(self, video):
        &quot;&quot;&quot;
        Converts a video (numpy array of shape (num_frames, height, width, channels))
        to a tensor of shape (num_frames, channels, height, width) by applying the
        standard ToTensor conversion to each frame.
        &quot;&quot;&quot;
        if isinstance(video, np.ndarray):
            frames = []
            for frame in video:
                # Convert frame to PIL Image if not already.
                if not isinstance(frame, Image.Image):
                    frame = Image.fromarray(frame)
                # Apply the standard ToTensor transform.
                frame_tensor = transforms.functional.to_tensor(frame)
                frames.append(frame_tensor)
            return torch.stack(frames)
        elif isinstance(video, list):
            # If video is a list of frames, process similarly.
            frames = []
            for frame in video:
                if not isinstance(frame, Image.Image):
                    frame = Image.fromarray(frame)
                frames.append(transforms.functional.to_tensor(frame))
            return torch.stack(frames)
        else:
            raise TypeError(&quot;Input video must be a numpy array or a list of frames.&quot;)
    def __repr__(self):
        return self.__class__.__name__ + &quot;()&quot;
class MultiaspectImage:
    @staticmethod
    def get_video_transforms():
        if not StateTracker.get_model_family() in [&quot;ltxvideo&quot;]:
            raise ValueError(
                f&quot;Cannot transform videos for {StateTracker.get_model_family()}.&quot;
            )
        # For videos, use the custom VideoToTensor transform.
        # Note: LTX Video applies its own normalisation later on.
        return transforms.Compose(
            [
                VideoToTensor(),
            ]
        )
    @staticmethod
    def get_image_transforms():
        if StateTracker.get_model_family() in [&quot;ltxvideo&quot;]:
            # LTX Video has its own normalisation, later on.
            return transforms.Compose(
                [
                    transforms.ToTensor(),
                ]
            )
        # default stable diffusion style latent normalisation.
        return transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5]),
            ]
        )
    @staticmethod
    def _round_to_nearest_multiple(value):
        &quot;&quot;&quot;Round a value to the nearest multiple.&quot;&quot;&quot;
        multiple = StateTracker.get_args().aspect_bucket_alignment
        rounded = round(value / multiple) * multiple
        return max(rounded, multiple)  # Ensure it&apos;s at least the value of &apos;multiple&apos;
    @staticmethod
    def is_image_too_large(image_size: tuple, resolution: float, resolution_type: str):
        &quot;&quot;&quot;
        Determine if an image is too large to be processed.
        Args:
            image (PIL.Image): The image to check.
            resolution (float): The maximum resolution to allow.
            resolution_type (str): What form of resolution to check, choices: &quot;pixel&quot;, &quot;area&quot;.
        Returns:
            bool: True if the image is too large, False otherwise.
        &quot;&quot;&quot;
        if resolution_type == &quot;pixel&quot;:
            return image_size[0] &gt; resolution or image_size[1] &gt; resolution
        elif resolution_type == &quot;area&quot;:
            image_area = image_size[0] * image_size[1]
            target_area = resolution * 1e6  # Convert megapixels to pixels
            logger.debug(
                f&quot;Image is too large? {image_area &gt; target_area} (image area: {image_area}, target area: {target_area})&quot;
            )
            return image_area &gt; target_area
        else:
            raise ValueError(f&quot;Unknown resolution type: {resolution_type}&quot;)
    @staticmethod
    def calculate_new_size_by_pixel_edge(
        aspect_ratio: float, resolution: int, original_size: tuple
    ):
        if type(aspect_ratio) != float:
            raise ValueError(f&quot;Aspect ratio must be a float, not {type(aspect_ratio)}&quot;)
        if type(resolution) != int and (
            type(resolution) != float or int(resolution) != resolution
        ):
            raise ValueError(f&quot;Resolution must be an int, not {type(resolution)}&quot;)
        W_original, H_original = original_size
        # Start by determining the potential initial sizes
        if W_original &lt; H_original:  # Portrait or square orientation
            W_initial = resolution
            H_initial = int(W_initial / aspect_ratio)
        else:  # Landscape orientation
            H_initial = resolution
            W_initial = int(H_initial * aspect_ratio)
        # Round down to ensure we do not exceed original dimensions
        W_adjusted = MultiaspectImage._round_to_nearest_multiple(W_initial)
        H_adjusted = MultiaspectImage._round_to_nearest_multiple(H_initial)
        # Intermediary size might be less than the reformed size.
        # This situation is difficult.
        # If the original image is roughly the size of the reformed image, and the intermediary is too small,
        #  we can&apos;t really just boost the size of the reformed image willy-nilly. The intermediary size needs to be larger.
        # We can&apos;t increase the intermediary size larger than the original size.
        if W_initial &lt; W_adjusted or H_initial &lt; H_adjusted:
            logger.debug(
                f&quot;Intermediary size {W_initial}x{H_initial} would be smaller than {W_adjusted}x{H_adjusted} (original size: {original_size}, aspect ratio: {aspect_ratio}).&quot;
            )
            # How much leeway to we have between the intermediary size and the reformed size?
            reformed_W_diff = W_adjusted - W_initial
            reformed_H_diff = H_adjusted - H_initial
            bigger_difference = max(reformed_W_diff, reformed_H_diff)
            logger.debug(
                f&quot;We have {reformed_W_diff}x{reformed_H_diff} leeway to the reformed image {W_adjusted}x{H_adjusted} from {W_initial}x{H_initial}, adjusting by {bigger_difference}px to both sides: {W_initial + bigger_difference}x{H_initial + bigger_difference}.&quot;
            )
            W_initial += bigger_difference
            H_initial += bigger_difference
        adjusted_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            (W_adjusted, H_adjusted)
        )
        return (W_adjusted, H_adjusted), (W_initial, H_initial), adjusted_aspect_ratio
    @staticmethod
    def calculate_new_size_by_pixel_area(
        aspect_ratio: float, megapixels: float, original_size: tuple
    ):
        if type(aspect_ratio) not in [float, np.float64]:
            raise ValueError(f&quot;Aspect ratio must be a float, not {type(aspect_ratio)}&quot;)
        target_pixel_area = (
            megapixels * 1e6
        )  # Convert megapixels to pixel area, eg. 1.0 mp = 1000000 pixels
        target_pixel_edge = MultiaspectImage._round_to_nearest_multiple(
            int(sqrt(target_pixel_area))
        )
        logger.debug(
            f&quot;Converted {megapixels} megapixels to {target_pixel_area} pixels with a square edge of {target_pixel_edge}.&quot;
        )
        W_initial, H_initial = original_size
        if aspect_ratio == 1.0:
            # If the aspect ratio is 1.0, we can just use the square edge as the target size.
            logger.debug(
                f&quot;Returning the square edge {target_pixel_edge}x{target_pixel_edge} as the target size and original size as intermediary.&quot;
            )
            if W_initial == H_initial:
                # if we have squares, resizing straight to the target is alright.
                return (
                    (target_pixel_edge, target_pixel_edge),
                    (target_pixel_edge, target_pixel_edge),
                    aspect_ratio,
                )
            return (
                (target_pixel_edge, target_pixel_edge),
                (W_initial, H_initial),
                aspect_ratio,
            )
        # Calculate the target size. This is what will be cropped-to.
        W_target = MultiaspectImage._round_to_nearest_multiple(
            target_pixel_edge * sqrt(aspect_ratio)
        )
        H_target = MultiaspectImage._round_to_nearest_multiple(
            target_pixel_edge / sqrt(aspect_ratio)
        )
        calculated_resulting_megapixels = (W_target * H_target) / 1e6
        target_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            (W_target, H_target)
        )
        if not np.isclose(calculated_resulting_megapixels, megapixels, rtol=1e-1):
            logger.debug(
                f&quot;-!- This image will not have the correct target megapixel size: {calculated_resulting_megapixels}&quot;
            )
        # Calculate the intermediary size. This will maintain aspect ratio and be resized-to.
        if W_target &lt; H_target:  # Portrait or square orientation
            W_intermediary = W_target
            H_intermediary = int(W_intermediary / aspect_ratio)
        else:  # Landscape orientation
            H_intermediary = H_target
            W_intermediary = int(H_intermediary * aspect_ratio)
        # retrieve the static mapping.
        adjusted_aspect_ratio = MultiaspectImage.calculate_image_aspect_ratio(
            (W_target, H_target)
        )
        previously_stored_resolution = StateTracker.get_resolution_by_aspect(
            dataloader_resolution=megapixels, aspect=adjusted_aspect_ratio
        )
        if previously_stored_resolution:
            logger.debug(
                f&quot;Using cached aspect-resolution map value for {adjusted_aspect_ratio}: {previously_stored_resolution}&quot;
            )
            W_target, H_target = previously_stored_resolution
        target_resolution = (W_target, H_target)
        # The intermediary size might be smaller than the target. This is bad.
        # If it happens, the cropped image will be cropped past the boundaries of the intermediary size.
        if W_target &gt; W_intermediary or H_target &gt; H_intermediary:
            _W_intermediary, _H_intermediary = W_intermediary, H_intermediary
            if W_target &gt; W_intermediary:
                W_diff = W_target - W_intermediary
                H_diff = int(W_diff / aspect_ratio)
            else:
                H_diff = H_target - H_intermediary
                W_diff = int(H_diff * aspect_ratio)
            H_intermediary += H_diff
            W_intermediary += W_diff
            logger.debug(
                f&quot;Intermediary size {_W_intermediary}x{_H_intermediary} would be smaller than {W_target}x{H_target} with a difference in size of {W_diff}x{H_diff}.&quot;
                f&quot; The size will be adjusted to maintain the aspect ratio: {W_intermediary}x{H_intermediary}.&quot;
            )
            calculated_resulting_megapixels = (W_intermediary * H_intermediary) / 1e6
        intermediary_resolution = (W_intermediary, H_intermediary)
        logger.debug(
            f&quot;Using target size of {megapixels} megapixels:&quot;
            f&quot;\n-&gt; initial size is {W_initial}x{H_initial}, original aspect ratio {aspect_ratio}.&quot;
            f&quot;\n-&gt; intermediary size is {W_intermediary}x{H_intermediary}, with aspect ratio {adjusted_aspect_ratio}.&quot;
            f&quot;\n-&gt; cropped size is {W_target}x{H_target}, with aspect ratio {target_aspect_ratio}.&quot;
            f&quot;\n-&gt; cropped sample will be {calculated_resulting_megapixels} megapixels&quot;
        )
        # Attempt to retrieve previously stored resolution by adjusted aspect ratio
        if not previously_stored_resolution:
            logger.debug(
                f&quot;No cached resolution found for aspect ratio {adjusted_aspect_ratio}. Storing {target_resolution}.&quot;
            )
            StateTracker.set_resolution_by_aspect(
                dataloader_resolution=megapixels,
                aspect=adjusted_aspect_ratio,
                resolution=target_resolution,
            )
        return (target_resolution, intermediary_resolution, adjusted_aspect_ratio)
    @staticmethod
    def adjust_resolution_to_bucket_interval(
        initial_resolution: tuple, target_resolution: tuple
    ):
        W_initial, H_initial = initial_resolution
        W_adjusted, H_adjusted = target_resolution
        # If W_initial or H_initial are &lt; W_adjusted or H_adjusted, add the greater of the two differences to both values.
        W_diff = W_adjusted - W_initial
        H_diff = H_adjusted - H_initial
        if W_diff &gt; 0 and (W_diff &gt; H_diff or W_diff == H_diff):
            logger.debug(
                f&quot;Intermediary size {W_initial}x{H_initial} would be smaller than {W_adjusted}x{H_adjusted} with a difference in size of {W_diff}x{H_diff}. Adjusting both sides by {max(W_diff, H_diff)} pixels.&quot;
            )
            H_initial += W_diff
            W_initial += W_diff
        elif H_diff &gt; 0 and H_diff &gt; W_diff:
            logger.debug(
                f&quot;Intermediary size {W_initial}x{H_initial} would be smaller than {W_adjusted}x{H_adjusted} with a difference in size of {W_diff}x{H_diff}. Adjusting both sides by {max(W_diff, H_diff)} pixels.&quot;
            )
            W_initial += H_diff
            H_initial += H_diff
        return W_initial, H_initial
    @staticmethod
    def calculate_image_aspect_ratio(image, rounding: int = 2):
        &quot;&quot;&quot;
        Calculate the aspect ratio of an image and round it to a specified precision.
        Args:
            image (PIL.Image): The image to calculate the aspect ratio for.
        Returns:
            float: The rounded aspect ratio of the image.
        &quot;&quot;&quot;
        to_round = StateTracker.get_args().aspect_bucket_rounding
        if to_round is None:
            to_round = rounding
        if isinstance(image, Image.Image):
            # An actual image was passed in.
            width, height = image.size
        elif isinstance(image, tuple) or isinstance(image, list):
            # An image.size or a similar (W, H) tuple was provided.
            width, height = image
        elif isinstance(image, float):
            # An externally-calculated aspect ratio was given to round.
            return round(image, to_round)
        elif isinstance(image, np.ndarray):
            # A video was passed in as a numpy array.
            width, height = image.shape[2], image.shape[1]
        else:
            raise ValueError(f&quot;Unexpected type {image}&quot;)
            width, height = image.size
        aspect_ratio = round(width / height, to_round)
        return aspect_ratio
resize_helpers = {
    &quot;pixel&quot;: MultiaspectImage.calculate_new_size_by_pixel_edge,
    &quot;area&quot;: MultiaspectImage.calculate_new_size_by_pixel_area,
}</file><file path="helpers/multiaspect/sampler.py">import torch
import logging
import random
import os
from helpers.training.multi_process import rank_info
from helpers.metadata.backends.base import MetadataBackend
from helpers.image_manipulation.training_sample import TrainingSample
from helpers.multiaspect.image import MultiaspectImage
from helpers.multiaspect.state import BucketStateManager
from helpers.data_backend.base import BaseDataBackend
from helpers.training.state_tracker import StateTracker
from helpers.training.exceptions import MultiDatasetExhausted
from helpers.prompts import PromptHandler
from accelerate.logging import get_logger
pil_logger = logging.getLogger(&quot;PIL.Image&quot;)
pil_logger.setLevel(logging.WARNING)
pil_logger = logging.getLogger(&quot;PIL.PngImagePlugin&quot;)
pil_logger.setLevel(logging.WARNING)
pil_logger = logging.getLogger(&quot;PIL.TiffImagePlugin&quot;)
pil_logger.setLevel(logging.WARNING)
class MultiAspectSampler(torch.utils.data.Sampler):
    def __init__(
        self,
        id: str,
        metadata_backend: MetadataBackend,
        data_backend: BaseDataBackend,
        accelerator,
        batch_size: int,
        debug_aspect_buckets: bool = False,
        delete_unwanted_images: bool = False,
        minimum_image_size: int = None,
        resolution: int = 1024,
        resolution_type: str = &quot;pixel&quot;,
        caption_strategy: str = &quot;filename&quot;,
        use_captions=True,
        prepend_instance_prompt=False,
        instance_prompt: str = None,
        conditioning_type: str = None,
        is_regularisation_data: bool = False,
        dataset_type: str = &quot;image&quot;,
    ):
        &quot;&quot;&quot;
        Initializes the sampler with provided settings.
        Parameters:
        - id: An identifier to link this with its VAECache and DataBackend objects.
        - metadata_backend: An initialised instance of MetadataBackend.
        - batch_size: Number of samples to draw per batch.
        - state_path: Path to store the current state of the sampler.
        - debug_aspect_buckets: Flag to log state for debugging purposes.
        - delete_unwanted_images: Flag to decide whether to delete unwanted (small) images or just remove from the bucket.
        - minimum_image_size: The minimum pixel length of the smallest side of an image.
        &quot;&quot;&quot;
        self.id = id
        if self.id != data_backend.id or self.id != metadata_backend.id:
            raise ValueError(
                f&quot;Sampler ID ({self.id}) must match DataBackend ID ({data_backend.id}) and MetadataBackend ID ({metadata_backend.id}).&quot;
            )
        # Update the logger name with the id:
        self.dataset_type = dataset_type
        self.sample_type_str = &quot;image&quot;
        self.sample_type_strs = &quot;images&quot;
        if dataset_type == &quot;video&quot;:
            self.sample_type_str = &quot;video&quot;
            self.sample_type_strs = &quot;videos&quot;
        self.logger = get_logger(
            f&quot;MultiAspectSampler-{self.id}&quot;,
            os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;),
        )
        if conditioning_type is not None:
            if conditioning_type not in [&quot;controlnet&quot;, &quot;mask&quot;]:
                raise ValueError(
                    f&quot;Unknown conditioning image type: {conditioning_type}&quot;
                )
        self.conditioning_type = conditioning_type
        self.is_regularisation_data = is_regularisation_data
        self.rank_info = rank_info()
        self.accelerator = accelerator
        self.metadata_backend = metadata_backend
        self.data_backend = data_backend
        self.current_bucket = None
        self.current_epoch = 1
        self.batch_size = batch_size
        if debug_aspect_buckets:
            self.logger.setLevel(logging.DEBUG)
        self.delete_unwanted_images = delete_unwanted_images
        self.minimum_image_size = minimum_image_size
        self.resolution = resolution
        self.resolution_type = resolution_type
        self.use_captions = use_captions
        self.caption_strategy = caption_strategy
        self.prepend_instance_prompt = prepend_instance_prompt
        self.instance_prompt = instance_prompt
        self.exhausted_buckets = []
        self.buckets = self.load_buckets()
        self.state_manager = BucketStateManager(self.id)
    def save_state(self, state_path: str):
        &quot;&quot;&quot;
        This method should be called when the accelerator save hook is called,
         so that the state is correctly restored with a given checkpoint.
        &quot;&quot;&quot;
        state = {
            &quot;aspect_ratio_bucket_indices&quot;: self.metadata_backend.aspect_ratio_bucket_indices,
            &quot;buckets&quot;: self.buckets,
            &quot;exhausted_buckets&quot;: self.exhausted_buckets,
            &quot;batch_size&quot;: self.batch_size,
            &quot;current_bucket&quot;: self.current_bucket,
            &quot;seen_images&quot;: self.metadata_backend.seen_images,
            &quot;current_epoch&quot;: self.current_epoch,
        }
        self.state_manager.save_state(state, state_path)
    def load_states(self, state_path: str):
        try:
            self.buckets = self.load_buckets()
            previous_state = self.state_manager.load_state(state_path)
        except Exception as e:
            raise e
        self.exhausted_buckets = []
        if &quot;exhausted_buckets&quot; in previous_state:
            self.logger.info(
                f&quot;Previous checkpoint had {len(previous_state[&apos;exhausted_buckets&apos;])} exhausted buckets.&quot;
            )
            self.exhausted_buckets = previous_state[&quot;exhausted_buckets&quot;]
        self.current_epoch = 1
        if &quot;current_epoch&quot; in previous_state:
            self.logger.info(
                f&quot;Previous checkpoint was on epoch {previous_state[&apos;current_epoch&apos;]}.&quot;
            )
            self.current_epoch = previous_state[&quot;current_epoch&quot;]
        # Merge seen_images into self.state_manager.seen_images Manager.dict:
        if &quot;seen_images&quot; in previous_state:
            self.logger.info(
                f&quot;Previous checkpoint had {len(previous_state[&apos;seen_images&apos;])} seen {self.sample_type_strs}.&quot;
            )
            self.metadata_backend.seen_images.update(previous_state[&quot;seen_images&quot;])
    def load_buckets(self):
        return list(
            self.metadata_backend.aspect_ratio_bucket_indices.keys()
        )  # These keys are a float value, eg. 1.78.
    def retrieve_validation_set(self, batch_size: int):
        &quot;&quot;&quot;
        Return random images from the set. They should be paired with their caption.
        Args:
            batch_size (int): Number of images to return.
        Returns:
            list: a list of tuples(validation_shortname, validation_prompt, validation_sample)
        &quot;&quot;&quot;
        results = (
            []
        )  # [tuple(validation_shortname, validation_prompt, validation_sample)]
        for img_idx in range(batch_size):
            image_path = self._yield_random_image()
            image_data = self.data_backend.read_image(image_path)
            image_metadata = self.metadata_backend.get_metadata_by_filepath(image_path)
            training_sample = TrainingSample(
                image=image_data,
                data_backend_id=self.id,
                image_metadata=image_metadata,
                image_path=image_path,
            )
            training_sample.prepare()
            validation_shortname = f&quot;{self.id}_{img_idx}&quot;
            validation_prompt = PromptHandler.magic_prompt(
                sampler_backend_id=self.id,
                data_backend=self.data_backend,
                image_path=image_path,
                caption_strategy=self.caption_strategy,
                use_captions=self.use_captions,
                prepend_instance_prompt=self.prepend_instance_prompt,
                instance_prompt=self.instance_prompt,
            )
            if type(validation_prompt) == list:
                validation_prompt = random.choice(validation_prompt)
                self.debug_log(
                    f&quot;Selecting random prompt from list: {validation_prompt}&quot;
                )
            results.append(
                (validation_shortname, validation_prompt, training_sample.image)
            )
        return results
    def _yield_n_from_exhausted_bucket(self, n: int, bucket: str):
        &quot;&quot;&quot;
        when a bucket is exhausted, and we have to populate the remainder of the batch,
        we shall use this quick and dirty method to retrieve n samples from the exhausted bucket.
        the thing is we can have a batch size of 4 and 1 image. so we&apos;ll have to just return the same image 4 times.
        &quot;&quot;&quot;
        available_images = self.metadata_backend.aspect_ratio_bucket_indices[bucket]
        if len(available_images) == 0:
            self.debug_log(f&quot;Bucket {bucket} is empty.&quot;)
            return []
        samples = []
        while len(samples) &lt; n:
            to_grab = min(n, len(available_images), (n - len(samples)))
            if to_grab == 0:
                break
            samples.extend(random.sample(available_images, k=to_grab))
        to_yield = self._validate_and_yield_images_from_samples(samples, bucket)
        return to_yield
    def _yield_random_image(self):
        bucket = random.choice(self.buckets)
        image_path = random.choice(
            self.metadata_backend.aspect_ratio_bucket_indices[bucket]
        )
        return image_path
    def yield_single_image(self, filepath: str):
        &quot;&quot;&quot;
        Yield a single image from the dataset by path.
        If the path prefix isn&apos;t in the path, we&apos;ll add it.
        &quot;&quot;&quot;
        if (
            self.metadata_backend.instance_data_dir is not None
            and self.metadata_backend.instance_data_dir not in filepath
            and not filepath.startswith(&quot;http&quot;)
        ):
            filepath = os.path.join(self.metadata_backend.instance_data_dir, filepath)
        image_data = self.data_backend.read_image(filepath)
        return image_data
    def _bucket_name_to_id(self, bucket_name: str) -&gt; int:
        &quot;&quot;&quot;
        Return a bucket array index, by its name.
        Args:
            bucket_name (str): Bucket name, eg. &quot;1.78&quot;
        Returns:
            int: Bucket array index, eg. 0
        &quot;&quot;&quot;
        if &quot;.&quot; not in str(bucket_name):
            self.debug_log(f&quot;Assuming {bucket_name} is already an index.&quot;)
            return int(bucket_name)
        return self.buckets.index(str(bucket_name))
    def _reset_buckets(self, raise_exhaustion_signal: bool = True):
        if (
            len(self.metadata_backend.seen_images) == 0
            and len(self._get_unseen_images()) == 0
        ):
            raise Exception(
                f&quot;No images found in the dataset: {self.metadata_backend.aspect_ratio_bucket_indices}&quot;
                f&quot;\n-&gt; Unseen {self.sample_type_strs}: {self._get_unseen_images()}&quot;
                f&quot;\n-&gt; Seen {self.sample_type_strs}: {self.metadata_backend.seen_images}&quot;
            )
        if StateTracker.get_args().print_sampler_statistics:
            self.logger.info(
                f&quot;Resetting seen {self.sample_type_str} list and refreshing buckets. State before reset:&quot;
            )
            self.log_state()
        # All buckets are exhausted, so we will move onto the next epoch.
        self.current_epoch += 1
        self.exhausted_buckets = []
        self.buckets = self.load_buckets()
        self.metadata_backend.reset_seen_images()
        self.change_bucket()
        if raise_exhaustion_signal:
            raise MultiDatasetExhausted()
    def _get_unseen_images(self, bucket=None):
        &quot;&quot;&quot;
        Get unseen {self.sample_type_strs} from the specified bucket.
        If bucket is None, get unseen {self.sample_type_strs} from all buckets.
        &quot;&quot;&quot;
        if bucket and bucket in self.metadata_backend.aspect_ratio_bucket_indices:
            return [
                (
                    os.path.join(self.metadata_backend.instance_data_dir, image)
                    if not image.startswith(&quot;http&quot;)
                    else image
                )
                for image in self.metadata_backend.aspect_ratio_bucket_indices[bucket]
                if not self.metadata_backend.is_seen(image)
            ]
        elif bucket is None:
            unseen_images = []
            for b, images in self.metadata_backend.aspect_ratio_bucket_indices.items():
                unseen_images.extend(
                    [
                        (
                            os.path.join(self.metadata_backend.instance_data_dir, image)
                            if not image.startswith(&quot;http&quot;)
                            else image
                        )
                        for image in images
                        if not self.metadata_backend.is_seen(image)
                    ]
                )
            return unseen_images
        else:
            return []
    def _handle_bucket_with_insufficient_images(self, bucket):
        &quot;&quot;&quot;
        Handle buckets with insufficient images. Return True if we changed or reset the bucket.
        &quot;&quot;&quot;
        if (
            len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])
            &lt; self.batch_size
        ):
            self.debug_log(
                f&quot;Bucket {bucket} has insufficient ({len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])}) images.&quot;
            )
            if bucket not in self.exhausted_buckets:
                self.debug_log(
                    f&quot;Bucket {bucket} is now exhausted and sleepy, and we have to move it to the sleepy list before changing buckets.&quot;
                )
                self.move_to_exhausted()
            self.debug_log(&quot;Changing bucket to another random selection.&quot;)
            self.change_bucket()
            return True
        self.debug_log(
            f&quot;Bucket {bucket} has sufficient ({len(self.metadata_backend.aspect_ratio_bucket_indices[bucket])}) images.&quot;
        )
        return False
    def _get_next_bucket(self):
        &quot;&quot;&quot;
        Get the next bucket excluding the exhausted ones.
        If all buckets are exhausted, first reset the seen {self.sample_type_strs} and exhausted buckets.
        &quot;&quot;&quot;
        available_buckets = [
            bucket for bucket in self.buckets if bucket not in self.exhausted_buckets
        ]
        if not available_buckets:
            # Raise MultiDatasetExhausted
            self._reset_buckets()
        if len(self.exhausted_buckets) &gt; 0:
            self.debug_log(f&quot;exhausted buckets: {self.exhausted_buckets}&quot;)
        # Sequentially get the next bucket
        if hasattr(self, &quot;current_bucket&quot;) and self.current_bucket is not None:
            self.current_bucket = (self.current_bucket + 1) % len(available_buckets)
        else:
            self.current_bucket = 0
        if self.buckets[self.current_bucket] not in available_buckets:
            random_bucket = random.choice(available_buckets)
            self.current_bucket = available_buckets.index(random_bucket)
        next_bucket = available_buckets[self.current_bucket]
        return next_bucket
    def change_bucket(self):
        &quot;&quot;&quot;
        Change the current bucket to a new one and exclude exhausted buckets from consideration.
        During _get_next_bucket(), if all buckets are exhausted, reset the exhausted list and seen {self.sample_type_strs}.
        &quot;&quot;&quot;
        next_bucket = self._get_next_bucket()
        self.current_bucket = self._bucket_name_to_id(next_bucket)
        self._clear_batch_accumulator()
    def move_to_exhausted(self):
        bucket = self.buckets[self.current_bucket]
        self.exhausted_buckets.append(bucket)
        self.buckets.remove(bucket)
        self.debug_log(
            f&quot;Bucket {bucket} is empty or doesn&apos;t have enough samples for a full batch. Removing from bucket list. {len(self.buckets)} remain.&quot;
        )
    def log_state(self, show_rank: bool = True, alt_stats: bool = False):
        self.debug_log(
            f&apos;Active Buckets: {&quot;, &quot;.join(self.convert_to_human_readable(float(b), self.metadata_backend.aspect_ratio_bucket_indices[b], self.resolution) for b in self.buckets)}&apos;
        )
        self.debug_log(
            f&apos;Exhausted Buckets: {&quot;, &quot;.join(self.convert_to_human_readable(float(b), self.metadata_backend.aspect_ratio_bucket_indices.get(b, &quot;N/A&quot;), self.resolution) for b in self.exhausted_buckets)}&apos;
        )
        if alt_stats:
            # Return an overview instead of a snapshot.
            # Eg. return totals, and not &quot;as it is now&quot;
            total_image_count = len(self.metadata_backend.seen_images) + len(
                self._get_unseen_images()
            )
            if self.accelerator.num_processes &gt; 1:
                # We don&apos;t know the direct count without more work, so we&apos;ll estimate it here for multi-GPU training.
                total_image_count *= self.accelerator.num_processes
                total_image_count = f&quot;~{total_image_count}&quot;
            data_backend_config = StateTracker.get_data_backend_config(self.id)
            printed_state = (
                f&quot;- Repeats: {data_backend_config.get(&apos;repeats&apos;, 0)}\n&quot;
                f&quot;- Total number of images: {total_image_count}\n&quot;
                f&quot;- Total number of aspect buckets: {len(self.buckets)}\n&quot;
                f&quot;- Resolution: {self.resolution} {&apos;megapixels&apos; if self.resolution_type == &apos;area&apos; else &apos;px&apos;}\n&quot;
                f&quot;- Cropped: {data_backend_config.get(&apos;crop&apos;)}\n&quot;
                f&quot;- Crop style: {&apos;None&apos; if not data_backend_config.get(&apos;crop&apos;) else data_backend_config.get(&apos;crop_style&apos;)}\n&quot;
                f&quot;- Crop aspect: {&apos;None&apos; if not data_backend_config.get(&apos;crop&apos;) else data_backend_config.get(&apos;crop_aspect&apos;)}\n&quot;
                f&quot;- Used for regularisation data: {&apos;Yes&apos; if self.is_regularisation_data else &apos;No&apos;}\n&quot;
            )
            if self.conditioning_type:
                printed_state += f&quot;- Conditioning type: {self.conditioning_type}\n&quot;
        else:
            # Return a snapshot of the current state during training.
            printed_state = (
                f&quot;\n{self.rank_info if show_rank else &apos;&apos;}    -&gt; Number of seen {self.sample_type_strs}: {len(self.metadata_backend.seen_images)}&quot;
                f&quot;\n{self.rank_info if show_rank else &apos;&apos;}    -&gt; Number of unseen {self.sample_type_strs}: {len(self._get_unseen_images())}&quot;
                f&quot;\n{self.rank_info if show_rank else &apos;&apos;}    -&gt; Current Bucket: {self.current_bucket}&quot;
                f&quot;\n{self.rank_info if show_rank else &apos;&apos;}    -&gt; {len(self.buckets)} Buckets: {self.buckets}&quot;
                f&quot;\n{self.rank_info if show_rank else &apos;&apos;}    -&gt; {len(self.exhausted_buckets)} Exhausted Buckets: {self.exhausted_buckets}&quot;
            )
        self.logger.info(printed_state)
        return printed_state
    def _validate_and_yield_images_from_samples(self, samples, bucket):
        &quot;&quot;&quot;
        Validate and yield images from given samples. Return a list of valid image paths.
        &quot;&quot;&quot;
        to_yield = []
        for image_path in samples:
            image_metadata = self.metadata_backend.get_metadata_by_filepath(image_path)
            if image_metadata is None:
                image_metadata = {}
            if (
                StateTracker.get_args().model_type
                not in [
                    &quot;legacy&quot;,
                    &quot;deepfloyd-full&quot;,
                    &quot;deepfloyd-lora&quot;,
                    &quot;deepfloyd-stage2&quot;,
                    &quot;deepfloyd-stage2-lora&quot;,
                ]
                and &quot;crop_coordinates&quot; not in image_metadata
            ):
                raise Exception(
                    f&quot;An image was discovered ({image_path}) that did not have its metadata: {self.metadata_backend.get_metadata_by_filepath(image_path)}&quot;
                )
            image_metadata[&quot;data_backend_id&quot;] = self.id
            image_metadata[&quot;image_path&quot;] = image_path
            # Use the magic prompt handler to retrieve the captions.
            instance_prompt = PromptHandler.magic_prompt(
                sampler_backend_id=self.id,
                data_backend=self.data_backend,
                image_path=image_metadata[&quot;image_path&quot;],
                caption_strategy=self.caption_strategy,
                use_captions=self.use_captions,
                prepend_instance_prompt=self.prepend_instance_prompt,
                instance_prompt=self.instance_prompt,
            )
            if type(instance_prompt) == list:
                instance_prompt = random.choice(instance_prompt)
                self.debug_log(f&quot;Selecting random prompt from list: {instance_prompt}&quot;)
            image_metadata[&quot;instance_prompt_text&quot;] = instance_prompt
            to_yield.append(image_metadata)
        return to_yield
    def _clear_batch_accumulator(self):
        self.batch_accumulator = []
    def get_conditioning_sample(self, original_sample_path: str) -&gt; str:
        &quot;&quot;&quot;
        Given an original dataset sample path, return a TrainingSample
        &quot;&quot;&quot;
        # strip leading /
        original_sample_path = original_sample_path.lstrip(&quot;/&quot;)
        full_path = os.path.join(
            self.metadata_backend.instance_data_dir, original_sample_path
        )
        try:
            conditioning_sample_data = self.data_backend.read_image(full_path)
        except Exception as e:
            self.logger.error(f&quot;Could not fetch conditioning sample: {e}&quot;)
            return None
        if not conditioning_sample_data:
            self.debug_log(f&quot;Could not fetch conditioning sample from {full_path}.&quot;)
            return None
        conditioning_sample = TrainingSample(
            image=conditioning_sample_data,
            data_backend_id=self.id,
            image_metadata=self.metadata_backend.get_metadata_by_filepath(full_path),
            image_path=full_path,
            conditioning_type=self.conditioning_type,
        )
        return conditioning_sample
    def connect_conditioning_samples(self, samples: tuple):
        # Locate the conditioning data
        conditioning_dataset = StateTracker.get_conditioning_dataset(self.id)
        if conditioning_dataset is None:
            return samples
        sampler = conditioning_dataset[&quot;sampler&quot;]
        outputs = list(samples)
        for sample in samples:
            sample_path = sample[&quot;image_path&quot;].split(
                self.metadata_backend.instance_data_dir
            )[-1]
            conditioning_sample = sampler.get_conditioning_sample(sample_path)
            outputs.append(conditioning_sample)
        return tuple(outputs)
    def __iter__(self):
        &quot;&quot;&quot;
        Iterate over the sampler to yield image paths in batches.
        &quot;&quot;&quot;
        self._clear_batch_accumulator()  # Initialize an empty list to accumulate images for a batch
        self.change_bucket()
        while True:
            all_buckets_exhausted = True  # Initial assumption
            # Loop through all buckets to find one with sufficient images
            for _ in range(len(self.buckets)):
                self._clear_batch_accumulator()
                available_images = self._get_unseen_images(
                    self.buckets[self.current_bucket]
                )
                self.debug_log(
                    f&quot;From {len(self.buckets)} buckets, selected {self.buckets[self.current_bucket]} ({self.buckets[self.current_bucket]}) -&gt; {len(available_images)} available images, and our accumulator has {len(self.batch_accumulator)} images ready for yielding.&quot;
                )
                if len(available_images) &gt; 0:
                    all_buckets_exhausted = False  # Found a non-exhausted bucket
                    break
                else:
                    # Current bucket doesn&apos;t have enough images, try the next bucket
                    self.move_to_exhausted()
                    self.change_bucket()
            while len(available_images) &gt; 0:
                if len(available_images) &lt; self.batch_size:
                    need_image_count = self.batch_size - len(available_images)
                    self.debug_log(
                        f&quot;Bucket {self.buckets[self.current_bucket]} has {len(available_images)} available images, but we need {need_image_count} more.&quot;
                    )
                    to_yield = self._yield_n_from_exhausted_bucket(
                        need_image_count, self.buckets[self.current_bucket]
                    )
                    # add the available images
                    to_yield.extend(
                        self._validate_and_yield_images_from_samples(
                            available_images, self.buckets[self.current_bucket]
                        )
                    )
                else:
                    all_buckets_exhausted = False  # Found a non-exhausted bucket
                    samples = random.sample(
                        available_images, k=min(len(available_images), self.batch_size)
                    )
                    to_yield = self._validate_and_yield_images_from_samples(
                        samples, self.buckets[self.current_bucket]
                    )
                self.debug_log(
                    f&quot;Building batch with {len(self.batch_accumulator)} samples.&quot;
                )
                if len(self.batch_accumulator) &lt; self.batch_size:
                    remaining_entries_needed = self.batch_size - len(
                        self.batch_accumulator
                    )
                    # Now we&apos;ll add only remaining_entries_needed amount to the accumulator:
                    if &quot;target_size&quot; in to_yield[0]:
                        self.debug_log(
                            f&quot;Current bucket: {self.current_bucket}. Adding samples with aspect ratios: {[MultiaspectImage.calculate_image_aspect_ratio(i[&apos;target_size&apos;]) for i in to_yield[:remaining_entries_needed]]}&quot;
                        )
                    self.batch_accumulator.extend(to_yield[:remaining_entries_needed])
                # If the batch is full, yield it
                if len(self.batch_accumulator) &gt;= self.batch_size:
                    final_yield = self.batch_accumulator[: self.batch_size]
                    self.debug_log(
                        f&quot;Yielding samples and marking {len(final_yield)} images as seen, we have {len(self.metadata_backend.seen_images.values())} seen {self.sample_type_strs} before adding.&quot;
                    )
                    self.metadata_backend.mark_batch_as_seen(
                        [instance[&quot;image_path&quot;] for instance in final_yield]
                    )
                    # if applicable, we&apos;ll append TrainingSample(s) to the end for conditioning inputs.
                    final_yield = self.connect_conditioning_samples(final_yield)
                    yield tuple(final_yield)
                    # Change bucket after a full batch is yielded
                    self.change_bucket()
                    # Break out of the while loop:
                    break
                # Update available images after yielding
                available_images = self._get_unseen_images(
                    self.buckets[self.current_bucket]
                )
                self.debug_log(
                    f&quot;Bucket {self.buckets[self.current_bucket]} now has {len(available_images)} available images after yielding.&quot;
                )
            # Handle exhausted bucket
            if len(available_images) &lt; self.batch_size:
                self.debug_log(
                    f&quot;Bucket {self.buckets[self.current_bucket]} is now exhausted and sleepy, and we have to move it to the sleepy list before changing buckets.&quot;
                )
                self.move_to_exhausted()
                self.change_bucket()
            # Check if all buckets are exhausted
            if all_buckets_exhausted:
                # If all buckets are exhausted, reset the seen {self.sample_type_strs} and refresh buckets
                self.logger.warning(
                    &quot;All buckets exhausted - since this is happening now, most likely you have chronically-underfilled buckets.&quot;
                )
                # Resetting buckets raises MultiDatasetExhausted
                self._reset_buckets()
    def __len__(self):
        backend_config = StateTracker.get_data_backend_config(self.id)
        repeats = backend_config.get(&quot;repeats&quot;, 0)
        # We need at least a multiplier of 1. Repeats is the number of extra sample steps.
        multiplier = repeats + 1 if repeats &gt; 0 else 1
        total_samples = (
            sum(
                len(indices)
                for indices in self.metadata_backend.aspect_ratio_bucket_indices.values()
            )
            * multiplier
        )
        # Calculate the total number of full batches
        total_batches = (total_samples + (self.batch_size - 1)) // self.batch_size
        return total_batches
    @staticmethod
    def convert_to_human_readable(
        aspect_ratio_float: float, bucket: iter, resolution: int = 1024
    ):
        if aspect_ratio_float &lt; 1:
            ratio_width = resolution
            ratio_height = int(resolution / aspect_ratio_float)
        else:
            ratio_width = int(resolution * aspect_ratio_float)
            ratio_height = resolution
        # Return the aspect ratio as a string in the format &quot;width:height&quot;
        return f&quot;{aspect_ratio_float} ({len(bucket)} samples)&quot;
        return f&quot;{ratio_width}:{ratio_height}&quot;
    def debug_log(self, msg: str):
        self.logger.debug(f&quot;{self.rank_info} {msg}&quot;, main_process_only=False)</file><file path="helpers/multiaspect/state.py">import json
import os
import logging
from multiprocessing.managers import DictProxy
logger = logging.getLogger(&quot;BucketStateManager&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class BucketStateManager:
    def __init__(self, id: str):
        self.id = id
    def mangle_state_path(self, state_path):
        # When saving the state, it goes into the checkpoint dir.
        # However, we need to save a single state for each data backend.
        # Thus, we split the state_path from its extension, add self.id to the end of the name, and rejoin:
        if self.id in os.path.basename(state_path):
            return state_path
        filename, ext = os.path.splitext(state_path)
        return f&quot;{filename}-{self.id}{ext}&quot;
    def load_seen_images(self, state_path: str):
        if os.path.exists(state_path):
            with open(state_path, &quot;r&quot;) as f:
                return json.load(f)
        else:
            return {}
    def save_seen_images(self, seen_images, state_path: str):
        with open(state_path, &quot;w&quot;) as f:
            json.dump(seen_images, f)
    def deep_convert_dict(self, d):
        if isinstance(d, dict):
            return {key: self.deep_convert_dict(value) for key, value in d.items()}
        elif isinstance(d, list):
            return [self.deep_convert_dict(value) for value in d]
        elif isinstance(d, DictProxy):
            return self.deep_convert_dict(dict(d))
        else:
            return d
    def save_state(self, state: dict, state_path: str):
        if state_path is None:
            raise ValueError(&quot;state_path must be specified&quot;)
        state_path = self.mangle_state_path(state_path)
        logger.debug(f&quot;Saving trainer state to {state_path}&quot;)
        final_state = self.deep_convert_dict(state)
        with open(state_path, &quot;w&quot;) as f:
            json.dump(final_state, f)
    def load_state(self, state_path: str):
        if state_path is None:
            raise ValueError(&quot;state_path must be specified&quot;)
        state_path = self.mangle_state_path(state_path)
        if os.path.exists(state_path):
            with open(state_path, &quot;r&quot;) as f:
                return json.load(f)
        else:
            logger.debug(f&quot;load_state found no file: {state_path}&quot;)
            return {}</file><file path="helpers/multiaspect/video.py">import cv2
import numpy as np
def resize_video_frames(
    video_frames: np.ndarray, dsize=None, fx=None, fy=None
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Resize each frame in a video (NumPy array with shape (num_frames, height, width, channels)).
    You can either provide a fixed destination size (dsize) or scaling factors (fx and fy).
    &quot;&quot;&quot;
    resized_frames = []
    for frame in video_frames:
        # Optionally, add a check to make sure frame is valid.
        if frame is None or frame.size == 0:
            continue
        resized_frame = cv2.resize(frame, dsize=dsize, fx=fx, fy=fy)
        resized_frames.append(resized_frame)
    if not resized_frames:
        raise ValueError(
            &quot;No frames were resized. Check your video data and resize parameters.&quot;
        )
    return np.stack(resized_frames, axis=0)</file><file path="helpers/prompt_expander/__init__.py">import torch
import random
import json
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
class PromptExpander:
    # Class variables to hold the model, tokenizer, and generator
    model = None
    tokenizer = None
    generator = None
    @staticmethod
    def initialize_model(model_path=&quot;meta-llama/Llama-3.2-1B-Instruct&quot;):
        &quot;&quot;&quot;
        Initializes the language model, tokenizer, and text generation pipeline.
        &quot;&quot;&quot;
        device = (
            &quot;cuda&quot;
            if torch.cuda.is_available()
            else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
        )
        PromptExpander.model = AutoModelForCausalLM.from_pretrained(model_path).to(
            device
        )
        PromptExpander.tokenizer = AutoTokenizer.from_pretrained(model_path)
        PromptExpander.generator = pipeline(
            &quot;text-generation&quot;,
            model=PromptExpander.model,
            tokenizer=PromptExpander.tokenizer,
            device=0 if device == &quot;cuda&quot; else -1,
        )
    @staticmethod
    def generate_prompts(trigger_phrase, num_prompts=25):
        &quot;&quot;&quot;
        Generates expanded prompts based on the provided trigger phrase.
        Args:
            trigger_phrase (str): The trigger phrase to include in the prompts.
            num_prompts (int): The number of prompts to generate.
        &quot;&quot;&quot;
        # Check if the model is initialized
        if PromptExpander.generator is None:
            print(&quot;Model not initialized. Please call initialize_model() first.&quot;)
            return
        # Define the list of prompt templates and styles
        prompt_templates = [
            &quot;An image of TRIGGERPHRASE in the style of {style}.&quot;,
            &quot;A painting of {style}, featuring TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE as depicted in {style} art.&quot;,
            &quot;{style} illustration showing TRIGGERPHRASE.&quot;,
            &quot;An abstract representation of TRIGGERPHRASE in {style}.&quot;,
            &quot;A realistic portrayal of TRIGGERPHRASE, inspired by {style}.&quot;,
            &quot;An artistic depiction of TRIGGERPHRASE using {style} techniques.&quot;,
            &quot;A {style} photograph of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE captured in the style of {style}.&quot;,
            &quot;An image featuring TRIGGERPHRASE with {style} elements.&quot;,
            &quot;A {style}-inspired scene with TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE illustrated in {style} style.&quot;,
            &quot;An artistic rendering of TRIGGERPHRASE in {style} fashion.&quot;,
            &quot;A {style}-themed artwork of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE portrayed in {style} aesthetics.&quot;,
            &quot;{style} art featuring TRIGGERPHRASE.&quot;,
            &quot;An expressive {style} depiction of TRIGGERPHRASE.&quot;,
            &quot;A creative {style} representation of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE shown in a {style} setting.&quot;,
            &quot;An imaginative {style} image of TRIGGERPHRASE.&quot;,
            &quot;A {style} design including TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE in a {style} composition.&quot;,
            &quot;An illustration of TRIGGERPHRASE with {style} influence.&quot;,
            &quot;A {style}-inspired portrait of TRIGGERPHRASE.&quot;,
            &quot;An image where TRIGGERPHRASE meets {style}.&quot;,
            &quot;TRIGGERPHRASE blended into a {style} background.&quot;,
            &quot;A {style} collage featuring TRIGGERPHRASE.&quot;,
            &quot;An artistic scene of TRIGGERPHRASE in {style} mood.&quot;,
            &quot;A {style} depiction of TRIGGERPHRASE in motion.&quot;,
            &quot;TRIGGERPHRASE rendered in {style} tones.&quot;,
            &quot;An atmospheric {style} image of TRIGGERPHRASE.&quot;,
            &quot;An expressive portrait of TRIGGERPHRASE in {style} style.&quot;,
            &quot;A surreal {style} painting of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE integrated into a {style} landscape.&quot;,
            &quot;An abstract {style} representation of TRIGGERPHRASE.&quot;,
            &quot;A {style} sketch of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE depicted in {style} illustration.&quot;,
            &quot;An image of TRIGGERPHRASE with {style} patterns.&quot;,
            &quot;A {style} poster featuring TRIGGERPHRASE.&quot;,
            &quot;An iconic {style} image of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE in a {style} artwork.&quot;,
            &quot;A vibrant {style} depiction of TRIGGERPHRASE.&quot;,
            &quot;An ethereal {style} image of TRIGGERPHRASE.&quot;,
            &quot;A dynamic {style} scene with TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE portrayed through {style} art.&quot;,
            &quot;A {style} mural of TRIGGERPHRASE.&quot;,
            &quot;An imaginative {style} illustration of TRIGGERPHRASE.&quot;,
            &quot;TRIGGERPHRASE set in a {style} environment.&quot;,
            &quot;A {style}-inspired depiction of TRIGGERPHRASE.&quot;,
            &quot;An image of TRIGGERPHRASE with {style} motifs.&quot;,
        ]
        styles = [
            &quot;Impressionism&quot;,
            &quot;Cubism&quot;,
            &quot;Surrealism&quot;,
            &quot;Pop Art&quot;,
            &quot;Futurism&quot;,
            &quot;Baroque&quot;,
            &quot;Gothic&quot;,
            &quot;Abstract Expressionism&quot;,
            &quot;Renaissance&quot;,
            &quot;Minimalism&quot;,
            &quot;Digital Art&quot;,
            &quot;Vintage Photography&quot;,
            &quot;Sci-Fi&quot;,
            &quot;Fantasy&quot;,
            &quot;Steampunk&quot;,
            &quot;Cyberpunk&quot;,
            &quot;Art Deco&quot;,
            &quot;Graffiti&quot;,
            &quot;Watercolor&quot;,
            &quot;Oil Painting&quot;,
            &quot;Black and White&quot;,
            &quot;Colorful&quot;,
            &quot;Retro&quot;,
            &quot;Comic Book&quot;,
            &quot;Manga&quot;,
            &quot;3D Rendering&quot;,
            &quot;Low Poly&quot;,
            &quot;Pixel Art&quot;,
            &quot;Line Art&quot;,
            &quot;Flat Design&quot;,
            &quot;Concept Art&quot;,
            &quot;Photorealism&quot;,
            &quot;High Contrast&quot;,
            &quot;Monochrome&quot;,
            &quot;Collage&quot;,
            &quot;Typography&quot;,
            &quot;Street Art&quot;,
            &quot;Ukiyo-e&quot;,
            &quot;Pop Surrealism&quot;,
            &quot;Digital Illustration&quot;,
            &quot;Neon&quot;,
            &quot;Expressionism&quot;,
            &quot;Anime&quot;,
            &quot;Realism&quot;,
            &quot;Dadaism&quot;,
            &quot;Constructivism&quot;,
            &quot;Avant-Garde&quot;,
            &quot;Hyperrealism&quot;,
            &quot;Symbolism&quot;,
            &quot;Fauvism&quot;,
        ]
        used_templates = []
        used_styles = []
        user_prompt_library = {}
        idx = 0
        for _ in range(num_prompts):
            idx += 1
            # Randomly select a prompt template and style
            prompt_template = None
            style = None
            while (prompt_template is None or prompt_template in used_templates) or (
                style is None or style in used_styles
            ):
                prompt_template = random.choice(prompt_templates)
                style = random.choice(styles)
            used_templates.append(prompt_template)
            used_styles.append(style)
            # Replace placeholders in the template
            prompt = prompt_template.replace(&quot;{style}&quot;, style.lower())
            prompt = prompt.replace(&quot;TRIGGERPHRASE&quot;, trigger_phrase)
            # Generate the text
            input_prompt = (
                &quot;You are a text-to-text interface that returns improved prompts. &quot;
                &quot;The captions should be expanded to be more descriptive. &quot;
                &quot;Captions look like sentence fragments and tags.\n\n&quot;
                f&quot;WITHOUT CHANGING ANY SPELLINGS: Clean this prompt, and return NOTHING but the upgraded prompt: {prompt}&quot;
            )
            import time
            begin = time.time()
            output_prompt = None
            def refused(output_prompt):
                triggers = [
                    &quot;i cannot&quot;,
                    &quot;can you&quot;,
                    &quot;what kind&quot;,
                    &quot;i can&apos;t&quot;,
                    &quot;i won&apos;t&quot;,
                    &quot;am unable to&quot;,
                    &quot;here is the&quot;,
                    &quot;here&apos;s the&quot;,
                    &quot;improving the caption&quot;,
                    &quot;improving the prompt&quot;,
                    &quot;should &quot;,
                    &quot;improving the text&quot;,
                    &quot;improving the sentence&quot;,
                    &quot;improving the description&quot;,
                    &quot;remember:&quot;,
                    &quot;please describe&quot;,
                    &quot;please&quot;,
                    &quot;improving the tag&quot;,
                    &quot;improving the fragment&quot;,
                    &quot;i am unable to&quot;,
                    &quot;i am not able to&quot;,
                    &quot;i am not capable of&quot;,
                    &quot;i am not capable to&quot;,
                    &quot;i am not capable&quot;,
                    &quot;i am not able&quot;,
                ]
                if output_prompt is None:
                    return None
                output_prompt_lower = output_prompt.lower()
                for trigger in triggers:
                    if trigger in output_prompt_lower:
                        return trigger
                return None
            refused_term = None
            while output_prompt is None or refused_term is not None:
                if output_prompt is not None and refused_term:
                    print(
                        f&quot;-&gt; (REFUSAL) Prompt contains &apos;{refused_term}&apos;. Generating a new prompt.&quot;,
                        end=&quot;\n\n&quot;,
                    )
                generated_caption = PromptExpander.generator(
                    input_prompt,
                    temperature=0.4,
                    max_new_tokens=77,
                    return_full_text=False,
                )
                end = time.time()
                output_prompt = (
                    generated_caption[0][&quot;generated_text&quot;]
                    .strip()
                    .lower()
                    .replace(&quot;create a &quot;, &quot;&quot;)
                    .replace(&quot;explore the&quot;, &quot;&quot;)
                    .replace(&quot;imagine a&quot;, &quot;&quot;)
                    .replace(&quot;find a&quot;, &quot;&quot;)
                    .replace(&apos;&quot;&apos;, &quot;&quot;)
                )
                refused_term = refused(output_prompt)
            time_taken = end - begin
            print(&quot;Prompt expanded in&quot;, round(time_taken, 2), &quot;seconds:&quot;)
            print(f&quot;Original prompt: {prompt}&quot;)
            print(f&quot;Expanded prompt: {output_prompt}&quot;, end=&quot;\n\n&quot;)
            user_prompt_library[f&quot;prompt_{idx}&quot;] = output_prompt
        # Output the generated prompts as JSON
        return user_prompt_library
if __name__ == &quot;__main__&quot;:
    # Example usage:
    # Initialize the model first
    PromptExpander.initialize_model()
    # Generate prompts with your trigger phrase
    user_prompt_library = PromptExpander.generate_prompts(
        trigger_phrase=&quot;your_trigger_phrase_here&quot;, num_prompts=25
    )
    # Write to disk
    with open(&quot;config/user_prompt_library.json&quot;, &quot;w&quot;) as f:
        json.dump(user_prompt_library, f, indent=4)</file><file path="helpers/publishing/huggingface.py">import os
import logging
from pathlib import Path
from helpers.training.state_tracker import StateTracker
from helpers.publishing.metadata import save_model_card
from huggingface_hub import create_repo, upload_folder, upload_file
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, logging.INFO))
LORA_SAFETENSORS_FILENAME = &quot;pytorch_lora_weights.safetensors&quot;
EMA_SAFETENSORS_FILENAME = &quot;ema_model.safetensors&quot;
class HubManager:
    def __init__(self, config, repo_id: str = None):
        self.config = config
        self.repo_id = (
            repo_id or self.config.hub_model_id or self.config.tracker_project_name
        )
        self.hub_token = self._load_hub_token()
        self.data_backends = StateTracker.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;])
        self._create_repo()
        self.validation_prompts = None
        self.validation_shortnames = None
        self.collected_data_backend_str = None
    def _create_repo(self):
        self._repo_id = create_repo(
            repo_id=self.config.hub_model_id or self.config.tracker_project_name,
            exist_ok=True,
        ).repo_id
    def _vae_string(self):
        if &quot;deepfloyd&quot; in self.config.model_type:
            return &quot;\nDeepFloyd Pixel diffusion (no VAE).&quot;
        else:
            return f&quot;\nVAE: {self.config.pretrained_vae_model_name_or_path}&quot;
    def _commit_message(self):
        return (
            f&quot;Trained for {StateTracker.get_epoch() - 1} epochs and {StateTracker.get_global_step()} steps.&quot;
            f&quot;\nTrained with datasets {self.collected_data_backend_str}&quot;
            f&quot;\nLearning rate {self.config.learning_rate}, batch size {self.config.train_batch_size}, and {self.config.gradient_accumulation_steps} gradient accumulation steps.&quot;
            f&quot;\nUsed DDPM noise scheduler for training with {self.config.prediction_type} prediction type and rescaled_betas_zero_snr={self.config.rescale_betas_zero_snr}&quot;
            f&quot;\nUsing &apos;{self.config.training_scheduler_timestep_spacing}&apos; timestep spacing.&quot;
            f&quot;\nBase model: {self.config.pretrained_model_name_or_path}&quot;
            f&quot;{self._vae_string()}&quot;
        )
    def _load_hub_token(self):
        token_path = os.path.join(os.path.expanduser(&quot;~&quot;), &quot;.cache/huggingface/token&quot;)
        if os.path.exists(token_path):
            with open(token_path, &quot;r&quot;) as f:
                return f.read().strip()
        raise ValueError(
            f&quot;No Hugging Face Hub token found ({token_path}). Please ensure you have logged in with &apos;huggingface-cli login&apos;.&quot;
        )
    def set_validation_prompts(self, validation_prompts, validation_shortnames):
        self.validation_prompts = validation_prompts
        self.validation_shortnames = validation_shortnames
    def upload_validation_folder(self, webhook_handler=None, override_path=None):
        try:
            upload_folder(
                repo_id=self._repo_id,
                folder_path=os.path.join(
                    override_path or self.config.output_dir, &quot;assets&quot;
                ),
                path_in_repo=&quot;assets/&quot;,
                commit_message=&quot;Validation images auto-generated by SimpleTuner&quot;,
            )
        except Exception as e:
            logger.error(f&quot;Error uploading validation images to Hugging Face Hub: {e}&quot;)
    def upload_model(self, validation_images, webhook_handler=None, override_path=None):
        if webhook_handler:
            webhook_handler.send(
                message=f&quot;Uploading {&apos;model&apos; if override_path is None else &apos;intermediary checkpoint&apos;} to Hugging Face Hub as `{self.repo_id}`.&quot;
            )
        save_model_card(
            repo_id=self.repo_id,
            images=validation_images,
            base_model=self.config.pretrained_model_name_or_path,
            train_text_encoder=self.config.train_text_encoder,
            prompt=self.config.validation_prompt,
            validation_prompts=self.validation_prompts,
            validation_shortnames=self.validation_shortnames,
            repo_folder=override_path
            or os.path.join(
                self.config.output_dir,
                &quot;pipeline&quot; if &quot;lora&quot; not in self.config.model_type else &quot;&quot;,
            ),
        )
        try:
            self.upload_validation_folder(
                webhook_handler=webhook_handler, override_path=override_path
            )
        except:
            logger.error(&quot;Error uploading validation images to Hugging Face Hub.&quot;)
        attempt = 0
        while attempt &lt; 3:
            attempt += 1
            try:
                if &quot;lora&quot; not in self.config.model_type:
                    self.upload_full_model(override_path=override_path)
                else:
                    self.upload_lora_model(override_path=override_path)
                    if self.config.use_ema:
                        self.upload_ema_model(override_path=override_path)
                break
            except Exception as e:
                if webhook_handler:
                    webhook_handler.send(
                        message=f&quot;(attempt {attempt}/3) Error uploading model to Hugging Face Hub: {e}. Retrying...&quot;
                    )
        if webhook_handler:
            webhook_handler.send(
                message=f&quot;Model is now available [on Hugging Face Hub](https://huggingface.co/{self._repo_id}).&quot;
            )
    def upload_full_model(self, override_path=None):
        folder_path = os.path.join(self.config.output_dir, &quot;pipeline&quot;)
        try:
            upload_folder(
                repo_id=self._repo_id,
                folder_path=override_path or folder_path,
                commit_message=self._commit_message(),
            )
        except Exception as e:
            logger.error(f&quot;Failed to upload pipeline to hub: {e}&quot;)
    def upload_lora_model(self, override_path=None):
        lora_weights_path = os.path.join(
            override_path or self.config.output_dir, LORA_SAFETENSORS_FILENAME
        )
        try:
            upload_file(
                repo_id=self._repo_id,
                path_in_repo=f&quot;/{LORA_SAFETENSORS_FILENAME}&quot;,
                path_or_fileobj=lora_weights_path,
                commit_message=self._commit_message(),
            )
            readme_path = os.path.join(
                override_path or self.config.output_dir, &quot;README.md&quot;
            )
            upload_file(
                repo_id=self._repo_id,
                path_in_repo=&quot;/README.md&quot;,
                path_or_fileobj=readme_path,
                commit_message=&quot;Model card auto-generated by SimpleTuner&quot;,
            )
        except Exception as e:
            logger.error(f&quot;Failed to upload LoRA weights to hub: {e}&quot;)
    def upload_ema_model(self, override_path=None):
        try:
            check_ema_paths = [&quot;transformer_ema&quot;, &quot;unet_ema&quot;, &quot;controlnet_ema&quot;, &quot;ema&quot;]
            # if any of the folder names are present in the checkpoint dir, we will upload them too
            for check_ema_path in check_ema_paths:
                print(f&quot;Checking for EMA path: {check_ema_path}&quot;)
                ema_path = os.path.join(
                    override_path or self.config.output_dir, check_ema_path
                )
                if os.path.exists(ema_path):
                    print(f&quot;Found EMA checkpoint!&quot;)
                    upload_folder(
                        repo_id=self._repo_id,
                        folder_path=ema_path,
                        path_in_repo=&quot;/ema&quot;,
                        commit_message=&quot;LoRA EMA checkpoint auto-generated by SimpleTuner&quot;,
                    )
        except Exception as e:
            logger.error(f&quot;Failed to upload LoRA EMA weights to hub: {e}&quot;)
    def find_latest_checkpoint(self):
        checkpoints = list(Path(self.config.output_dir).rglob(&quot;checkpoint-*&quot;))
        highest_checkpoint_value = None
        highest_checkpoint = None
        if len(checkpoints) &gt; 0:
            highest_checkpoint_value = 0
            for checkpoint in checkpoints:
                # split by -
                parts = checkpoint.stem.split(&quot;-&quot;)
                checkpoint_value = int(parts[-1])
                if checkpoint_value &gt; highest_checkpoint_value:
                    highest_checkpoint_value = checkpoint_value
                    highest_checkpoint = checkpoint
        return highest_checkpoint
    def upload_latest_checkpoint(self, validation_images: dict, webhook_handler=None):
        checkpoint_path = self.find_latest_checkpoint()
        if checkpoint_path:
            logging.info(f&quot;Checkpoint path: {checkpoint_path}&quot;)
            try:
                self.upload_model(
                    validation_images=validation_images,
                    override_path=checkpoint_path,
                    webhook_handler=webhook_handler,
                )
            except Exception as e:
                logger.error(f&quot;Failed to upload latest checkpoint: {e}&quot;)
    def upload_validation_images(
        self, validation_images, webhook_handler=None, override_path=None
    ):
        logging.info(f&quot;Validation images for upload: {validation_images}&quot;)
        if validation_images and len(validation_images) &gt; 0:
            idx = 0
            for shortname, images in (
                validation_images.items()
                if type(validation_images) is dict
                else validation_images
            ):
                # print(f&quot;Shortname {shortname} images: {images}&quot;)
                if type(images) is not list:
                    images = [images]
                sub_idx = 0
                for image in images:
                    image_path = os.path.join(
                        override_path or self.config.output_dir,
                        &quot;assets&quot;,
                        f&quot;image_{idx}_{sub_idx}.png&quot;,
                    )
                    image.save(image_path, format=&quot;PNG&quot;)
                    attempt = 0
                    while attempt &lt; 3:
                        attempt += 1
                        try:
                            upload_file(
                                repo_id=self._repo_id,
                                path_in_repo=f&quot;/assets/image_{idx}_{sub_idx}.png&quot;,
                                path_or_fileobj=image_path,
                                commit_message=&quot;Validation image auto-generated by SimpleTuner&quot;,
                            )
                        except Exception as e:
                            if webhook_handler:
                                webhook_handler.send(
                                    message=f&quot;(attempt {attempt}/3) Error uploading validation image to Hugging Face Hub: {e}. Retrying...&quot;
                                )
                    sub_idx += 1
                    idx += 1</file><file path="helpers/publishing/metadata.py">import os
import logging
import json
import torch
from helpers.training.state_tracker import StateTracker
from typing import Union
import numpy as np
from PIL import Image
from diffusers.utils.export_utils import export_to_gif
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
licenses = {
    &quot;flux&quot;: &quot;flux-1-dev-non-commercial-license&quot;,
    &quot;sdxl&quot;: &quot;creativeml-openrail-m&quot;,
    &quot;legacy&quot;: &quot;openrail++&quot;,
    &quot;pixart_sigma&quot;: &quot;openrail++&quot;,
    &quot;kolors&quot;: &quot;apache-2.0&quot;,
    &quot;smoldit&quot;: &quot;apache-2.0&quot;,
    &quot;sd3&quot;: &quot;stabilityai-ai-community&quot;,
}
allowed_licenses = [
    &quot;apache-2.0&quot;,
    &quot;mit&quot;,
    &quot;openrail&quot;,
    &quot;bigscience-openrail-m&quot;,
    &quot;creativeml-openrail-m&quot;,
    &quot;bigscience-bloom-rail-1.0&quot;,
    &quot;bigcode-openrail-m&quot;,
    &quot;afl-3.0&quot;,
    &quot;artistic-2.0&quot;,
    &quot;bsl-1.0&quot;,
    &quot;bsd&quot;,
    &quot;bsd-2-clause&quot;,
    &quot;bsd-3-clause&quot;,
    &quot;bsd-3-clause-clear&quot;,
    &quot;c-uda&quot;,
    &quot;cc&quot;,
    &quot;cc0-1.0&quot;,
    &quot;cc-by-2.0&quot;,
    &quot;cc-by-2.5&quot;,
    &quot;cc-by-3.0&quot;,
    &quot;cc-by-4.0&quot;,
    &quot;cc-by-sa-3.0&quot;,
    &quot;cc-by-sa-4.0&quot;,
    &quot;cc-by-nc-2.0&quot;,
    &quot;cc-by-nc-3.0&quot;,
    &quot;cc-by-nc-4.0&quot;,
    &quot;cc-by-nd-4.0&quot;,
    &quot;cc-by-nc-nd-3.0&quot;,
    &quot;cc-by-nc-nd-4.0&quot;,
    &quot;cc-by-nc-sa-2.0&quot;,
    &quot;cc-by-nc-sa-3.0&quot;,
    &quot;cc-by-nc-sa-4.0&quot;,
    &quot;cdla-sharing-1.0&quot;,
    &quot;cdla-permissive-1.0&quot;,
    &quot;cdla-permissive-2.0&quot;,
    &quot;wtfpl&quot;,
    &quot;ecl-2.0&quot;,
    &quot;epl-1.0&quot;,
    &quot;epl-2.0&quot;,
    &quot;etalab-2.0&quot;,
    &quot;eupl-1.1&quot;,
    &quot;agpl-3.0&quot;,
    &quot;gfdl&quot;,
    &quot;gpl&quot;,
    &quot;gpl-2.0&quot;,
    &quot;gpl-3.0&quot;,
    &quot;lgpl&quot;,
    &quot;lgpl-2.1&quot;,
    &quot;lgpl-3.0&quot;,
    &quot;isc&quot;,
    &quot;lppl-1.3c&quot;,
    &quot;ms-pl&quot;,
    &quot;apple-ascl&quot;,
    &quot;mpl-2.0&quot;,
    &quot;odc-by&quot;,
    &quot;odbl&quot;,
    &quot;openrail++&quot;,
    &quot;osl-3.0&quot;,
    &quot;postgresql&quot;,
    &quot;ofl-1.1&quot;,
    &quot;ncsa&quot;,
    &quot;unlicense&quot;,
    &quot;zlib&quot;,
    &quot;pddl&quot;,
    &quot;lgpl-lr&quot;,
    &quot;deepfloyd-if-license&quot;,
    &quot;llama2&quot;,
    &quot;llama3&quot;,
    &quot;llama3.1&quot;,
    &quot;gemma&quot;,
    &quot;unknown&quot;,
    &quot;other&quot;,
    &quot;array&quot;,
]
for _model, _license in licenses.items():
    if _license not in allowed_licenses:
        licenses[_model] = &quot;other&quot;
def _model_imports(args):
    output = &quot;import torch\n&quot;
    output += &quot;from diffusers import DiffusionPipeline&quot;
    if &quot;lycoris&quot; == args.lora_type.lower() and &quot;lora&quot; in args.model_type:
        output += &quot;\nfrom lycoris import create_lycoris_from_weights&quot;
    return f&quot;{output}&quot;
def ema_info(args):
    if args.use_ema:
        ema_information = &quot;&quot;&quot;
## Exponential Moving Average (EMA)
SimpleTuner generates a safetensors variant of the EMA weights and a pt file.
The safetensors file is intended to be used for inference, and the pt file is for continuing finetuning.
The EMA model may provide a more well-rounded result, but typically will feel undertrained compared to the full model as it is a running decayed average of the model weights.
&quot;&quot;&quot;
        return ema_information
    return &quot;&quot;
def lycoris_download_info():
    &quot;&quot;&quot;output a function to download the adapter&quot;&quot;&quot;
    output_fn = &quot;&quot;&quot;
def download_adapter(repo_id: str):
    import os
    from huggingface_hub import hf_hub_download
    adapter_filename = &quot;pytorch_lora_weights.safetensors&quot;
    cache_dir = os.environ.get(&apos;HF_PATH&apos;, os.path.expanduser(&apos;~/.cache/huggingface/hub/models&apos;))
    cleaned_adapter_path = repo_id.replace(&quot;/&quot;, &quot;_&quot;).replace(&quot;\\\\&quot;, &quot;_&quot;).replace(&quot;:&quot;, &quot;_&quot;)
    path_to_adapter = os.path.join(cache_dir, cleaned_adapter_path)
    path_to_adapter_file = os.path.join(path_to_adapter, adapter_filename)
    os.makedirs(path_to_adapter, exist_ok=True)
    hf_hub_download(
        repo_id=repo_id, filename=adapter_filename, local_dir=path_to_adapter
    )
    return path_to_adapter_file
    &quot;&quot;&quot;
    return output_fn
def _model_component_name(args):
    model_component_name = &quot;pipeline.transformer&quot;
    if args.model_family in [&quot;sdxl&quot;, &quot;kolors&quot;, &quot;legacy&quot;, &quot;deepfloyd&quot;]:
        model_component_name = &quot;pipeline.unet&quot;
    return model_component_name
def _model_load(args, repo_id: str = None):
    model_component_name = _model_component_name(args)
    hf_user_name = StateTracker.get_hf_username()
    if hf_user_name is not None:
        repo_id = f&quot;{hf_user_name}/{repo_id}&quot; if hf_user_name else repo_id
    if &quot;lora&quot; in args.model_type:
        if args.lora_type.lower() == &quot;standard&quot;:
            output = (
                f&quot;model_id = &apos;{args.pretrained_model_name_or_path}&apos;&quot;
                f&quot;\nadapter_id = &apos;{repo_id if repo_id is not None else args.output_dir}&apos;&quot;
                f&quot;\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype={StateTracker.get_weight_dtype()}) # loading directly in bf16&quot;
                f&quot;\npipeline.load_lora_weights(adapter_id)&quot;
            )
        elif args.lora_type.lower() == &quot;lycoris&quot;:
            output = (
                f&quot;{lycoris_download_info()}&quot;
                f&quot;\nmodel_id = &apos;{args.pretrained_model_name_or_path}&apos;&quot;
                f&quot;\nadapter_repo_id = &apos;{repo_id if repo_id is not None else args.output_dir}&apos;&quot;
                f&quot;\nadapter_filename = &apos;pytorch_lora_weights.safetensors&apos;&quot;
                f&quot;\nadapter_file_path = download_adapter(repo_id=adapter_repo_id)&quot;
                f&quot;\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype={StateTracker.get_weight_dtype()}) # loading directly in bf16&quot;
                &quot;\nlora_scale = 1.0&quot;
            )
    else:
        output = (
            f&quot;model_id = &apos;{repo_id if repo_id else os.path.join(args.output_dir, &apos;pipeline&apos;)}&apos;&quot;
            f&quot;\npipeline = DiffusionPipeline.from_pretrained(model_id, torch_dtype={StateTracker.get_weight_dtype()}) # loading directly in bf16&quot;
        )
    if args.model_type == &quot;lora&quot; and args.lora_type.lower() == &quot;lycoris&quot;:
        output += f&quot;\nwrapper, _ = create_lycoris_from_weights(lora_scale, adapter_file_path, {model_component_name})&quot;
        output += &quot;\nwrapper.merge_to()&quot;
    return output
def _torch_device():
    return &quot;&quot;&quot;&apos;cuda&apos; if torch.cuda.is_available() else &apos;mps&apos; if torch.backends.mps.is_available() else &apos;cpu&apos;&quot;&quot;&quot;
def _negative_prompt(args, in_call: bool = False):
    if args.model_family.lower() == &quot;flux&quot;:
        return &quot;&quot;
    if not in_call:
        return f&quot;negative_prompt = &apos;{args.validation_negative_prompt}&apos;&quot;
    return &quot;\n    negative_prompt=negative_prompt,&quot;
def _guidance_rescale(args):
    if args.model_family.lower() in [&quot;sd3&quot;, &quot;flux&quot;, &quot;pixart_sigma&quot;, &quot;ltxvideo&quot;, &quot;sana&quot;]:
        return &quot;&quot;
    return f&quot;\n    guidance_rescale={args.validation_guidance_rescale},&quot;
def _skip_layers(args):
    if (
        args.model_family.lower() not in [&quot;sd3&quot;]
        or args.validation_guidance_skip_layers is None
    ):
        return &quot;&quot;
    return f&quot;\n    skip_guidance_layers={args.validation_guidance_skip_layers},&quot;
def _pipeline_move_to(args):
    output = f&quot;pipeline.to({_torch_device()}) # the pipeline is already in its target precision level&quot;
    return output
def _pipeline_quanto(args):
    # return some optional lines to run Quanto on the model pipeline
    if args.model_type == &quot;full&quot;:
        return &quot;&quot;
    model_component_name = _model_component_name(args)
    comment_character = &quot;&quot;
    was_quantised = &quot;The model was quantised during training, and so it is recommended to do the same during inference time.&quot;
    if args.base_model_precision == &quot;no_change&quot;:
        comment_character = &quot;#&quot;
        was_quantised = &quot;The model was not quantised during training, so it is not necessary to quantise it during inference time.&quot;
    output = f&quot;&quot;&quot;
## Optional: quantise the model to save on vram.
## Note: {was_quantised}
{comment_character}from optimum.quanto import quantize, freeze, qint8
{comment_character}quantize({model_component_name}, weights=qint8)
{comment_character}freeze({model_component_name})
    &quot;&quot;&quot;
    return output
def _validation_resolution(args):
    if args.validation_resolution == &quot;&quot; or args.validation_resolution is None:
        return f&quot;width=1024,\n&quot; f&quot;    height=1024,&quot;
    resolutions = [args.validation_resolution]
    if &quot;,&quot; in args.validation_resolution:
        # split the resolution into a list of resolutions
        resolutions = args.validation_resolution.split(&quot;,&quot;)
    for resolution in resolutions:
        if &quot;x&quot; in resolution:
            return (
                f&quot;width={resolution.split(&apos;x&apos;)[0]},\n&quot;
                f&quot;    height={resolution.split(&apos;x&apos;)[1]},&quot;
            )
        return f&quot;width={resolution},\n&quot; f&quot;    height={resolution},&quot;
def _output_attribute(args):
    if args.model_family in [&quot;ltxvideo&quot;]:
        return &quot;frames[0]&quot;
    return &quot;images[0]&quot;
def _output_save_call(args):
    if args.model_family in [&quot;ltxvideo&quot;]:
        return f&quot;&quot;&quot;
from diffusers.utils.export_utils import export_to_gif
export_to_gif(model_output, &quot;output.gif&quot;, fps={args.framerate})
&quot;&quot;&quot;
    return f&quot;&quot;&quot;
model_output.save(&quot;output.png&quot;, format=&quot;PNG&quot;)
&quot;&quot;&quot;
def code_example(args, repo_id: str = None):
    &quot;&quot;&quot;Return a string with the code example.&quot;&quot;&quot;
    code_example = f&quot;&quot;&quot;
```python
{_model_imports(args)}
{_model_load(args, repo_id)}
prompt = &quot;{args.validation_prompt if args.validation_prompt else &apos;An astronaut is riding a horse through the jungles of Thailand.&apos;}&quot;
{_negative_prompt(args)}
{_pipeline_quanto(args)}
{_pipeline_move_to(args)}
model_output = pipeline(
    prompt=prompt,{_negative_prompt(args, in_call=True) if args.model_family.lower() != &apos;flux&apos; else &apos;&apos;}
    num_inference_steps={args.validation_num_inference_steps},
    generator=torch.Generator(device={_torch_device()}).manual_seed({args.validation_seed or args.seed or 42}),
    {_validation_resolution(args)}
    guidance_scale={args.validation_guidance},{_guidance_rescale(args)}{_skip_layers(args)}
).{_output_attribute(args)}
{_output_save_call(args)}
```
&quot;&quot;&quot;
    return code_example
def model_type(args):
    if &quot;lora&quot; in args.model_type:
        if &quot;standard&quot; == args.lora_type.lower():
            return &quot;standard PEFT LoRA&quot;
        if &quot;lycoris&quot; == args.lora_type.lower():
            return &quot;LyCORIS adapter&quot;
    else:
        return &quot;full rank finetune&quot;
def lora_info(args):
    &quot;&quot;&quot;Return a string with the LORA information.&quot;&quot;&quot;
    if &quot;lora&quot; not in args.model_type:
        return &quot;&quot;
    if args.lora_type.lower() == &quot;standard&quot;:
        return f&quot;&quot;&quot;- LoRA Rank: {args.lora_rank}
- LoRA Alpha: {args.lora_alpha}
- LoRA Dropout: {args.lora_dropout}
- LoRA initialisation style: {args.lora_init_type}
    &quot;&quot;&quot;
    if args.lora_type.lower() == &quot;lycoris&quot;:
        lycoris_config_file = args.lycoris_config
        # read the json file
        with open(lycoris_config_file, &quot;r&quot;) as file:
            try:
                lycoris_config = json.load(file)
            except:
                lycoris_config = {&quot;error&quot;: &quot;could not locate or load LyCORIS config.&quot;}
        return f&quot;&quot;&quot;
### LyCORIS Config:\n```json\n{json.dumps(lycoris_config, indent=4)}\n```
&quot;&quot;&quot;
def model_card_note(args):
    &quot;&quot;&quot;Return a string with the model card note.&quot;&quot;&quot;
    note_contents = args.model_card_note if args.model_card_note else &quot;&quot;
    if note_contents is None or note_contents == &quot;&quot;:
        return &quot;&quot;
    return f&quot;\n**Note:** {note_contents}\n&quot;
def flux_schedule_info(args):
    if args.model_family.lower() != &quot;flux&quot;:
        return &quot;&quot;
    output_args = []
    if args.flux_fast_schedule:
        output_args.append(&quot;flux_fast_schedule&quot;)
    if args.flow_schedule_auto_shift:
        output_args.append(&quot;flow_schedule_auto_shift&quot;)
    if args.flow_schedule_shift is not None:
        output_args.append(f&quot;shift={args.flow_schedule_shift}&quot;)
    output_args.append(f&quot;flux_guidance_mode={args.flux_guidance_mode}&quot;)
    if args.flux_guidance_value:
        output_args.append(f&quot;flux_guidance_value={args.flux_guidance_value}&quot;)
    if args.flux_guidance_min:
        output_args.append(f&quot;flux_guidance_min={args.flux_guidance_min}&quot;)
    if args.flux_guidance_mode == &quot;random-range&quot;:
        output_args.append(f&quot;flux_guidance_max={args.flux_guidance_max}&quot;)
        output_args.append(f&quot;flux_guidance_min={args.flux_guidance_min}&quot;)
    if args.flow_use_beta_schedule:
        output_args.append(f&quot;flow_beta_schedule_alpha={args.flow_beta_schedule_alpha}&quot;)
        output_args.append(f&quot;flow_beta_schedule_beta={args.flow_beta_schedule_beta}&quot;)
    if args.flux_attention_masked_training:
        output_args.append(&quot;flux_attention_masked_training&quot;)
    if args.t5_padding != &quot;unmodified&quot;:
        output_args.append(f&quot;t5_padding={args.t5_padding}&quot;)
    output_args.append(f&quot;flow_matching_loss={args.flow_matching_loss}&quot;)
    if (
        args.model_type == &quot;lora&quot;
        and args.lora_type == &quot;standard&quot;
        and args.flux_lora_target is not None
    ):
        output_args.append(f&quot;flux_lora_target={args.flux_lora_target}&quot;)
    output_str = (
        f&quot; (extra parameters={output_args})&quot;
        if output_args
        else &quot; (no special parameters set)&quot;
    )
    return output_str
def sd3_schedule_info(args):
    if args.model_family.lower() != &quot;sd3&quot;:
        return &quot;&quot;
    output_args = []
    if args.flow_schedule_auto_shift:
        output_args.append(&quot;flow_schedule_auto_shift&quot;)
    if args.flow_schedule_shift is not None:
        output_args.append(f&quot;shift={args.flow_schedule_shift}&quot;)
    if args.flow_use_beta_schedule:
        output_args.append(f&quot;flow_beta_schedule_alpha={args.flow_beta_schedule_alpha}&quot;)
        output_args.append(f&quot;flow_beta_schedule_beta={args.flow_beta_schedule_beta}&quot;)
    if args.flow_use_uniform_schedule:
        output_args.append(f&quot;flow_use_uniform_schedule&quot;)
    # if args.model_type == &quot;lora&quot; and args.lora_type == &quot;standard&quot;:
    #     output_args.append(f&quot;flux_lora_target={args.flux_lora_target}&quot;)
    output_str = (
        f&quot; (extra parameters={output_args})&quot;
        if output_args
        else &quot; (no special parameters set)&quot;
    )
    return output_str
def ddpm_schedule_info(args):
    &quot;&quot;&quot;Information about DDPM schedules, eg. rescaled betas or offset noise&quot;&quot;&quot;
    output_args = []
    if args.snr_gamma:
        output_args.append(f&quot;snr_gamma={args.snr_gamma}&quot;)
    if args.use_soft_min_snr:
        output_args.append(f&quot;use_soft_min_snr&quot;)
        if args.soft_min_snr_sigma_data:
            output_args.append(
                f&quot;soft_min_snr_sigma_data={args.soft_min_snr_sigma_data}&quot;
            )
    if args.rescale_betas_zero_snr:
        output_args.append(f&quot;rescale_betas_zero_snr&quot;)
    if args.offset_noise:
        output_args.append(f&quot;offset_noise&quot;)
        output_args.append(f&quot;noise_offset={args.noise_offset}&quot;)
        output_args.append(f&quot;noise_offset_probability={args.noise_offset_probability}&quot;)
    output_args.append(
        f&quot;training_scheduler_timestep_spacing={args.training_scheduler_timestep_spacing}&quot;
    )
    output_args.append(
        f&quot;inference_scheduler_timestep_spacing={args.inference_scheduler_timestep_spacing}&quot;
    )
    output_str = (
        f&quot; (extra parameters={output_args})&quot;
        if output_args
        else &quot; (no special parameters set)&quot;
    )
    return output_str
def model_schedule_info(args):
    if args.model_family == &quot;flux&quot;:
        return flux_schedule_info(args)
    if args.model_family == &quot;sd3&quot;:
        return sd3_schedule_info(args)
    else:
        return ddpm_schedule_info(args)
def save_metadata_sample(
    image_path: str,
    image: Union[Image.Image, np.ndarray, list],
):
    if isinstance(image, list):
        file_extension = &quot;gif&quot;
        output_path = f&quot;{image_path}.{file_extension}&quot;
        export_to_gif(
            image=image,
            output_gif_path=output_path,
            fps=StateTracker.get_args().framerate,
        )
    elif isinstance(image, Image.Image):
        file_extension = &quot;png&quot;
        output_path = f&quot;{image_path}.{file_extension}&quot;
        image.save(output_path, format=&quot;PNG&quot;)
    else:
        raise ValueError(f&quot;Cannot export sample type {type(image)} yet.&quot;)
    return output_path, file_extension
def _model_card_family_tag(model_family: str):
    if model_family == &quot;ltxvideo&quot;:
        # the hub has a hyphen.
        return &quot;ltx-video&quot;
    return model_family
def _pipeline_tag(args):
    return &quot;text-to-image&quot; if args.model_family not in [&quot;ltxvideo&quot;] else &quot;text-to-video&quot;
def save_model_card(
    repo_id: str,
    images=None,
    base_model: str = &quot;&quot;,
    train_text_encoder: bool = False,
    prompt: str = &quot;&quot;,
    validation_prompts: list = None,
    validation_shortnames: list = None,
    repo_folder: str = None,
):
    if repo_folder is None:
        raise ValueError(&quot;The repo_folder must be specified and not be None.&quot;)
    if type(validation_prompts) is not list:
        raise ValueError(
            f&quot;The validation_prompts must be a list. Received {validation_prompts}&quot;
        )
    # if we have more than one &apos;/&apos; in the base_model, we will turn it into unknown/model
    model_family = StateTracker.get_model_family()
    if base_model.count(&quot;/&quot;) &gt; 1:
        base_model = f&quot;{model_family}/unknown-model&quot;
    logger.debug(f&quot;Validating from prompts: {validation_prompts}&quot;)
    assets_folder = os.path.join(repo_folder, &quot;assets&quot;)
    optimizer_config = StateTracker.get_args().optimizer_config
    if optimizer_config is None:
        optimizer_config = &quot;&quot;
    os.makedirs(assets_folder, exist_ok=True)
    datasets_str = &quot;&quot;
    datasettypes = [&quot;image&quot;, &quot;video&quot;]
    for dataset in StateTracker.get_data_backends(_types=datasettypes).keys():
        if &quot;sampler&quot; in StateTracker.get_data_backends(_types=datasettypes)[dataset]:
            datasets_str += f&quot;### {dataset}\n&quot;
            datasets_str += f&quot;{StateTracker.get_data_backends(_types=datasettypes)[dataset][&apos;sampler&apos;].log_state(show_rank=False, alt_stats=True)}&quot;
    widget_str = &quot;&quot;
    idx = 0
    shortname_idx = 0
    negative_prompt_text = str(StateTracker.get_args().validation_negative_prompt)
    if negative_prompt_text == &quot;&quot;:
        negative_prompt_text = &quot;&apos;&apos;&quot;
    if images is not None and len(images) &gt; 0:
        widget_str = &quot;widget:&quot;
        for image_list in images.values() if isinstance(images, dict) else images:
            if not isinstance(image_list, list):
                image_list = [image_list]
            sub_idx = 0
            for image in image_list:
                image_path, image_extension = save_metadata_sample(
                    image_path=os.path.join(assets_folder, f&quot;image_{idx}_{sub_idx}&quot;),
                    image=image,
                )
                validation_prompt = &quot;no prompt available&quot;
                if validation_prompts is not None:
                    try:
                        validation_prompt = validation_prompts[shortname_idx]
                    except IndexError:
                        validation_prompt = f&quot;prompt not found ({validation_shortnames[shortname_idx] if validation_shortnames is not None and shortname_idx in validation_shortnames else shortname_idx})&quot;
                if validation_prompt == &quot;&quot;:
                    validation_prompt = &quot;unconditional (blank prompt)&quot;
                else:
                    # Escape anything that YAML won&apos;t like
                    validation_prompt = validation_prompt.replace(&quot;&apos;&quot;, &quot;&apos;&apos;&quot;)
                widget_str += f&quot;\n- text: &apos;{validation_prompt}&apos;&quot;
                widget_str += &quot;\n  parameters:&quot;
                widget_str += f&quot;\n    negative_prompt: &apos;{negative_prompt_text}&apos;&quot;
                widget_str += &quot;\n  output:&quot;
                widget_str += (
                    f&quot;\n    url: ./assets/image_{idx}_{sub_idx}.{image_extension}&quot;
                )
                idx += 1
                sub_idx += 1
            shortname_idx += 1
    args = StateTracker.get_args()
    yaml_content = f&quot;&quot;&quot;---
license: {licenses.get(model_family, &quot;other&quot;)}
base_model: &quot;{base_model}&quot;
tags:
  - {_model_card_family_tag(model_family)}
  - {f&apos;{_model_card_family_tag(model_family)}-diffusers&apos; if &apos;deepfloyd&apos; not in args.model_type else &apos;deepfloyd-if-diffusers&apos;}
  - {_pipeline_tag(args)}
  - {&apos;image-to-image&apos; if args.model_family not in [&quot;ltxvideo&quot;] else &apos;image-to-video&apos;}
  - diffusers
  - simpletuner
  - {&apos;not-for-all-audiences&apos; if not args.model_card_safe_for_work else &apos;safe-for-work&apos;}
  - {args.model_type}
{&apos;  - template:sd-lora&apos; if &apos;lora&apos; in args.model_type else &apos;&apos;}
{&apos;  - video-to-video&apos; if args.model_family in [&quot;ltxvideo&quot;] else &apos;&apos;}
{f&apos;  - {args.lora_type}&apos; if &apos;lora&apos; in args.model_type else &apos;&apos;}
pipeline_tag: {_pipeline_tag(args)}
inference: true
{widget_str}
---
&quot;&quot;&quot;
    model_card_content = f&quot;&quot;&quot;# {repo_id}
This is a {model_type(args)} derived from [{base_model}](https://huggingface.co/{base_model}).
{&apos;This is a **diffusion** model trained using DDPM objective instead of Flow matching. **Be sure to set the appropriate scheduler configuration.**&apos; if args.model_family == &quot;sd3&quot; and args.flow_matching_loss == &quot;diffusion&quot; else &apos;&apos;}
{&apos;The main validation prompt used during training was:&apos; if prompt else &apos;Validation used ground-truth images as an input for partial denoising (img2img).&apos; if args.validation_using_datasets else &apos;No validation prompt was used during training.&apos;}
{&apos;```&apos; if prompt else &apos;&apos;}
{prompt}
{&apos;```&apos; if prompt else &apos;&apos;}
{model_card_note(args)}
## Validation settings
- CFG: `{StateTracker.get_args().validation_guidance}`
- CFG Rescale: `{StateTracker.get_args().validation_guidance_rescale}`
- Steps: `{StateTracker.get_args().validation_num_inference_steps}`
- Sampler: `{&apos;FlowMatchEulerDiscreteScheduler&apos; if args.model_family in [&apos;sd3&apos;, &apos;flux&apos;, &apos;sana&apos;, &apos;ltxvideo&apos;] else StateTracker.get_args().validation_noise_scheduler}`
- Seed: `{StateTracker.get_args().validation_seed}`
- Resolution{&apos;s&apos; if &apos;,&apos; in StateTracker.get_args().validation_resolution else &apos;&apos;}: `{StateTracker.get_args().validation_resolution}`
{f&quot;- Skip-layer guidance: {_skip_layers(args)}&quot; if args.model_family in [&apos;sd3&apos;, &apos;flux&apos;] else &apos;&apos;}
Note: The validation settings are not necessarily the same as the [training settings](#training-settings).
{&apos;You can find some example images in the following gallery:&apos; if images is not None else &apos;&apos;}\n
&lt;Gallery /&gt;
The text encoder {&apos;**was**&apos; if train_text_encoder else &apos;**was not**&apos;} trained.
{&apos;You may reuse the base model text encoder for inference.&apos; if not train_text_encoder else &apos;If the text encoder from this repository is not used at inference time, unexpected or bad results could occur.&apos;}
## Training settings
- Training epochs: {StateTracker.get_epoch() - 1}
- Training steps: {StateTracker.get_global_step()}
- Learning rate: {StateTracker.get_args().learning_rate}
  - Learning rate schedule: {StateTracker.get_args().lr_scheduler}
  - Warmup steps: {StateTracker.get_args().lr_warmup_steps}
- Max grad {StateTracker.get_args().grad_clip_method}: {StateTracker.get_args().max_grad_norm}
- Effective batch size: {StateTracker.get_args().train_batch_size * StateTracker.get_args().gradient_accumulation_steps * StateTracker.get_accelerator().num_processes}
  - Micro-batch size: {StateTracker.get_args().train_batch_size}
  - Gradient accumulation steps: {StateTracker.get_args().gradient_accumulation_steps}
  - Number of GPUs: {StateTracker.get_accelerator().num_processes}
- Gradient checkpointing: {StateTracker.get_args().gradient_checkpointing}
- Prediction type: {&apos;flow-matching&apos; if (StateTracker.get_args().model_family in [&quot;sd3&quot;, &quot;flux&quot;, &quot;sana&quot;, &quot;ltxvideo&quot;]) else StateTracker.get_args().prediction_type}{model_schedule_info(args=StateTracker.get_args())}
- Optimizer: {StateTracker.get_args().optimizer}{optimizer_config if optimizer_config is not None else &apos;&apos;}
- Trainable parameter precision: {&apos;Pure BF16&apos; if torch.backends.mps.is_available() or StateTracker.get_args().mixed_precision == &quot;bf16&quot; else &apos;FP32&apos;}
- Base model precision: `{args.base_model_precision}`
- Caption dropout probability: {StateTracker.get_args().caption_dropout_probability * 100}%
{&apos;- Xformers: Enabled&apos; if StateTracker.get_args().attention_mechanism == &apos;xformers&apos; else &apos;&apos;}
{f&apos;- SageAttention: Enabled {StateTracker.get_args().sageattention_usage}&apos; if StateTracker.get_args().attention_mechanism == &apos;sageattention&apos; else &apos;&apos;}
{lora_info(args=StateTracker.get_args())}
## Datasets
{datasets_str}
## Inference
{code_example(args=StateTracker.get_args(), repo_id=repo_id)}
{ema_info(args=StateTracker.get_args())}
&quot;&quot;&quot;
    logger.debug(f&quot;YAML:\n{yaml_content}&quot;)
    logger.debug(f&quot;Model Card:\n{model_card_content}&quot;)
    with open(os.path.join(repo_folder, &quot;README.md&quot;), &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.write(yaml_content + model_card_content)</file><file path="helpers/training/default_settings/__init__.py">CURRENT_VERSION = 2
LATEST_DEFAULTS = {1: {&quot;hash_filenames&quot;: False}, 2: {&quot;hash_filenames&quot;: True}}
def default(setting: str, current_version: int = None, default_value=None):
    if current_version &lt;= 0 or current_version is None:
        current_version = CURRENT_VERSION
    if current_version in LATEST_DEFAULTS:
        return LATEST_DEFAULTS[current_version].get(setting, default_value)
    return default_value
def latest_config_version():
    return CURRENT_VERSION</file><file path="helpers/training/default_settings/safety_check.py">import logging, sys, os
from os import environ
from diffusers.utils import is_wandb_available
from helpers.training.multi_process import _get_rank as get_rank
from helpers.training.state_tracker import StateTracker
from torch.version import cuda as cuda_version
logger = logging.getLogger(__name__)
if get_rank() == 0:
    logger.setLevel(environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(logging.ERROR)
from helpers.training.error_handling import validate_deepspeed_compat_from_args
def safety_check(args, accelerator):
    if accelerator is not None and accelerator.num_processes &gt; 1:
        # mulit-gpu safety checks &amp; warnings
        if args.model_type == &quot;lora&quot; and args.lora_type == &quot;standard&quot;:
            # multi-gpu PEFT checks &amp; warnings
            if args.base_model_precision in [&quot;fp8-quanto&quot;]:
                logger.error(
                    f&quot;{args.base_model_precision} is incompatible with multi-GPU training on PEFT LoRA.&quot;
                    &quot; Use LORA_TYPE (--lora_type) lycoris for quantised multi-GPU training of LoKr models in FP8.&quot;
                )
                args.base_model_precision = &quot;int8-quanto&quot;
    if (
        args.base_model_precision in [&quot;fp8-quanto&quot;, &quot;int4-quanto&quot;]
        or (args.base_model_precision != &quot;no_change&quot; and args.quantize_activations)
    ) and (
        accelerator is not None
        and accelerator.state.dynamo_plugin.backend.lower() == &quot;inductor&quot;
    ):
        logger.warning(
            f&quot;{args.base_model_precision} is not supported with Dynamo backend. Disabling Dynamo.&quot;
        )
        from accelerate.utils import DynamoBackend
        accelerator.state.dynamo_plugin.backend = DynamoBackend.NO
    if args.report_to == &quot;wandb&quot;:
        if not is_wandb_available():
            raise ImportError(
                &quot;Make sure to install wandb if you want to use it for logging during training.&quot;
            )
        import wandb
    if accelerator is not None and (
        hasattr(accelerator.state, &quot;deepspeed_plugin&quot;)
        and accelerator.state.deepspeed_plugin is not None
    ):
        validate_deepspeed_compat_from_args(accelerator, args)
    if args.controlnet:
        if args.model_family not in [&quot;legacy&quot; &quot;sdxl&quot;]:
            raise ValueError(
                f&quot;ControlNet is not yet supported with {args.model_family} models. Please disable --controlnet, or switch model types.&quot;
            )
    if &quot;lora&quot; in args.model_type and &quot;standard&quot; == args.lora_type.lower():
        if args.model_family == &quot;pixart_sigma&quot;:
            raise Exception(f&quot;{args.model_type} does not support LoRA model training.&quot;)
    if &quot;lora&quot; in args.model_type and args.train_text_encoder:
        if args.lora_type.lower() == &quot;lycoris&quot;:
            logger.error(
                &quot;LyCORIS training is not meant to be combined with --train_text_encoder. It is powerful enough on its own!&quot;
            )
            sys.exit(1)
    if args.user_prompt_library and not os.path.exists(args.user_prompt_library):
        raise FileNotFoundError(
            f&quot;User prompt library not found at {args.user_prompt_library}. Please check the path and try again.&quot;
        )
    # optimizer memory limit check for SOAP w/ 24G
    if (
        accelerator is not None
        and accelerator.device.type == &quot;cuda&quot;
        and accelerator.is_main_process
        and cuda_version is not None
    ):
        import subprocess
        output = subprocess.check_output(
            [
                &quot;nvidia-smi&quot;,
                &quot;--query-gpu=memory.total&quot;,
                &quot;--format=csv,noheader,nounits&quot;,
            ]
        ).split(b&quot;\n&quot;)[get_rank()]
        total_memory = int(output.decode().strip()) / 1024
        from math import ceil
        total_memory_gb = ceil(total_memory)
        if total_memory_gb &lt; 32 and total_memory_gb &gt; 16 and args.optimizer == &quot;soap&quot;:
            logger.warning(
                f&quot;Your GPU has {total_memory_gb}GB of memory. The SOAP optimiser may require more than this. Setting --accelerator_cache_clear_interval=10 may help to eliminate OOM.&quot;
            )
        elif total_memory_gb &lt; 24 and args.optimizer == &quot;soap&quot;:
            logger.error(
                f&quot;Your GPU has {total_memory_gb}GB of memory. The SOAP optimiser requires a GPU with at least 24G of memory.&quot;
            )
            sys.exit(1)
    if (
        args.model_type != &quot;lora&quot;
        and not args.controlnet
        and args.base_model_precision != &quot;no_change&quot;
        and not args.i_know_what_i_am_doing
    ):
        logger.error(
            f&quot;{args.model_type} tuning is not compatible with quantisation. Please set --base_model_precision to &apos;no_change&apos; or train LyCORIS/LoRA.&quot;
        )
        sys.exit(1)
    if (
        args.flow_schedule_shift is not None
        and args.flow_schedule_shift &gt; 0
        and args.flow_schedule_auto_shift
    ):
        logger.error(
            f&quot;--flow_schedule_auto_shift cannot be combined with --flow_schedule_shift. Please set --flow_schedule_shift to 0 if you want to train with --flow_schedule_auto_shift.&quot;
        )
        sys.exit(1)
    if &quot;sageattention&quot; in args.attention_mechanism:
        if args.sageattention_usage != &quot;inference&quot;:
            logger.error(
                f&quot;SageAttention usage is set to &apos;{args.sageattention_usage}&apos; instead of &apos;inference&apos;. This is not an officially supported configuration, please be sure you understand the implications. It is recommended to set this value to &apos;inference&apos; for safety.&quot;
            )
        if args.enable_xformers_memory_efficient_attention:
            logger.error(
                f&quot;--enable_xformers_memory_efficient_attention is only compatible with --attention_mechanism=diffusers. Please set --attention_mechanism=diffusers to enable this feature or disable xformers to use alternative attention mechanisms.&quot;
            )
            sys.exit(1)
        if &quot;nf4&quot; in args.base_model_precision:
            logger.error(
                f&quot;{args.base_model_precision} is not supported with SageAttention. Please select from int8 or fp8, or, disable quantisation to use SageAttention.&quot;
            )
            sys.exit(1)
        if args.model_family == &quot;sana&quot;:
            logger.error(
                f&quot;{args.model_family} is not supported with SageAttention at this point. Disabling SageAttention.&quot;
            )
            args.attention_mechanism = &quot;diffusers&quot;
    gradient_checkpointing_interval_supported_models = [&quot;flux&quot;, &quot;sana&quot;, &quot;sdxl&quot;, &quot;sd3&quot;]
    if args.gradient_checkpointing_interval is not None:
        if (
            args.model_family.lower()
            not in gradient_checkpointing_interval_supported_models
        ):
            logger.error(
                f&quot;Gradient checkpointing interval is not supported with {args.model_family} models. Please disable --gradient_checkpointing_interval by setting it to None, or remove it from your configuration. Currently supported models: {gradient_checkpointing_interval_supported_models}&quot;
            )
            sys.exit(1)
        if args.gradient_checkpointing_interval == 0:
            raise ValueError(
                &quot;Gradient checkpointing interval must be greater than 0. Please set it to a positive integer.&quot;
            )
    if (
        args.report_to == &quot;none&quot;
        and args.eval_steps_interval is not None
        and args.eval_steps_interval &gt; 0
    ):
        logger.warning(
            &quot;Evaluation steps interval is set, but no reporting is enabled. Evaluation will not be logged.&quot;
        )</file><file path="helpers/training/optimizers/adamw_bfloat16/stochastic/__init__.py">import torch
from torch import Tensor, FloatTensor
def swap_first_and_last_dims(tensor: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Swap the first dimension with the last dimension of a tensor.
    Args:
        tensor (torch.Tensor): The input tensor of any shape.
    Returns:
        torch.Tensor: A tensor with the first dimension swapped with the last.
    &quot;&quot;&quot;
    # Get the total number of dimensions
    num_dims = len(tensor.shape)
    # Create a new order of dimensions
    new_order = list(range(1, num_dims)) + [0]
    # Permute the tensor according to the new order
    return tensor.permute(*new_order)
def swap_back_first_and_last_dims(tensor: torch.Tensor) -&gt; torch.Tensor:
    &quot;&quot;&quot;
    Swap back the first dimension with the last dimension of a tensor
    to its original shape after a swap.
    Args:
        tensor (torch.Tensor): The tensor that had its first and last dimensions swapped.
    Returns:
        torch.Tensor: A tensor with its original shape restored.
    &quot;&quot;&quot;
    # Get the total number of dimensions
    num_dims = len(tensor.shape)
    # Create a new order to reverse the previous swapping
    new_order = [num_dims - 1] + list(range(0, num_dims - 1))
    # Permute the tensor according to the new order
    return tensor.permute(*new_order)
def copy_stochastic_(target: Tensor, source: Tensor):
    &quot;&quot;&quot;
    copies source into target using stochastic rounding
    Args:
        target: the target tensor with dtype=bfloat16
        source: the target tensor with dtype=float32
    &quot;&quot;&quot;
    # create a random 16 bit integer
    result = torch.randint_like(
        source,
        dtype=torch.int32,
        low=0,
        high=(1 &lt;&lt; 16),
    )
    # add the random number to the lower 16 bit of the mantissa
    result.add_(source.view(dtype=torch.int32))
    # mask off the lower 16 bit of the mantissa
    result.bitwise_and_(-65536)  # -65536 = FFFF0000 as a signed int32
    # copy the higher 16 bit into the target tensor
    target.copy_(result.view(dtype=torch.float32))
    del result
def add_stochastic_(_input: Tensor, other: Tensor, alpha: float = 1.0):
    &quot;&quot;&quot;
    Adds other to input using stochastic rounding.
    There is a hack to fix a bug on MPS where uneven final dimensions cause
    a crash.
    Args:
        _input: the input tensor with dtype=bfloat16
        other: the other tensor
        alpha: a multiplier for other
    &quot;&quot;&quot;
    _input_original = _input
    if _input.device.type == &quot;mps&quot;:
        _input = _input.to(dtype=torch.float32)
    if other.dtype == torch.float32:
        result = other.clone()
    else:
        result = other.to(dtype=torch.float32)
    if _input.device.type == &quot;mps&quot;:
        result.add_(_input, alpha=torch.tensor(alpha, dtype=torch.float32))
    else:
        result.add_(_input, alpha=alpha)
    copy_stochastic_(_input, result)
    if _input.device.type == &quot;mps&quot;:
        _input_original.copy_(_input.view(dtype=torch.float32))
def addcdiv_stochastic_(
    _input: Tensor, tensor1: Tensor, tensor2: Tensor, value: float = 1.0
):
    &quot;&quot;&quot;
    adds (tensor1 / tensor2 * value) to input using stochastic rounding
    Args:
        _input: the input tensor with dtype=bfloat16
        tensor1: the numerator tensor
        tensor2: the denominator tensor
        value: a multiplier for tensor1/tensor2
    &quot;&quot;&quot;
    if _input.dtype == torch.float32:
        result = _input.clone()
    else:
        result = _input.to(dtype=torch.float32)
    result.addcdiv_(tensor1, tensor2, value=value)
    copy_stochastic_(_input, result)</file><file path="helpers/training/optimizers/adamw_bfloat16/__init__.py">&quot;&quot;&quot;
Different versions appeared, 
they have identical interface, but sutiable for different scenarios.
&quot;&quot;&quot;
__version__ = &quot;0.2.0&quot;
__all__ = [&quot;AdamW_BF16&quot;]
&quot;&quot;&quot;
This implementation uses torch.compile to speed up,
should be suitable for different backends.
&quot;&quot;&quot;
import torch
from torch.optim.optimizer import Optimizer
from .stochastic import (
    add_stochastic_,
    addcdiv_stochastic_,
)
class AdamWBF16(Optimizer):
    decay_threshold = 5e-3
    def __init__(
        self,
        params,
        *,
        lr=1e-4,
        betas=(0.9, 0.999),
        eps=1e-8,
        weight_decay=0,
    ):
        &quot;&quot;&quot;
        Implements AdamW optimization specifically for bfloat16 models.
        No other dtype is supported.
        Compatible with cuda graphs.
        Uses delayed accumulation for decays and compensated summation for Adam steps.
        Uses only one additional bfloat16 weight for keeping correction.
        Do not use schedulers - those can&apos;t affect cuda graphs.
        :param lr_function: a callable that maps torch scalar (step) to torch scalar (learning rate)
        &quot;&quot;&quot;
        if not 0.0 &lt;= eps:
            raise ValueError(f&quot;Invalid epsilon value: {eps}&quot;)
        if not 0.0 &lt;= betas[0] &lt; 1.0:
            raise ValueError(f&quot;Invalid beta parameter at index 0: {betas[0]}&quot;)
        if not 0.0 &lt;= betas[1] &lt; 1.0:
            raise ValueError(f&quot;Invalid beta parameter at index 1: {betas[1]}&quot;)
        if not 0.0 &lt;= weight_decay:
            raise ValueError(f&quot;Invalid weight_decay value: {weight_decay}&quot;)
        defaults = dict(betas=betas, eps=eps, weight_decay=weight_decay, lr=lr)
        super().__init__(params, defaults)
    @torch.no_grad()
    def step(self, zero_grad: bool = False):
        &quot;&quot;&quot;Performs a single optimization step.&quot;&quot;&quot;
        for group in self.param_groups:
            beta1, beta2 = group[&quot;betas&quot;]
            for p in group[&quot;params&quot;]:
                if p.grad is not None:
                    state = self.state[p]
                    # Lazy state initialization
                    if len(state) == 0:
                        assert p.dtype == torch.bfloat16, &quot;only bfloat 16 is supported.&quot;
                        state[&quot;step&quot;] = 0.0
                        # Exponential moving average of gradient values
                        state[&quot;exp_avg&quot;] = torch.zeros_like(
                            p, memory_format=torch.preserve_format
                        )
                        # Exponential moving average of squared gradient values
                        state[&quot;exp_avg_sq&quot;] = torch.zeros_like(
                            p, memory_format=torch.preserve_format
                        )
                        # accumulated shift that should be added to p, but wasn&apos;t because of truncation
                        # true value is p + shift
                        state[&quot;shift&quot;] = torch.zeros_like(
                            p, memory_format=torch.preserve_format
                        )
                        # using decay at each step will work only for float32, so we just remember how much owe to decay
                        # and decay once in n iterations
                        # Each weight has its own starting point to avoid simultaneous updates in all weights
                        state[&quot;accumulated_decay&quot;] = float(
                            torch.rand([]) * self.decay_threshold
                        )
                    grad = p.grad
                    state[&quot;step&quot;] += 1
                    lr = group[&quot;lr&quot;]
                    state[&quot;accumulated_decay&quot;] += group[&quot;weight_decay&quot;] * lr
                    accum_decay = state[&quot;accumulated_decay&quot;]
                    decay_this_iteration = (
                        accum_decay &gt; self.decay_threshold
                    ) * accum_decay
                    state[&quot;accumulated_decay&quot;] -= decay_this_iteration
                    _make_step(
                        grad,
                        p,
                        state[&quot;shift&quot;],
                        state[&quot;exp_avg&quot;],
                        state[&quot;exp_avg_sq&quot;],
                        beta1=beta1,
                        beta2=beta2,
                        step=state[&quot;step&quot;],
                        lr=lr,
                        eps=group[&quot;eps&quot;],
                        decay_this_iteration=decay_this_iteration,
                        zero_grad=zero_grad,
                    )
def _make_step(
    grad,
    p,
    shift,
    exp_avg,
    exp_avg_sq,
    beta1: float,
    beta2: float,
    step: float,
    lr: float,
    eps: float,
    decay_this_iteration: float,
    zero_grad: bool,
):
    # Originally:
    # exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
    exp_avg.mul_(beta1)
    add_stochastic_(exp_avg, grad, alpha=1 - beta1)
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
    denom_correction = (1 - beta2**step) ** 0.5
    # Originally:
    # shift.addcdiv_(
    #     exp_avg,
    #     exp_avg_sq.sqrt().add_(eps, alpha=1),
    #     value=-lr * denom_correction,
    # )
    addcdiv_stochastic_(
        shift,
        exp_avg,
        exp_avg_sq.sqrt().add_(eps, alpha=1),
        value=-lr * denom_correction,
    )
    buffer = p.clone()
    # Originally:
    # p.add_(shift)
    add_stochastic_(p, shift)
    # Originally:
    # shift.add_(buffer.sub_(p))
    add_stochastic_(shift, buffer.sub_(p))
    if decay_this_iteration &gt; 0:
        shift.add_(p, alpha=-decay_this_iteration)
        # Do NOT do this, it will cause the model to become unstable.
        # add_stochastic_(shift, p, alpha=-decay_this_iteration)
    if zero_grad:
        grad.zero_()</file><file path="helpers/training/optimizers/adamw_schedulefree/__init__.py">import torch
from torch.optim.optimizer import Optimizer
import math
from typing import Iterable
from helpers.training.state_tracker import StateTracker
class AdamWScheduleFreeKahan(Optimizer):
    &quot;&quot;&quot;AdamW optimizer with schedule-free adjustments and Kahan summation.
    Args:
        params: Iterable of parameters to optimize or dicts defining parameter groups.
        lr: Learning rate.
        betas: Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.999)).
        eps: Added to denominator to improve numerical stability (default: 1e-8).
        weight_decay: Weight decay coefficient (default: 1e-2).
        warmup_steps: Number of steps to warm up the learning rate (default: 0).
        kahan_sum: Enables Kahan summation for more accurate parameter updates when training in low precision.
    &quot;&quot;&quot;
    def __init__(
        self,
        params: Iterable,
        lr: float = 1e-3,
        betas: tuple = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 1e-2,
        warmup_steps: int = 0,
        kahan_sum: bool = True,
    ):
        defaults = dict(
            lr=lr,
            betas=betas,
            eps=eps,
            weight_decay=weight_decay,
            warmup_steps=warmup_steps,
            kahan_sum=kahan_sum,
        )
        super(AdamWScheduleFreeKahan, self).__init__(params, defaults)
        self.k = 0
        self.lr_max = -1.0
        self.last_lr = -1.0
        self.weight_sum = 0.0
    def _initialize_state(self, state, p):
        if &quot;step&quot; not in state:
            state[&quot;step&quot;] = 0
            state[&quot;exp_avg&quot;] = torch.zeros_like(p, memory_format=torch.preserve_format)
            state[&quot;exp_avg_sq&quot;] = torch.zeros_like(
                p, memory_format=torch.preserve_format
            )
            if self.defaults[&quot;kahan_sum&quot;]:
                state[&quot;kahan_comp&quot;] = torch.zeros_like(
                    p, memory_format=torch.preserve_format
                )
    def eval(self):
        for group in self.param_groups:
            train_mode = group.get(&quot;train_mode&quot;, True)
            beta1, _ = group[&quot;betas&quot;]
            if train_mode:
                for p in group[&quot;params&quot;]:
                    state = self.state[p]
                    if &quot;z&quot; in state:
                        # Set p.data to x
                        p.data.lerp_(
                            end=state[&quot;z&quot;].to(p.data.device), weight=1 - 1 / beta1
                        )
                group[&quot;train_mode&quot;] = False
    def train(self):
        for group in self.param_groups:
            train_mode = group.get(&quot;train_mode&quot;, False)
            beta1, _ = group[&quot;betas&quot;]
            if not train_mode:
                for p in group[&quot;params&quot;]:
                    state = self.state[p]
                    if &quot;z&quot; in state:
                        # Set p.data to y
                        p.data.lerp_(end=state[&quot;z&quot;].to(p.data.device), weight=1 - beta1)
                group[&quot;train_mode&quot;] = True
    def step(self, closure=None):
        &quot;&quot;&quot;Performs a single optimization step.&quot;&quot;&quot;
        loss = None
        if closure is not None:
            loss = closure()
        for group in self.param_groups:
            beta1, beta2 = group[&quot;betas&quot;]
            lr = group[&quot;lr&quot;]
            eps = group[&quot;eps&quot;]
            weight_decay = group[&quot;weight_decay&quot;]
            warmup_steps = group.get(&quot;warmup_steps&quot;, 0)
            kahan_sum = group[&quot;kahan_sum&quot;]
            k = self.k
            # Adjust learning rate with warmup
            if k &lt; warmup_steps:
                sched = (k + 1) / warmup_steps
            else:
                sched = 1.0
            bias_correction2 = 1 - beta2 ** (k + 1)
            adjusted_lr = lr * sched * (bias_correction2**0.5)
            self.lr_max = max(adjusted_lr, self.lr_max)
            for p in group[&quot;params&quot;]:
                if p.grad is None:
                    continue
                grad = p.grad.data
                state = self.state[p]
                self._initialize_state(state, p)
                exp_avg, exp_avg_sq = state[&quot;exp_avg&quot;], state[&quot;exp_avg_sq&quot;]
                if kahan_sum:
                    kahan_comp = state[&quot;kahan_comp&quot;]
                    grad.add_(kahan_comp)
                # Decay the first and second moment running average coefficient
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = adjusted_lr / (bias_correction2**0.5)
                if weight_decay != 0:
                    p.data.add_(p.data, alpha=-weight_decay)
                # Kahan summation to improve precision
                step = exp_avg / denom
                p.data.add_(-step_size * step)
                if kahan_sum:
                    buffer = p.data.add(-step_size * step)
                    kahan_comp.copy_(p.data.sub(buffer).add(buffer.sub_(p.data)))
            self.k += 1
            self.last_lr = adjusted_lr
            StateTracker.set_last_lr(adjusted_lr)
        return loss
    def get_last_lr(self):
        return self.last_lr</file><file path="helpers/training/optimizers/soap/__init__.py">import torch
import torch.nn as nn
import torch.optim as optim
from itertools import chain
# Parts of the code are modifications of Pytorch&apos;s AdamW optimizer
# Parts of the code are modifications of code from https://github.com/jiaweizzhao/GaLore/blob/master/galore_torch/galore_projector.py
class SOAP(optim.Optimizer):
    &quot;&quot;&quot;
    Implements SOAP algorithm (https://arxiv.org/abs/2409.11321).
    Parameters:
        params (`Iterable[nn.parameter.Parameter]`):
            Iterable of parameters to optimize or dictionaries defining parameter groups.
        lr (`float`, *optional*, defaults to 0.003):
            The learning rate to use.
        betas (`Tuple[float,float]`, *optional*, defaults to `(0.95, 0.95)`):
            Adam&apos;s betas parameters (b1, b2).
        shampoo_beta (`float`, *optional*, defaults to -1):
            If &gt;= 0, use this beta for the preconditioner (L and R in paper, state[&apos;GG&apos;] below) moving average instead of betas[1].
        eps (`float`, *optional*, defaults to 1e-08):
            Adam&apos;s epsilon for numerical stability.
        weight_decay (`float`, *optional*, defaults to 0.01): weight decay coefficient.
        precondition_frequency (`int`, *optional*, defaults to 10):
            How often to update the preconditioner.
        max_precond_dim (`int`, *optional*, defaults to 10000):
            Maximum dimension of the preconditioner.
            Set to 10000, so that we exclude most common vocab sizes while including layers.
        merge_dims (`bool`, *optional*, defaults to `False`):
            Whether or not to merge dimensions of the preconditioner.
        precondition_1d (`bool`, *optional*, defaults to `False`):
            Whether or not to precondition 1D gradients.
        normalize_grads (`bool`, *optional*, defaults to `False`):
            Whether or not to normalize gradients per layer.
            Helps at large precondition_frequency (~100 in our experiments),
            but hurts performance at small precondition_frequency (~10 in our experiments).
        data_format (`str`, *optional*, defaults to `channels_first`):
            Data format of the input for convolutional layers.
            Should be &quot;channels_last&quot; for data_format of NHWC and &quot;channels_first&quot; for NCHW.
        correct_bias (`bool`, *optional*, defaults to `True`):
            Whether or not to use bias correction in Adam.
    &quot;&quot;&quot;
    def __init__(
        self,
        params,
        lr: float = 3e-3,
        betas=(0.95, 0.95),
        shampoo_beta: float = -1,
        eps: float = 1e-8,
        weight_decay: float = 0.01,
        precondition_frequency: int = 10,
        max_precond_dim: int = 10000,  #
        merge_dims: bool = False,  # Merge dimensions till the product of the dimensions is less than or equal to max_precond_dim.
        precondition_1d: bool = False,
        normalize_grads: bool = False,
        data_format: str = &quot;channels_first&quot;,
        correct_bias: bool = True,
    ):
        defaults = {
            &quot;lr&quot;: lr,
            &quot;betas&quot;: betas,
            &quot;shampoo_beta&quot;: shampoo_beta,
            &quot;eps&quot;: eps,
            &quot;weight_decay&quot;: weight_decay,
            &quot;precondition_frequency&quot;: precondition_frequency,
            &quot;max_precond_dim&quot;: max_precond_dim,
            &quot;merge_dims&quot;: merge_dims,
            &quot;precondition_1d&quot;: precondition_1d,
            &quot;normalize_grads&quot;: normalize_grads,
            &quot;correct_bias&quot;: correct_bias,
        }
        super().__init__(params, defaults)
        self._data_format = data_format
    def merge_dims(self, grad, max_precond_dim):
        &quot;&quot;&quot;
        Merges dimensions of the gradient tensor till the product of the dimensions is less than or equal to max_precond_dim.
        &quot;&quot;&quot;
        assert self._data_format in [&quot;channels_first&quot;, &quot;channels_last&quot;]
        if self._data_format == &quot;channels_last&quot; and grad.dim() == 4:
            grad = grad.permute(0, 3, 1, 2)
        shape = grad.shape
        new_shape = []
        curr_shape = 1
        for sh in shape:
            temp_shape = curr_shape * sh
            if temp_shape &gt; max_precond_dim:
                if curr_shape &gt; 1:
                    new_shape.append(curr_shape)
                    curr_shape = sh
                else:
                    new_shape.append(sh)
                    curr_shape = 1
            else:
                curr_shape = temp_shape
        if curr_shape &gt; 1 or len(new_shape) == 0:
            new_shape.append(curr_shape)
        new_grad = grad.reshape(new_shape)
        return new_grad
    @torch.no_grad()
    def step(self, closure=None):
        &quot;&quot;&quot;
        Performs a single optimization step.
        Arguments:
            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.
        &quot;&quot;&quot;
        loss = None
        if closure is not None:
            loss = closure()
        for group in self.param_groups:
            for p in group[&quot;params&quot;]:
                if p.grad is None:
                    continue
                grad = p.grad
                state = self.state[p]
                if &quot;step&quot; not in state:
                    state[&quot;step&quot;] = 0
                # State initialization
                if &quot;exp_avg&quot; not in state:
                    # Exponential moving average of gradient values
                    state[&quot;exp_avg&quot;] = torch.zeros_like(grad)
                    # Exponential moving average of squared gradient values
                    state[&quot;exp_avg_sq&quot;] = torch.zeros_like(grad)
                if &quot;Q&quot; not in state:
                    self.init_preconditioner(
                        grad,
                        state,
                        precondition_frequency=group[&quot;precondition_frequency&quot;],
                        precondition_1d=group[&quot;precondition_1d&quot;],
                        shampoo_beta=(
                            group[&quot;shampoo_beta&quot;]
                            if group[&quot;shampoo_beta&quot;] &gt;= 0
                            else group[&quot;betas&quot;][1]
                        ),
                        max_precond_dim=group[&quot;max_precond_dim&quot;],
                        merge_dims=group[&quot;merge_dims&quot;],
                    )
                    self.update_preconditioner(
                        grad,
                        state,
                        max_precond_dim=group[&quot;max_precond_dim&quot;],
                        merge_dims=group[&quot;merge_dims&quot;],
                        precondition_1d=group[&quot;precondition_1d&quot;],
                    )
                    continue  # first step is skipped so that we never use the current gradients in the projection.
                # Projecting gradients to the eigenbases of Shampoo&apos;s preconditioner
                # i.e. projecting to the eigenbases of matrices in state[&apos;GG&apos;]
                grad_projected = self.project(
                    grad,
                    state,
                    merge_dims=group[&quot;merge_dims&quot;],
                    max_precond_dim=group[&quot;max_precond_dim&quot;],
                )
                exp_avg, exp_avg_sq = state[&quot;exp_avg&quot;], state[&quot;exp_avg_sq&quot;]
                beta1, beta2 = group[&quot;betas&quot;]
                state[&quot;step&quot;] += 1
                # Decay the first and second moment running average coefficient
                # In-place operations to update the averages at the same time
                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))
                exp_avg_sq.mul_(beta2).add_(
                    grad_projected.square(), alpha=(1.0 - beta2)
                )
                denom = exp_avg_sq.sqrt().add_(group[&quot;eps&quot;])
                # Projecting the exponential moving average of gradients to the eigenbases of Shampoo&apos;s preconditioner
                # i.e. projecting to the eigenbases of matrices in state[&apos;GG&apos;]
                exp_avg_projected = self.project(
                    exp_avg,
                    state,
                    merge_dims=group[&quot;merge_dims&quot;],
                    max_precond_dim=group[&quot;max_precond_dim&quot;],
                )
                step_size = group[&quot;lr&quot;]
                if group[&quot;correct_bias&quot;]:
                    bias_correction1 = 1.0 - beta1 ** (state[&quot;step&quot;])
                    bias_correction2 = 1.0 - beta2 ** (state[&quot;step&quot;])
                    step_size = step_size * (bias_correction2**0.5) / bias_correction1
                # Projecting back the preconditioned (by Adam) exponential moving average of gradients
                # to the original space
                norm_grad = self.project_back(
                    exp_avg_projected / denom,
                    state,
                    merge_dims=group[&quot;merge_dims&quot;],
                    max_precond_dim=group[&quot;max_precond_dim&quot;],
                )
                if group[&quot;normalize_grads&quot;]:
                    norm_grad = norm_grad / (1e-30 + torch.mean(norm_grad**2) ** 0.5)
                p.add_(norm_grad, alpha=-step_size)
                # From AdamW code: Just adding the square of the weights to the loss function is *not*
                # the correct way of using L2 regularization/weight decay with Adam,
                # since that will interact with the m and v parameters in strange ways.
                #
                # Instead we want to decay the weights in a manner that doesn&apos;t interact
                # with the m/v parameters. This is equivalent to adding the square
                # of the weights to the loss with plain (non-momentum) SGD.
                # Add weight decay at the end (fixed version)
                if group[&quot;weight_decay&quot;] &gt; 0.0:
                    p.add_(p, alpha=(-group[&quot;lr&quot;] * group[&quot;weight_decay&quot;]))
                # Update is done after the gradient step to avoid using current gradients in the projection.
                self.update_preconditioner(
                    grad,
                    state,
                    max_precond_dim=group[&quot;max_precond_dim&quot;],
                    merge_dims=group[&quot;merge_dims&quot;],
                    precondition_1d=group[&quot;precondition_1d&quot;],
                )
        return loss
    def init_preconditioner(
        self,
        grad,
        state,
        precondition_frequency=10,
        shampoo_beta=0.95,
        max_precond_dim=10000,
        precondition_1d=False,
        merge_dims=False,
    ):
        &quot;&quot;&quot;
        Initializes the preconditioner matrices (L and R in the paper).
        &quot;&quot;&quot;
        state[&quot;GG&quot;] = (
            []
        )  # Will hold all the preconditioner matrices (L and R in the paper).
        if grad.dim() == 1:
            if not precondition_1d or grad.shape[0] &gt; max_precond_dim:
                state[&quot;GG&quot;].append([])
            else:
                state[&quot;GG&quot;].append(
                    torch.zeros(grad.shape[0], grad.shape[0], device=grad.device)
                )
        else:
            if merge_dims:
                grad = self.merge_dims(grad, max_precond_dim)
            for sh in grad.shape:
                if sh &gt; max_precond_dim:
                    state[&quot;GG&quot;].append([])
                else:
                    state[&quot;GG&quot;].append(torch.zeros(sh, sh, device=grad.device))
        state[&quot;Q&quot;] = None  # Will hold all the eigenbases of the preconditioner.
        state[&quot;precondition_frequency&quot;] = precondition_frequency
        state[&quot;shampoo_beta&quot;] = shampoo_beta
    def project(self, grad, state, merge_dims=False, max_precond_dim=10000):
        &quot;&quot;&quot;
        Projects the gradient to the eigenbases of the preconditioner.
        &quot;&quot;&quot;
        original_shape = grad.shape
        if merge_dims:
            if grad.dim() == 4 and self._data_format == &quot;channels_last&quot;:
                permuted_shape = grad.permute(0, 3, 1, 2).shape
            grad = self.merge_dims(grad, max_precond_dim)
        for mat in state[&quot;Q&quot;]:
            if len(mat) &gt; 0:
                grad = torch.tensordot(
                    grad,
                    mat.to(grad.dtype),
                    dims=[[0], [0]],
                )
            else:
                permute_order = list(range(1, len(grad.shape))) + [0]
                grad = grad.permute(permute_order)
        if merge_dims:
            if self._data_format == &quot;channels_last&quot; and len(original_shape) == 4:
                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                grad = grad.reshape(original_shape)
        return grad
    def update_preconditioner(
        self,
        grad,
        state,
        max_precond_dim=10000,
        merge_dims=False,
        precondition_1d=False,
    ):
        &quot;&quot;&quot;
        Updates the preconditioner matrices and the eigenbases (L, R, Q_L, Q_R in the paper).
        &quot;&quot;&quot;
        if grad.dim() == 1:
            if precondition_1d and grad.shape[0] &lt;= max_precond_dim:
                state[&quot;GG&quot;][0].lerp_(
                    grad.unsqueeze(1) @ grad.unsqueeze(0), 1 - state[&quot;shampoo_beta&quot;]
                )
        else:
            if merge_dims:
                new_grad = self.merge_dims(grad, max_precond_dim)
                for idx, sh in enumerate(new_grad.shape):
                    if sh &lt;= max_precond_dim:
                        outer_product = torch.tensordot(
                            new_grad,
                            new_grad,
                            dims=[
                                [
                                    *chain(
                                        range(idx), range(idx + 1, len(new_grad.shape))
                                    )
                                ]
                            ]
                            * 2,
                        )
                        state[&quot;GG&quot;][idx].lerp_(outer_product, 1 - state[&quot;shampoo_beta&quot;])
            else:
                for idx, sh in enumerate(grad.shape):
                    if sh &lt;= max_precond_dim:
                        outer_product = torch.tensordot(
                            grad,
                            grad,
                            # Contracts across all dimensions except for k.
                            dims=[[*chain(range(idx), range(idx + 1, len(grad.shape)))]]
                            * 2,
                        )
                        state[&quot;GG&quot;][idx].lerp_(
                            outer_product.to(state[&quot;GG&quot;][idx].dtype),
                            1 - state[&quot;shampoo_beta&quot;],
                        )
        if state[&quot;Q&quot;] is None:
            state[&quot;Q&quot;] = self.get_orthogonal_matrix(state[&quot;GG&quot;])
        if state[&quot;step&quot;] &gt; 0 and state[&quot;step&quot;] % state[&quot;precondition_frequency&quot;] == 0:
            state[&quot;Q&quot;] = self.get_orthogonal_matrix_QR(
                state, max_precond_dim, merge_dims
            )
    def project_back(self, grad, state, merge_dims=False, max_precond_dim=10000):
        &quot;&quot;&quot;
        Projects the gradient back to the original space.
        &quot;&quot;&quot;
        original_shape = grad.shape
        if merge_dims:
            if self._data_format == &quot;channels_last&quot; and grad.dim() == 4:
                permuted_shape = grad.permute(0, 3, 1, 2).shape
            grad = self.merge_dims(grad, max_precond_dim)
        for mat in state[&quot;Q&quot;]:
            if len(mat) &gt; 0:
                grad = torch.tensordot(
                    grad.to(mat.dtype),
                    mat,
                    dims=[[0], [1]],
                )
            else:
                permute_order = list(range(1, len(grad.shape))) + [0]
                grad = grad.permute(permute_order)
        if merge_dims:
            if self._data_format == &quot;channels_last&quot; and len(original_shape) == 4:
                grad = grad.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                grad = grad.reshape(original_shape)
        return grad
    def get_orthogonal_matrix(self, mat):
        &quot;&quot;&quot;
        Computes the eigenbases of the preconditioner using torch.linalg.eigh decomposition.
        &quot;&quot;&quot;
        matrix = []
        for m in mat:
            if len(m) == 0:
                matrix.append([])
                continue
            if m.data.dtype != torch.float:
                float_data = False
                original_type = m.data.dtype
                original_device = m.data.device
                matrix.append(m.data.float())
            else:
                float_data = True
                matrix.append(m.data)
        final = []
        for m in matrix:
            if len(m) == 0:
                final.append([])
                continue
            try:
                _, Q = torch.linalg.eigh(
                    m + 1e-30 * torch.eye(m.shape[0], device=m.device)
                )
            except:
                _, Q = torch.linalg.eigh(
                    m.to(torch.float64) + 1e-30 * torch.eye(m.shape[0], device=m.device)
                )
                Q = Q.to(m.dtype)
            Q = torch.flip(Q, [1])
            if not float_data:
                Q = Q.to(original_device).type(original_type)
            final.append(Q)
        return final
    def get_orthogonal_matrix_QR(self, state, max_precond_dim=10000, merge_dims=False):
        &quot;&quot;&quot;
        Computes the eigenbases of the preconditioner using one round of power iteration
        followed by torch.linalg.qr decomposition.
        &quot;&quot;&quot;
        precond_list = state[&quot;GG&quot;]
        orth_list = state[&quot;Q&quot;]
        matrix = []
        orth_matrix = []
        for m, o in zip(precond_list, orth_list):
            if len(m) == 0:
                matrix.append([])
                orth_matrix.append([])
                continue
            if m.data.dtype != torch.float:
                float_data = False
                original_type = m.data.dtype
                original_device = m.data.device
                matrix.append(m.data.float())
                orth_matrix.append(o.data.float())
            else:
                float_data = True
                matrix.append(m.data.float())
                orth_matrix.append(o.data.float())
        orig_shape = state[&quot;exp_avg_sq&quot;].shape
        if self._data_format == &quot;channels_last&quot; and len(orig_shape) == 4:
            permuted_shape = state[&quot;exp_avg_sq&quot;].permute(0, 3, 1, 2).shape
        if merge_dims:
            exp_avg_sq = self.merge_dims(state[&quot;exp_avg_sq&quot;], max_precond_dim)
        else:
            exp_avg_sq = state[&quot;exp_avg_sq&quot;]
        final = []
        for ind, (m, o) in enumerate(zip(matrix, orth_matrix)):
            if len(m) == 0:
                final.append([])
                continue
            est_eig = torch.diag(o.T @ m @ o)
            sort_idx = torch.argsort(est_eig, descending=True)
            exp_avg_sq = exp_avg_sq.index_select(ind, sort_idx)
            o = o[:, sort_idx]
            power_iter = m @ o
            Q, _ = torch.linalg.qr(power_iter)
            if not float_data:
                Q = Q.to(original_device).type(original_type)
            final.append(Q)
        if merge_dims:
            if self._data_format == &quot;channels_last&quot; and len(orig_shape) == 4:
                exp_avg_sq = exp_avg_sq.reshape(permuted_shape).permute(0, 2, 3, 1)
            else:
                exp_avg_sq = exp_avg_sq.reshape(orig_shape)
        state[&quot;exp_avg_sq&quot;] = exp_avg_sq
        return final</file><file path="helpers/training/quantisation/__init__.py">from helpers.training.multi_process import should_log
from helpers.training.state_tracker import StateTracker
import logging
import torch, os
logger = logging.getLogger(__name__)
if should_log():
    logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
else:
    logger.setLevel(logging.ERROR)
def _quanto_type_map(model_precision: str):
    if model_precision == &quot;no_change&quot;:
        return None
    from optimum.quanto import (
        qfloat8,
        qfloat8_e4m3fnuz,
        qint8,
        qint4,
        qint2,
    )
    if model_precision == &quot;int2-quanto&quot;:
        quant_level = qint2
    elif model_precision == &quot;int4-quanto&quot;:
        quant_level = qint4
    elif model_precision == &quot;int8-quanto&quot;:
        quant_level = qint8
    elif model_precision == &quot;fp8-quanto&quot; or model_precision == &quot;fp8uz-quanto&quot;:
        if torch.backends.mps.is_available():
            logger.warning(
                &quot;MPS doesn&apos;t support dtype float8, you must select another precision level such as bf16, int2, int8, or int8.&quot;
            )
            return None
        if model_precision == &quot;fp8-quanto&quot;:
            quant_level = qfloat8
        elif model_precision == &quot;fp8uz-quanto&quot;:
            quant_level = qfloat8_e4m3fnuz
    else:
        raise ValueError(f&quot;Invalid quantisation level: {model_precision}&quot;)
    return quant_level
def _quanto_model(
    model,
    model_precision,
    base_model_precision=None,
    quantize_activations: bool = False,
):
    try:
        from helpers.training.quantisation import quanto_workarounds
        from optimum.quanto import (
            freeze,
            quantize,
            QTensor,
        )
    except ImportError as e:
        raise ImportError(
            f&quot;To use Quanto, please install the optimum library: `pip install optimum-quanto`: {e}&quot;
        )
    if model_precision is None:
        model_precision = base_model_precision
    if model is None:
        return model
    if model_precision == &quot;no_change&quot; or model_precision is None:
        logger.info(f&quot;...No quantisation applied to {model.__class__.__name__}.&quot;)
        return model
    logger.info(f&quot;Quantising {model.__class__.__name__}. Using {model_precision}.&quot;)
    weight_quant = _quanto_type_map(model_precision)
    extra_quanto_args = {}
    if StateTracker.get_args().model_family == &quot;sd3&quot;:
        extra_quanto_args[&quot;exclude&quot;] = [
            &quot;*.norm&quot;,
            &quot;*.norm1&quot;,
            &quot;*.norm1_context&quot;,
            &quot;*.norm_q&quot;,
            &quot;*.norm_k&quot;,
            &quot;*.norm_added_q&quot;,
            &quot;*.norm_added_k&quot;,
            &quot;proj_out&quot;,
            &quot;pos_embed&quot;,
            &quot;norm_out&quot;,
            &quot;context_embedder&quot;,
            &quot;time_text_embed&quot;,
        ]
    elif StateTracker.get_args().model_family == &quot;flux&quot;:
        extra_quanto_args[&quot;exclude&quot;] = [
            &quot;*.norm&quot;,
            &quot;*.norm1&quot;,
            &quot;*.norm2&quot;,
            &quot;*.norm2_context&quot;,
            &quot;proj_out&quot;,
            &quot;x_embedder&quot;,
            &quot;norm_out&quot;,
            &quot;context_embedder&quot;,
        ]
    if quantize_activations:
        logger.info(&quot;Freezing model weights and activations&quot;)
        extra_quanto_args[&quot;activations&quot;] = weight_quant
    else:
        logger.info(&quot;Freezing model weights only&quot;)
    try:
        quantize(model, weights=weight_quant, **extra_quanto_args)
        freeze(model)
    except Exception as e:
        if &quot;out of memory&quot; in str(e).lower():
            logger.error(
                &quot;GPU ran out of memory during quantisation. Use --quantize_via=cpu to use the slower CPU method.&quot;
            )
        raise e
    return model
def _torchao_filter_fn(mod: torch.nn.Module, fqn: str):
    # don&apos;t convert the output module
    if fqn == &quot;proj_out&quot;:
        return False
    # don&apos;t convert linear modules with weight dimensions not divisible by 16
    if isinstance(mod, torch.nn.Linear):
        if mod.in_features % 16 != 0 or mod.out_features % 16 != 0:
            return False
    return True
def _torchao_model(
    model,
    model_precision,
    base_model_precision=None,
    quantize_activations: bool = False,
):
    if model_precision is None:
        model_precision = base_model_precision
    if model is None:
        return model
    if model_precision == &quot;no_change&quot; or model_precision is None:
        logger.info(f&quot;...No quantisation applied to {model.__class__.__name__}.&quot;)
        return model
    try:
        from helpers.training.quantisation import torchao_workarounds
        from torchao.float8 import convert_to_float8_training, Float8LinearConfig
        from torchao.prototype.quantized_training import (
            int8_weight_only_quantized_training,
        )
        import torchao
        from torchao.quantization import quantize_
    except ImportError as e:
        raise ImportError(
            f&quot;To use torchao, please install the torchao library: `pip install torchao`: {e}&quot;
        )
    logger.info(f&quot;Quantising {model.__class__.__name__}. Using {model_precision}.&quot;)
    if quantize_activations:
        logger.warning(
            &quot;Activation quantisation is not used in TorchAO. This will be ignored.&quot;
        )
    if model_precision == &quot;int8-torchao&quot;:
        quantize_(
            model,
            int8_weight_only_quantized_training(),  # , filter_fn=_torchao_filter_fn
        )
    elif model_precision == &quot;fp8-torchao&quot;:
        model = convert_to_float8_training(
            model,
            module_filter_fn=_torchao_filter_fn,
            config=Float8LinearConfig(pad_inner_dim=True),
        )
    else:
        raise ValueError(
            f&quot;Invalid quantisation level. model_precision={model_precision}, base_model_precision={base_model_precision}&quot;
        )
    return model
def get_quant_fn(base_model_precision):
    &quot;&quot;&quot;
    Determine the quantization function based on the base model precision.
    Args:
        base_model_precision (str): The precision specification for the base model.
    Returns:
        function: The corresponding quantization function.
    Raises:
        ValueError: If the precision specification is unsupported.
    &quot;&quot;&quot;
    precision = base_model_precision.lower()
    if precision == &quot;no_change&quot;:
        return None
    if &quot;quanto&quot; in precision:
        return _quanto_model
    elif &quot;torchao&quot; in precision:
        return _torchao_model
    else:
        return None
def quantise_model(
    unet=None,
    transformer=None,
    text_encoder_1=None,
    text_encoder_2=None,
    text_encoder_3=None,
    controlnet=None,
    ema=None,
    args=None,
    return_dict: bool = False,
):
    &quot;&quot;&quot;
    Quantizes the provided models using the specified precision settings.
    Args:
        unet: The UNet model to quantize.
        transformer: The Transformer model to quantize.
        text_encoder_1: The first text encoder to quantize.
        text_encoder_2: The second text encoder to quantize.
        text_encoder_3: The third text encoder to quantize.
        controlnet: The ControlNet model to quantize.
        ema: An EMAModel to quantize.
        args: An object containing precision settings and other arguments.
    Returns:
        tuple: A tuple containing the quantized models in the order:
               (unet, transformer, text_encoder_1, text_encoder_2, text_encoder_3, controlnet)
    &quot;&quot;&quot;
    models = [
        (
            transformer,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.base_model_precision),
                &quot;model_precision&quot;: args.base_model_precision,
                &quot;quantize_activations&quot;: args.quantize_activations,
            },
        ),
        (
            unet,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.base_model_precision),
                &quot;model_precision&quot;: args.base_model_precision,
                &quot;quantize_activations&quot;: args.quantize_activations,
            },
        ),
        (
            controlnet,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.base_model_precision),
                &quot;model_precision&quot;: args.base_model_precision,
                &quot;quantize_activations&quot;: args.quantize_activations,
            },
        ),
        (
            text_encoder_1,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.text_encoder_1_precision),
                &quot;model_precision&quot;: args.text_encoder_1_precision,
                &quot;base_model_precision&quot;: args.base_model_precision,
            },
        ),
        (
            text_encoder_2,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.text_encoder_2_precision),
                &quot;model_precision&quot;: args.text_encoder_2_precision,
                &quot;base_model_precision&quot;: args.base_model_precision,
            },
        ),
        (
            text_encoder_3,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.text_encoder_3_precision),
                &quot;model_precision&quot;: args.text_encoder_3_precision,
                &quot;base_model_precision&quot;: args.base_model_precision,
            },
        ),
        (
            ema,
            {
                &quot;quant_fn&quot;: get_quant_fn(args.base_model_precision),
                &quot;model_precision&quot;: args.base_model_precision,
                &quot;quantize_activations&quot;: args.quantize_activations,
            },
        ),
    ]
    # Iterate over the models and apply quantization if the model is not None
    for i, (model, quant_args) in enumerate(models):
        quant_fn = quant_args[&quot;quant_fn&quot;]
        if quant_fn is None:
            continue
        if model is not None:
            quant_args_combined = {
                &quot;model_precision&quot;: quant_args[&quot;model_precision&quot;],
                &quot;base_model_precision&quot;: quant_args.get(
                    &quot;base_model_precision&quot;, args.base_model_precision
                ),
                &quot;quantize_activations&quot;: quant_args.get(
                    &quot;quantize_activations&quot;, args.quantize_activations
                ),
            }
            models[i] = (quant_fn(model, **quant_args_combined), quant_args)
    # Unpack the quantized models
    (
        transformer,
        unet,
        controlnet,
        text_encoder_1,
        text_encoder_2,
        text_encoder_3,
        ema,
    ) = [model for model, _ in models]
    if return_dict:
        return {
            &quot;unet&quot;: unet,
            &quot;transformer&quot;: transformer,
            &quot;text_encoder_1&quot;: text_encoder_1,
            &quot;text_encoder_2&quot;: text_encoder_2,
            &quot;text_encoder_3&quot;: text_encoder_3,
            &quot;controlnet&quot;: controlnet,
            &quot;ema&quot;: ema,
        }
    return (
        unet,
        transformer,
        text_encoder_1,
        text_encoder_2,
        text_encoder_3,
        controlnet,
        ema,
    )</file><file path="helpers/training/quantisation/peft_workarounds.py"># Copyright 2024-present the HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import annotations
import math
import warnings
from typing import Any, Optional
import torch
from torch import nn
from torch.nn import functional as F
from peft.import_utils import is_quanto_available
from peft.tuners.lora.layer import LoraLayer
from peft.tuners.tuners_utils import BaseTunerLayer, check_adapters_to_merge
from peft.utils.other import transpose
if is_quanto_available:
    # ensure that there are no quanto imports unless optimum.quanto is installed
    from optimum.quanto import QConv2d, QLinear
else:
    QConv2d, QLinear = None, None
class QuantoLoraLinear(torch.nn.Module, LoraLayer):
    &quot;&quot;&quot;LoRA layer implementation for quanto QLinear&quot;&quot;&quot;
    def __init__(
        self,
        base_layer,
        adapter_name,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.0,
        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        init_lora_weights: bool = True,
        use_rslora: bool = False,
        use_dora: bool = False,
        **kwargs,
    ):
        if use_dora:
            raise ValueError(
                f&quot;{self.__class__.__name__} does not support DoRA yet, please set it to False&quot;
            )
        super().__init__()
        LoraLayer.__init__(self, base_layer)
        self.fan_in_fan_out = fan_in_fan_out
        self._active_adapter = adapter_name
        self.update_layer(
            adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora
        )
    def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:
        result = self.base_layer(x)
        adapter_names = kwargs.pop(&quot;adapter_names&quot;, None)
        if adapter_names is not None:
            raise ValueError(
                f&quot;{self.__class__.__name__} does not support mixed_batch_forward yet.&quot;
            )
        if self.disable_adapters:
            return result
        if self.disable_adapters:
            if self.merged:
                self.unmerge()
            result = self.base_layer(x, *args, **kwargs)
        elif self.merged:
            result = self.base_layer(x, *args, **kwargs)
        else:
            for active_adapter in self.active_adapters:
                if active_adapter not in self.lora_A.keys():
                    continue
                lora_A = self.lora_A[active_adapter]
                lora_B = self.lora_B[active_adapter]
                dropout = self.lora_dropout[active_adapter]
                scaling = self.scaling[active_adapter]
                requires_conversion = not torch.is_autocast_enabled()
                if requires_conversion:
                    expected_dtype = result.dtype
                    x = x.to(lora_A.weight.dtype)
                output = lora_B(lora_A(dropout(x)))
                if requires_conversion:
                    output = output.to(expected_dtype)
                output = output * scaling
                result = result + output
        return result
    def get_delta_weight(self, adapter):
        return (
            transpose(
                self.lora_B[adapter].weight @ self.lora_A[adapter].weight,
                fan_in_fan_out=self.fan_in_fan_out,
            )
            * self.scaling[adapter]
        )
    def merge(
        self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None
    ) -&gt; None:
        from optimum.quanto import quantize_weight
        adapter_names = check_adapters_to_merge(self, adapter_names)
        if not adapter_names:
            # no adapter to merge
            return
        base_layer = self.get_base_layer()
        orig_weight = base_layer.weight
        for active_adapter in adapter_names:
            delta_weight = self.get_delta_weight(active_adapter)
            # note: no in-place for safe_merge=False
            new_weight_data = orig_weight + delta_weight
            if safe_merge:
                if torch.isfinite(new_weight_data).all():
                    raise ValueError(
                        f&quot;NaNs detected in the merged weights. The adapter {active_adapter} seems to be broken&quot;
                    )
            quantized = quantize_weight(
                new_weight_data, qtype=orig_weight.qtype, axis=orig_weight.axis
            )
            base_layer.weight._data = quantized._data
            base_layer.weight._scale = quantized._scale
            self.merged_adapters.append(active_adapter)
    def unmerge(self) -&gt; None:
        from optimum.quanto import quantize_weight
        if not self.merged:
            warnings.warn(&quot;Already unmerged. Nothing to do.&quot;)
            return
        while len(self.merged_adapters) &gt; 0:
            active_adapter = self.merged_adapters.pop()
            if active_adapter not in self.lora_A.keys():
                continue
            base_layer = self.get_base_layer()
            orig_weight = base_layer.weight
            new_weight_data = orig_weight - self.get_delta_weight(active_adapter)
            quantized = quantize_weight(
                new_weight_data, qtype=orig_weight.qtype, axis=orig_weight.axis
            )
            base_layer.weight._data = quantized._data
            base_layer.weight._scale = quantized._scale
    def __repr__(self) -&gt; str:
        rep = super().__repr__()
        return &quot;lora.&quot; + rep
class QuantoLoraConv2d(torch.nn.Module, LoraLayer):
    &quot;&quot;&quot;LoRA layer implementation for quanto QConv2d&quot;&quot;&quot;
    def __init__(
        self,
        base_layer,
        adapter_name,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.0,
        init_lora_weights: bool = True,
        use_rslora: bool = False,
        use_dora: bool = False,
        **kwargs,
    ):
        if use_dora:
            raise ValueError(
                f&quot;{self.__class__.__name__} does not support DoRA yet, please set it to False&quot;
            )
        super().__init__()
        LoraLayer.__init__(self, base_layer)
        self._active_adapter = adapter_name
        self.update_layer(
            adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora
        )
    def update_layer(
        self,
        adapter_name,
        r,
        lora_alpha,
        lora_dropout,
        init_lora_weights,
        use_rslora,
        use_dora,
    ):
        # same as lora.layer.Conv2d
        if r &lt;= 0:
            raise ValueError(
                f&quot;`r` should be a positive integer value but the value passed is {r}&quot;
            )
        self.r[adapter_name] = r
        self.lora_alpha[adapter_name] = lora_alpha
        if lora_dropout &gt; 0.0:
            lora_dropout_layer = nn.Dropout(p=lora_dropout)
        else:
            lora_dropout_layer = nn.Identity()
        self.lora_dropout[adapter_name] = lora_dropout_layer
        # Actual trainable parameters
        base_layer = self.get_base_layer()
        kernel_size = base_layer.kernel_size
        stride = base_layer.stride
        padding = base_layer.padding
        self.lora_A[adapter_name] = nn.Conv2d(
            self.in_features, r, kernel_size, stride, padding, bias=False
        )
        self.lora_B[adapter_name] = nn.Conv2d(
            r, self.out_features, (1, 1), (1, 1), bias=False
        )
        if use_rslora:
            self.scaling[adapter_name] = lora_alpha / math.sqrt(r)
        else:
            self.scaling[adapter_name] = lora_alpha / r
        if init_lora_weights == &quot;loftq&quot;:
            self.loftq_init(adapter_name)
        elif init_lora_weights:
            self.reset_lora_parameters(adapter_name, init_lora_weights)
        # call this before dora_init
        self._move_adapter_to_device_of_base_layer(adapter_name)
        if use_dora:
            # TODO: Implement DoRA
            self.dora_init(adapter_name)
            self.use_dora[adapter_name] = True
        else:
            self.use_dora[adapter_name] = False
        self.set_adapter(self.active_adapters)
    def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -&gt; torch.Tensor:
        result = self.base_layer(x)
        adapter_names = kwargs.pop(&quot;adapter_names&quot;, None)
        if adapter_names is not None:
            raise ValueError(
                f&quot;{self.__class__.__name__} does not support mixed_batch_forward yet.&quot;
            )
        if self.disable_adapters:
            return result
        if self.disable_adapters:
            if self.merged:
                self.unmerge()
            result = self.base_layer(x, *args, **kwargs)
        elif self.merged:
            result = self.base_layer(x, *args, **kwargs)
        else:
            for active_adapter in self.active_adapters:
                if active_adapter not in self.lora_A.keys():
                    continue
                lora_A = self.lora_A[active_adapter]
                lora_B = self.lora_B[active_adapter]
                dropout = self.lora_dropout[active_adapter]
                scaling = self.scaling[active_adapter]
                requires_conversion = not torch.is_autocast_enabled()
                if requires_conversion:
                    expected_dtype = result.dtype
                    x = x.to(lora_A.weight.dtype)
                output = lora_B(lora_A(dropout(x)))
                if requires_conversion:
                    output = output.to(expected_dtype)
                output = output * scaling
                result = result + output
        return result
    def get_delta_weight(self, adapter):
        # same as lora.layer.Conv2d
        device = self.lora_B[adapter].weight.device
        dtype = self.lora_A[adapter].weight.dtype
        # In case users wants to merge the adapter weights that are in
        # (b)float16 while being on CPU, we need to cast the weights to float32, perform the merge and then cast back to
        # (b)float16 because some CPUs have slow bf16/fp16 matmuls.
        cast_to_fp32 = device.type == &quot;cpu&quot; and (
            dtype == torch.float16 or dtype == torch.bfloat16
        )
        weight_A = self.lora_A[adapter].weight
        weight_B = self.lora_B[adapter].weight
        if cast_to_fp32:
            weight_A = weight_A.float()
            weight_B = weight_B.float()
        # https://github.com/bmaltais/kohya_ss/blob/feb6728762a8f463d15ba936d189d4c3abfaa1ab/networks/lora.py#L117
        if self.get_base_layer().weight.size()[2:4] == (1, 1):
            # conv2d 1x1
            output_tensor = (
                weight_B.squeeze(3).squeeze(2) @ weight_A.squeeze(3).squeeze(2)
            ).unsqueeze(2).unsqueeze(3) * self.scaling[adapter]
        else:
            # conv2d 3x3
            output_tensor = (
                F.conv2d(
                    weight_A.permute(1, 0, 2, 3),
                    weight_B,
                ).permute(1, 0, 2, 3)
                * self.scaling[adapter]
            )
        if cast_to_fp32:
            output_tensor = output_tensor.to(dtype=dtype)
            # cast back the weights
            self.lora_A[adapter].weight.data = weight_A.to(dtype)
            self.lora_B[adapter].weight.data = weight_B.to(dtype)
        return output_tensor
    def merge(
        self, safe_merge: bool = False, adapter_names: Optional[list[str]] = None
    ) -&gt; None:
        # same as lora.quanto.QuantoLoraLinear
        from optimum.quanto import quantize_weight
        adapter_names = check_adapters_to_merge(self, adapter_names)
        if not adapter_names:
            # no adapter to merge
            return
        base_layer = self.get_base_layer()
        orig_weight = base_layer.weight
        for active_adapter in adapter_names:
            delta_weight = self.get_delta_weight(active_adapter)
            # note: no in-place for safe_merge=False
            new_weight_data = orig_weight + delta_weight
            if safe_merge:
                if torch.isfinite(new_weight_data).all():
                    raise ValueError(
                        f&quot;NaNs detected in the merged weights. The adapter {active_adapter} seems to be broken&quot;
                    )
            quantized = quantize_weight(
                new_weight_data, qtype=orig_weight.qtype, axis=orig_weight.axis
            )
            base_layer.weight._data = quantized._data
            base_layer.weight._scale = quantized._scale
            self.merged_adapters.append(active_adapter)
    def unmerge(self) -&gt; None:
        # same as lora.quanto.QuantoLoraLinear
        from optimum.quanto import quantize_weight
        if not self.merged:
            warnings.warn(&quot;Already unmerged. Nothing to do.&quot;)
            return
        while len(self.merged_adapters) &gt; 0:
            active_adapter = self.merged_adapters.pop()
            if active_adapter not in self.lora_A.keys():
                continue
            base_layer = self.get_base_layer()
            orig_weight = base_layer.weight
            new_weight_data = orig_weight - self.get_delta_weight(active_adapter)
            quantized = quantize_weight(
                new_weight_data, qtype=orig_weight.qtype, axis=orig_weight.axis
            )
            base_layer.weight._data = quantized._data
            base_layer.weight._scale = quantized._scale
    def __repr__(self) -&gt; str:
        rep = super().__repr__()
        return &quot;lora.&quot; + rep
def dispatch_quanto(
    target: torch.nn.Module,
    adapter_name: str,
    **kwargs: Any,
) -&gt; Optional[torch.nn.Module]:
    new_module = None
    if isinstance(target, BaseTunerLayer):
        target_base_layer = target.get_base_layer()
    else:
        target_base_layer = target
    if is_quanto_available() and isinstance(target_base_layer, QLinear):
        new_module = QuantoLoraLinear(target, adapter_name, **kwargs)
        target.weight = target_base_layer.weight
        if hasattr(target, &quot;bias&quot;):
            target.bias = target_base_layer.bias
    elif is_quanto_available() and isinstance(target_base_layer, QConv2d):
        new_module = QuantoLoraConv2d(target, adapter_name, **kwargs)
        target.weight = target_base_layer.weight
        if hasattr(target, &quot;bias&quot;):
            target.bias = target_base_layer.bias
    return new_module
custom_module_mapping = {QConv2d: QuantoLoraConv2d, QLinear: QuantoLoraLinear}</file><file path="helpers/training/quantisation/quanto_workarounds.py">import torch
import optimum
if torch.cuda.is_available():
    # the marlin fp8 kernel needs some help with dtype casting for some reason
    # see: https://github.com/huggingface/optimum-quanto/pull/296#issuecomment-2380719201
    from optimum.quanto.library.extensions.cuda import ext as quanto_ext
    # Save the original operator
    original_gemm_f16f8_marlin = torch.ops.quanto.gemm_f16f8_marlin
    def fp8_marlin_gemm_wrapper(
        a: torch.Tensor,
        b_q_weight: torch.Tensor,
        b_scales: torch.Tensor,
        workspace: torch.Tensor,
        num_bits: int,
        size_m: int,
        size_n: int,
        size_k: int,
    ) -&gt; torch.Tensor:
        # Ensure &apos;a&apos; has the correct dtype
        a = a.to(b_scales.dtype)
        # Call the original operator
        return original_gemm_f16f8_marlin(
            a,
            b_q_weight,
            b_scales,
            workspace,
            num_bits,
            size_m,
            size_n,
            size_k,
        )
    # Monkey-patch the operator
    torch.ops.quanto.gemm_f16f8_marlin = fp8_marlin_gemm_wrapper
    class TinyGemmQBitsLinearFunction(
        optimum.quanto.tensor.function.QuantizedLinearFunction
    ):
        @staticmethod
        def forward(ctx, input, other, bias):
            ctx.save_for_backward(input, other)
            if type(input) is not torch.Tensor:
                input = input.dequantize()
            in_features = input.shape[-1]
            out_features = other.shape[0]
            output_shape = input.shape[:-1] + (out_features,)
            output = torch._weight_int4pack_mm(
                input.view(-1, in_features).to(dtype=other.dtype),
                other._data._data,
                other._group_size,
                other._scale_shift,
            )
            output = output.view(output_shape)
            if bias is not None:
                output = output + bias
            return output
    from optimum.quanto.tensor.weights import tinygemm
    tinygemm.qbits.TinyGemmQBitsLinearFunction = TinyGemmQBitsLinearFunction
class WeightQBytesLinearFunction(
    optimum.quanto.tensor.function.QuantizedLinearFunction
):
    @staticmethod
    def forward(ctx, input, other, bias=None):
        ctx.save_for_backward(input, other)
        if isinstance(input, optimum.quanto.tensor.QBytesTensor):
            output = torch.ops.quanto.qbytes_mm(
                input._data, other._data, input._scale * other._scale
            )
        else:
            in_features = input.shape[-1]
            out_features = other.shape[0]
            output_shape = input.shape[:-1] + (out_features,)
            output = torch.ops.quanto.qbytes_mm(
                input.reshape(-1, in_features), other._data, other._scale
            )
            output = output.view(output_shape)
        if bias is not None:
            output = output + bias
        return output
optimum.quanto.tensor.weights.qbytes.WeightQBytesLinearFunction = (
    WeightQBytesLinearFunction
)
def reshape_qlf_backward(ctx, gO):
    # another one where we need .reshape instead of .view
    input_gO = other_gO = bias_gO = None
    input, other = ctx.saved_tensors
    out_features, in_features = other.shape
    if ctx.needs_input_grad[0]:
        # grad(A@(B.t()) = gO =&gt; grad(A) = gO@(B.t().t()) = gO@B
        input_gO = torch.matmul(gO, other)
    if ctx.needs_input_grad[1]:
        # grad(B@A.t()) = gO.t() =&gt; grad(B) = gO.t()@(A.t().t()) = gO.t()@A
        other_gO = torch.matmul(
            gO.reshape(-1, out_features).t(),
            input.to(gO.dtype).reshape(-1, in_features),
        )
    if ctx.needs_input_grad[2]:
        # Bias gradient is the sum on all dimensions but the last one
        dim = tuple(range(gO.ndim - 1))
        bias_gO = gO.sum(dim)
    return input_gO, other_gO, bias_gO
optimum.quanto.tensor.function.QuantizedLinearFunction.backward = reshape_qlf_backward</file><file path="helpers/training/quantisation/torchao_workarounds.py">import torchao, torch
from torch import Tensor
from typing import Optional
from torchao.prototype.quantized_training.int8 import Int8QuantizedTrainingLinearWeight
class _Int8WeightOnlyLinear(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        input: Tensor,
        weight: Int8QuantizedTrainingLinearWeight,
        bias: Optional[Tensor] = None,
    ):
        ctx.save_for_backward(input, weight)
        ctx.bias = bias is not None
        # NOTE: we have to .T before .to(input.dtype) for torch.compile() mixed matmul to work
        out = (input @ weight.int_data.T.to(input.dtype)) * weight.scale
        out = out + bias if bias is not None else out
        return out
    @staticmethod
    def backward(ctx, grad_output):
        input, weight = ctx.saved_tensors
        grad_input = (grad_output * weight.scale) @ weight.int_data.to(
            grad_output.dtype
        )
        # print(f&quot;dtypes: grad_output {grad_output.dtype}, input {input.dtype}, weight {weight.dtype}&quot;)
        # here is the patch: we will cast the input to the grad_output dtype.
        grad_weight = grad_output.view(-1, weight.shape[0]).T @ input.to(
            grad_output.dtype
        ).reshape(-1, weight.shape[1])
        grad_bias = grad_output.view(-1, weight.shape[0]).sum(0) if ctx.bias else None
        return grad_input, grad_weight, grad_bias
torchao.prototype.quantized_training.int8._Int8WeightOnlyLinear = _Int8WeightOnlyLinear</file><file path="helpers/training/__init__.py">quantised_precision_levels = [
    &quot;no_change&quot;,
    &quot;int8-quanto&quot;,
    &quot;int4-quanto&quot;,
    &quot;int2-quanto&quot;,
    &quot;int8-torchao&quot;,
]
import torch
if torch.cuda.is_available():
    quantised_precision_levels.extend(
        [
            &quot;nf4-bnb&quot;,
            # &quot;fp4-bnb&quot;,
            # &quot;fp8-bnb&quot;,
            &quot;fp8-quanto&quot;,
            &quot;fp8uz-quanto&quot;,
        ]
    )
    primary_device = torch.cuda.get_device_properties(0)
    if primary_device.major &gt;= 9:
        # Hopper! Or blackwell+.
        quantised_precision_levels.append(&quot;fp8-torchao&quot;)
image_file_extensions = set(
    [
        &quot;jpg&quot;,
        &quot;jpeg&quot;,
        &quot;png&quot;,
        &quot;webp&quot;,
        &quot;bmp&quot;,
        &quot;tiff&quot;,
        &quot;tif&quot;,
        &quot;mp4&quot;,
        &quot;avi&quot;,
        &quot;gif&quot;,
        &quot;mov&quot;,
        &quot;webm&quot;,
    ]
)
video_file_extensions = set([&quot;mp4&quot;, &quot;avi&quot;, &quot;gif&quot;, &quot;mov&quot;, &quot;webm&quot;])
lycoris_defaults = {
    &quot;lora&quot;: {
        &quot;algo&quot;: &quot;lora&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 64,
        &quot;linear_alpha&quot;: 32,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
    &quot;loha&quot;: {
        &quot;algo&quot;: &quot;loha&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 32,
        &quot;linear_alpha&quot;: 16,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
    &quot;lokr&quot;: {
        &quot;algo&quot;: &quot;lokr&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 10000,  # Full dimension
        &quot;linear_alpha&quot;: 1,  # Ignored in full dimension
        &quot;factor&quot;: 16,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
    &quot;full&quot;: {
        &quot;algo&quot;: &quot;full&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 1024,  # Example full matrix size
        &quot;linear_alpha&quot;: 512,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
        },
    },
    &quot;ia3&quot;: {
        &quot;algo&quot;: &quot;ia3&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: None,  # No network arguments
        &quot;linear_alpha&quot;: None,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
        },
    },
    &quot;dylora&quot;: {
        &quot;algo&quot;: &quot;dylora&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 128,
        &quot;linear_alpha&quot;: 64,
        &quot;block_size&quot;: 1,  # Update one row/col per step
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
    &quot;diag-oft&quot;: {
        &quot;algo&quot;: &quot;diag-oft&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 64,  # Block size
        &quot;constraint&quot;: False,
        &quot;rescaled&quot;: False,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
    &quot;boft&quot;: {
        &quot;algo&quot;: &quot;boft&quot;,
        &quot;multiplier&quot;: 1.0,
        &quot;linear_dim&quot;: 64,  # Block size
        &quot;constraint&quot;: False,
        &quot;rescaled&quot;: False,
        &quot;apply_preset&quot;: {
            &quot;target_module&quot;: [&quot;Attention&quot;, &quot;FeedForward&quot;],
            &quot;module_algo_map&quot;: {
                &quot;Attention&quot;: {&quot;factor&quot;: 16},
                &quot;FeedForward&quot;: {&quot;factor&quot;: 8},
            },
        },
    },
}
def steps_remaining_in_epoch(current_step: int, steps_per_epoch: int) -&gt; int:
    &quot;&quot;&quot;
    Calculate the number of steps remaining in the current epoch.
    Args:
        current_step (int): The current step within the epoch.
        steps_per_epoch (int): Total number of steps in the epoch.
    Returns:
        int: Number of steps remaining in the current epoch.
    &quot;&quot;&quot;
    remaining_steps = steps_per_epoch - (current_step % steps_per_epoch)
    return remaining_steps</file><file path="helpers/training/adapter.py">import peft
import torch
import safetensors.torch
def determine_adapter_target_modules(args, unet, transformer):
    if unet is not None:
        return [&quot;to_k&quot;, &quot;to_q&quot;, &quot;to_v&quot;, &quot;to_out.0&quot;]
    elif transformer is not None:
        target_modules = [&quot;to_k&quot;, &quot;to_q&quot;, &quot;to_v&quot;, &quot;to_out.0&quot;]
        if args.model_family.lower() == &quot;flux&quot; and args.flux_lora_target == &quot;all&quot;:
            # target_modules = mmdit layers here
            target_modules = [
                &quot;to_k&quot;,
                &quot;to_q&quot;,
                &quot;to_v&quot;,
                &quot;add_k_proj&quot;,
                &quot;add_q_proj&quot;,
                &quot;add_v_proj&quot;,
                &quot;to_out.0&quot;,
                &quot;to_add_out&quot;,
            ]
        elif args.flux_lora_target == &quot;context&quot;:
            # i think these are the text input layers.
            target_modules = [
                &quot;add_k_proj&quot;,
                &quot;add_q_proj&quot;,
                &quot;add_v_proj&quot;,
                &quot;to_add_out&quot;,
            ]
        elif args.flux_lora_target == &quot;context+ffs&quot;:
            # i think these are the text input layers.
            target_modules = [
                &quot;add_k_proj&quot;,
                &quot;add_q_proj&quot;,
                &quot;add_v_proj&quot;,
                &quot;to_add_out&quot;,
                &quot;ff_context.net.0.proj&quot;,
                &quot;ff_context.net.2&quot;,
            ]
        elif args.flux_lora_target == &quot;all+ffs&quot;:
            target_modules = [
                &quot;to_k&quot;,
                &quot;to_q&quot;,
                &quot;to_v&quot;,
                &quot;add_k_proj&quot;,
                &quot;add_q_proj&quot;,
                &quot;add_v_proj&quot;,
                &quot;to_out.0&quot;,
                &quot;to_add_out&quot;,
                &quot;ff.net.0.proj&quot;,
                &quot;ff.net.2&quot;,
                &quot;ff_context.net.0.proj&quot;,
                &quot;ff_context.net.2&quot;,
                &quot;proj_mlp&quot;,
                &quot;proj_out&quot;,
            ]
        elif args.flux_lora_target == &quot;ai-toolkit&quot;:
            # from ostris&apos; ai-toolkit, possibly required to continue finetuning one.
            target_modules = [
                &quot;to_q&quot;,
                &quot;to_k&quot;,
                &quot;to_v&quot;,
                &quot;add_q_proj&quot;,
                &quot;add_k_proj&quot;,
                &quot;add_v_proj&quot;,
                &quot;to_out.0&quot;,
                &quot;to_add_out&quot;,
                &quot;ff.net.0.proj&quot;,
                &quot;ff.net.2&quot;,
                &quot;ff_context.net.0.proj&quot;,
                &quot;ff_context.net.2&quot;,
                &quot;norm.linear&quot;,
                &quot;norm1.linear&quot;,
                &quot;norm1_context.linear&quot;,
                &quot;proj_mlp&quot;,
                &quot;proj_out&quot;,
            ]
        elif args.flux_lora_target == &quot;tiny&quot;:
            # From TheLastBen
            # https://www.reddit.com/r/StableDiffusion/comments/1f523bd/good_flux_loras_can_be_less_than_45mb_128_dim/
            target_modules = [
                &quot;single_transformer_blocks.7.proj_out&quot;,
                &quot;single_transformer_blocks.20.proj_out&quot;,
            ]
        elif args.flux_lora_target == &quot;nano&quot;:
            # From TheLastBen
            # https://www.reddit.com/r/StableDiffusion/comments/1f523bd/good_flux_loras_can_be_less_than_45mb_128_dim/
            target_modules = [
                &quot;single_transformer_blocks.7.proj_out&quot;,
            ]
        return target_modules
@torch.no_grad()
def load_lora_weights(dictionary, filename, loraKey=&quot;default&quot;, use_dora=False):
    additional_keys = set()
    state_dict = safetensors.torch.load_file(filename)
    for prefix, model in dictionary.items():
        lora_layers = {
            (prefix + &quot;.&quot; + x): y
            for (x, y) in model.named_modules()
            if isinstance(y, peft.tuners.lora.layer.Linear)
        }
    missing_keys = set(
        [x + &quot;.lora_A.weight&quot; for x in lora_layers.keys()]
        + [x + &quot;.lora_B.weight&quot; for x in lora_layers.keys()]
        + (
            [x + &quot;.lora_magnitude_vector.weight&quot; for x in lora_layers.keys()]
            if use_dora
            else []
        )
    )
    for k, v in state_dict.items():
        if &quot;lora_A&quot; in k:
            kk = k.replace(&quot;.lora_A.weight&quot;, &quot;&quot;)
            if kk in lora_layers:
                lora_layers[kk].lora_A[loraKey].weight.copy_(v)
                missing_keys.remove(k)
            else:
                additional_keys.add(k)
        elif &quot;lora_B&quot; in k:
            kk = k.replace(&quot;.lora_B.weight&quot;, &quot;&quot;)
            if kk in lora_layers:
                lora_layers[kk].lora_B[loraKey].weight.copy_(v)
                missing_keys.remove(k)
            else:
                additional_keys.add(k)
        elif &quot;.alpha&quot; in k or &quot;.lora_alpha&quot; in k:
            kk = k.replace(&quot;.lora_alpha&quot;, &quot;&quot;).replace(&quot;.alpha&quot;, &quot;&quot;)
            if kk in lora_layers:
                lora_layers[kk].lora_alpha[loraKey] = v
        elif &quot;.lora_magnitude_vector&quot; in k:
            kk = k.replace(&quot;.lora_magnitude_vector.weight&quot;, &quot;&quot;)
            if kk in lora_layers:
                lora_layers[kk].lora_magnitude_vector[loraKey].weight.copy_(v)
                missing_keys.remove(k)
            else:
                additional_keys.add(k)
    return (additional_keys, missing_keys)</file><file path="helpers/training/collate.py">import torch
import logging
import concurrent.futures
import numpy as np
from os import environ
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import rank_info
from helpers.image_manipulation.training_sample import TrainingSample
from concurrent.futures import ThreadPoolExecutor
logger = logging.getLogger(&quot;collate_fn&quot;)
logger.setLevel(environ.get(&quot;SIMPLETUNER_COLLATE_LOG_LEVEL&quot;, &quot;INFO&quot;))
rank_text = rank_info()
from torchvision.transforms import ToTensor
# Convert PIL Image to PyTorch Tensor
to_tensor = ToTensor()
def debug_log(msg: str):
    logger.debug(f&quot;{rank_text}{msg}&quot;)
def compute_time_ids(
    intermediary_size: tuple,
    target_size: tuple,
    weight_dtype,
    vae_downscale_factor: int = 8,
    crop_coordinates: list = None,
):
    if intermediary_size is None or target_size is None:
        raise Exception(
            f&quot;Cannot continue, the intermediary_size or target_size were not provided: {intermediary_size}, {target_size}&quot;
        )
    logger.debug(
        f&quot;Computing time ids for:&quot;
        f&quot;\n-&gt; intermediary_size = {intermediary_size}&quot;
        f&quot;\n-&gt; target_size = {target_size}&quot;
    )
    # The dimensions of tensors are &quot;transposed&quot;, as:
    # (batch_size, height, width)
    # An image would look like:
    # (width, height)
    # SDXL conditions are:
    # [h, w, h, w, h, w]
    original_width = intermediary_size[0]
    original_height = intermediary_size[1]
    target_width = int(target_size[2] * vae_downscale_factor)
    target_height = int(target_size[1] * vae_downscale_factor)
    final_target_size = (target_height, target_width)
    if original_width is None:
        raise ValueError(&quot;Original width must be specified.&quot;)
    if original_height is None:
        raise ValueError(&quot;Original height must be specified.&quot;)
    if crop_coordinates is None:
        raise ValueError(&quot;Crop coordinates were not collected during collate.&quot;)
    if StateTracker.is_sdxl_refiner():
        fake_aesthetic_score = StateTracker.get_args().data_aesthetic_score
        add_time_ids = list(
            (original_height, original_width)
            + tuple(crop_coordinates)
            + (fake_aesthetic_score,)
        )
    else:
        add_time_ids = list(
            (original_height, original_width)
            + tuple(crop_coordinates)
            + final_target_size
        )
    add_time_ids = torch.tensor([add_time_ids], dtype=weight_dtype)
    logger.debug(
        f&quot;compute_time_ids returning {add_time_ids.shape} shaped time ids: {add_time_ids}&quot;
    )
    return add_time_ids
def extract_filepaths(examples):
    filepaths = []
    for example in examples:
        filepaths.append(example[&quot;image_path&quot;])
    return filepaths
def fetch_pixel_values(fp, data_backend_id: str):
    &quot;&quot;&quot;Worker method to fetch pixel values for a single image.&quot;&quot;&quot;
    debug_log(
        f&quot; -&gt; pull pixels for fp {fp} from cache via data backend {data_backend_id}&quot;
    )
    data_backend = StateTracker.get_data_backend(data_backend_id)
    image = data_backend[&quot;data_backend&quot;].read_image(fp)
    training_sample = TrainingSample(
        image=image,
        data_backend_id=data_backend_id,
    )
    return training_sample.prepare(return_tensor=True).image
def fetch_latent(fp, data_backend_id: str):
    &quot;&quot;&quot;Worker method to fetch latent for a single image.&quot;&quot;&quot;
    debug_log(
        f&quot; -&gt; pull latents for fp {fp} from cache via data backend {data_backend_id}&quot;
    )
    latent = StateTracker.get_vaecache(id=data_backend_id).retrieve_from_cache(fp)
    # Move to CPU and pin memory if it&apos;s not on the GPU
    if not torch.backends.mps.is_available():
        debug_log(&quot; -&gt; push latents to GPU via pinned memory&quot;)
        if isinstance(latent, dict):
            latent[&quot;latents&quot;] = latent[&quot;latents&quot;].to(&quot;cpu&quot;).pin_memory()
        else:
            latent = latent.to(&quot;cpu&quot;).pin_memory()
    return latent
def deepfloyd_pixels(filepaths, data_backend_id: str):
    &quot;&quot;&quot;DeepFloyd doesn&apos;t use the VAE. We retrieve, normalise, and stack the pixel tensors directly.&quot;&quot;&quot;
    # Use a thread pool to fetch latents concurrently
    try:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            pixels = list(
                executor.map(
                    fetch_pixel_values, filepaths, [data_backend_id] * len(filepaths)
                )
            )
    except Exception as e:
        logger.error(f&quot;(id={data_backend_id}) Error while computing pixels: {e}&quot;)
        raise
    pixels = torch.stack(pixels)
    pixels = pixels.to(memory_format=torch.contiguous_format).float()
    return pixels
def fetch_conditioning_pixel_values(
    fp, training_fp, conditioning_data_backend_id: str, training_data_backend_id: str
):
    &quot;&quot;&quot;Worker method to fetch pixel values for a single image.&quot;&quot;&quot;
    # Retrieve data backends
    conditioning_data_backend = StateTracker.get_data_backend(
        conditioning_data_backend_id
    )
    training_data_backend = StateTracker.get_data_backend(training_data_backend_id)
    # Use the provided training file path directly
    training_sample = TrainingSample.from_image_path(
        image_path=training_fp,
        data_backend_id=training_data_backend_id,
    )
    conditioning_sample = TrainingSample.from_image_path(
        image_path=fp,
        data_backend_id=conditioning_data_backend_id,
    )
    # Prepare the conditioning sample to match the training sample
    prepared_like = conditioning_sample.prepare_like(
        training_sample, return_tensor=True
    ).image
    return prepared_like
def conditioning_pixels(
    filepaths,
    training_filepaths,
    conditioning_data_backend_id: str,
    training_data_backend_id: str,
):
    &quot;&quot;&quot;For pixel-based conditioning images that must be prepared matching a paired image&apos;s metadata..&quot;&quot;&quot;
    try:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            pixels = list(
                executor.map(
                    fetch_conditioning_pixel_values,
                    filepaths,
                    training_filepaths,
                    [conditioning_data_backend_id] * len(filepaths),
                    [training_data_backend_id] * len(filepaths),
                )
            )
    except Exception as e:
        logger.error(
            f&quot;(conditioning_data_backend_id={conditioning_data_backend_id}) Error while retrieving or transforming pixels (training data id={training_data_backend_id}): {e}&quot;
        )
        raise
    pixels = torch.stack(pixels)
    pixels = pixels.to(memory_format=torch.contiguous_format).float()
    return pixels
def compute_latents(filepaths, data_backend_id: str):
    # Use a thread pool to fetch latents concurrently
    try:
        if &quot;deepfloyd&quot; in StateTracker.get_args().model_type:
            latents = deepfloyd_pixels(filepaths, data_backend_id)
            return latents
        if StateTracker.get_args().vae_cache_ondemand:
            latents = StateTracker.get_vaecache(id=data_backend_id).encode_images(
                [None] * len(filepaths), filepaths
            )
        else:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                latents = list(
                    executor.map(
                        fetch_latent, filepaths, [data_backend_id] * len(filepaths)
                    )
                )
    except Exception as e:
        logger.error(f&quot;(id={data_backend_id}) Error while computing latents: {e}&quot;)
        raise
    return latents
def compute_single_embedding(
    caption, text_embed_cache, is_sdxl, is_sd3: bool = False, is_flux: bool = False
):
    &quot;&quot;&quot;Worker function to compute embedding for a single caption.&quot;&quot;&quot;
    if caption == &quot;&quot; or not caption:
        # Grab the default text embed backend for null caption.
        text_embed_cache = StateTracker.get_default_text_embed_cache()
        debug_log(
            f&quot;Hashing caption &apos;{caption}&apos; on text embed cache: {text_embed_cache.id} using data backend {text_embed_cache.data_backend.id}&quot;
        )
    if is_sdxl:
        (
            prompt_embeds,
            pooled_prompt_embeds,
        ) = text_embed_cache.compute_embeddings_for_sdxl_prompts([caption])
        return (
            prompt_embeds[0],
            pooled_prompt_embeds[0],
        )  # Unpack the first (and only) element
    elif is_sd3:
        prompt_embeds, pooled_prompt_embeds = (
            text_embed_cache.compute_embeddings_for_sd3_prompts(prompts=[caption])
        )
        return prompt_embeds[0], pooled_prompt_embeds[0]
    elif is_flux:
        prompt_embeds, pooled_prompt_embeds, time_ids, masks = (
            text_embed_cache.compute_embeddings_for_flux_prompts(prompts=[caption])
        )
        return (
            prompt_embeds[0],
            pooled_prompt_embeds[0],
            time_ids[0],
            masks[0] if masks is not None else None,
        )
    else:
        prompt_embeds = text_embed_cache.compute_embeddings_for_legacy_prompts(
            [caption]
        )
        if type(prompt_embeds) == tuple:
            if StateTracker.get_model_family() in [&quot;pixart_sigma&quot;, &quot;smoldit&quot;]:
                # PixArt requires the attn mask be returned, too.
                prompt_embeds, attn_mask = prompt_embeds
                return prompt_embeds, attn_mask
            elif &quot;deepfloyd&quot; in StateTracker.get_args().model_type:
                # DeepFloyd doesn&apos;t use the attn mask on the unet inputs, we discard it
                prompt_embeds = prompt_embeds[0]
            prompt_embeds = prompt_embeds[0]
        result = torch.squeeze(prompt_embeds[0])
        debug_log(f&quot;Torch shape: {result}&quot;)
        return result, None  # Unpack and return None for the second element
def compute_prompt_embeddings(captions, text_embed_cache):
    &quot;&quot;&quot;
    Retrieve / compute text embeds in parallel.
    Args:
        captions: List of strings
        text_embed_cache: TextEmbedCache instance
    Returns:
        prompt_embeds_all: Tensor of shape (batch_size, 512)
        add_text_embeds_all: Tensor of shape (batch_size, 512)
    &quot;&quot;&quot;
    debug_log(&quot; -&gt; get embed from cache&quot;)
    is_sdxl = (
        text_embed_cache.model_type == &quot;sdxl&quot; or text_embed_cache.model_type == &quot;kolors&quot;
    )
    is_sd3 = text_embed_cache.model_type == &quot;sd3&quot;
    is_pixart_sigma = text_embed_cache.model_type == &quot;pixart_sigma&quot;
    is_smoldit = text_embed_cache.model_type == &quot;smoldit&quot;
    is_flux = text_embed_cache.model_type == &quot;flux&quot;
    # Use a thread pool to compute embeddings concurrently
    with ThreadPoolExecutor() as executor:
        embeddings = list(
            executor.map(
                compute_single_embedding,
                captions,
                [text_embed_cache] * len(captions),
                [is_sdxl] * len(captions),
                [is_sd3] * len(captions),
                [is_flux] * len(captions),
            )
        )
    debug_log(f&quot;Got embeddings: {embeddings}&quot;)
    if is_sdxl:
        # Separate the tuples
        prompt_embeds = [t[0] for t in embeddings]
        add_text_embeds = [t[1] for t in embeddings]
        return (torch.stack(prompt_embeds), torch.stack(add_text_embeds))
    elif is_sd3:
        # Separate the tuples
        prompt_embeds = [t[0] for t in embeddings]
        add_text_embeds = [t[1] for t in embeddings]
        return (torch.stack(prompt_embeds), torch.stack(add_text_embeds))
    elif is_pixart_sigma or is_smoldit:
        # the tuples here are the text encoder hidden states and the attention masks
        prompt_embeds, attn_masks = [], []
        for embed in embeddings:
            prompt_embeds.append(embed[0][0])
            attn_masks.append(embed[1][0])
        if len(prompt_embeds[0].shape) == 3:
            # some tensors are already expanded due to the way they were saved
            prompt_embeds = [t.squeeze(0) for t in prompt_embeds]
        return (torch.stack(prompt_embeds), torch.stack(attn_masks))
    elif is_flux:
        # Separate the tuples
        prompt_embeds = [t[0] for t in embeddings]
        add_text_embeds = [t[1] for t in embeddings]
        time_ids = [t[2] for t in embeddings]
        masks = [t[3] for t in embeddings]
        return (
            torch.stack(prompt_embeds),
            torch.stack(add_text_embeds),
            torch.stack(time_ids),
            torch.stack(masks) if None not in masks else None,
        )
    else:
        # Separate the tuples
        prompt_embeds = [t[0] for t in embeddings]
        return (torch.stack(prompt_embeds), None)
def gather_conditional_pixart_size_features(examples, latents, weight_dtype):
    bsz = len(examples)
    # 1/8th scale VAE
    LATENT_COMPRESSION_F = 8
    batch_height = latents.shape[2] * LATENT_COMPRESSION_F
    batch_width = latents.shape[3] * LATENT_COMPRESSION_F
    resolution = torch.tensor([batch_height, batch_width]).repeat(bsz, 1)
    aspect_ratio = torch.tensor([float(batch_height / batch_width)]).repeat(bsz, 1)
    resolution = resolution.to(
        dtype=weight_dtype, device=StateTracker.get_accelerator().device
    )
    aspect_ratio = aspect_ratio.to(
        dtype=weight_dtype, device=StateTracker.get_accelerator().device
    )
    return {&quot;resolution&quot;: resolution, &quot;aspect_ratio&quot;: aspect_ratio}
def gather_conditional_sdxl_size_features(examples, latents, weight_dtype):
    batch_time_ids_list = []
    if len(examples) != len(latents):
        raise ValueError(
            f&quot;Number of examples ({len(examples)}) and latents ({len(latents)}) must match.&quot;
        )
    for idx, example in enumerate(examples):
        # Compute time IDs for all examples
        # - We use the intermediary size as the original size for SDXL.
        # - This is because we first resize to intermediary_size before cropping.
        time_ids = compute_time_ids(
            intermediary_size=tuple(
                example.get(&quot;intermediary_size&quot;, example.get(&quot;original_size&quot;))
            ),
            target_size=latents[idx].shape,
            crop_coordinates=example[&quot;crop_coordinates&quot;],
            weight_dtype=weight_dtype,
        )
        # Overwrite with zeros if conditioning is to be dropped
        if example[&quot;drop_conditioning&quot;]:
            time_ids = torch.zeros_like(time_ids)
        batch_time_ids_list.append(time_ids)
    return torch.stack(batch_time_ids_list, dim=0)
def check_latent_shapes(latents, filepaths, data_backend_id, batch):
    # Validate shapes
    test_shape = latents[0].shape
    # 5D tensors (B, F, C, H, W) are for LTX Video currently, and we&apos;ll just test the C, H, W shape
    if len(test_shape) == 5:
        test_shape = test_shape[1:]
    # Check all &quot;aspect_ratio&quot; values and raise error if any differ, with the two differing values:
    for example in batch:
        if example[&quot;aspect_ratio&quot;] != batch[0][&quot;aspect_ratio&quot;]:
            error_msg = f&quot;(id=({data_backend_id}) Aspect ratio mismatch: {example[&apos;aspect_ratio&apos;]} != {batch[0][0][&apos;aspect_ratio&apos;]}&quot;
            logger.error(error_msg)
            logger.error(f&quot;Erroneous batch: {batch}&quot;)
            raise ValueError(error_msg)
    for idx, latent in enumerate(latents):
        # Are there any inf or nan positions?
        if latent is None:
            logger.debug(f&quot;Error batch: {batch}&quot;)
            error_msg = f&quot;(id={data_backend_id}) File {filepaths[idx]} latent is None. Filepath: {filepaths[idx]}, data_backend_id: {data_backend_id}&quot;
            logger.error(error_msg)
            raise ValueError(error_msg)
        if torch.isnan(latent).any() or torch.isinf(latent).any():
            # get the data_backend
            data_backend = StateTracker.get_data_backend(data_backend_id)
            # remove the object
            data_backend[&quot;vaecache&quot;].cache_data_backend.delete(filepaths[idx])
            raise ValueError(
                f&quot;(id={data_backend_id}) Deleted cache file {filepaths[idx]}: contains NaN or Inf values: {latent}&quot;
            )
        if len(latent.shape) == 5:
            if latent.shape[1:] != test_shape:
                raise ValueError(
                    f&quot;(id={data_backend_id}) File {filepaths[idx]} latent shape mismatch: {latent.shape[1:]} != {test_shape}&quot;
                )
        elif latent.shape != test_shape:
            raise ValueError(
                f&quot;(id={data_backend_id}) File {filepaths[idx]} latent shape mismatch: {latent.shape} != {test_shape}&quot;
            )
    debug_log(f&quot; -&gt; stacking {len(latents)} latents: {latents}&quot;)
    return torch.stack(
        [latent.to(StateTracker.get_accelerator().device) for latent in latents], dim=0
    )
def collate_fn(batch):
    if len(batch) != 1:
        raise ValueError(
            &quot;This trainer is not designed to handle multiple batches in a single collate.&quot;
        )
    debug_log(&quot;Begin collate_fn on batch&quot;)
    # SDXL Dropout
    dropout_probability = StateTracker.get_args().caption_dropout_probability
    batch = batch[0]
    examples = batch[&quot;training_samples&quot;]
    conditioning_examples = batch[&quot;conditioning_samples&quot;]
    is_regularisation_data = batch.get(&quot;is_regularisation_data&quot;, False)
    is_i2v_data = batch.get(&quot;is_i2v_data&quot;, False)
    if StateTracker.get_args().controlnet and len(examples) != len(
        conditioning_examples
    ):
        raise ValueError(
            &quot;Number of training samples and conditioning samples must match for ControlNet.&quot;
            f&quot;\n-&gt; Training samples: {examples}&quot;
            f&quot;\n-&gt; Conditioning samples: {conditioning_examples}&quot;
        )
    # Randomly drop captions/conditioning based on dropout_probability
    for example in examples:
        data_backend_id = example[&quot;data_backend_id&quot;]
        if (
            dropout_probability is not None
            and dropout_probability &gt; 0
            and np.random.rand() &lt; dropout_probability
        ):
            example[&quot;instance_prompt_text&quot;] = &quot;&quot;  # Drop caption
            example[&quot;drop_conditioning&quot;] = True  # Flag to drop conditioning
        else:
            example[&quot;drop_conditioning&quot;] = False
    debug_log(&quot;Collect luminance values&quot;)
    if &quot;luminance&quot; in examples[0]:
        batch_luminance = [example[&quot;luminance&quot;] for example in examples]
    else:
        batch_luminance = [0] * len(examples)
    # average it
    batch_luminance = sum(batch_luminance) / len(batch_luminance)
    debug_log(&quot;Extract filepaths&quot;)
    filepaths = extract_filepaths(examples)
    debug_log(&quot;Compute latents&quot;)
    batch_data = compute_latents(filepaths, data_backend_id)
    if isinstance(batch_data[0], dict):
        latent_batch = [v[&quot;latents&quot;] for v in batch_data]
    else:
        latent_batch = batch_data
    if &quot;deepfloyd&quot; not in StateTracker.get_args().model_type:
        debug_log(&quot;Check latents&quot;)
        latent_batch = check_latent_shapes(
            latent_batch, filepaths, data_backend_id, examples
        )
    conditioning_filepaths = []
    training_filepaths = []
    conditioning_type = None
    conditioning_pixel_values = None
    if len(conditioning_examples) &gt; 0:
        if len(conditioning_examples) != len(examples):
            raise ValueError(
                &quot;The number of conditioning examples must match the number of training examples.&quot;
            )
        data_backend = StateTracker.get_data_backend(data_backend_id)
        conditioning_data_backend_id = data_backend.get(&quot;conditioning_data&quot;, {}).get(
            &quot;id&quot;
        )
        for cond_example, train_example in zip(conditioning_examples, examples):
            # Ensure conditioning types match
            cond_type = cond_example.get_conditioning_type()
            if conditioning_type is None:
                conditioning_type = cond_type
            elif cond_type != conditioning_type:
                raise ValueError(
                    f&quot;Conditioning type mismatch: {conditioning_type} != {cond_type}&quot;
                    &quot;\n-&gt; Ensure all conditioning samples are of the same type.&quot;
                )
            # Collect conditioning and training file paths
            conditioning_filepaths.append(cond_example.image_path(basename_only=False))
            training_filepaths.append(train_example[&quot;image_path&quot;])
        # Pass both file paths to `conditioning_pixels`
        conditioning_pixel_values = conditioning_pixels(
            conditioning_filepaths,
            training_filepaths,
            conditioning_data_backend_id,
            data_backend_id,
        )
        conditioning_pixel_values = torch.stack(
            [
                latent.to(StateTracker.get_accelerator().device)
                for latent in conditioning_pixel_values
            ]
        )
    # Compute embeddings and handle dropped conditionings
    debug_log(&quot;Extract captions&quot;)
    captions = [example[&quot;instance_prompt_text&quot;] for example in examples]
    debug_log(&quot;Pull cached text embeds&quot;)
    text_embed_cache = StateTracker.get_data_backend(data_backend_id)[
        &quot;text_embed_cache&quot;
    ]
    attn_mask = None
    batch_time_ids = None
    if StateTracker.get_model_family() == &quot;flux&quot;:
        debug_log(&quot;Compute and stack Flux time ids&quot;)
        prompt_embeds_all, add_text_embeds_all, batch_time_ids, attn_mask = (
            compute_prompt_embeddings(captions, text_embed_cache)
        )
    else:
        prompt_embeds_all, add_text_embeds_all = compute_prompt_embeddings(
            captions, text_embed_cache
        )
    if (
        StateTracker.get_model_family() == &quot;sdxl&quot;
        or StateTracker.get_model_family() == &quot;kolors&quot;
    ):
        debug_log(&quot;Compute and stack SDXL time ids&quot;)
        batch_time_ids = gather_conditional_sdxl_size_features(
            examples, latent_batch, StateTracker.get_weight_dtype()
        )
        debug_log(f&quot;Time ids stacked to {batch_time_ids.shape}: {batch_time_ids}&quot;)
    elif StateTracker.get_model_family() == &quot;pixart_sigma&quot;:
        debug_log(&quot;Compute and stack PixArt time ids&quot;)
        batch_time_ids = gather_conditional_pixart_size_features(
            examples, latent_batch, StateTracker.get_weight_dtype()
        )
        attn_mask = add_text_embeds_all
    elif StateTracker.get_model_family() == &quot;smoldit&quot;:
        attn_mask = add_text_embeds_all
    return {
        &quot;latent_batch&quot;: latent_batch,
        &quot;prompt_embeds&quot;: prompt_embeds_all,
        &quot;add_text_embeds&quot;: add_text_embeds_all,
        &quot;batch_time_ids&quot;: batch_time_ids,
        &quot;batch_luminance&quot;: batch_luminance,
        &quot;conditioning_pixel_values&quot;: conditioning_pixel_values,
        &quot;encoder_attention_mask&quot;: attn_mask,
        &quot;is_regularisation_data&quot;: is_regularisation_data,
        &quot;is_i2v_data&quot;: is_i2v_data,
        &quot;conditioning_type&quot;: conditioning_type,
    }</file><file path="helpers/training/custom_schedule.py">from torch.optim.lr_scheduler import LambdaLR
import torch
import math
import accelerate
import os
import logging
from torch.optim.lr_scheduler import LRScheduler
from helpers.training.state_tracker import StateTracker
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def segmented_timestep_selection(
    actual_num_timesteps, bsz, weights, use_refiner_range: bool = False
):
    args = StateTracker.get_args()
    # Determine the range of timesteps to use
    num_timesteps = actual_num_timesteps
    if use_refiner_range or args.refiner_training:
        if args.refiner_training_invert_schedule:
            # Inverted schedule calculation: we start from the last timestep and move downwards
            start_timestep = (
                actual_num_timesteps - 1
            )  # Start from the last timestep, e.g., 999
            # Calculate the end of the range based on the inverse of the training strength
            end_timestep = int(args.refiner_training_strength * actual_num_timesteps)
        else:
            # Normal refiner training schedule
            start_timestep = (
                int(actual_num_timesteps * args.refiner_training_strength) - 1
            )
            end_timestep = 0
        num_timesteps = start_timestep - end_timestep + 1
    else:
        start_timestep = actual_num_timesteps - 1
        end_timestep = 0
    # logger.debug(
    #     f&quot;{&apos;Using SDXL refiner&apos; if StateTracker.is_sdxl_refiner() else &apos;Training base model &apos;} with {num_timesteps} timesteps from a full schedule of {actual_num_timesteps} and a segment size of {num_timesteps // bsz} timesteps.&quot;
    # )
    segment_size = max(num_timesteps // bsz, 1)
    selected_timesteps = []
    # Select one timestep from each segment based on the weights
    for i in range(bsz):
        start = start_timestep - i * segment_size
        end = max(start - segment_size, end_timestep) if i != bsz - 1 else end_timestep
        # logger.debug(f&quot;Segment from {start} to {end}&quot;)
        segment_weights = weights[end : start + 1]
        # Normalize segment weights to ensure they sum to 1
        segment_weights /= segment_weights.sum()
        # Sample one timestep from the segment
        segment_timesteps = torch.arange(end, start + 1)
        selected_timestep = torch.multinomial(segment_weights, 1).item()
        selected_timesteps.append(segment_timesteps[selected_timestep])
    # logger.debug(f&quot;Selected timesteps: {selected_timesteps}&quot;)
    return torch.tensor(selected_timesteps)
def get_sd3_sigmas(
    accelerator, noise_scheduler_copy, timesteps, n_dim=4, dtype=torch.float32
):
    sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)
    # print(f&apos;sigmas: {sigmas.shape}&apos;)
    schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)
    timesteps = timesteps.to(accelerator.device)
    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
    # print(f&apos;step_indices: {step_indices}&apos;)
    sigma = sigmas[step_indices].flatten()
    while len(sigma.shape) &lt; n_dim:
        # print(&apos;unsqueeze&apos;)
        sigma = sigma.unsqueeze(-1)
    # print(&apos;return&apos;)
    return sigma
def generate_timestep_weights(args, num_timesteps):
    weights = torch.ones(num_timesteps)
    # Determine the indices to bias
    num_to_bias = int(args.timestep_bias_portion * num_timesteps)
    if args.timestep_bias_strategy == &quot;later&quot;:
        bias_indices = slice(-num_to_bias, None)
    elif args.timestep_bias_strategy == &quot;earlier&quot;:
        bias_indices = slice(0, num_to_bias)
    elif args.timestep_bias_strategy == &quot;range&quot;:
        # Out of the possible 1000 timesteps, we might want to focus on eg. 200-500.
        range_begin = args.timestep_bias_begin
        range_end = args.timestep_bias_end
        if range_begin &lt; 0:
            raise ValueError(
                &quot;When using the range strategy for timestep bias, you must provide a beginning timestep greater or equal to zero.&quot;
            )
        if range_end &gt; num_timesteps:
            raise ValueError(
                &quot;When using the range strategy for timestep bias, you must provide an ending timestep smaller than the number of timesteps.&quot;
            )
        bias_indices = slice(range_begin, range_end)
    else:  # &apos;none&apos; or any other string
        return weights
    if args.timestep_bias_multiplier &lt;= 0:
        raise ValueError(
            &quot;The parameter --timestep_bias_multiplier is not intended to be used to disable the training of specific timesteps.&quot;
            &quot; If it was intended to disable timestep bias, use `--timestep_bias_strategy none` instead.&quot;
            &quot; A timestep bias multiplier less than or equal to 0 is not allowed.&quot;
        )
    # Apply the bias
    weights[bias_indices] *= args.timestep_bias_multiplier
    # Normalize
    weights /= weights.sum()
    return weights
def get_polynomial_decay_schedule_with_warmup(
    optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    lr_end: float = 1e-7,
    power: float = 1.0,
    last_epoch: int = -1,
):
    &quot;&quot;&quot;
    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
    optimizer to end lr defined by *lr_end*, after a warmup period during which it increases linearly from 0 to the
    initial lr set in the optimizer.
    Args:
        optimizer ([`~torch.optim.Optimizer`]):
            The optimizer for which to schedule the learning rate.
        num_warmup_steps (`int`):
            The number of steps for the warmup phase.
        num_training_steps (`int`):
            The total number of training steps.
        lr_end (`float`, *optional*, defaults to 1e-7):
            The end LR.
        power (`float`, *optional*, defaults to 1.0):
            Power factor.
        last_epoch (`int`, *optional*, defaults to -1):
            The index of the last epoch when resuming training.
    Note: *power* defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
    implementation at
    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37
    Return:
        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.
    &quot;&quot;&quot;
    lr_init = optimizer.defaults[&quot;lr&quot;]
    if not (float(lr_init) &gt; float(lr_end)):
        raise ValueError(
            f&quot;lr_end ({lr_end}) must be be smaller than initial lr ({lr_init})&quot;
        )
    def lr_lambda(current_step: int):
        if current_step &lt; num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        elif current_step &gt; num_training_steps:
            return float(lr_end) / float(lr_init)  # as LambdaLR multiplies by lr_init
        else:
            lr_range = float(lr_init) - float(lr_end)
            decay_steps = int(num_training_steps) - int(num_warmup_steps)
            pct_remaining = 1 - (current_step - int(num_warmup_steps)) / decay_steps
            decay = lr_range * pct_remaining**power + float(lr_end)
            return decay / float(lr_init)  # as LambdaLR multiplies by lr_init
    return LambdaLR(optimizer, lr_lambda, last_epoch)
def enforce_zero_terminal_snr(betas):
    # Convert betas to alphas_bar_sqrt
    alphas = 1 - betas
    alphas_bar = alphas.cumprod(0)
    alphas_bar_sqrt = alphas_bar.sqrt()
    # Store old values.
    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()
    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()
    # Shift so last timestep is zero.
    alphas_bar_sqrt -= alphas_bar_sqrt_T
    # Scale so first timestep is back to old value.
    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)
    # Convert alphas_bar_sqrt to betas
    alphas_bar = alphas_bar_sqrt**2
    alphas = alphas_bar[1:] / alphas_bar[:-1]
    alphas = torch.cat([alphas_bar[0:1], alphas])
    betas = 1 - alphas
    return betas
def patch_scheduler_betas(scheduler):
    scheduler.betas = enforce_zero_terminal_snr(scheduler.betas)
class _enable_get_lr_call:
    def __init__(self, o):
        self.o = o
    def __enter__(self):
        self.o._get_lr_called_within_step = True
        return self
    def __exit__(self, type, value, traceback):
        self.o._get_lr_called_within_step = False
        return self
class Cosine(LRScheduler):
    r&quot;&quot;&quot;Use a cosine schedule for the learning rate, without restarts.
    This makes a nice and pretty chart on the tensorboard.
    Args:
        optimizer (Optimizer): Wrapped optimizer.
        T_0 (int): Number of iterations for the first restart.
        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.
        eta_min (float, optional): Minimum learning rate. Default: 0.
        last_epoch (int, optional): The index of last epoch. Default: -1.
        verbose (bool): If ``True``, prints a message to stdout for
            each update. Default: ``False``.
    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
        https://arxiv.org/abs/1608.03983
    &quot;&quot;&quot;
    def __init__(
        self,
        optimizer,
        T_0,
        steps_per_epoch=-1,
        T_mult=1,
        eta_min=0,
        last_step=-1,
        last_epoch=-1,
        verbose=False,
    ):
        if T_0 &lt;= 0 or not isinstance(T_0, int):
            raise ValueError(
                f&quot;Cosine learning rate expects to use warmup steps as its interval. Expected positive integer T_0, but got {T_0}&quot;
            )
        if T_mult &lt; 1 or not isinstance(T_mult, int):
            raise ValueError(f&quot;Expected integer T_mult &gt;= 1, but got {T_mult}&quot;)
        if last_epoch != -1 and last_step != -1:
            last_epoch = last_step
        elif last_epoch != -1 and last_step == -1:
            last_step = last_epoch
        self.T_0 = T_0
        self.steps_per_epoch = steps_per_epoch
        self.T_i = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.T_cur = last_step
        super().__init__(optimizer, last_step, verbose)
    def get_lr(self):
        lrs = [
            self.eta_min
            + (base_lr - self.eta_min)
            * (1 + math.cos(math.pi * self.T_cur / self.T_i))
            / 2
            for base_lr in self.base_lrs
        ]
        return lrs
    def step(self, step=None):
        if step is None and self.last_epoch &lt; 0:
            step = 0
        if step is None:
            step = self.last_epoch + 1
            self.T_cur = (step // self.steps_per_epoch) + (
                step % self.steps_per_epoch
            ) / self.steps_per_epoch
        else:
            self.T_cur = (step // self.steps_per_epoch) + (
                step % self.steps_per_epoch
            ) / self.steps_per_epoch
        if self.T_cur &gt;= self.T_i:
            self.T_cur = self.T_cur - self.T_i
            self.T_i = self.T_i * self.T_mult
        self.last_epoch = step
        with _enable_get_lr_call(self):
            for i, data in enumerate(zip(self.optimizer.param_groups, self.get_lr())):
                param_group, lr = data
                param_group[&quot;lr&quot;] = math.floor(lr * 1e9) / 1e9
                self.print_lr(self.verbose, i, lr, step)
        self._last_lr = [group[&quot;lr&quot;] for group in self.optimizer.param_groups]
    def print_lr(self, is_verbose, group, lr, epoch=None):
        &quot;&quot;&quot;Display the current learning rate.&quot;&quot;&quot;
        if is_verbose:
            if epoch is None:
                print(
                    &quot;Adjusting learning rate&quot;
                    &quot; of group {} to {:.8e}.&quot;.format(group, lr)
                )
            else:
                epoch_str = (&quot;%.2f&quot; if isinstance(epoch, float) else &quot;%.5d&quot;) % epoch
                print(
                    &quot;Epoch {}: adjusting learning rate&quot;
                    &quot; of group {} to {:.8e}.&quot;.format(epoch_str, group, lr)
                )
class CosineAnnealingHardRestarts(LRScheduler):
    r&quot;&quot;&quot;Set the learning rate of each parameter group using a cosine annealing
    schedule, where :math:`\eta_{max}` is set to the initial lr, :math:`T_{cur}`
    is the number of epochs since the last restart and :math:`T_{i}` is the number
    of epochs between two warm restarts in SGDR:
    .. math::
        \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 +
        \cos\left(\frac{T_{cur}}{T_{i}}\pi\right)\right)
    When :math:`T_{cur}=T_{i}`, set :math:`\eta_t = \eta_{min}`.
    When :math:`T_{cur}=0` after restart, set :math:`\eta_t=\eta_{max}`.
    It has been proposed in
    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.
    Args:
        optimizer (Optimizer): Wrapped optimizer.
        T_0 (int): Number of iterations for the first restart.
        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.
        eta_min (float, optional): Minimum learning rate. Default: 0.
        last_epoch (int, optional): The index of last epoch. Default: -1.
        verbose (bool): If ``True``, prints a message to stdout for
            each update. Default: ``False``.
    .. _SGDR\: Stochastic Gradient Descent with Warm Restarts:
        https://arxiv.org/abs/1608.03983
    &quot;&quot;&quot;
    def __init__(
        self,
        optimizer,
        T_0,
        steps_per_epoch=-1,
        T_mult=1,
        eta_min=0,
        last_step=-1,
        last_epoch=-1,
        verbose=False,
    ):
        if T_0 &lt;= 0 or not isinstance(T_0, int):
            raise ValueError(f&quot;Expected positive integer T_0, but got {T_0}&quot;)
        if T_mult &lt; 1 or not isinstance(T_mult, int):
            raise ValueError(f&quot;Expected integer T_mult &gt;= 1, but got {T_mult}&quot;)
        if last_epoch != -1 and last_step != -1:
            last_epoch = last_step
        elif last_epoch != -1 and last_step == -1:
            last_step = last_epoch
        self.T_0 = T_0
        self.steps_per_epoch = steps_per_epoch
        self.T_i = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.T_cur = last_step
        self.last_step = last_step
        super().__init__(optimizer, last_step, verbose)
    def get_lr(self):
        lrs = [
            self.eta_min
            + (base_lr - self.eta_min)
            * (1 + math.cos(math.pi * self.T_cur / self.T_i))
            / 2
            for base_lr in self.base_lrs
        ]
        return lrs
    def step(self, step=None):
        # Check if the step argument is provided, if not, increment the last_step counter
        if step is None:
            step = self.last_step + 1
        # Calculate T_cur: This represents the current step within the current cycle
        # % operator ensures T_cur is always within the range of the current cycle
        self.T_cur = step % self.steps_per_epoch
        # Check if T_cur has reached the end of the current cycle (T_i)
        # If so, it&apos;s time for a warm restart
        if self.T_cur &gt;= self.T_i:
            self.T_cur = 0  # Reset T_cur to start a new cycle
            self.T_i *= self.T_mult  # Increase the length of the next cycle
        # Update the last step with the current step
        self.last_step = step
        # This context manager ensures that the learning rate is updated correctly
        with _enable_get_lr_call(self):
            # Loop through each parameter group and its corresponding learning rate
            for i, data in enumerate(zip(self.optimizer.param_groups, self.get_lr())):
                param_group, lr = data
                # Update the learning rate for this parameter group
                # We use math.floor to truncate the precision to avoid numerical issues
                param_group[&quot;lr&quot;] = math.floor(lr * 1e9) / 1e9
                # Print the updated learning rate if verbose mode is enabled
                self.print_lr(self.verbose, i, lr, step)
        # Update the last learning rate values for each parameter group
        self._last_lr = [group[&quot;lr&quot;] for group in self.optimizer.param_groups]
    def print_lr(self, is_verbose, group, lr, epoch=None):
        &quot;&quot;&quot;Display the current learning rate.&quot;&quot;&quot;
        if is_verbose:
            if epoch is None:
                print(
                    &quot;Adjusting learning rate&quot;
                    &quot; of group {} to {:.8e}.&quot;.format(group, lr)
                )
            else:
                epoch_str = (&quot;%.2f&quot; if isinstance(epoch, float) else &quot;%.5d&quot;) % epoch
                print(
                    &quot;Epoch {}: adjusting learning rate&quot;
                    &quot; of group {} to {:.8e}.&quot;.format(epoch_str, group, lr)
                )
class Sine(LRScheduler):
    def __init__(
        self, optimizer, T_0, T_mult=1, eta_min=0, last_step=-1, verbose=False
    ):
        if T_0 &lt;= 0 or not isinstance(T_0, int):
            raise ValueError(
                f&quot;Sine learning rate expects positive integer T_0, but got {T_0}&quot;
            )
        if T_mult &lt; 1 or not isinstance(T_mult, int):
            raise ValueError(f&quot;Expected integer T_mult &gt;= 1, but got {T_mult}&quot;)
        self.optimizer = optimizer
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.T_i = T_0
        self.T_cur = last_step
        self.last_epoch = last_step
        self.base_lrs = [group[&quot;lr&quot;] for group in optimizer.param_groups]
        self.verbose = verbose
        self._last_lr = self.base_lrs
        self.total_steps = 0  # Track total steps for a continuous wave
        super().__init__(optimizer, last_step, verbose)
    def get_lr(self):
        # Calculate learning rates using a continuous sine function based on total steps
        lrs = [
            self.eta_min
            + (base_lr - self.eta_min)
            * (0.5 * (1 + math.sin(math.pi * self.total_steps / self.T_0)))
            for base_lr in self.base_lrs
        ]
        return lrs
    def step(self, step=None):
        if step is None:
            step = self.last_epoch + 1
        self.total_steps = step  # Use total steps instead of resetting per interval
        self.last_epoch = step
        for i, (param_group, lr) in enumerate(
            zip(self.optimizer.param_groups, self.get_lr())
        ):
            param_group[&quot;lr&quot;] = math.floor(lr * 1e9) / 1e9
            self.print_lr(self.verbose, i, lr, step)
        self._last_lr = [group[&quot;lr&quot;] for group in self.optimizer.param_groups]
    def print_lr(self, is_verbose, group, lr, epoch=None):
        if is_verbose:
            epoch_str = (&quot;%.2f&quot; if isinstance(epoch, float) else &quot;%.5d&quot;) % epoch
            print(
                f&quot;Epoch {epoch_str}: adjusting learning rate of group {group} to {lr:.8e}.&quot;
            )
from diffusers.optimization import get_scheduler
def get_lr_scheduler(
    args, optimizer, accelerator, logger, use_deepspeed_scheduler=False
):
    if use_deepspeed_scheduler:
        logger.info(&quot;Using DeepSpeed learning rate scheduler&quot;)
        lr_scheduler = accelerate.utils.DummyScheduler(
            optimizer,
            total_num_steps=args.max_train_steps,
            warmup_num_steps=args.lr_warmup_steps,
        )
    elif args.lr_scheduler == &quot;cosine_with_restarts&quot;:
        logger.info(&quot;Using Cosine with Restarts learning rate scheduler.&quot;)
        logger.warning(
            &quot;cosine_with_restarts is currently misbehaving, and may not do what you expect. sine is recommended instead.&quot;
        )
        from helpers.training.custom_schedule import CosineAnnealingHardRestarts
        lr_scheduler = CosineAnnealingHardRestarts(
            optimizer=optimizer,
            T_0=int(args.lr_warmup_steps * accelerator.num_processes),
            T_mult=int(1),
            eta_min=float(args.lr_end),
            last_step=-1,
            verbose=os.environ.get(&quot;SIMPLETUNER_SCHEDULER_VERBOSE&quot;, &quot;false&quot;).lower()
            == &quot;true&quot;,
        )
    elif args.lr_scheduler == &quot;sine&quot;:
        logger.info(&quot;Using Sine learning rate scheduler.&quot;)
        from helpers.training.custom_schedule import Sine
        lr_scheduler = Sine(
            optimizer=optimizer,
            T_0=int(args.lr_warmup_steps * accelerator.num_processes),
            T_mult=int(1),
            eta_min=float(args.lr_end),
            last_step=-1,
            verbose=os.environ.get(&quot;SIMPLETUNER_SCHEDULER_VERBOSE&quot;, &quot;false&quot;).lower()
            == &quot;true&quot;,
        )
    elif args.lr_scheduler == &quot;cosine&quot;:
        logger.info(&quot;Using Cosine learning rate scheduler.&quot;)
        from helpers.training.custom_schedule import Cosine
        lr_scheduler = Cosine(
            optimizer=optimizer,
            T_0=int(args.lr_warmup_steps * accelerator.num_processes),
            T_mult=int(1),
            eta_min=float(args.lr_end),
            last_step=-1,
            verbose=os.environ.get(&quot;SIMPLETUNER_SCHEDULER_VERBOSE&quot;, &quot;false&quot;).lower()
            == &quot;true&quot;,
        )
    elif args.lr_scheduler == &quot;polynomial&quot;:
        logger.info(
            f&quot;Using Polynomial learning rate scheduler with last epoch {StateTracker.get_global_step() - 2}.&quot;
        )
        lr_scheduler = get_polynomial_decay_schedule_with_warmup(
            optimizer=optimizer,
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
            num_training_steps=args.max_train_steps * accelerator.num_processes,
            lr_end=args.lr_end,
            power=args.lr_power,
            last_epoch=StateTracker.get_global_step() - 1,
        )
    else:
        logger.info(f&quot;Using generic &apos;{args.lr_scheduler}&apos; learning rate scheduler.&quot;)
        lr_scheduler = get_scheduler(
            name=args.lr_scheduler,
            optimizer=optimizer,
            num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
            num_training_steps=args.max_train_steps * accelerator.num_processes,
            num_cycles=args.lr_num_cycles,
            power=args.lr_power,
        )
    return lr_scheduler
# from huggingface/diffusers#8449 (author: @leffff)
# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# DISCLAIMER: This code is strongly influenced by https://github.com/leffff/euler-scheduler
from dataclasses import dataclass
from typing import Tuple, Optional, Union
import torch
from diffusers.configuration_utils import ConfigMixin, register_to_config
from diffusers.utils import BaseOutput
from diffusers.schedulers.scheduling_utils import (
    SchedulerMixin,
)
@dataclass
class FlowMatchingEulerSchedulerOutput(BaseOutput):
    &quot;&quot;&quot;
    Output class for the scheduler&apos;s `step` function output.
    Args:
        prev_sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)` for images):
            Computed sample `(x_{t-1})` of previous timestep (which in flow-matching notation should be noted as
            `(x_{t+h})`). `prev_sample` should be used as next model input in the denoising loop.
        pred_original_sample (`torch.Tensor` of shape `(batch_size, num_channels, height, width)` for images):
            The predicted denoised sample `(x_{0})` (which in flow-matching notation should be noted as
            `(x_{1})`) based on the model output from the current timestep.
            `pred_original_sample` can be used to preview progress or for guidance.
    &quot;&quot;&quot;
    prev_sample: torch.Tensor
    pred_original_sample: Optional[torch.Tensor] = None
def get_time_coefficients(timestep: torch.Tensor, ndim: int) -&gt; torch.Tensor:
    return timestep.reshape((timestep.shape[0], *([1] * (ndim - 1))))
class FlowMatchingEulerScheduler(SchedulerMixin, ConfigMixin):
    &quot;&quot;&quot;
    `FlowMatchingEulerScheduler` is a scheduler for training and inferencing Conditional Flow Matching models (CFMs).
    Flow Matching (FM) is a novel, simulation-free methodology for training Continuous Normalizing Flows (CNFs) by
    regressing vector fields of predetermined conditional probability paths, facilitating scalable training and
    efficient sample generation through the utilization of various probability paths, including Gaussian and
    Optimal Transport (OT) paths, thereby enhancing model performance and generalization capabilities
    Args:
        num_inference_steps (`int`, defaults to 100):
            The number of steps on inference.
    &quot;&quot;&quot;
    @register_to_config
    def __init__(self, num_inference_steps: int = 100):
        self.timesteps = None
        self.num_inference_steps = None
        self.h = None
        if num_inference_steps is not None:
            self.set_timesteps(num_inference_steps)
    @staticmethod
    def add_noise(
        original_samples: torch.Tensor, noise: torch.Tensor, timestep: torch.Tensor
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Add noise to the given sample
        Args:
            original_samples (`torch.Tensor`):
                The original sample that is to be noised
            noise (`torch.Tensor`):
                The noise that is used to noise the image
            timestep (`torch.Tensor`):
                Timestep used to create linear interpolation `x_t = t * x_1 + (1 - t) * x_0`.
                Where x_1 is a target distribution, x_0 is a source distribution and t (timestep) ∈ [0, 1]
        &quot;&quot;&quot;
        t = get_time_coefficients(timestep, original_samples.ndim)
        noised_sample = t * original_samples + (1 - t) * noise
        return noised_sample
    def set_timesteps(self, num_inference_steps: int = 100) -&gt; None:
        &quot;&quot;&quot;
        Set number of inference steps (Euler intagration steps)
        Args:
            num_inference_steps (`int`, defaults to 100):
                The number of steps on inference.
        &quot;&quot;&quot;
        self.num_inference_steps = num_inference_steps
        self.h = 1 / num_inference_steps
        self.timesteps = torch.arange(0, 1, self.h)
    def step(
        self,
        model_output: torch.Tensor,
        timestep: torch.Tensor,
        sample: torch.Tensor,
        return_dict: bool = True,
    ) -&gt; Union[FlowMatchingEulerSchedulerOutput, Tuple]:
        &quot;&quot;&quot;
        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion
        process from the learned model outputs (most often the predicted noise).
        Args:
            model_output (`torch.Tensor`):
                The direct output from learned diffusion model.
            timestep (`float`):
                Timestep used to perform Euler Method `x_t = h * f(x_t, t) + x_{t-1}`.
                Where x_1 is a target distribution, x_0 is a source distribution and t (timestep) ∈ [0, 1]
            sample (`torch.Tensor`):
                A current instance of a sample created by the diffusion process.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`.
        Returns:
            [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] or `tuple`:
                If return_dict is `True`, [`~schedulers.scheduling_ddpm.DDPMSchedulerOutput`] is returned, otherwise a
                tuple is returned where the first element is the sample tensor.
        &quot;&quot;&quot;
        step = FlowMatchingEulerSchedulerOutput(
            prev_sample=sample + self.h * model_output,
            pred_original_sample=sample
            + (1 - get_time_coefficients(timestep, model_output.ndim)) * model_output,
        )
        if return_dict:
            return step
        return (step.prev_sample,)
    @staticmethod
    def get_velocity(
        original_samples: torch.Tensor, noise: torch.Tensor
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Predict the sample from the previous timestep by reversing the SDE. This function propagates the diffusion
        process from the learned model outputs (most often the predicted noise).
        Args:
            original_samples (`torch.Tensor`):
                The original sample that is to be noised
            noise (`torch.Tensor`):
                The noise that is used to noise the image
        Returns:
            `torch.Tensor`
        &quot;&quot;&quot;
        return original_samples - noise
    @staticmethod
    def scale_model_input(
        sample: torch.Tensor, timestep: Optional[int] = None
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;
         Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
         current timestep.
        Args:
            sample (`torch.Tensor`):
                The input sample.
            timestep (`int`, *optional*):
                The current timestep in the diffusion chain.
        Returns:
            `torch.Tensor`:
                A scaled input sample.
        &quot;&quot;&quot;
        return sample</file><file path="helpers/training/deepspeed.py">import accelerate, logging, os, contextlib, transformers
from accelerate.state import AcceleratorState
from transformers.integrations import HfDeepSpeedConfig
logger = logging.getLogger(&quot;DeepSpeed&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
from transformers.integrations.deepspeed import (
    is_deepspeed_zero3_enabled,
    set_hf_deepspeed_config,
    unset_hf_deepspeed_config,
)
@contextlib.contextmanager
def temporarily_disable_deepspeed_zero3():
    # https://github.com/huggingface/transformers/issues/28106
    deepspeed_plugin = (
        AcceleratorState().deepspeed_plugin
        if accelerate.state.is_initialized()
        else None
    )
    if deepspeed_plugin is None:
        print(&quot;DeepSpeed was not enabled.&quot;)
        return []
    if deepspeed_plugin and is_deepspeed_zero3_enabled():
        print(&quot;DeepSpeed being disabled.&quot;)
        _hf_deepspeed_config_weak_ref = (
            transformers.integrations.deepspeed._hf_deepspeed_config_weak_ref
        )
        unset_hf_deepspeed_config()
        yield
        print(&quot;DeepSpeed being enabled.&quot;)
        set_hf_deepspeed_config(HfDeepSpeedConfig(deepspeed_plugin.deepspeed_config))
        transformers.integrations.deepspeed._hf_deepspeed_config_weak_ref = (
            _hf_deepspeed_config_weak_ref
        )
    else:
        print(f&quot;Doing nothing, deepspeed zero3 was not enabled?&quot;)
        yield
def deepspeed_zero_init_disabled_context_manager():
    &quot;&quot;&quot;
    returns either a context list that includes one that will disable zero.Init or an empty context list
    &quot;&quot;&quot;
    deepspeed_plugin = (
        AcceleratorState().deepspeed_plugin
        if accelerate.state.is_initialized()
        else None
    )
    if deepspeed_plugin is None:
        logger.debug(&quot;DeepSpeed context manager disabled, no DeepSpeed detected.&quot;)
        return []
    logger.debug(
        f&quot;DeepSpeed context manager enabled, DeepSpeed detected: {deepspeed_plugin}&quot;
    )
    return [
        deepspeed_plugin.zero3_init_context_manager(enable=False),
        temporarily_disable_deepspeed_zero3(),
    ]
def prepare_model_for_deepspeed(accelerator, args):
    use_deepspeed_optimizer = False
    use_deepspeed_scheduler = False
    if (
        hasattr(accelerator, &quot;state&quot;)
        and hasattr(accelerator.state, &quot;deepspeed_plugin&quot;)
        and getattr(accelerator.state, &quot;deepspeed_plugin&quot;) is not None
    ):
        offload_param = accelerator.state.deepspeed_plugin.deepspeed_config[
            &quot;zero_optimization&quot;
        ][&quot;offload_param&quot;]
        accelerator.state.deepspeed_plugin.deepspeed_config[&quot;zero_optimization&quot;][
            &quot;offload_param&quot;
        ][&quot;pin_memory&quot;] = True
        if offload_param[&quot;device&quot;] == &quot;nvme&quot;:
            if offload_param[&quot;nvme_path&quot;] == &quot;none&quot;:
                if args.offload_param_path is None:
                    raise ValueError(
                        f&quot;DeepSpeed is using {offload_param[&apos;device&apos;]} but nvme_path is not specified. The configuration has &apos;{offload_param[&apos;nvme_path&apos;]}&apos; for &apos;nvme_path&apos;.&quot;
                    )
                else:
                    offload_buffer = 100000000.0
                    if args.model_family in [&quot;flux&quot;]:
                        # flux is big
                        offload_buffer = 131600000.0
                    logger.info(
                        f&quot;Attempting to allocate {offload_buffer} size byte buffer.&quot;
                    )
                    accelerator.state.deepspeed_plugin.deepspeed_config[
                        &quot;zero_optimization&quot;
                    ][&quot;offload_param&quot;][&quot;buffer_size&quot;] = offload_buffer
                    accelerator.state.deepspeed_plugin.deepspeed_config[
                        &quot;zero_optimization&quot;
                    ][&quot;offload_param&quot;][&quot;nvme_path&quot;] = args.offload_param_path
            logger.info(
                f&quot;Using DeepSpeed NVMe offload at {accelerator.state.deepspeed_plugin.deepspeed_config[&apos;zero_optimization&apos;][&apos;offload_param&apos;][&apos;nvme_path&apos;]}.&quot;
            )
        use_deepspeed_optimizer = True
        if &quot;optimizer&quot; not in accelerator.state.deepspeed_plugin.deepspeed_config:
            logger.info(&quot;Using DeepSpeed optimizer (AdamW).&quot;)
            accelerator.state.deepspeed_plugin.deepspeed_config[&quot;optimizer&quot;] = {
                &quot;type&quot;: &quot;AdamW&quot;,
                &quot;params&quot;: {
                    &quot;lr&quot;: args.learning_rate,
                    &quot;betas&quot;: [args.adam_beta1, args.adam_beta2],
                    &quot;eps&quot;: args.adam_epsilon,
                    &quot;weight_decay&quot;: args.adam_weight_decay,
                },
            }
        use_deepspeed_scheduler = True
        if &quot;scheduler&quot; not in accelerator.state.deepspeed_plugin.deepspeed_config:
            logger.info(&quot;Using DeepSpeed scheduler (WarmupLR).&quot;)
            accelerator.state.deepspeed_plugin.deepspeed_config[&quot;scheduler&quot;] = {
                &quot;type&quot;: &quot;WarmupLR&quot;,
                &quot;params&quot;: {
                    &quot;warmup_min_lr&quot;: 0,
                    &quot;warmup_max_lr&quot;: args.learning_rate,
                    &quot;warmup_num_steps&quot;: args.lr_warmup_steps,
                },
            }
    return use_deepspeed_optimizer, use_deepspeed_scheduler</file><file path="helpers/training/diffusion_model.py">import os
from accelerate.logging import get_logger
from helpers.models import get_model_config_path
logger = get_logger(__name__, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
def determine_subfolder(folder_value: str = None):
    if folder_value is None or str(folder_value).lower() == &quot;none&quot;:
        return None
    return str(folder_value)
def load_diffusion_model(args, weight_dtype):
    pretrained_load_args = {
        &quot;revision&quot;: args.revision,
        &quot;variant&quot;: args.variant,
        &quot;torch_dtype&quot;: weight_dtype,
        &quot;use_safetensors&quot;: True,
    }
    unet = None
    transformer = None
    pretrained_transformer_path = (
        args.pretrained_transformer_model_name_or_path
        or args.pretrained_model_name_or_path
    )
    if &quot;nf4-bnb&quot; == args.base_model_precision:
        import torch
        from diffusers import BitsAndBytesConfig
        pretrained_load_args[&quot;quantization_config&quot;] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type=&quot;nf4&quot;,
            bnb_4bit_compute_dtype=weight_dtype,
        )
    if args.model_family == &quot;sd3&quot;:
        # Stable Diffusion 3 uses a Diffusion transformer.
        logger.info(&quot;Loading Stable Diffusion 3 diffusion transformer..&quot;)
        try:
            from helpers.models.sd3.transformer import SD3Transformer2DModel
        except Exception as e:
            logger.error(
                f&quot;Can not load SD3 model class. This release requires the latest version of Diffusers: {e}&quot;
            )
        transformer = SD3Transformer2DModel.from_pretrained(
            args.pretrained_transformer_model_name_or_path
            or args.pretrained_model_name_or_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
    elif args.model_family == &quot;ltxvideo&quot;:
        # LTXVideo uses a Diffusion transformer.
        logger.info(&quot;Loading LTX Video diffusion transformer..&quot;)
        try:
            from diffusers import LTXVideoTransformer3DModel
        except Exception as e:
            logger.error(
                f&quot;Can not load LTXVideoTransformer3DModel model class. This release requires the latest version of Diffusers: {e}&quot;
            )
        transformer = LTXVideoTransformer3DModel.from_pretrained(
            args.pretrained_transformer_model_name_or_path
            or args.pretrained_model_name_or_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
    elif (
        args.model_family.lower() == &quot;flux&quot; and not args.flux_attention_masked_training
    ):
        from helpers.models.flux.transformer import (
            FluxTransformer2DModelWithMasking as FluxTransformer2DModel,
        )
        import torch
        if torch.cuda.is_available():
            rank = (
                torch.distributed.get_rank()
                if torch.distributed.is_initialized()
                else 0
            )
            primary_device = torch.cuda.get_device_properties(0)
            if primary_device.major &gt;= 9:
                try:
                    from flash_attn_interface import flash_attn_func
                    import diffusers
                    from helpers.models.flux.attention import (
                        FluxAttnProcessor3_0,
                        FluxSingleAttnProcessor3_0,
                    )
                    diffusers.models.attention_processor.FluxSingleAttnProcessor2_0 = (
                        FluxSingleAttnProcessor3_0
                    )
                    diffusers.models.attention_processor.FluxAttnProcessor2_0 = (
                        FluxAttnProcessor3_0
                    )
                    if rank == 0:
                        print(&quot;Using FlashAttention3_0 for H100 GPU (Single block)&quot;)
                except:
                    if rank == 0:
                        logger.warning(
                            &quot;No flash_attn is available, using slower FlashAttention_2_0. Install flash_attn to make use of FA3 for Hopper or newer arch.&quot;
                        )
        transformer_load_fn = FluxTransformer2DModel.from_pretrained
        if pretrained_transformer_path.lower().endswith(&quot;.safetensors&quot;):
            transformer_load_fn = FluxTransformer2DModel.from_single_file
        transformer = transformer_load_fn(
            pretrained_transformer_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
    elif args.model_family.lower() == &quot;flux&quot; and args.flux_attention_masked_training:
        from helpers.models.flux.transformer import (
            FluxTransformer2DModelWithMasking,
        )
        transformer_load_fn = FluxTransformer2DModelWithMasking.from_pretrained
        if pretrained_transformer_path.lower().endswith(&quot;.safetensors&quot;):
            transformer_load_fn = FluxTransformer2DModelWithMasking.from_single_file
        transformer = transformer_load_fn(
            pretrained_transformer_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
        if args.gradient_checkpointing_interval is not None:
            transformer.set_gradient_checkpointing_interval(
                int(args.gradient_checkpointing_interval)
            )
    elif args.model_family == &quot;pixart_sigma&quot;:
        from diffusers.models import PixArtTransformer2DModel
        transformer_load_fn = PixArtTransformer2DModel.from_pretrained
        if pretrained_transformer_path.lower().endswith(&quot;.safetensors&quot;):
            # transformer_load_fn = PixArtTransformer2DModel.from_single_file
            raise ValueError(&quot;PixArt does not support single file loading.&quot;)
        transformer = transformer_load_fn(
            pretrained_transformer_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
    elif args.model_family == &quot;smoldit&quot;:
        logger.info(&quot;Loading SmolDiT model..&quot;)
        if args.validation_noise_scheduler is None:
            args.validation_noise_scheduler = &quot;ddpm&quot;
        transformer_variant = None
        from helpers.models.smoldit import SmolDiT2DModel, SmolDiTConfigurations
        if args.smoldit_config not in SmolDiTConfigurations:
            raise ValueError(
                f&quot;Invalid SmolDiT size configuration: {args.smoldit_config}&quot;
            )
        transformer = SmolDiT2DModel(**SmolDiTConfigurations[args.smoldit_config])
        if &quot;lora&quot; in args.model_type:
            raise ValueError(&quot;SmolDiT does not yet support LoRA training.&quot;)
    elif args.model_family == &quot;sana&quot;:
        from helpers.models.sana.transformer import SanaTransformer2DModel
        logger.info(&quot;Loading Sana flow-matching diffusion transformer..&quot;)
        transformer_load_fn = SanaTransformer2DModel.from_pretrained
        if pretrained_transformer_path.lower().endswith(&quot;.safetensors&quot;):
            # transformer_load_fn = SanaTransformer2DModel.from_single_file
            raise ValueError(&quot;Sana does not support single file loading.&quot;)
        transformer = transformer_load_fn(
            pretrained_transformer_path,
            subfolder=determine_subfolder(args.pretrained_transformer_subfolder),
            **pretrained_load_args,
        )
    else:
        from diffusers import UNet2DConditionModel
        logger.info(&quot;Loading U-net..&quot;)
        unet_variant = args.variant
        if (
            args.model_family == &quot;kolors&quot;
            and args.pretrained_model_name_or_path.lower()
            == &quot;kwai-kolors/kolors-diffusers&quot;
        ):
            unet_variant = &quot;fp16&quot;
        pretrained_load_args[&quot;variant&quot;] = unet_variant
        unet_load_fn = UNet2DConditionModel.from_pretrained
        pretrained_unet_path = (
            args.pretrained_unet_model_name_or_path
            or args.pretrained_model_name_or_path
        )
        if pretrained_unet_path.lower().endswith(&quot;.safetensors&quot;):
            unet_load_fn = UNet2DConditionModel.from_single_file
        unet = unet_load_fn(
            pretrained_unet_path,
            subfolder=determine_subfolder(args.pretrained_unet_subfolder),
            **pretrained_load_args,
        )
        if (
            args.gradient_checkpointing_interval is not None
            and args.gradient_checkpointing_interval &gt; 0
        ):
            logger.warning(
                &quot;Using experimental gradient checkpointing monkeypatch for a checkpoint interval of {}&quot;.format(
                    args.gradient_checkpointing_interval
                )
            )
            # monkey-patch the gradient checkpointing function for pytorch to run every nth call only.
            # definitely one of the more awful things I&apos;ve ever done while programming, but it&apos;s easier than
            # modifying every one of the unet blocks&apos; forward calls in Diffusers to make it work properly.
            from helpers.training.gradient_checkpointing_interval import (
                set_checkpoint_interval,
            )
            set_checkpoint_interval(int(args.gradient_checkpointing_interval))
    if (
        args.gradient_checkpointing_interval is not None
        and args.gradient_checkpointing_interval &gt; 1
    ):
        if transformer is not None and hasattr(
            transformer, &quot;set_gradient_checkpointing_interval&quot;
        ):
            logger.info(&quot;Setting gradient checkpointing interval for transformer..&quot;)
            transformer.set_gradient_checkpointing_interval(
                int(args.gradient_checkpointing_interval)
            )
        if unet is not None and hasattr(unet, &quot;set_gradient_checkpointing_interval&quot;):
            logger.info(&quot;Checking gradient checkpointing interval for U-Net..&quot;)
            unet.set_gradient_checkpointing_interval(
                int(args.gradient_checkpointing_interval)
            )
    if args.pretrained_model_name_or_path.endswith(&quot;.safetensors&quot;):
        args.pretrained_model_name_or_path = get_model_config_path(
            args.model_family, args.pretrained_model_name_or_path
        )
    return unet, transformer</file><file path="helpers/training/ema.py">import torch
import copy
import logging
import os
import contextlib
import transformers
from typing import Any, Dict, Iterable, Optional, Union
from diffusers.utils.deprecation_utils import deprecate
from diffusers.utils import is_transformers_available
from helpers.training.state_tracker import StateTracker
logger = logging.getLogger(&quot;EMAModel&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;WARNING&quot;))
def should_update_ema(args, step):
    if args.ema_update_interval is None:
        # If the EMA update interval is not set, always update the EMA.
        return True
    else:
        should_update = step % args.ema_update_interval == 0
        if should_update:
            logger.debug(&quot;Updating EMA weights...&quot;)
        return should_update
class EMAModel:
    &quot;&quot;&quot;
    Exponential Moving Average of models weights
    &quot;&quot;&quot;
    def __init__(
        self,
        args,
        accelerator,
        parameters: Iterable[torch.nn.Parameter],
        decay: float = 0.9999,
        min_decay: float = 0.0,
        update_after_step: int = 0,
        use_ema_warmup: bool = False,
        inv_gamma: Union[float, int] = 1.0,
        power: Union[float, int] = 2 / 3,
        foreach: bool = True,
        model_cls: Optional[Any] = None,
        model_config: Dict[str, Any] = None,
        **kwargs,
    ):
        &quot;&quot;&quot;
        Args:
            parameters (Iterable[torch.nn.Parameter]): The parameters to track.
            decay (float): The decay factor for the exponential moving average.
            min_decay (float): The minimum decay factor for the exponential moving average.
            update_after_step (int): The number of steps to wait before starting to update the EMA weights.
            use_ema_warmup (bool): Whether to use EMA warmup.
            inv_gamma (float):
                Inverse multiplicative factor of EMA warmup. Default: 1. Only used if `use_ema_warmup` is True.
            power (float): Exponential factor of EMA warmup. Default: 2/3. Only used if `use_ema_warmup` is True.
            foreach (bool): Use torch._foreach functions for updating shadow parameters. Should be faster.
            device (Optional[Union[str, torch.device]]): The device to store the EMA weights on. If None, the EMA
                        weights will be stored on CPU.
        @crowsonkb&apos;s notes on EMA Warmup:
            If gamma=1 and power=1, implements a simple average. gamma=1, power=2/3 are good values for models you plan
            to train for a million or more steps (reaches decay factor 0.999 at 31.6K steps, 0.9999 at 1M steps),
            gamma=1, power=3/4 for models you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999
            at 215.4k steps).
        &quot;&quot;&quot;
        if isinstance(parameters, torch.nn.Module):
            deprecation_message = (
                &quot;Passing a `torch.nn.Module` to `ExponentialMovingAverage` is deprecated. &quot;
                &quot;Please pass the parameters of the module instead.&quot;
            )
            deprecate(
                &quot;passing a `torch.nn.Module` to `ExponentialMovingAverage`&quot;,
                &quot;1.0.0&quot;,
                deprecation_message,
                standard_warn=False,
            )
            parameters = parameters.parameters()
            # set use_ema_warmup to True if a torch.nn.Module is passed for backwards compatibility
            use_ema_warmup = True
        if kwargs.get(&quot;max_value&quot;, None) is not None:
            deprecation_message = (
                &quot;The `max_value` argument is deprecated. Please use `decay` instead.&quot;
            )
            deprecate(&quot;max_value&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
            decay = kwargs[&quot;max_value&quot;]
        if kwargs.get(&quot;min_value&quot;, None) is not None:
            deprecation_message = &quot;The `min_value` argument is deprecated. Please use `min_decay` instead.&quot;
            deprecate(&quot;min_value&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
            min_decay = kwargs[&quot;min_value&quot;]
        parameters = list(parameters)
        self.shadow_params = [p.clone().detach() for p in parameters]
        if kwargs.get(&quot;device&quot;, None) is not None:
            deprecation_message = (
                &quot;The `device` argument is deprecated. Please use `to` instead.&quot;
            )
            deprecate(&quot;device&quot;, &quot;1.0.0&quot;, deprecation_message, standard_warn=False)
            self.to(device=kwargs[&quot;device&quot;])
        self.temp_stored_params = None
        self.decay = decay
        self.min_decay = min_decay
        self.update_after_step = update_after_step
        self.use_ema_warmup = use_ema_warmup
        self.inv_gamma = inv_gamma
        self.power = power
        self.optimization_step = 0
        self.cur_decay_value = None  # set in `step()`
        self.foreach = foreach
        self.model_cls = model_cls
        self.model_config = model_config
        self.args = args
        self.accelerator = accelerator
        self.training = True  # To emulate nn.Module&apos;s training mode
    def save_state_dict(self, path: str) -&gt; None:
        &quot;&quot;&quot;
        Save the EMA model&apos;s state directly to a file.
        Args:
            path (str): The file path where the EMA state will be saved.
        &quot;&quot;&quot;
        # if the folder containing the path does not exist, create it
        os.makedirs(os.path.dirname(path), exist_ok=True)
        # grab state dict
        state_dict = self.state_dict()
        # save it using torch.save
        torch.save(state_dict, path)
        logger.info(f&quot;EMA model state saved to {path}&quot;)
    def load_state_dict(self, path: str) -&gt; None:
        &quot;&quot;&quot;
        Load the EMA model&apos;s state from a file and apply it to this instance.
        Args:
            path (str): The file path from where the EMA state will be loaded.
        &quot;&quot;&quot;
        state_dict = torch.load(path, map_location=&quot;cpu&quot;, weights_only=True)
        # Load metadata
        self.decay = state_dict.get(&quot;decay&quot;, self.decay)
        self.min_decay = state_dict.get(&quot;min_decay&quot;, self.min_decay)
        self.optimization_step = state_dict.get(
            &quot;optimization_step&quot;, self.optimization_step
        )
        self.update_after_step = state_dict.get(
            &quot;update_after_step&quot;, self.update_after_step
        )
        self.use_ema_warmup = state_dict.get(&quot;use_ema_warmup&quot;, self.use_ema_warmup)
        self.inv_gamma = state_dict.get(&quot;inv_gamma&quot;, self.inv_gamma)
        self.power = state_dict.get(&quot;power&quot;, self.power)
        # Load shadow parameters
        shadow_params = []
        idx = 0
        while f&quot;shadow_params.{idx}&quot; in state_dict:
            shadow_params.append(state_dict[f&quot;shadow_params.{idx}&quot;])
            idx += 1
        if len(shadow_params) != len(self.shadow_params):
            raise ValueError(
                f&quot;Mismatch in number of shadow parameters: expected {len(self.shadow_params)}, &quot;
                f&quot;but found {len(shadow_params)} in the state dict.&quot;
            )
        for current_param, loaded_param in zip(self.shadow_params, shadow_params):
            current_param.data.copy_(loaded_param.data)
        logger.info(f&quot;EMA model state loaded from {path}&quot;)
    @classmethod
    def from_pretrained(cls, path, model_cls) -&gt; &quot;EMAModel&quot;:
        _, ema_kwargs = model_cls.load_config(path, return_unused_kwargs=True)
        model = model_cls.from_pretrained(path)
        ema_model = cls(
            model.parameters(), model_cls=model_cls, model_config=model.config
        )
        ema_model.load_state_dict(ema_kwargs)
        return ema_model
    def save_pretrained(self, path, max_shard_size: str = &quot;10GB&quot;):
        if self.model_cls is None:
            raise ValueError(
                &quot;`save_pretrained` can only be used if `model_cls` was defined at __init__.&quot;
            )
        if self.model_config is None:
            raise ValueError(
                &quot;`save_pretrained` can only be used if `model_config` was defined at __init__.&quot;
            )
        model = self.model_cls.from_config(self.model_config)
        state_dict = self.state_dict(exclude_params=True)
        state_dict.pop(&quot;shadow_params&quot;, None)
        model.register_to_config(**state_dict)
        self.copy_to(model.parameters())
        model.save_pretrained(path, max_shard_size=max_shard_size)
    def get_decay(self, optimization_step: int = None) -&gt; float:
        &quot;&quot;&quot;
        Compute the decay factor for the exponential moving average.
        &quot;&quot;&quot;
        if optimization_step is None:
            optimization_step = self.optimization_step
        step = max(0, optimization_step - self.update_after_step - 1)
        if step &lt;= 0:
            return 0.0
        if self.use_ema_warmup:
            cur_decay_value = 1 - (1 + step / self.inv_gamma) ** -self.power
        else:
            cur_decay_value = (1 + step) / (10 + step)
        cur_decay_value = min(cur_decay_value, self.decay)
        # make sure decay is not smaller than min_decay
        cur_decay_value = max(cur_decay_value, self.min_decay)
        return cur_decay_value
    @torch.no_grad()
    def step(self, parameters: Iterable[torch.nn.Parameter], global_step: int = None):
        if not should_update_ema(self.args, global_step):
            return
        if self.args.ema_device == &quot;cpu&quot; and not self.args.ema_cpu_only:
            # Move EMA to accelerator for faster update.
            self.to(device=self.accelerator.device, non_blocking=True)
        if isinstance(parameters, torch.nn.Module):
            deprecation_message = (
                &quot;Passing a `torch.nn.Module` to `ExponentialMovingAverage.step` is deprecated. &quot;
                &quot;Please pass the parameters of the module instead.&quot;
            )
            deprecate(
                &quot;passing a `torch.nn.Module` to `ExponentialMovingAverage.step`&quot;,
                &quot;1.0.0&quot;,
                deprecation_message,
                standard_warn=False,
            )
            parameters = parameters.parameters()
        parameters = list(parameters)
        if global_step is not None:
            # When we&apos;re updating the EMA periodically, we can&apos;t trust the counter.
            self.optimization_step = global_step
        else:
            self.optimization_step += 1
        # Compute the decay factor for the exponential moving average.
        decay = self.get_decay(self.optimization_step)
        self.cur_decay_value = decay
        one_minus_decay = 1 - decay
        context_manager = contextlib.nullcontext
        if (
            is_transformers_available()
            and transformers.integrations.deepspeed.is_deepspeed_zero3_enabled()
        ):
            import deepspeed
        if self.foreach:
            if (
                is_transformers_available()
                and transformers.deepspeed.is_deepspeed_zero3_enabled()
            ):
                context_manager = deepspeed.zero.GatheredParameters(
                    parameters, modifier_rank=None
                )
            with context_manager():
                params_grad = [param for param in parameters if param.requires_grad]
                s_params_grad = [
                    s_param
                    for s_param, param in zip(self.shadow_params, parameters)
                    if param.requires_grad
                ]
                if len(params_grad) &lt; len(parameters):
                    torch._foreach_copy_(
                        [
                            s_param
                            for s_param, param in zip(self.shadow_params, parameters)
                            if not param.requires_grad
                        ],
                        [param for param in parameters if not param.requires_grad],
                        non_blocking=True,
                    )
                torch._foreach_sub_(
                    s_params_grad,
                    torch._foreach_sub(s_params_grad, params_grad),
                    alpha=one_minus_decay,
                )
        else:
            for s_param, param in zip(self.shadow_params, parameters):
                if (
                    is_transformers_available()
                    and transformers.integrations.deepspeed.is_deepspeed_zero3_enabled()
                ):
                    context_manager = deepspeed.zero.GatheredParameters(
                        param, modifier_rank=None
                    )
                with context_manager():
                    if param.requires_grad:
                        s_param.sub_(
                            one_minus_decay * (s_param - param.to(s_param.device))
                        )
                    else:
                        s_param.copy_(param)
        if self.args.ema_device == &quot;cpu&quot; and not self.args.ema_cpu_only:
            # Move back to CPU for safe-keeping.
            self.to(device=self.args.ema_device, non_blocking=True)
    def copy_to(self, parameters: Iterable[torch.nn.Parameter]) -&gt; None:
        &quot;&quot;&quot;
        Copy current averaged parameters into given collection of parameters.
        Args:
            parameters: Iterable of `torch.nn.Parameter`; the parameters to be
                updated with the stored moving averages. If `None`, the parameters with which this
                `ExponentialMovingAverage` was initialized will be used.
        &quot;&quot;&quot;
        parameters = list(parameters)
        if self.foreach:
            torch._foreach_copy_(
                [param.data for param in parameters],
                [
                    s_param.to(param.device).data
                    for s_param, param in zip(self.shadow_params, parameters)
                ],
            )
        else:
            for s_param, param in zip(self.shadow_params, parameters):
                param.data.copy_(s_param.to(param.device).data)
    def pin_memory(self) -&gt; None:
        r&quot;&quot;&quot;
        Move internal buffers of the ExponentialMovingAverage to pinned memory. Useful for non-blocking transfers for
        offloading EMA params to the host.
        &quot;&quot;&quot;
        if torch.backends.mps.is_available():
            logger.warning(&quot;Apple silicon does not support pinned memory. Skipping.&quot;)
            return
        if self.args.ema_cpu_only:
            return
        # This probably won&apos;t work, but we&apos;ll do it anyway.
        self.shadow_params = [p.pin_memory() for p in self.shadow_params]
    def to(self, *args, **kwargs):
        for param in self.shadow_params:
            param.data = param.data.to(*args, **kwargs)
        return self
    def cuda(self, device=None):
        return self.to(device=&quot;cuda&quot; if device is None else f&quot;cuda:{device}&quot;)
    def cpu(self):
        return self.to(device=&quot;cpu&quot;)
    def state_dict(
        self, destination=None, prefix=&quot;&quot;, keep_vars=False, exclude_params: bool = False
    ):
        r&quot;&quot;&quot;
        Returns a dictionary containing a whole state of the EMA model.
        &quot;&quot;&quot;
        state_dict = {
            &quot;decay&quot;: self.decay,
            &quot;min_decay&quot;: self.min_decay,
            &quot;optimization_step&quot;: self.optimization_step,
            &quot;update_after_step&quot;: self.update_after_step,
            &quot;use_ema_warmup&quot;: self.use_ema_warmup,
            &quot;inv_gamma&quot;: self.inv_gamma,
            &quot;power&quot;: self.power,
        }
        if exclude_params:
            return state_dict
        for idx, param in enumerate(self.shadow_params):
            state_dict[f&quot;{prefix}shadow_params.{idx}&quot;] = (
                param if keep_vars else param.detach()
            )
        return state_dict
    def store(self, parameters: Iterable[torch.nn.Parameter]) -&gt; None:
        r&quot;&quot;&quot;
        Save the current parameters for restoring later.
        &quot;&quot;&quot;
        self.temp_stored_params = [param.detach().cpu().clone() for param in parameters]
    def restore(self, parameters: Iterable[torch.nn.Parameter]) -&gt; None:
        r&quot;&quot;&quot;
        Restore the parameters stored with the `store` method.
        &quot;&quot;&quot;
        if self.temp_stored_params is None:
            raise RuntimeError(
                &quot;This ExponentialMovingAverage has no `store()`ed weights &quot;
                &quot;to `restore()`&quot;
            )
        if self.foreach:
            torch._foreach_copy_(
                [param.data for param in parameters],
                [c_param.data for c_param in self.temp_stored_params],
            )
        else:
            for c_param, param in zip(self.temp_stored_params, parameters):
                param.data.copy_(c_param.data)
        # Better memory-wise.
        self.temp_stored_params = None
    def parameter_count(self) -&gt; int:
        return sum(p.numel() for p in self.shadow_params)
    # Implementing nn.Module methods to emulate its behavior
    def named_children(self):
        # No child modules
        return iter([])
    def children(self):
        return iter([])
    def modules(self):
        yield self
    def named_modules(self, memo=None, prefix=&quot;&quot;):
        yield prefix, self
    def parameters(self, recurse=True):
        return iter(self.shadow_params)
    def named_parameters(self, prefix=&quot;&quot;, recurse=True):
        for i, param in enumerate(self.shadow_params):
            name = f&quot;{prefix}shadow_params.{i}&quot;
            yield name, param
    def buffers(self, recurse=True):
        return iter([])
    def named_buffers(self, prefix=&quot;&quot;, recurse=True):
        return iter([])
    def train(self, mode=True):
        self.training = mode
        return self
    def eval(self):
        return self.train(False)
    def zero_grad(self):
        # No gradients to zero in EMA model
        pass</file><file path="helpers/training/error_handling.py">import os
import sys
from accelerate.logging import get_logger
logger = get_logger(__name__, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
def validate_deepspeed_compat_from_args(accelerator, args):
    if &quot;lora&quot; in args.model_type:
        logger.error(
            &quot;LoRA can not be trained with DeepSpeed. Please disable DeepSpeed via &apos;accelerate config&apos; before reattempting.&quot;
        )
        sys.exit(1)
    if (
        &quot;gradient_accumulation_steps&quot;
        in accelerator.state.deepspeed_plugin.deepspeed_config
    ):
        args.gradient_accumulation_steps = (
            accelerator.state.deepspeed_plugin.deepspeed_config[
                &quot;gradient_accumulation_steps&quot;
            ]
        )
        logger.info(
            f&quot;Updated gradient_accumulation_steps to the value provided by DeepSpeed: {args.gradient_accumulation_steps}&quot;
        )</file><file path="helpers/training/evaluation.py">from functools import partial
from torchmetrics.functional.multimodal import clip_score
from torchvision import transforms
import torch, logging, os
import numpy as np
from PIL import Image
from helpers.training.state_tracker import StateTracker
logger = logging.getLogger(&quot;ModelEvaluator&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
model_evaluator_map = {
    &quot;clip&quot;: &quot;CLIPModelEvaluator&quot;,
}
class ModelEvaluator:
    def __init__(self, pretrained_model_name_or_path):
        raise NotImplementedError(
            &quot;Subclasses is incomplete, no __init__ method was found.&quot;
        )
    def evaluate(self, images, prompts):
        raise NotImplementedError(&quot;Subclasses should implement the evaluate() method.&quot;)
    @staticmethod
    def from_config(args):
        &quot;&quot;&quot;Instantiate a ModelEvaluator from the training config, if set to do so.&quot;&quot;&quot;
        if not StateTracker.get_accelerator().is_main_process:
            return None
        if (
            args.evaluation_type is not None
            and args.evaluation_type.lower() != &quot;&quot;
            and args.evaluation_type.lower() != &quot;none&quot;
        ):
            model_evaluator = model_evaluator_map[args.evaluation_type]
            return globals()[model_evaluator](
                args.pretrained_evaluation_model_name_or_path
            )
        return None
class CLIPModelEvaluator(ModelEvaluator):
    def __init__(
        self, pretrained_model_name_or_path=&quot;openai/clip-vit-large-patch14-336&quot;
    ):
        self.clip_score_fn = partial(
            clip_score, model_name_or_path=pretrained_model_name_or_path
        )
        self.preprocess = transforms.Compose([transforms.ToTensor()])
    def evaluate(self, images, prompts):
        # Preprocess images
        images_tensor = torch.stack([self.preprocess(img) * 255 for img in images])
        # Compute CLIP scores
        result = self.clip_score_fn(images_tensor, prompts).detach().cpu()
        return result</file><file path="helpers/training/exceptions.py">class MultiDatasetExhausted(Exception):
    pass</file><file path="helpers/training/gradient_checkpointing_interval.py">import torch
from torch.utils.checkpoint import checkpoint as original_checkpoint
# Global variables to keep track of the checkpointing state
_checkpoint_call_count = 0
_checkpoint_interval = 4  # You can set this to any interval you prefer
def reset_checkpoint_counter():
    &quot;&quot;&quot;Resets the checkpoint call counter. Call this at the beginning of the forward pass.&quot;&quot;&quot;
    global _checkpoint_call_count
    _checkpoint_call_count = 0
def set_checkpoint_interval(n):
    &quot;&quot;&quot;Sets the interval at which checkpointing is skipped.&quot;&quot;&quot;
    global _checkpoint_interval
    _checkpoint_interval = n
def checkpoint_wrapper(function, *args, use_reentrant=True, **kwargs):
    &quot;&quot;&quot;Wrapper function for torch.utils.checkpoint.checkpoint.&quot;&quot;&quot;
    global _checkpoint_call_count, _checkpoint_interval
    _checkpoint_call_count += 1
    if (
        _checkpoint_interval &gt; 0
        and (_checkpoint_call_count % _checkpoint_interval) == 0
    ):
        # Use the original checkpoint function
        return original_checkpoint(
            function, *args, use_reentrant=use_reentrant, **kwargs
        )
    else:
        # Skip checkpointing: execute the function directly
        # Do not pass &apos;use_reentrant&apos; to the function
        return function(*args, **kwargs)
# Monkeypatch torch.utils.checkpoint.checkpoint
torch.utils.checkpoint.checkpoint = checkpoint_wrapper</file><file path="helpers/training/min_snr_gamma.py"># From Diffusers repository: examples/research_projects/onnxruntime/text_to_image/train_text_to_image.py
def compute_snr(timesteps, noise_scheduler, use_soft_min: bool = False, sigma_data=1.0):
    &quot;&quot;&quot;
    Computes SNR using two different methods based on the `use_soft_min` flag.
    Args:
        timesteps (torch.Tensor): The timesteps at which SNR is computed.
        noise_scheduler (NoiseScheduler): An object that contains the alpha_cumprod values.
        use_soft_min (bool): If True, use the _weighting_soft_min_snr method to compute SNR.
        sigma_data (torch.Tensor or None): The standard deviation of the data used in the soft min weighting method.
    Returns:
        torch.Tensor: The computed SNR values.
    &quot;&quot;&quot;
    alphas_cumprod = noise_scheduler.alphas_cumprod
    sqrt_alphas_cumprod = alphas_cumprod**0.5
    sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5
    # Expand the tensors.
    sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device=timesteps.device)[
        timesteps
    ].float()
    while len(sqrt_alphas_cumprod.shape) &lt; len(timesteps.shape):
        sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]
    alpha = sqrt_alphas_cumprod.expand(timesteps.shape)
    sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(
        device=timesteps.device
    )[timesteps].float()
    while len(sqrt_one_minus_alphas_cumprod.shape) &lt; len(timesteps.shape):
        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]
    sigma = sqrt_one_minus_alphas_cumprod.expand(timesteps.shape)
    # Choose the method to compute SNR
    if use_soft_min:
        if sigma_data is None:
            raise ValueError(
                &quot;sigma_data must be provided when using soft min SNR calculation.&quot;
            )
        snr = (sigma * sigma_data) ** 2 / (sigma**2 + sigma_data**2) ** 2
    else:
        # Default SNR computation
        snr = (alpha / sigma) ** 2
    return snr</file><file path="helpers/training/model_freeze.py">import logging
import os, re
from torch import nn
logger = logging.getLogger(&quot;ModelFreeze&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
def freeze_transformer_blocks(
    model: nn.Module,
    target_blocks: str,
    first_unfrozen_dit_layer: int = 0,
    first_unfrozen_mmdit_layer: int = 0,
    freeze_direction: str = &quot;up&quot;,
    use_bitfit: bool = False,
):
    if target_blocks not in [&quot;any&quot;, &quot;dit&quot;, &quot;mmdit&quot;]:
        raise ValueError(
            f&quot;Invalid target_blocks value {target_blocks}. Choose from &apos;any&apos;, &apos;dit&apos;, &apos;mmdit&apos;.&quot;
        )
    if freeze_direction not in [&quot;up&quot;, &quot;down&quot;]:
        raise ValueError(
            f&quot;Invalid freeze_direction value {freeze_direction}. Choose from &apos;up&apos;, &apos;down&apos;.&quot;
        )
    if first_unfrozen_dit_layer &lt; 0 or first_unfrozen_mmdit_layer &lt; 0:
        raise ValueError(f&quot;Invalid first_unfrozen layer value. Must be greater than 0.&quot;)
    for name, param in model.named_parameters():
        # Example names:
        #  single_transformer_blocks.31.ff.c_proj.weight
        #  joint_transformer_blocks.1.ff.c_proj.weight
        try:
            layer_group = name.split(&quot;.&quot;)[0]
            layer_number = int(name.split(&quot;.&quot;)[1])
        except Exception as e:
            # non-numeric layer.
            continue
        try:
            if hasattr(param, &quot;requires_grad&quot;):
                # freeze by default.
                param.requires_grad = False
            else:
                continue
            if target_blocks != &quot;any&quot;:
                # We will exclude entire categories of blocks here if they aren&apos;t defined to be trained.
                if (
                    target_blocks == &quot;dit&quot;
                    and layer_group != &quot;single_transformer_blocks&quot;
                ):
                    continue
                if (
                    target_blocks == &quot;mmdit&quot;
                    and layer_group != &quot;joint_transformer_blocks&quot;
                ):
                    continue
            should_train = False
            if first_unfrozen_dit_layer is not None:
                if layer_group == &quot;single_transformer_blocks&quot; or target_blocks == &quot;any&quot;:
                    if first_unfrozen_dit_layer == 0:
                        should_train = True
                    if (
                        freeze_direction == &quot;up&quot;
                        and layer_number &lt; first_unfrozen_dit_layer
                    ) or (
                        freeze_direction == &quot;down&quot;
                        and layer_number &gt; first_unfrozen_dit_layer
                    ):
                        should_train = True
            if first_unfrozen_mmdit_layer is not None:
                if layer_group == &quot;joint_transformer_blocks&quot; or target_blocks == &quot;any&quot;:
                    if first_unfrozen_mmdit_layer == 0:
                        should_train = True
                    if (
                        freeze_direction == &quot;up&quot;
                        and layer_number &lt; first_unfrozen_mmdit_layer
                    ) or (
                        freeze_direction == &quot;down&quot;
                        and layer_number &gt; first_unfrozen_mmdit_layer
                    ):
                        should_train = True
            if should_train:
                param.requires_grad = True
                logger.debug(f&quot;Unfreezing {name}.&quot;)
        except Exception as e:
            logger.error(e)
            raise e
    return model
def apply_bitfit_freezing(model, args):
    model_type = args.model_type
    if &quot;lora&quot; in model_type:
        # LoRAs don&apos;t have bias and arrive pre-frozen on the bottom.
        return model
    logger.debug(&quot;Applying BitFit freezing strategy for u-net tuning.&quot;)
    for name, param in model.named_parameters():
        if not hasattr(param, &quot;requires_grad&quot;):
            logger.debug(
                f&quot;Skipping {name} as it does not have &apos;requires_grad&apos; attribute.&quot;
            )
            continue
        # Freeze everything that&apos;s not a bias
        if &quot;bias&quot; not in name:
            param.requires_grad = False
        else:
            # Unfreeze biases
            param.requires_grad = True
    return model
def freeze_entire_component(component):
    for name, param in component.named_parameters():
        if hasattr(param, &quot;requires_grad&quot;):
            param.requires_grad = False
    return component
def freeze_text_encoder(args, component):
    from transformers import T5EncoderModel
    if (
        not args.train_text_encoder
        or not args.freeze_encoder
        or type(component) is T5EncoderModel
    ):
        if args.train_text_encoder:
            logger.info(&quot;Not freezing text encoder. Live dangerously and prosper!&quot;)
        return component
    method = args.freeze_encoder_strategy
    first_layer = args.freeze_encoder_before
    last_layer = args.freeze_encoder_after
    total_count = 0
    for name, param in component.named_parameters():
        total_count += 1
        pieces = name.split(&quot;.&quot;)
        if pieces[1] != &quot;encoder&quot; and pieces[2] != &quot;layers&quot;:
            logger.info(f&quot;Ignoring non-encoder layer: {name}&quot;)
            continue
        else:
            logger.debug(f&quot;Freezing layer: {name}, which has keys: {pieces}&quot;)
        current_layer = int(pieces[3])
        freeze_param = False
        if method == &quot;between&quot;:
            freeze_param = current_layer &gt; first_layer or current_layer &lt; last_layer
        elif method == &quot;outside&quot;:
            freeze_param = first_layer &lt;= current_layer &lt;= last_layer
        elif method == &quot;before&quot;:
            freeze_param = current_layer &lt; first_layer
        elif method == &quot;after&quot;:
            freeze_param = current_layer &gt; last_layer
        else:
            raise ValueError(
                f&quot;Invalid method {method}. Choose between &apos;between&apos;, &apos;outside&apos;, &apos;before&apos; or &apos;after&apos;.&quot;
            )
        if freeze_param:
            if hasattr(param, &quot;requires_grad&quot;):
                param.requires_grad = False
                # logger.debug(
                #     f&quot;Froze layer {name} with method {method} and range {first_layer} - {last_layer}&quot;
                # )
            else:
                # logger.info(
                #     f&quot;Ignoring layer that does not mark as gradient capable: {name}&quot;
                # )
                pass
    logger.info(
        f&quot;Applied {method} method with range {first_layer} - {last_layer} to {total_count} total layers.&quot;
    )
    return component</file><file path="helpers/training/multi_process.py">import torch.distributed as dist
def _get_rank():
    if dist.is_available() and dist.is_initialized():
        return dist.get_rank()
    else:
        return 0
def rank_info():
    try:
        return f&quot;(Rank: {_get_rank()}) &quot;
    except:
        return &quot;&quot;
def should_log():
    return _get_rank() == 0</file><file path="helpers/training/optimizer_param.py">import os
from accelerate.logging import get_logger
import accelerate
import torch
logger = get_logger(__name__, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
is_optimi_available = False
from helpers.training.optimizers.adamw_bfloat16 import AdamWBF16
from helpers.training.optimizers.adamw_schedulefree import AdamWScheduleFreeKahan
from helpers.training.optimizers.soap import SOAP
try:
    from optimum.quanto import QTensor
except:
    pass
try:
    from torchao.prototype.low_bit_optim import (
        AdamW8bit as AOAdamW8Bit,
        AdamW4bit as AOAdamW4Bit,
        AdamFp8 as AOAdamFp8,
        AdamWFp8 as AOAdamWFp8,
        CPUOffloadOptimizer as AOCPUOffloadOptimizer,
    )
    if torch.backends.mps.is_available():
        import torch._dynamo
        torch._dynamo.config.suppress_errors = True
except Exception as e:
    print(&quot;You need torchao installed for its low-precision optimizers.&quot;)
    raise e
try:
    import optimi
    is_optimi_available = True
except:
    logger.error(
        &quot;Could not load optimi library. Please install `torch-optimi` for better memory efficiency.&quot;
    )
is_bitsandbytes_available = False
try:
    import bitsandbytes
    is_bitsandbytes_available = True
except:
    if torch.cuda.is_available():
        logger.warning(
            &quot;Could not load bitsandbytes library. BnB-specific optimisers and other functionality will be unavailable.&quot;
        )
# Some optimizers are not available in multibackend bitsandbytes as of January 2025.
is_ademamix_available = False
if is_bitsandbytes_available:
    if &quot;AdEMAMix&quot; in dir(bitsandbytes.optim):
        is_ademamix_available = True
is_prodigy_available = False
try:
    import prodigyplus
    is_prodigy_available = True
except:
    if torch.cuda.is_available():
        logger.warning(
            &quot;Could not load prodigyplus library. Prodigy will not be available.&quot;
        )
optimizer_choices = {
    &quot;adamw_bf16&quot;: {
        &quot;precision&quot;: &quot;bf16&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AdamWBF16,
    },
    &quot;ao-adamw8bit&quot;: {
        &quot;gradient_precision&quot;: &quot;bf16&quot;,
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AOAdamW8Bit,
    },
    &quot;ao-adamw4bit&quot;: {
        &quot;gradient_precision&quot;: &quot;bf16&quot;,
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AOAdamW4Bit,
    },
    &quot;ao-adamfp8&quot;: {
        &quot;gradient_precision&quot;: &quot;bf16&quot;,
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AOAdamFp8,
    },
    &quot;ao-adamwfp8&quot;: {
        &quot;gradient_precision&quot;: &quot;bf16&quot;,
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AOAdamWFp8,
    },
    &quot;adamw_schedulefree&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;override_lr_scheduler&quot;: True,
        &quot;is_schedulefree&quot;: True,
        &quot;can_warmup&quot;: True,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-8,
        },
        &quot;class&quot;: AdamWScheduleFreeKahan,
    },
    &quot;adamw_schedulefree+aggressive&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;override_lr_scheduler&quot;: True,
        &quot;is_schedulefree&quot;: True,
        &quot;can_warmup&quot;: True,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-3,
            &quot;eps&quot;: 1e-6,
        },
        &quot;class&quot;: AdamWScheduleFreeKahan,
    },
    &quot;adamw_schedulefree+no_kahan&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;override_lr_scheduler&quot;: True,
        &quot;is_schedulefree&quot;: True,
        &quot;can_warmup&quot;: True,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.999),
            &quot;weight_decay&quot;: 1e-3,
            &quot;eps&quot;: 1e-6,
            &quot;use_kahan&quot;: False,
        },
        &quot;class&quot;: AdamWScheduleFreeKahan,
    },
    &quot;optimi-stableadamw&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;weight_decay&quot;: 1e-2,
            &quot;eps&quot;: 1e-6,
            &quot;decouple_lr&quot;: False,
            &quot;max_lr&quot;: None,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.StableAdamW,
    },
    &quot;optimi-adamw&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;eps&quot;: 1e-6,
            &quot;weight_decay&quot;: 0.0,
            &quot;decouple_lr&quot;: False,
            &quot;kahan_sum&quot;: True,
            &quot;max_lr&quot;: None,
        },
        &quot;class&quot;: optimi.AdamW,
    },
    &quot;optimi-lion&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;weight_decay&quot;: 0.0,
            &quot;decouple_lr&quot;: False,
            &quot;max_lr&quot;: None,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.Lion,
    },
    &quot;optimi-radam&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;weight_decay&quot;: 0.0,
            &quot;eps&quot;: 1e-6,
            &quot;decouple_wd&quot;: True,
            &quot;decouple_lr&quot;: False,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.RAdam,
    },
    &quot;optimi-ranger&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;weight_decay&quot;: 0.0,
            &quot;eps&quot;: 1e-6,
            &quot;k&quot;: 6,
            &quot;alpha&quot;: 0.5,
            &quot;decouple_wd&quot;: True,
            &quot;decouple_lr&quot;: False,
            &quot;max_lr&quot;: None,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.Ranger,
    },
    &quot;optimi-adan&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.98, 0.92, 0.999),
            &quot;weight_decay&quot;: 2e-2,
            &quot;eps&quot;: 1e-6,
            &quot;decouple_lr&quot;: False,
            &quot;max_lr&quot;: None,
            &quot;adam_wd&quot;: False,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.Adan,
    },
    &quot;optimi-adam&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.9, 0.99),
            &quot;eps&quot;: 1e-6,
            &quot;weight_decay&quot;: 0.0,
            &quot;decouple_wd&quot;: False,
            &quot;decouple_lr&quot;: False,
            &quot;kahan_sum&quot;: True,
            &quot;max_lr&quot;: None,
        },
        &quot;class&quot;: optimi.Adam,
    },
    &quot;optimi-sgd&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;momentum&quot;: 0,
            &quot;weight_decay&quot;: 0.0,
            &quot;dampening&quot;: False,
            &quot;decouple_wd&quot;: False,
            &quot;decouple_lr&quot;: False,
            &quot;max_lr&quot;: None,
            &quot;torch_init&quot;: False,
            &quot;kahan_sum&quot;: True,
            &quot;foreach&quot;: True,
        },
        &quot;class&quot;: optimi.SGD,
    },
    &quot;soap&quot;: {
        &quot;precision&quot;: &quot;any&quot;,
        &quot;default_settings&quot;: {
            &quot;betas&quot;: (0.95, 0.95),
            &quot;shampoo_beta&quot;: -1,
            &quot;eps&quot;: 1e-8,
            &quot;weight_decay&quot;: 0.01,
            &quot;precondition_frequency&quot;: 10,
            &quot;max_precond_dim&quot;: 10000,
            &quot;merge_dims&quot;: False,
            &quot;precondition_1d&quot;: False,
            &quot;normalize_grads&quot;: False,
            &quot;data_format&quot;: &quot;channels_first&quot;,
            &quot;correct_bias&quot;: True,
        },
        &quot;class&quot;: SOAP,
    },
}
if is_bitsandbytes_available:
    optimizer_choices.update(
        {
            &quot;bnb-adagrad&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;lr_decay&quot;: 0,
                    &quot;weight_decay&quot;: 0,
                    &quot;initial_accumulator_value&quot;: 0,
                    &quot;eps&quot;: 1e-10,
                    &quot;min_8bit_size&quot;: 4096,
                    &quot;percentile_clipping&quot;: 100,
                },
                &quot;class&quot;: bitsandbytes.optim.Adagrad,
            },
            &quot;bnb-adagrad8bit&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;lr_decay&quot;: 0,
                    &quot;weight_decay&quot;: 0,
                    &quot;initial_accumulator_value&quot;: 0,
                    &quot;eps&quot;: 1e-10,
                    &quot;min_8bit_size&quot;: 4096,
                    &quot;percentile_clipping&quot;: 100,
                },
                &quot;class&quot;: bitsandbytes.optim.Adagrad8bit,
            },
            &quot;bnb-adam&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0,
                    &quot;amsgrad&quot;: False,
                    &quot;min_8bit_size&quot;: 4096,
                    &quot;percentile_clipping&quot;: 100,
                },
                &quot;class&quot;: bitsandbytes.optim.Adam,
            },
            &quot;bnb-adam8bit&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0,
                    &quot;amsgrad&quot;: False,
                    &quot;min_8bit_size&quot;: 4096,
                    &quot;percentile_clipping&quot;: 100,
                },
                &quot;class&quot;: bitsandbytes.optim.Adam8bit,
            },
            &quot;bnb-adamw&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;weight_decay&quot;: 1e-2,
                    &quot;eps&quot;: 1e-6,
                },
                &quot;class&quot;: bitsandbytes.optim.AdamW,
            },
            &quot;bnb-adamw8bit&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;weight_decay&quot;: 1e-2,
                    &quot;eps&quot;: 1e-6,
                },
                &quot;class&quot;: bitsandbytes.optim.AdamW8bit,
            },
            &quot;bnb-adamw-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;weight_decay&quot;: 1e-2,
                    &quot;eps&quot;: 1e-6,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedAdamW,
            },
            &quot;bnb-adamw8bit-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999),
                    &quot;weight_decay&quot;: 1e-2,
                    &quot;eps&quot;: 1e-6,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedAdamW8bit,
            },
            &quot;bnb-lion&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.99),
                    &quot;weight_decay&quot;: 0.0,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.Lion,
            },
            &quot;bnb-lion8bit&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.99),
                    &quot;weight_decay&quot;: 0.0,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.Lion8bit,
            },
            &quot;bnb-lion-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.99),
                    &quot;weight_decay&quot;: 0.0,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedLion,
            },
            &quot;bnb-lion8bit-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.99),
                    &quot;weight_decay&quot;: 0.0,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedLion8bit,
            },
        }
    )
if is_ademamix_available:
    optimizer_choices.update(
        {
            &quot;bnb-ademamix&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999, 0.9999),
                    &quot;alpha&quot;: 5.0,
                    &quot;t_alpha&quot;: None,
                    &quot;t_beta3&quot;: None,
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0.01,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.AdEMAMix,
            },
            &quot;bnb-ademamix8bit&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999, 0.9999),
                    &quot;alpha&quot;: 5.0,
                    &quot;t_alpha&quot;: None,
                    &quot;t_beta3&quot;: None,
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0.01,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.AdEMAMix8bit,
            },
            &quot;bnb-ademamix-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999, 0.9999),
                    &quot;alpha&quot;: 5.0,
                    &quot;t_alpha&quot;: None,
                    &quot;t_beta3&quot;: None,
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0.01,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedAdEMAMix,
            },
            &quot;bnb-ademamix8bit-paged&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;default_settings&quot;: {
                    &quot;betas&quot;: (0.9, 0.999, 0.9999),
                    &quot;alpha&quot;: 5.0,
                    &quot;t_alpha&quot;: None,
                    &quot;t_beta3&quot;: None,
                    &quot;eps&quot;: 1e-08,
                    &quot;weight_decay&quot;: 0.01,
                    &quot;min_8bit_size&quot;: 4096,
                },
                &quot;class&quot;: bitsandbytes.optim.PagedAdEMAMix8bit,
            },
        }
    )
if is_prodigy_available:
    optimizer_choices.update(
        {
            &quot;prodigy&quot;: {
                &quot;precision&quot;: &quot;any&quot;,
                &quot;override_lr_scheduler&quot;: False,
                &quot;is_schedulefree&quot;: True,
                &quot;can_warmup&quot;: False,
                &quot;default_settings&quot;: {
                    &quot;lr&quot;: 1.0,
                    &quot;betas&quot;: (0.9, 0.99),
                    &quot;beta3&quot;: None,
                    &quot;weight_decay&quot;: 0.0,
                    &quot;weight_decay_by_lr&quot;: True,
                    &quot;use_bias_correction&quot;: False,
                    &quot;d0&quot;: 1e-6,
                    &quot;d_coef&quot;: 1,
                    &quot;prodigy_steps&quot;: 0,
                    &quot;use_speed&quot;: False,
                    &quot;eps&quot;: 1e-8,
                    &quot;split_groups&quot;: True,
                    &quot;split_groups_mean&quot;: True,
                    &quot;factored&quot;: True,
                    &quot;factored_fp32&quot;: True,
                    &quot;fused_back_pass&quot;: False,
                    &quot;use_stableadamw&quot;: True,
                    &quot;use_muon_pp&quot;: False,
                    &quot;use_cautious&quot;: False,
                    &quot;use_grams&quot;: False,
                    &quot;use_adopt&quot;: False,
                    &quot;stochastic_rounding&quot;: True,
                },
                &quot;class&quot;: prodigyplus.prodigy_plus_schedulefree.ProdigyPlusScheduleFree,
            }
        }
    )
args_to_optimizer_mapping = {
    &quot;use_adafactor_optimizer&quot;: &quot;adafactor&quot;,
    &quot;use_prodigy_optimizer&quot;: &quot;prodigy&quot;,
    &quot;use_dadaptation_optimizer&quot;: &quot;dadaptation&quot;,
    &quot;adam_bfloat16&quot;: &quot;adamw_bf16&quot;,
    &quot;use_8bit_adam&quot;: &quot;adamw8bit&quot;,
}
deprecated_optimizers = {
    &quot;dadaptation&quot;: &quot;D-adaptation optimiser has been removed due to issues with precision levels and convergence. Please use adamw_schedulefree instead.&quot;,
    &quot;adafactor&quot;: &quot;Adafactor optimiser has been removed in favour of optimi-stableadamw, which offers improved memory efficiency and convergence.&quot;,
    &quot;adamw8bit&quot;: &quot;AdamW8Bit has been removed in favour of optimi-adamw optimiser, which offers better low-precision support. Please use this or adamw_bf16 instead.&quot;,
}
def convert_arg_to_parameters(args):
    &quot;&quot;&quot;--optimizer_config can have a format like --optimizer_config=eps=1e-6,weight_decay=0.0&quot;&quot;&quot;
    out = {}
    if args.optimizer_config is not None and args.optimizer_config:
        optimizer_params = [
            param.split(&quot;=&quot;) for param in args.optimizer_config.split(&quot;,&quot;)
        ]
        for param in optimizer_params:
            if &quot;.&quot; in param[1]:
                out[param[0]] = float(param[1])
            elif str(param[1]).isdigit():
                out[param[0]] = int(param[1])
            elif param[1].lower() == &quot;true&quot;:
                out[param[0]] = True
            elif param[1].lower() == &quot;false&quot;:
                out[param[0]] = False
            elif param[1].lower() == &quot;none&quot;:
                out[param[0]] = None
            elif &quot;e-&quot; in param[1]:
                out[param[0]] = float(param[1])
            else:
                out[param[0]] = param[1]
        return out
    if args.optimizer_beta1 is not None and args.optimizer_beta2 is not None:
        # the user has supplied a beta1 and beta2 value
        out[&quot;betas&quot;] = tuple([args.optimizer_beta1, args.optimizer_beta2])
    return out
def optimizer_parameters(optimizer, args):
    &quot;&quot;&quot;Return the parameters for the optimizer&quot;&quot;&quot;
    if optimizer in optimizer_choices:
        optimizer_details = optimizer_choices.get(optimizer)
        optimizer_class = optimizer_choices.get(optimizer).get(&quot;class&quot;)
        optimizer_params = optimizer_choices.get(optimizer).get(&quot;default_settings&quot;)
        optimizer_params.update(convert_arg_to_parameters(args))
        if args.optimizer_release_gradients and &quot;optimi-&quot; in optimizer:
            optimizer_params[&quot;gradient_release&quot;] = True
        optimizer_details[&quot;default_settings&quot;] = optimizer_params
        if args.optimizer == &quot;prodigy&quot;:
            prodigy_steps = args.prodigy_steps
            if prodigy_steps and prodigy_steps &gt; 0:
                optimizer_params[&quot;prodigy_steps&quot;] = int(prodigy_steps)
            print(
                f&quot;Using Prodigy optimiser with {optimizer_params[&apos;prodigy_steps&apos;]} steps of learning rate adjustment.&quot;
            )
        return optimizer_class, optimizer_details
    else:
        raise ValueError(f&quot;Optimizer {optimizer} not found.&quot;)
def is_lr_scheduler_disabled(optimizer: str):
    &quot;&quot;&quot;Check if the optimizer has a built-in LR scheduler&quot;&quot;&quot;
    is_disabled = False
    if optimizer in optimizer_choices:
        is_disabled = optimizer_choices.get(optimizer).get(
            &quot;override_lr_scheduler&quot;, False
        )
    return is_disabled
def is_lr_schedulefree(optimizer: str):
    &quot;&quot;&quot;
    Check if the optimizer has ScheduleFree logic.
    This is separate from the disabling of LR schedulers, because some optimizers
    that contain ScheduleFree logic (Prodigy) can use an LR scheduler.
    &quot;&quot;&quot;
    is_schedulefree = False
    if optimizer in optimizer_choices:
        is_schedulefree = optimizer_choices.get(optimizer).get(&quot;is_schedulefree&quot;, False)
    return is_schedulefree
def show_optimizer_defaults(optimizer: str = None):
    &quot;&quot;&quot;we&apos;ll print the defaults on a single line, eg. foo=bar, buz=baz&quot;&quot;&quot;
    if optimizer is None:
        for key in optimizer_choices:
            print(f&quot;{key}={optimizer_choices[key].get(&apos;default_settings&apos;)}&quot;)
    else:
        print(f&quot;{optimizer}={optimizer_choices.get(optimizer).get(&apos;default_settings&apos;)}&quot;)
def is_optimizer_deprecated(optimizer: str) -&gt; bool:
    if optimizer in deprecated_optimizers:
        raise ValueError(deprecated_optimizers.get(optimizer))
def map_deprecated_optimizer_parameter(optimizer: str) -&gt; str:
    return args_to_optimizer_mapping.get(optimizer, None)
def is_optimizer_bf16(optimizer: str) -&gt; bool:
    optimizer_precision = optimizer_choices.get(optimizer, {}).get(&quot;precision&quot;, &quot;fp32&quot;)
    if optimizer_precision in [&quot;any&quot;, &quot;bf16&quot;]:
        return True
    return False
def is_optimizer_grad_fp32(optimizer: str) -&gt; bool:
    optimizer_precision = optimizer_choices.get(optimizer, {}).get(
        &quot;gradient_precision&quot;, None
    )
    if optimizer_precision == &quot;fp32&quot;:
        return True
    return False
def cpu_offload_optimizer(
    params_to_optimize,
    optimizer_cls,
    optimizer_parameters: dict,
    offload_gradients: bool = True,
    fused: bool = True,
    offload_mechanism: str = None,
):
    if not offload_mechanism or offload_mechanism == &quot;none&quot;:
        return optimizer_cls(params_to_optimize, **optimizer_parameters)
    if offload_mechanism != &quot;torchao&quot;:
        raise ValueError(
            f&quot;Unknown CPU optimiser offload mechanism: {offload_mechanism}&quot;
        )
    if offload_gradients:
        optimizer_parameters[&quot;offload_gradients&quot;] = offload_gradients
    if fused:
        optimizer_parameters[&quot;fused&quot;] = fused
    optimizer_parameters[&quot;optimizer_class&quot;] = optimizer_cls
    return AOCPUOffloadOptimizer(params_to_optimize, **optimizer_parameters)
def determine_optimizer_class_with_config(
    args, use_deepspeed_optimizer, is_quantized, enable_adamw_bf16
) -&gt; tuple:
    extra_optimizer_args = {}
    if use_deepspeed_optimizer:
        optimizer_class = accelerate.utils.DummyOptim
        extra_optimizer_args[&quot;lr&quot;] = float(args.learning_rate)
        extra_optimizer_args[&quot;betas&quot;] = (args.adam_beta1, args.adam_beta2)
        extra_optimizer_args[&quot;eps&quot;] = args.adam_epsilon
        extra_optimizer_args[&quot;weight_decay&quot;] = args.adam_weight_decay
        default_settings = extra_optimizer_args
        optimizer_details = {}
    elif is_quantized and not enable_adamw_bf16 and args.optimizer == &quot;adamw_bf16&quot;:
        logger.error(
            f&quot;When --base_model_default_dtype=fp32, AdamWBF16 may not be used. Switching to AdamW.&quot;
        )
        optimizer_class, optimizer_details = optimizer_parameters(&quot;optimi-adamw&quot;, args)
        default_settings = optimizer_details.get(&quot;default_settings&quot;)
    else:
        optimizer_class, optimizer_details = optimizer_parameters(args.optimizer, args)
        default_settings = optimizer_details.get(&quot;default_settings&quot;)
    if optimizer_details.get(&quot;can_warmup&quot;, False):
        logger.info(
            f&quot;Optimizer contains LR scheduler, warmup steps will be set to {args.lr_warmup_steps}.&quot;
        )
        default_settings[&quot;warmup_steps&quot;] = args.lr_warmup_steps
    logger.info(f&quot;cls: {optimizer_class}, settings: {default_settings}&quot;)
    return default_settings, optimizer_class
def determine_params_to_optimize(
    args,
    controlnet,
    unet,
    transformer,
    text_encoder_1,
    text_encoder_2,
    model_type_label,
    lycoris_wrapped_network,
):
    if args.model_type == &quot;full&quot;:
        if args.controlnet:
            params_to_optimize = controlnet.parameters()
        elif unet is not None:
            params_to_optimize = list(
                filter(lambda p: p.requires_grad, unet.parameters())
            )
        elif transformer is not None:
            params_to_optimize = list(
                filter(lambda p: p.requires_grad, transformer.parameters())
            )
        if args.train_text_encoder:
            raise ValueError(
                &quot;Full model tuning does not currently support text encoder training.&quot;
            )
    elif &quot;lora&quot; in args.model_type:
        if args.controlnet:
            raise ValueError(
                &quot;SimpleTuner does not currently support training a ControlNet LoRA.&quot;
            )
        if unet is not None:
            params_to_optimize = list(
                filter(lambda p: p.requires_grad, unet.parameters())
            )
        if transformer is not None:
            params_to_optimize = list(
                filter(lambda p: p.requires_grad, transformer.parameters())
            )
        if args.train_text_encoder:
            if args.model_family in [&quot;sd3&quot;, &quot;pixart_sigma&quot;]:
                raise ValueError(
                    f&quot;{model_type_label} does not support finetuning the text encoders, as T5 does not benefit from it.&quot;
                )
            else:
                # add the first text encoder&apos;s parameters
                params_to_optimize = params_to_optimize + list(
                    filter(lambda p: p.requires_grad, text_encoder_1.parameters())
                )
                # if text_encoder_2 is not None, add its parameters
                if text_encoder_2 is None and args.model_family not in [&quot;flux&quot;]:
                    # but not flux. it has t5 as enc 2.
                    params_to_optimize = params_to_optimize + list(
                        filter(lambda p: p.requires_grad, text_encoder_2.parameters())
                    )
        if args.lora_type == &quot;lycoris&quot; and lycoris_wrapped_network is not None:
            params_to_optimize = list(
                filter(lambda p: p.requires_grad, lycoris_wrapped_network.parameters())
            )
    return params_to_optimize</file><file path="helpers/training/peft_init.py">import torch
def approximate_normal_tensor(inp, target, scale=1.0):
    device = inp.device
    tensor = torch.randn_like(target).to(device)
    desired_norm = inp.norm().to(device)
    desired_mean = inp.mean().to(device)
    desired_std = inp.std().to(device)
    current_norm = tensor.norm()
    tensor = tensor * (desired_norm / current_norm)
    current_std = tensor.std()
    tensor = tensor * (desired_std / current_std)
    tensor = tensor - tensor.mean() + desired_mean
    tensor.mul_(scale)
    target.copy_(tensor)
def init_lokr_network_with_perturbed_normal(lycoris, scale=1e-3):
    with torch.no_grad():
        for lora in lycoris.loras:
            lora.lokr_w1.fill_(1.0)
            approximate_normal_tensor(lora.org_weight, lora.lokr_w2, scale=scale)</file><file path="helpers/training/save_hooks.py">from helpers.training.ema import EMAModel
from helpers.training.wrappers import unwrap_model
from helpers.training.multi_process import _get_rank as get_rank
from diffusers.utils import (
    convert_state_dict_to_diffusers,
    convert_unet_state_dict_to_peft,
)
from peft import set_peft_model_state_dict
from peft.utils import get_peft_model_state_dict
from helpers.models.sdxl.pipeline import StableDiffusionXLPipeline
from helpers.training.state_tracker import StateTracker
from helpers.models.smoldit import SmolDiT2DModel, SmolDiTPipeline
from helpers.models.sd3.transformer import SD3Transformer2DModel
import os
import logging
import shutil
import json
from safetensors import safe_open
from safetensors.torch import save_file
from tqdm import tqdm
logger = logging.getLogger(&quot;SaveHookManager&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;WARNING&quot;))
try:
    from diffusers import (
        UNet2DConditionModel,
        StableDiffusion3Pipeline,
        StableDiffusionPipeline,
        FluxPipeline,
        PixArtSigmaPipeline,
        ControlNetModel,
        HunyuanDiTPipeline,
    )
except ImportError:
    logger.error(&quot;This release requires the latest version of Diffusers.&quot;)
try:
    from diffusers.models import PixArtTransformer2DModel
except Exception as e:
    logger.error(
        f&quot;Can not load Pixart Sigma model class. This release requires the latest version of Diffusers: {e}&quot;
    )
    raise e
try:
    from diffusers.models import FluxTransformer2DModel
except Exception as e:
    logger.error(
        f&quot;Can not load FluxTransformer2DModel model class. This release requires the latest version of Diffusers: {e}&quot;
    )
    raise e
try:
    from diffusers.models import HunyuanDiT2DModel
except Exception as e:
    logger.error(
        f&quot;Can not load Hunyuan DiT model class. This release requires the latest version of Diffusers: {e}&quot;
    )
    raise e
def merge_safetensors_files(directory):
    json_file_name = &quot;diffusion_pytorch_model.safetensors.index.json&quot;
    json_file_path = os.path.join(directory, json_file_name)
    if not os.path.exists(json_file_path):
        return
    # Step 2: Load the JSON file and extract the weight map
    with open(json_file_path, &quot;r&quot;) as file:
        data = json.load(file)
        weight_map = data.get(&quot;weight_map&quot;)
        if weight_map is None:
            raise KeyError(&quot;&apos;weight_map&apos; key not found in the JSON file.&quot;)
    # Collect all unique safetensors files from weight_map
    files_to_load = set(weight_map.values())
    all_tensors = {}
    # Load tensors from each unique file
    for file_name in files_to_load:
        part_file_path = os.path.join(directory, file_name)
        if not os.path.exists(part_file_path):
            raise FileNotFoundError(f&quot;Part file {file_name} not found.&quot;)
        with safe_open(part_file_path, framework=&quot;pt&quot;, device=&quot;cpu&quot;) as f:
            for tensor_key in f.keys():
                if tensor_key in weight_map:
                    all_tensors[tensor_key] = f.get_tensor(tensor_key)
    # Step 4: Save all loaded tensors into a single new safetensors file
    output_file_path = os.path.join(directory, &quot;diffusion_pytorch_model.safetensors&quot;)
    save_file(all_tensors, output_file_path)
    # Step 5: If the file now exists, remove the index and part files
    if os.path.exists(output_file_path):
        os.remove(json_file_path)
        for file_name in files_to_load:
            os.remove(os.path.join(directory, file_name))
    logger.info(f&quot;All tensors have been merged and saved into {output_file_path}&quot;)
class SaveHookManager:
    def __init__(
        self,
        args,
        unet,
        transformer,
        ema_model,
        text_encoder_1,
        text_encoder_2,
        accelerator,
        use_deepspeed_optimizer,
    ):
        self.args = args
        self.unet = unet
        self.transformer = transformer
        if self.unet is not None and self.transformer is not None:
            raise ValueError(&quot;Both `unet` and `transformer` cannot be set.&quot;)
        self.text_encoder_1 = text_encoder_1
        self.text_encoder_2 = text_encoder_2
        self.ema_model = ema_model
        self.accelerator = accelerator
        self.use_deepspeed_optimizer = use_deepspeed_optimizer
        self.denoiser_class = None
        self.denoiser_subdir = None
        self.pipeline_class = None
        if self.unet is not None:
            self.denoiser_class = UNet2DConditionModel
            self.denoiser_subdir = &quot;unet&quot;
            self.pipeline_class = StableDiffusionXLPipeline
            if StateTracker.get_model_family() == &quot;legacy&quot;:
                self.pipeline_class = StableDiffusionPipeline
        elif self.transformer is not None:
            if args.model_family == &quot;sd3&quot;:
                self.denoiser_class = SD3Transformer2DModel
                self.pipeline_class = StableDiffusion3Pipeline
            elif (
                args.model_family.lower() == &quot;flux&quot;
                and not args.flux_attention_masked_training
            ):
                self.denoiser_class = FluxTransformer2DModel
                self.pipeline_class = FluxPipeline
            elif (
                args.model_family.lower() == &quot;flux&quot;
                and args.flux_attention_masked_training
            ):
                from helpers.models.flux.transformer import (
                    FluxTransformer2DModelWithMasking,
                )
                self.denoiser_class = FluxTransformer2DModelWithMasking
                self.pipeline_class = FluxPipeline
            elif hasattr(args, &quot;hunyuan_dit&quot;) and args.hunyuan_dit:
                self.denoiser_class = HunyuanDiT2DModel
                self.pipeline_class = HunyuanDiTPipeline
            elif args.model_family == &quot;pixart_sigma&quot;:
                self.denoiser_class = PixArtTransformer2DModel
                self.pipeline_class = PixArtSigmaPipeline
            elif args.model_family == &quot;smoldit&quot;:
                self.denoiser_class = SmolDiT2DModel
                self.pipeline_class = SmolDiTPipeline
            elif args.model_family == &quot;sana&quot;:
                from diffusers import SanaPipeline, SanaTransformer2DModel
                self.denoiser_class = SanaTransformer2DModel
                self.pipeline_class = SanaPipeline
            elif args.model_family == &quot;ltxvideo&quot;:
                from diffusers import LTXPipeline, LTXVideoTransformer3DModel
                self.denoiser_class = LTXVideoTransformer3DModel
                self.pipeline_class = LTXPipeline
            self.denoiser_subdir = &quot;transformer&quot;
        if args.controlnet:
            self.denoiser_class = ControlNetModel
            self.denoiser_subdir = &quot;controlnet&quot;
        logger.debug(f&quot;Denoiser class set to: {self.denoiser_class.__name__}.&quot;)
        logger.debug(f&quot;Pipeline class set to: {self.pipeline_class.__name__}.&quot;)
        self.ema_model_cls = None
        self.ema_model_subdir = None
        if unet is not None:
            self.ema_model_subdir = &quot;unet_ema&quot;
            self.ema_model_cls = unet.__class__
        if transformer is not None:
            self.ema_model_subdir = &quot;transformer_ema&quot;
            self.ema_model_cls = transformer.__class__
        self.training_state_path = &quot;training_state.json&quot;
        if self.accelerator is not None:
            rank = get_rank()
            if rank &gt; 0:
                self.training_state_path = f&quot;training_state-rank{rank}.json&quot;
    def _primary_model(self):
        if self.args.controlnet:
            return self.controlnet
        if self.unet is not None:
            return self.unet
        if self.transformer is not None:
            return self.transformer
    def _save_lora(self, models, weights, output_dir):
        # for SDXL/others, there are only two options here. Either are just the unet attn processor layers
        # or there are the unet and text encoder atten layers.
        unet_lora_layers_to_save = None
        transformer_lora_layers_to_save = None
        text_encoder_1_lora_layers_to_save = None
        text_encoder_2_lora_layers_to_save = None
        # Diffusers does not train the third text encoder.
        # text_encoder_3_lora_layers_to_save = None
        if self.args.use_ema:
            # we&apos;ll temporarily overwrite teh LoRA parameters with the EMA parameters to save it.
            logger.info(&quot;Saving EMA model to disk.&quot;)
            trainable_parameters = [
                p for p in self._primary_model().parameters() if p.requires_grad
            ]
            self.ema_model.store(trainable_parameters)
            self.ema_model.copy_to(trainable_parameters)
            if self.transformer is not None:
                self.pipeline_class.save_lora_weights(
                    os.path.join(output_dir, &quot;ema&quot;),
                    transformer_lora_layers=convert_state_dict_to_diffusers(
                        get_peft_model_state_dict(self._primary_model())
                    ),
                )
            elif self.unet is not None:
                self.pipeline_class.save_lora_weights(
                    os.path.join(output_dir, &quot;ema&quot;),
                    unet_lora_layers=convert_state_dict_to_diffusers(
                        get_peft_model_state_dict(self._primary_model())
                    ),
                )
            self.ema_model.restore(trainable_parameters)
        for model in models:
            if isinstance(model, type(unwrap_model(self.accelerator, self.unet))):
                unet_lora_layers_to_save = convert_state_dict_to_diffusers(
                    get_peft_model_state_dict(model)
                )
            elif isinstance(
                model, type(unwrap_model(self.accelerator, self.text_encoder_1))
            ):
                text_encoder_1_lora_layers_to_save = convert_state_dict_to_diffusers(
                    get_peft_model_state_dict(model)
                )
            elif isinstance(
                model, type(unwrap_model(self.accelerator, self.text_encoder_2))
            ):
                text_encoder_2_lora_layers_to_save = convert_state_dict_to_diffusers(
                    get_peft_model_state_dict(model)
                )
            elif not isinstance(
                model, type(unwrap_model(self.accelerator, HunyuanDiT2DModel))
            ):
                if isinstance(
                    model, type(unwrap_model(self.accelerator, self.transformer))
                ):
                    transformer_lora_layers_to_save = get_peft_model_state_dict(model)
            elif not self.use_deepspeed_optimizer:
                raise ValueError(f&quot;unexpected save model: {model.__class__}&quot;)
            # make sure to pop weight so that corresponding model is not saved again
            if weights:
                weights.pop()
        if self.args.model_family == &quot;flux&quot;:
            self.pipeline_class.save_lora_weights(
                output_dir,
                transformer_lora_layers=transformer_lora_layers_to_save,
                text_encoder_lora_layers=text_encoder_1_lora_layers_to_save,
            )
        elif self.args.model_family == &quot;ltxvideo&quot;:
            self.pipeline_class.save_lora_weights(
                output_dir,
                transformer_lora_layers=transformer_lora_layers_to_save,
            )
        elif self.args.model_family == &quot;sd3&quot;:
            self.pipeline_class.save_lora_weights(
                output_dir,
                transformer_lora_layers=transformer_lora_layers_to_save,
                text_encoder_lora_layers=text_encoder_1_lora_layers_to_save,
                text_encoder_2_lora_layers=text_encoder_2_lora_layers_to_save,
            )
        elif self.args.model_family == &quot;legacy&quot;:
            self.pipeline_class.save_lora_weights(
                output_dir,
                unet_lora_layers=unet_lora_layers_to_save,
                text_encoder_lora_layers=text_encoder_1_lora_layers_to_save,
            )
        elif self.args.model_family == &quot;sdxl&quot; or self.args.model_family == &quot;kolors&quot;:
            self.pipeline_class.save_lora_weights(
                output_dir,
                unet_lora_layers=unet_lora_layers_to_save,
                text_encoder_lora_layers=text_encoder_1_lora_layers_to_save,
                text_encoder_2_lora_layers=text_encoder_2_lora_layers_to_save,
            )
        else:
            raise ValueError(f&quot;unexpected model family: {self.args.model_family}&quot;)
    def _save_lycoris(self, models, weights, output_dir):
        &quot;&quot;&quot;
        save wrappers for lycoris. For now, text encoders are not trainable
        via lycoris.
        &quot;&quot;&quot;
        from helpers.publishing.huggingface import (
            LORA_SAFETENSORS_FILENAME,
            EMA_SAFETENSORS_FILENAME,
        )
        for _ in models:
            if weights:
                weights.pop()
        lycoris_config = None
        with open(self.args.lycoris_config, &quot;r&quot;) as f:
            lycoris_config = json.load(f)
        self.accelerator._lycoris_wrapped_network.save_weights(
            os.path.join(output_dir, LORA_SAFETENSORS_FILENAME),
            list(self.accelerator._lycoris_wrapped_network.parameters())[0].dtype,
            {&quot;lycoris_config&quot;: json.dumps(lycoris_config)},  # metadata
        )
        if self.args.use_ema:
            # we&apos;ll store lycoris weights.
            self.ema_model.store(self.accelerator._lycoris_wrapped_network.parameters())
            # we&apos;ll write EMA to the lycoris adapter temporarily.
            self.ema_model.copy_to(
                self.accelerator._lycoris_wrapped_network.parameters()
            )
            # now we can write the lycoris weights using the EMA_SAFETENSORS_FILENAME instead.
            os.makedirs(os.path.join(output_dir, &quot;ema&quot;), exist_ok=True)
            self.accelerator._lycoris_wrapped_network.save_weights(
                os.path.join(output_dir, &quot;ema&quot;, EMA_SAFETENSORS_FILENAME),
                list(self.accelerator._lycoris_wrapped_network.parameters())[0].dtype,
                {&quot;lycoris_config&quot;: json.dumps(lycoris_config)},  # metadata
            )
            self.ema_model.restore(
                self.accelerator._lycoris_wrapped_network.parameters()
            )
        # copy the config into the repo
        shutil.copy2(
            self.args.lycoris_config, os.path.join(output_dir, &quot;lycoris_config.json&quot;)
        )
        logger.info(&quot;LyCORIS weights have been saved to disk&quot;)
    def _save_full_model(self, models, weights, output_dir):
        # Create a temporary directory for atomic saves
        temporary_dir = output_dir.replace(&quot;checkpoint&quot;, &quot;temporary&quot;)
        os.makedirs(temporary_dir, exist_ok=True)
        if self.args.use_ema and self.accelerator.is_main_process:
            # even with deepspeed, EMA should only save on the main process.
            ema_model_path = os.path.join(
                temporary_dir, self.ema_model_subdir, &quot;ema_model.pt&quot;
            )
            logger.info(f&quot;Saving EMA model to {ema_model_path}&quot;)
            try:
                self.ema_model.save_state_dict(ema_model_path)
            except Exception as e:
                logger.error(f&quot;Error saving EMA model: {e}&quot;)
            logger.info(f&quot;Saving EMA safetensors variant.&quot;)
            self.ema_model.save_pretrained(
                os.path.join(temporary_dir, self.ema_model_subdir),
                max_shard_size=&quot;10GB&quot;,
            )
        if self.unet is not None:
            sub_dir = &quot;unet&quot;
        if self.transformer is not None:
            sub_dir = &quot;transformer&quot;
        if self.args.controlnet:
            sub_dir = &quot;controlnet&quot;
        for model in models:
            model.save_pretrained(
                os.path.join(temporary_dir, sub_dir), max_shard_size=&quot;10GB&quot;
            )
            merge_safetensors_files(os.path.join(temporary_dir, sub_dir))
            if weights:
                weights.pop()  # Pop the last weight
        # Copy contents of temporary directory to output directory
        for item in os.listdir(temporary_dir):
            s = os.path.join(temporary_dir, item)
            d = os.path.join(output_dir, item)
            if os.path.isdir(s):
                shutil.copytree(s, d, dirs_exist_ok=True)  # Python 3.8+
            else:
                shutil.copy2(s, d)
        # Remove the temporary directory
        shutil.rmtree(temporary_dir, ignore_errors=True)
    def save_model_hook(self, models, weights, output_dir):
        # Write &quot;training_state.json&quot; to the output directory containing the training state
        StateTracker.save_training_state(
            os.path.join(output_dir, self.training_state_path)
        )
        if not self.accelerator.is_main_process:
            return
        if self.args.use_ema:
            # we&apos;ll save this EMA checkpoint for restoring the state easier.
            ema_model_path = os.path.join(
                output_dir, self.ema_model_subdir, &quot;ema_model.pt&quot;
            )
            logger.info(f&quot;Saving EMA model to {ema_model_path}&quot;)
            try:
                self.ema_model.save_state_dict(ema_model_path)
            except Exception as e:
                logger.error(f&quot;Error saving EMA model: {e}&quot;)
        if &quot;lora&quot; in self.args.model_type and self.args.lora_type == &quot;standard&quot;:
            self._save_lora(models=models, weights=weights, output_dir=output_dir)
            return
        elif &quot;lora&quot; in self.args.model_type and self.args.lora_type == &quot;lycoris&quot;:
            self._save_lycoris(models=models, weights=weights, output_dir=output_dir)
            return
        else:
            self._save_full_model(models=models, weights=weights, output_dir=output_dir)
    def _load_lora(self, models, input_dir):
        logger.info(f&quot;Loading LoRA weights from Path: {input_dir}&quot;)
        unet_ = None
        transformer_ = None
        denoiser = None
        text_encoder_one_ = None
        text_encoder_two_ = None
        while len(models) &gt; 0:
            model = models.pop()
            if isinstance(
                unwrap_model(self.accelerator, model),
                type(unwrap_model(self.accelerator, self.unet)),
            ):
                unet_ = model
                denoiser = unet_
            elif isinstance(
                unwrap_model(self.accelerator, model),
                type(unwrap_model(self.accelerator, self.transformer)),
            ):
                transformer_ = model
                denoiser = transformer_
            elif isinstance(
                unwrap_model(self.accelerator, model),
                type(unwrap_model(self.accelerator, self.text_encoder_1)),
            ):
                text_encoder_one_ = model
            elif isinstance(
                unwrap_model(self.accelerator, model),
                type(unwrap_model(self.accelerator, self.text_encoder_2)),
            ):
                text_encoder_two_ = model
            else:
                raise ValueError(
                    f&quot;unexpected save model: {model.__class__}&quot;
                    f&quot;\nunwrapped: {unwrap_model(self.accelerator, model).__class__}&quot;
                    f&quot;\nunet: {unwrap_model(self.accelerator, self.unet).__class__}&quot;
                )
        if self.transformer is not None:
            key_to_replace = &quot;transformer&quot;
            lora_state_dict = self.pipeline_class.lora_state_dict(input_dir)
        elif self.unet is not None:
            key_to_replace = &quot;unet&quot;
            lora_state_dict, _ = self.pipeline_class.lora_state_dict(input_dir)
        else:
            raise Exception(&quot;No model to save LoRA for.&quot;)
        denoiser_state_dict = {
            f&apos;{k.replace(f&quot;{key_to_replace}.&quot;, &quot;&quot;)}&apos;: v
            for k, v in lora_state_dict.items()
            if k.startswith(f&quot;{key_to_replace}.&quot;)
        }
        denoiser_state_dict = convert_unet_state_dict_to_peft(denoiser_state_dict)
        incompatible_keys = set_peft_model_state_dict(
            denoiser, denoiser_state_dict, adapter_name=&quot;default&quot;
        )
        if incompatible_keys is not None:
            # check only for unexpected keys
            unexpected_keys = getattr(incompatible_keys, &quot;unexpected_keys&quot;, None)
            if unexpected_keys:
                logger.warning(
                    f&quot;Loading adapter weights from state_dict led to unexpected keys not found in the model: &quot;
                    f&quot; {unexpected_keys}. &quot;
                )
        if self.args.train_text_encoder:
            # Do we need to call `scale_lora_layers()` here?
            from diffusers.training_utils import _set_state_dict_into_text_encoder
            _set_state_dict_into_text_encoder(
                lora_state_dict,
                prefix=&quot;text_encoder.&quot;,
                text_encoder=text_encoder_one_,
            )
            _set_state_dict_into_text_encoder(
                lora_state_dict,
                prefix=&quot;text_encoder_2.&quot;,
                text_encoder=text_encoder_two_,
            )
        logger.info(&quot;Completed loading LoRA weights.&quot;)
    def _load_lycoris(self, models, input_dir):
        from helpers.publishing.huggingface import LORA_SAFETENSORS_FILENAME
        while len(models) &gt; 0:
            model = models.pop()
        state = self.accelerator._lycoris_wrapped_network.load_weights(
            os.path.join(input_dir, LORA_SAFETENSORS_FILENAME)
        )
        if len(state.keys()) &gt; 0:
            logging.error(f&quot;LyCORIS failed to load: {state}&quot;)
            raise RuntimeError(&quot;Loading of LyCORIS model failed&quot;)
        weight_dtype = StateTracker.get_weight_dtype()
        if self.transformer is not None:
            self.accelerator._lycoris_wrapped_network.to(
                device=self.accelerator.device, dtype=weight_dtype
            )
        elif self.unet is not None:
            self.accelerator._lycoris_wrapped_network.to(
                device=self.accelerator.device, dtype=weight_dtype
            )
        else:
            raise ValueError(&quot;No model found to load LyCORIS weights into.&quot;)
        logger.info(&quot;LyCORIS weights have been loaded from disk&quot;)
        # disable LyCORIS spam logging
        lycoris_logger = logging.getLogger(&quot;LyCORIS&quot;)
        lycoris_logger.setLevel(logging.ERROR)
    def _load_full_model(self, models, input_dir):
        if self.args.model_type == &quot;full&quot;:
            return_exception = False
            for i in range(len(models)):
                try:
                    # pop models so that they are not loaded again
                    model = models.pop()
                    load_model = self.denoiser_class.from_pretrained(
                        input_dir, subfolder=self.denoiser_subdir
                    )
                    if (
                        self.args.model_family == &quot;sd3&quot;
                        and not self.args.train_text_encoder
                    ):
                        logger.info(
                            &quot;Unloading text encoders for full SD3 training without --train_text_encoder&quot;
                        )
                        (self.text_encoder_1, self.text_encoder_2) = (None, None)
                    model.register_to_config(**load_model.config)
                    model.load_state_dict(load_model.state_dict())
                    del load_model
                except Exception as e:
                    import traceback
                    return_exception = f&quot;Could not load model: {e}, traceback: {traceback.format_exc()}&quot;
            if return_exception:
                raise Exception(return_exception)
    def load_model_hook(self, models, input_dir):
        # Check the checkpoint dir for a &quot;training_state.json&quot; file to load
        training_state_path = os.path.join(input_dir, self.training_state_path)
        if (
            not os.path.exists(training_state_path)
            and self.training_state_path != &quot;training_state.json&quot;
        ):
            logger.warning(
                f&quot;Could not find {training_state_path} in checkpoint dir {input_dir}. Trying the default path.&quot;
            )
            training_state_path = os.path.join(input_dir, &quot;training_state.json&quot;)
        if os.path.exists(training_state_path):
            StateTracker.load_training_state(training_state_path)
        else:
            logger.warning(
                f&quot;Could not find {training_state_path} in checkpoint dir {input_dir}&quot;
            )
        if self.args.use_ema and self.accelerator.is_main_process:
            try:
                self.ema_model.load_state_dict(
                    os.path.join(input_dir, self.ema_model_subdir, &quot;ema_model.pt&quot;)
                )
                # self.ema_model.to(self.accelerator.device)
            except Exception as e:
                logger.error(f&quot;Could not load EMA model: {e}&quot;)
        if &quot;lora&quot; in self.args.model_type and self.args.lora_type == &quot;standard&quot;:
            self._load_lora(models=models, input_dir=input_dir)
        elif &quot;lora&quot; in self.args.model_type and self.args.lora_type == &quot;lycoris&quot;:
            self._load_lycoris(models=models, input_dir=input_dir)
        else:
            self._load_full_model(models=models, input_dir=input_dir)</file><file path="helpers/training/schedulers.py">import os
from accelerate.logging import get_logger
from helpers.models import get_model_config_path
logger = get_logger(__name__, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
def load_scheduler_from_args(args):
    flow_matching = False
    if (
        args.model_family == &quot;sd3&quot; and args.flow_matching_loss != &quot;diffusion&quot;
    ) or args.model_family in [&quot;flux&quot;, &quot;sana&quot;, &quot;ltxvideo&quot;]:
        # Flow-matching models.
        flow_matching = True
        from diffusers import FlowMatchEulerDiscreteScheduler
        noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
            get_model_config_path(
                args.model_family, args.pretrained_model_name_or_path
            ),
            subfolder=&quot;scheduler&quot;,
            shift=1 if args.model_family == &quot;sd3&quot; else 3,
        )
    else:
        if args.model_family == &quot;legacy&quot;:
            args.rescale_betas_zero_snr = True
            args.training_scheduler_timestep_spacing = &quot;trailing&quot;
        from diffusers import DDPMScheduler
        noise_scheduler = DDPMScheduler.from_pretrained(
            get_model_config_path(
                args.model_family, args.pretrained_model_name_or_path
            ),
            subfolder=&quot;scheduler&quot;,
            rescale_betas_zero_snr=args.rescale_betas_zero_snr,
            timestep_spacing=args.training_scheduler_timestep_spacing,
        )
        args.prediction_type = noise_scheduler.config.prediction_type
        if flow_matching and args.flow_matching_loss == &quot;diffusion&quot;:
            logger.warning(
                &quot;Since --flow_matching_loss=diffusion, we will be reparameterising the model to v-prediction diffusion objective. This will break things for a while. Perhaps forever..&quot;
            )
    return args, flow_matching, noise_scheduler</file><file path="helpers/training/state_tracker.py">from os import environ
from pathlib import Path
import json
import logging
logger = logging.getLogger(&quot;StateTracker&quot;)
logger.setLevel(environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
filename_mapping = {
    &quot;all_image_files&quot;: &quot;image&quot;,
    &quot;all_vae_cache_files&quot;: &quot;vae&quot;,
    &quot;all_text_cache_files&quot;: &quot;text&quot;,
}
class StateTracker:
    config_path = None
    # Class variables
    model_type = &quot;&quot;
    # Job ID for FastAPI. None if local.
    job_id = None
    ## Training state
    global_step = 0
    global_resume_step = None
    epoch_step = 0
    epoch_micro_step = 0
    epoch = 1
    ## Caches
    all_image_files = {}
    all_vae_cache_files = {}
    all_text_cache_files = {}
    all_caption_files = None
    ## Backend entities for retrieval
    default_text_embed_cache = None
    _is_sdxl_refiner = False
    accelerator = None
    data_backends = {}
    parquet_databases = {}
    # A list of backend IDs to exhaust.
    exhausted_backends = []
    # A dict of backend IDs to the number of times they have been repeated.
    repeats = {}
    # The images we&apos;ll use for upscaling at validation time. Stored at startup.
    validation_sample_images = []
    vae = None
    vae_dtype = None
    weight_dtype = None
    args = None
    # Aspect to resolution map, we&apos;ll store once generated for consistency.
    aspect_resolution_map = {}
    # for schedulefree
    last_lr = 0.0
    # hugging face hub user details
    hf_user = None
    webhook_handler = None
    @classmethod
    def delete_cache_files(
        cls, data_backend_id: str = None, preserve_data_backend_cache=False
    ):
        for cache_name in [
            &quot;all_image_files&quot;,
            &quot;all_vae_cache_files&quot;,
            &quot;all_text_cache_files&quot;,
        ]:
            if filename_mapping[cache_name] in str(preserve_data_backend_cache):
                continue
            data_backend_id_suffix = &quot;&quot;
            if data_backend_id:
                data_backend_id_suffix = f&quot;_{data_backend_id}&quot;
            cache_path = (
                Path(cls.args.output_dir) / f&quot;{cache_name}{data_backend_id_suffix}.json&quot;
            )
            if cache_path.exists():
                try:
                    cache_path.unlink()
                except:
                    pass
    @classmethod
    def _load_from_disk(cls, cache_name):
        cache_path = Path(cls.args.output_dir) / f&quot;{cache_name}.json&quot;
        if cache_path.exists():
            try:
                with cache_path.open(&quot;r&quot;) as f:
                    return json.load(f)
            except Exception as e:
                logger.error(
                    f&quot;Invalidating cache: error loading {cache_name} from disk. {e}&quot;
                )
                return None
        return None
    @classmethod
    def _save_to_disk(cls, cache_name, data):
        cache_path = Path(cls.args.output_dir) / f&quot;{cache_name}.json&quot;
        with cache_path.open(&quot;w&quot;) as f:
            json.dump(data, f)
    @classmethod
    def set_config_path(cls, config_path: str):
        cls.config_path = config_path
    @classmethod
    def get_config_path(cls):
        return cls.config_path
    @classmethod
    def set_model_family(cls, model_type: str):
        if model_type not in [
            &quot;legacy&quot;,
            &quot;ltxvideo&quot;,
            &quot;sdxl&quot;,
            &quot;sd3&quot;,
            &quot;pixart_sigma&quot;,
            &quot;kolors&quot;,
            &quot;smoldit&quot;,
            &quot;flux&quot;,
            &quot;sana&quot;,
        ]:
            raise ValueError(f&quot;Unknown model type: {model_type}&quot;)
        cls.model_type = model_type
    @classmethod
    def get_model_family(cls):
        return cls.model_type
    @classmethod
    def get_hf_user(cls):
        return cls.hf_user
    @classmethod
    def set_hf_user(cls, hf_user):
        cls.hf_user = hf_user
    @classmethod
    def get_hf_username(cls):
        if cls.hf_user is not None and &quot;name&quot; in cls.hf_user:
            return cls.hf_user[&quot;name&quot;]
        return None
    @classmethod
    def is_sdxl_refiner(cls, set_value=None):
        if set_value is not None:
            cls._is_sdxl_refiner = set_value
        return cls._is_sdxl_refiner
    @classmethod
    def set_parquet_database(cls, data_backend_id: str, parquet_database: tuple):
        &quot;&quot;&quot;parquet_database is a tuple (dataframe, filename_column, caption_column, fallback_caption_column)&quot;&quot;&quot;
        cls.parquet_databases[data_backend_id] = parquet_database
    @classmethod
    def get_parquet_database(cls, data_backend_id: str):
        return cls.parquet_databases.get(data_backend_id, (None, None, None, None))
    @classmethod
    def set_image_files(cls, raw_file_list: list, data_backend_id: str):
        if cls.all_image_files[data_backend_id] is not None:
            cls.all_image_files[data_backend_id].clear()
        else:
            cls.all_image_files[data_backend_id] = {}
        for subdirectory_list in raw_file_list:
            _, _, files = subdirectory_list
            for image in files:
                cls.all_image_files[data_backend_id][image] = False
        cls._save_to_disk(
            &quot;all_image_files_{}&quot;.format(data_backend_id),
            cls.all_image_files[data_backend_id],
        )
        logger.debug(
            f&quot;set_image_files found {len(cls.all_image_files[data_backend_id])} images.&quot;
        )
        return cls.all_image_files[data_backend_id]
    @classmethod
    def get_image_files(cls, data_backend_id: str):
        if data_backend_id not in cls.all_image_files:
            cls.all_image_files[data_backend_id] = cls._load_from_disk(
                &quot;all_image_files_{}&quot;.format(data_backend_id)
            )
        return cls.all_image_files[data_backend_id]
    @classmethod
    def get_global_resume_step(cls):
        return cls.global_resume_step
    @classmethod
    def set_global_resume_step(cls, global_resume_step: int):
        cls.global_resume_step = global_resume_step
    @classmethod
    def get_global_step(cls):
        return cls.global_step
    @classmethod
    def set_global_step(cls, global_step: int):
        cls.global_step = global_step
    @classmethod
    def get_epoch(cls):
        return cls.epoch
    @classmethod
    def set_epoch(cls, epoch: int):
        logger.debug(f&quot;Current training state: {cls.get_training_state()}&quot;)
        cls.epoch = epoch
    @classmethod
    def get_epoch_step(cls):
        return cls.epoch_step
    @classmethod
    def set_epoch_step(cls, epoch_step: int):
        cls.epoch_step = epoch_step
    @classmethod
    def set_repeats(cls, repeats: dict):
        cls.repeats = repeats
    @classmethod
    def load_training_state(cls, state_path: str):
        try:
            with open(state_path, &quot;r&quot;) as f:
                training_state = json.load(f)
        except OSError as e:
            logger.error(f&quot;Error loading training state: {e}&quot;)
            training_state = {}
        except Exception as e:
            logger.error(f&quot;Error loading training state: {e}&quot;)
            training_state = {}
        cls.set_global_step(training_state.get(&quot;global_step&quot;, 0))
        cls.set_epoch_step(training_state.get(&quot;epoch_step&quot;, 0))
        cls.set_epoch(training_state.get(&quot;epoch&quot;, 1))
        cls.set_exhausted_backends(training_state.get(&quot;exhausted_backends&quot;, []))
        cls.init_repeats(training_state.get(&quot;repeats&quot;, {}))
        logging.debug(f&quot;Training state loaded: {cls.get_training_state()}&quot;)
    @classmethod
    def save_training_state(cls, state_path: str):
        training_state = {
            &quot;global_step&quot;: cls.global_step,
            &quot;epoch_step&quot;: cls.epoch_step,
            &quot;epoch&quot;: cls.epoch,
            &quot;exhausted_backends&quot;: cls.exhausted_backends,
            &quot;repeats&quot;: cls.repeats,
        }
        logger.debug(f&quot;Saving training state: {training_state}&quot;)
        with open(state_path, &quot;w&quot;) as f:
            json.dump(training_state, f)
    @classmethod
    def get_training_state(cls):
        return {
            &quot;global_step&quot;: cls.global_step,
            &quot;epoch_step&quot;: cls.epoch_step,
            &quot;epoch&quot;: cls.epoch,
            &quot;exhausted_backends&quot;: cls.exhausted_backends,
            &quot;repeats&quot;: cls.repeats,
        }
    @classmethod
    def set_repeats(cls, repeats: int, data_backend_id: str = None):
        if data_backend_id is None:
            # set every entry in repeats to zero
            for key in cls.repeats.keys():
                cls.repeats[key] = repeats
        else:
            cls.repeats[data_backend_id] = repeats
    @classmethod
    def init_repeats(cls, repeats: int):
        cls.repeats = repeats
    @classmethod
    def get_repeats(cls, data_backend_id: str):
        if data_backend_id not in cls.repeats:
            return 0
        return cls.repeats[data_backend_id]
    @classmethod
    def increment_repeats(cls, data_backend_id: str):
        cls.set_repeats(
            data_backend_id=data_backend_id,
            repeats=cls.get_repeats(data_backend_id) + 1,
        )
    @classmethod
    def backend_status(cls, data_backend_id: str):
        return data_backend_id in cls.exhausted_backends
    @classmethod
    def backend_exhausted(cls, data_backend_id: str):
        cls.exhausted_backends.append(data_backend_id)
    @classmethod
    def backend_enable(cls, data_backend_id: str):
        cls.exhausted_backends.remove(data_backend_id)
    @classmethod
    def set_exhausted_backends(cls, exhausted_backends: list):
        cls.exhausted_backends = exhausted_backends
    @classmethod
    def clear_exhausted_buckets(cls):
        cls.exhausted_backends = []
    @classmethod
    def set_vae_cache_files(cls, raw_file_list: list, data_backend_id: str):
        if cls.all_vae_cache_files.get(data_backend_id) is not None:
            cls.all_vae_cache_files[data_backend_id].clear()
        else:
            cls.all_vae_cache_files[data_backend_id] = {}
        for subdirectory_list in raw_file_list:
            _, _, files = subdirectory_list
            for image in files:
                cls.all_vae_cache_files[data_backend_id][image] = False
        cls._save_to_disk(
            &quot;all_vae_cache_files_{}&quot;.format(data_backend_id),
            cls.all_vae_cache_files[data_backend_id],
        )
        logger.debug(
            f&quot;set_vae_cache_files found {len(cls.all_vae_cache_files[data_backend_id])} images.&quot;
        )
    @classmethod
    def get_vae_cache_files(cls: list, data_backend_id: str):
        if (
            data_backend_id not in cls.all_vae_cache_files
            or cls.all_vae_cache_files.get(data_backend_id) is None
        ):
            cls.all_vae_cache_files[data_backend_id] = cls._load_from_disk(
                &quot;all_vae_cache_files_{}&quot;.format(data_backend_id)
            )
        return cls.all_vae_cache_files[data_backend_id] or {}
    @classmethod
    def set_text_cache_files(cls, raw_file_list: list, data_backend_id: str):
        if cls.all_text_cache_files[data_backend_id] is not None:
            cls.all_text_cache_files[data_backend_id].clear()
        else:
            cls.all_text_cache_files[data_backend_id] = {}
        for subdirectory_list in raw_file_list:
            _, _, files = subdirectory_list
            for text_embed_path in files:
                cls.all_text_cache_files[data_backend_id][text_embed_path] = False
        cls._save_to_disk(
            &quot;all_text_cache_files_{}&quot;.format(data_backend_id),
            cls.all_text_cache_files[data_backend_id],
        )
        logger.debug(
            f&quot;set_text_cache_files found {len(cls.all_text_cache_files[data_backend_id])} images.&quot;
        )
    @classmethod
    def get_text_cache_files(cls: list, data_backend_id: str):
        if data_backend_id not in cls.all_text_cache_files:
            cls.all_text_cache_files[data_backend_id] = cls._load_from_disk(
                &quot;all_text_cache_files_{}&quot;.format(data_backend_id)
            )
        return cls.all_text_cache_files[data_backend_id]
    @classmethod
    def set_caption_files(cls, caption_files):
        cls.all_caption_files = caption_files
        cls._save_to_disk(&quot;all_caption_files&quot;, cls.all_caption_files)
    @classmethod
    def get_caption_files(cls):
        if not cls.all_caption_files:
            cls.all_caption_files = cls._load_from_disk(&quot;all_caption_files&quot;)
        return cls.all_caption_files
    @classmethod
    def get_validation_sample_images(cls):
        return cls.validation_sample_images
    @classmethod
    def set_validation_sample_images(cls, validation_sample_images):
        cls.validation_sample_images = validation_sample_images
    @classmethod
    def register_data_backend(cls, data_backend):
        cls.data_backends[data_backend[&quot;id&quot;]] = data_backend
    @classmethod
    def get_data_backend(cls, id: str):
        return cls.data_backends[id]
    @classmethod
    def get_dataset_size(cls, data_backend_id: str):
        if &quot;sampler&quot; in cls.data_backends[data_backend_id]:
            return len(cls.data_backends[data_backend_id][&quot;sampler&quot;])
        return 0
    @classmethod
    def set_conditioning_dataset(
        cls, data_backend_id: str, conditioning_backend_id: str
    ):
        cls.data_backends[data_backend_id][&quot;conditioning_data&quot;] = cls.data_backends[
            conditioning_backend_id
        ]
    @classmethod
    def get_conditioning_dataset(cls, data_backend_id: str):
        return cls.data_backends[data_backend_id].get(&quot;conditioning_data&quot;, None)
    @classmethod
    def get_data_backend_config(cls, data_backend_id: str):
        return cls.data_backends.get(data_backend_id, {}).get(&quot;config&quot;, {})
    @classmethod
    def set_data_backend_config(cls, data_backend_id: str, config: dict):
        if data_backend_id not in cls.data_backends:
            cls.data_backends[data_backend_id] = {}
        cls.data_backends[data_backend_id][&quot;config&quot;] = config
    @classmethod
    def clear_data_backends(cls):
        cls.data_backends = {}
    @classmethod
    def get_data_backends(cls, _type=&quot;image&quot;, _types=[&quot;image&quot;, &quot;video&quot;]):
        output = {}
        for backend_id, backend in dict(cls.data_backends).items():
            if backend.get(&quot;dataset_type&quot;, &quot;image&quot;) == _type or (
                type(_types) is list and backend.get(&quot;dataset_type&quot;, &quot;image&quot;) in _types
            ):
                output[backend_id] = backend
        return output
    @classmethod
    def set_accelerator(cls, accelerator):
        cls.accelerator = accelerator
    @classmethod
    def get_accelerator(cls):
        return cls.accelerator
    @classmethod
    def get_webhook_handler(cls):
        return cls.webhook_handler
    @classmethod
    def set_webhook_handler(cls, webhook_handler):
        cls.webhook_handler = webhook_handler
    @classmethod
    def set_job_id(cls, job_id: str):
        cls.job_id = job_id
    @classmethod
    def get_job_id(cls):
        return cls.job_id
    @classmethod
    def set_vae(cls, vae):
        cls.vae = vae
    @classmethod
    def get_vae(cls):
        return cls.vae
    @classmethod
    def set_vae_dtype(cls, vae_dtype):
        cls.vae_dtype = vae_dtype
    @classmethod
    def get_vae_dtype(cls):
        return cls.vae_dtype
    @classmethod
    def set_weight_dtype(cls, weight_dtype):
        cls.weight_dtype = weight_dtype
    @classmethod
    def get_weight_dtype(cls):
        return cls.weight_dtype
    @classmethod
    def set_args(cls, args):
        cls.args = args
    @classmethod
    def get_args(cls):
        return cls.args
    @classmethod
    def get_vaecache(cls, id: str):
        return cls.data_backends[id][&quot;vaecache&quot;]
    @classmethod
    def set_default_text_embed_cache(cls, default_text_embed_cache):
        cls.default_text_embed_cache = default_text_embed_cache
    @classmethod
    def get_default_text_embed_cache(cls):
        return cls.default_text_embed_cache
    @classmethod
    def get_embedcache(cls, data_backend_id: str):
        return cls.data_backends[data_backend_id][&quot;text_embed_cache&quot;]
    @classmethod
    def get_metadata_by_filepath(cls, filepath, data_backend_id: str):
        for _, data_backend in cls.get_data_backends(_types=[&quot;image&quot;, &quot;video&quot;]).items():
            if &quot;metadata_backend&quot; not in data_backend:
                continue
            if data_backend_id != data_backend[&quot;metadata_backend&quot;].id:
                continue
            metadata = data_backend[&quot;metadata_backend&quot;].get_metadata_by_filepath(
                filepath
            )
            if metadata is not None:
                return metadata
        return None
    @classmethod
    def get_resolution_by_aspect(cls, dataloader_resolution: float, aspect: float):
        return cls.aspect_resolution_map.get(dataloader_resolution, {}).get(
            str(aspect), None
        )
    @classmethod
    def set_resolution_by_aspect(
        cls, dataloader_resolution: float, aspect: float, resolution: int
    ):
        if dataloader_resolution not in cls.aspect_resolution_map:
            cls.aspect_resolution_map[dataloader_resolution] = {}
        cls.aspect_resolution_map[dataloader_resolution][str(aspect)] = resolution
        cls._save_to_disk(
            f&quot;aspect_resolution_map-{dataloader_resolution}&quot;,
            cls.aspect_resolution_map[dataloader_resolution],
        )
        logger.debug(
            f&quot;Aspect resolution map: {cls.aspect_resolution_map[dataloader_resolution]}&quot;
        )
    @classmethod
    def save_aspect_resolution_map(cls, dataloader_resolution: float):
        cls._save_to_disk(
            f&quot;aspect_resolution_map-{dataloader_resolution}&quot;,
            cls.aspect_resolution_map[dataloader_resolution],
        )
    @classmethod
    def load_aspect_resolution_map(cls, dataloader_resolution: float):
        if dataloader_resolution not in cls.aspect_resolution_map:
            cls.aspect_resolution_map = {dataloader_resolution: {}}
        cls.aspect_resolution_map[dataloader_resolution] = (
            cls._load_from_disk(f&quot;aspect_resolution_map-{dataloader_resolution}&quot;) or {}
        )
        logger.debug(
            f&quot;Aspect resolution map: {cls.aspect_resolution_map[dataloader_resolution]}&quot;
        )
    @classmethod
    def get_last_lr(cls):
        return cls.last_lr
    @classmethod
    def set_last_lr(cls, last_lr: float):
        cls.last_lr = float(last_lr)</file><file path="helpers/training/text_encoding.py">from transformers import PretrainedConfig
import os
from accelerate.logging import get_logger
from helpers.models import get_model_config_path
from .state_tracker import StateTracker
logger = get_logger(__name__, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
def import_model_class_from_model_name_or_path(
    pretrained_model_name_or_path: str,
    revision: str,
    args,
    subfolder: str = &quot;text_encoder&quot;,
):
    if args.model_family.lower() == &quot;smoldit&quot;:
        from transformers import AutoModelForSeq2SeqLM
        return AutoModelForSeq2SeqLM
    text_encoder_config = PretrainedConfig.from_pretrained(
        pretrained_model_name_or_path, subfolder=subfolder, revision=revision
    )
    model_class = text_encoder_config.architectures[0]
    if model_class == &quot;CLIPTextModel&quot;:
        from transformers import CLIPTextModel
        return CLIPTextModel
    elif model_class == &quot;CLIPTextModelWithProjection&quot;:
        from transformers import CLIPTextModelWithProjection
        return CLIPTextModelWithProjection
    elif model_class == &quot;T5EncoderModel&quot;:
        from transformers import T5EncoderModel
        return T5EncoderModel
    elif model_class == &quot;UMT5EncoderModel&quot;:
        from transformers import UMT5EncoderModel
        return UMT5EncoderModel
    elif model_class == &quot;ChatGLMModel&quot;:
        from diffusers.pipelines.kolors.text_encoder import ChatGLMModel
        return ChatGLMModel
    elif model_class == &quot;Gemma2Model&quot;:
        from transformers import Gemma2Model
        return Gemma2Model
    else:
        raise ValueError(f&quot;{model_class} is not supported.&quot;)
def get_tokenizers(args):
    tokenizer_1, tokenizer_2, tokenizer_3 = None, None, None
    try:
        if args.model_family.lower() == &quot;smoldit&quot;:
            from transformers import AutoTokenizer
            tokenizer_1 = AutoTokenizer.from_pretrained(
                &quot;EleutherAI/pile-t5-base&quot;, pad_token=&quot;[PAD]&quot;
            )
            return tokenizer_1, tokenizer_2, tokenizer_3
        tokenizer_kwargs = {
            &quot;pretrained_model_name_or_path&quot;: get_model_config_path(
                args.model_family, args.pretrained_model_name_or_path
            ),
            &quot;subfolder&quot;: &quot;tokenizer&quot;,
            &quot;revision&quot;: args.revision,
        }
        is_t5_model = False
        if args.model_family.lower() in [&quot;ltxvideo&quot;, &quot;pixart_sigma&quot;]:
            from transformers import T5Tokenizer
            tokenizer_cls = T5Tokenizer
            is_t5_model = True
        elif args.model_family == &quot;sana&quot;:
            from transformers import Gemma2Model, GemmaTokenizerFast
            tokenizer_cls = GemmaTokenizerFast
            is_t5_model = False
            tokenizer_1 = tokenizer_cls.from_pretrained(
                get_model_config_path(
                    args.model_family, args.pretrained_model_name_or_path
                ),
                subfolder=&quot;tokenizer&quot;,
                revision=args.revision,
                use_fast=False,
            )
        elif args.model_family.lower() == &quot;kolors&quot;:
            from diffusers.pipelines.kolors.tokenizer import ChatGLMTokenizer
            tokenizer_cls = ChatGLMTokenizer
            tokenizer_1 = tokenizer_cls.from_pretrained(
                get_model_config_path(
                    args.model_family, args.pretrained_model_name_or_path
                ),
                subfolder=&quot;tokenizer&quot;,
                revision=args.revision,
                use_fast=False,
            )
        else:
            from transformers import CLIPTokenizer
            tokenizer_1 = CLIPTokenizer.from_pretrained(**tokenizer_kwargs)
        if is_t5_model:
            text_encoder_path = (
                args.pretrained_t5_model_name_or_path
                if args.pretrained_t5_model_name_or_path is not None
                else get_model_config_path(
                    args.model_family, args.pretrained_model_name_or_path
                )
            )
            logger.info(
                f&quot;Tokenizer path: {text_encoder_path}, custom T5 model path: {args.pretrained_t5_model_name_or_path} revision: {args.revision}&quot;
            )
            try:
                tokenizer_1 = tokenizer_cls.from_pretrained(
                    text_encoder_path,
                    subfolder=&quot;tokenizer&quot;,
                    revision=args.revision,
                    use_fast=False,
                )
            except Exception as e:
                logger.warning(
                    f&quot;Failed to load tokenizer 1: {e}, attempting no subfolder&quot;
                )
                tokenizer_1 = tokenizer_cls.from_pretrained(
                    text_encoder_path,
                    subfolder=None,
                    revision=args.revision,
                    use_fast=False,
                )
    except Exception as e:
        import traceback
        logger.warning(
            &quot;Primary tokenizer (CLIP-L/14) failed to load. Continuing to test whether we have just the secondary tokenizer..&quot;
            f&quot;\nError: -&gt; {e}&quot;
            f&quot;\nTraceback: {traceback.format_exc()}&quot;
        )
        if args.model_family in [&quot;sd3&quot;]:
            raise e
    from transformers import T5TokenizerFast
    if args.model_family not in [&quot;pixart_sigma&quot;, &quot;kolors&quot;, &quot;sana&quot;, &quot;ltxvideo&quot;]:
        try:
            tokenizer_2_cls = CLIPTokenizer
            if args.model_family.lower() == &quot;flux&quot;:
                tokenizer_2_cls = T5TokenizerFast
            tokenizer_2 = tokenizer_2_cls.from_pretrained(
                args.pretrained_model_name_or_path,
                subfolder=&quot;tokenizer_2&quot;,
                revision=args.revision,
                use_fast=False,
            )
            if tokenizer_1 is None:
                logger.info(&quot;Seems that we are training an SDXL refiner model.&quot;)
                StateTracker.is_sdxl_refiner(True)
                if args.validation_using_datasets is None:
                    logger.warning(
                        &quot;Since we are training the SDXL refiner and --validation_using_datasets was not specified, it is now being enabled.&quot;
                    )
                    args.validation_using_datasets = True
        except Exception as e:
            logger.warning(
                f&quot;Could not load secondary tokenizer ({&apos;OpenCLIP-G/14&apos; if args.model_family != &apos;flux&apos; else &apos;T5 XXL&apos;}). Cannot continue: {e}&quot;
            )
            if args.model_family in [&quot;flux&quot;, &quot;sd3&quot;]:
                raise e
        if not tokenizer_1 and not tokenizer_2:
            raise Exception(&quot;Failed to load tokenizer&quot;)
    else:
        if not tokenizer_1:
            raise Exception(&quot;Failed to load tokenizer&quot;)
    if args.model_family == &quot;sd3&quot;:
        try:
            tokenizer_3 = T5TokenizerFast.from_pretrained(
                args.pretrained_model_name_or_path,
                subfolder=&quot;tokenizer_3&quot;,
                revision=args.revision,
                use_fast=True,
            )
        except:
            raise ValueError(
                &quot;Could not load tertiary tokenizer (T5-XXL v1.1). Cannot continue.&quot;
            )
    return tokenizer_1, tokenizer_2, tokenizer_3
def determine_te_path_subfolder(args):
    if args.model_family.lower() == &quot;kolors&quot;:
        text_encoder_path = args.pretrained_model_name_or_path
        text_encoder_subfolder = &quot;text_encoder&quot;
    elif args.model_family.lower() == &quot;smoldit&quot;:
        text_encoder_path = &quot;EleutherAI/pile-t5-base&quot;
        text_encoder_subfolder = None
    elif args.model_family.lower() == &quot;flux&quot;:
        text_encoder_path = args.pretrained_model_name_or_path
        text_encoder_subfolder = &quot;text_encoder&quot;
    elif args.model_family.lower() in [&quot;ltxvideo&quot;, &quot;pixart_sigma&quot;]:
        text_encoder_path = (
            args.pretrained_t5_model_name_or_path
            if args.pretrained_t5_model_name_or_path is not None
            else args.pretrained_model_name_or_path
        )
        # Google&apos;s version of the T5 XXL model doesn&apos;t have a subfolder :()
        text_encoder_subfolder = &quot;text_encoder&quot;
    else:
        # sdxl and sd3 use the sd 1.5 clip-L/14 as number one.
        # sd2.x uses openclip vit-H/14
        logger.info(&quot;Load CLIP text encoder..&quot;)
        text_encoder_path = args.pretrained_model_name_or_path
        text_encoder_subfolder = &quot;text_encoder&quot;
    return text_encoder_path, text_encoder_subfolder
def load_tes(
    args,
    text_encoder_classes,
    tokenizers,
    weight_dtype,
    text_encoder_path,
    text_encoder_subfolder,
):
    text_encoder_cls_1, text_encoder_cls_2, text_encoder_cls_3 = text_encoder_classes
    tokenizer_1, tokenizer_2, tokenizer_3 = tokenizers
    text_encoder_1, text_encoder_2, text_encoder_3 = None, None, None
    text_encoder_variant = args.variant
    if tokenizer_1 is not None and not args.model_family == &quot;smoldit&quot;:
        if args.model_family.lower() in [&quot;pixart_sigma&quot;, &quot;ltxvideo&quot;]:
            logger.info(
                f&quot;Loading T5-XXL v1.1 text encoder from {text_encoder_path}/{text_encoder_subfolder}..&quot;
            )
        elif args.model_family.lower() == &quot;flux&quot;:
            logger.info(
                f&quot;Loading OpenAI CLIP-L text encoder from {text_encoder_path}/{text_encoder_subfolder}..&quot;
            )
        elif args.model_family.lower() == &quot;kolors&quot;:
            logger.info(
                f&quot;Loading ChatGLM language model from {text_encoder_path}/{text_encoder_subfolder}..&quot;
            )
            text_encoder_variant = &quot;fp16&quot;
        elif args.model_family.lower() == &quot;sana&quot;:
            logger.info(
                f&quot;Loading Gemma2 language model from {text_encoder_path}/{text_encoder_subfolder}..&quot;
            )
        else:
            logger.info(
                f&quot;Loading CLIP text encoder from {text_encoder_path}/{text_encoder_subfolder}..&quot;
            )
        text_encoder_1 = text_encoder_cls_1.from_pretrained(
            text_encoder_path,
            subfolder=text_encoder_subfolder,
            revision=args.revision,
            variant=text_encoder_variant,
            torch_dtype=weight_dtype,
        )
    elif args.model_family.lower() == &quot;smoldit&quot;:
        text_encoder_1 = text_encoder_cls_1.from_pretrained(
            &quot;EleutherAI/pile-t5-base&quot;,
            torch_dtype=weight_dtype,
        ).encoder
    if tokenizer_2 is not None:
        if args.model_family.lower() == &quot;flux&quot;:
            logger.info(
                f&quot;Loading T5 XXL v1.1 text encoder from {args.pretrained_model_name_or_path}/text_encoder_2..&quot;
            )
        else:
            logger.info(&quot;Loading LAION OpenCLIP-G/14 text encoder..&quot;)
        text_encoder_2 = text_encoder_cls_2.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder=&quot;text_encoder_2&quot;,
            revision=args.revision,
            torch_dtype=weight_dtype,
            variant=args.variant,
        )
    if tokenizer_3 is not None and args.model_family == &quot;sd3&quot;:
        logger.info(&quot;Loading T5-XXL v1.1 text encoder..&quot;)
        text_encoder_3 = text_encoder_cls_3.from_pretrained(
            args.pretrained_model_name_or_path,
            subfolder=&quot;text_encoder_3&quot;,
            torch_dtype=weight_dtype,
            revision=args.revision,
            variant=args.variant,
        )
    for te in [text_encoder_1, text_encoder_2, text_encoder_3]:
        if te is None:
            continue
        te.eval()
    return text_encoder_variant, text_encoder_1, text_encoder_2, text_encoder_3</file><file path="helpers/training/trainer.py">import logging, os
import huggingface_hub
from helpers.training.default_settings.safety_check import safety_check
from helpers.publishing.huggingface import HubManager
from configure import model_labels
import shutil
import hashlib
import json
import copy
import random
import math
import sys
import glob
import wandb
from helpers import log_format  # noqa
from helpers.configuration.loader import load_config
from helpers.caching.memory import reclaim_memory
from helpers.training.multi_process import _get_rank as get_rank
from helpers.training.validation import Validation, prepare_validation_prompt_list
from helpers.training.evaluation import ModelEvaluator
from helpers.training.state_tracker import StateTracker
from helpers.training.schedulers import load_scheduler_from_args
from helpers.training.custom_schedule import get_lr_scheduler
from helpers.training.adapter import determine_adapter_target_modules, load_lora_weights
from helpers.training.diffusion_model import load_diffusion_model
from helpers.training.text_encoding import (
    load_tes,
    determine_te_path_subfolder,
    import_model_class_from_model_name_or_path,
    get_tokenizers,
)
from helpers.training.optimizer_param import (
    determine_optimizer_class_with_config,
    determine_params_to_optimize,
    is_lr_scheduler_disabled,
    is_lr_schedulefree,
    cpu_offload_optimizer,
)
from helpers.data_backend.factory import BatchFetcher
from helpers.training.deepspeed import (
    deepspeed_zero_init_disabled_context_manager,
    prepare_model_for_deepspeed,
)
from helpers.training.wrappers import unwrap_model
from helpers.data_backend.factory import configure_multi_databackend
from helpers.data_backend.factory import random_dataloader_iterator
from helpers.training import steps_remaining_in_epoch
from helpers.training.custom_schedule import (
    generate_timestep_weights,
    segmented_timestep_selection,
)
from helpers.training.min_snr_gamma import compute_snr
from helpers.training.peft_init import init_lokr_network_with_perturbed_normal
from accelerate.logging import get_logger
from diffusers.models.embeddings import get_2d_rotary_pos_embed
from helpers.models.smoldit import get_resize_crop_region_for_grid
from helpers.models.ltxvideo import (
    pack_ltx_latents,
    unpack_ltx_latents,
    apply_first_frame_protection,
    make_i2v_conditioning_mask,
)
from helpers.models import get_model_config_path
logger = get_logger(
    &quot;SimpleTuner&quot;, log_level=os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
)
filelock_logger = get_logger(&quot;filelock&quot;)
connection_logger = get_logger(&quot;urllib3.connectionpool&quot;)
training_logger = get_logger(&quot;training-loop&quot;)
# More important logs.
target_level = os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;)
logger.setLevel(target_level)
training_logger_level = os.environ.get(&quot;SIMPLETUNER_TRAINING_LOOP_LOG_LEVEL&quot;, &quot;INFO&quot;)
training_logger.setLevel(training_logger_level)
# Less important logs.
filelock_logger.setLevel(&quot;WARNING&quot;)
connection_logger.setLevel(&quot;WARNING&quot;)
import torch
import diffusers
import accelerate
import transformers
import torch.nn.functional as F
import torch.utils.checkpoint
from accelerate import Accelerator
from accelerate.utils import set_seed
from configure import model_classes
from torch.distributions import Beta
try:
    from lycoris import LycorisNetwork
except:
    print(&quot;[ERROR] Lycoris not available. Please install &quot;)
from tqdm.auto import tqdm
from transformers import PretrainedConfig, CLIPTokenizer
from helpers.models.sdxl.pipeline import StableDiffusionXLPipeline
from diffusers import StableDiffusion3Pipeline
from diffusers import (
    ControlNetModel,
    DDIMScheduler,
    DDPMScheduler,
    UNet2DConditionModel,
    FluxTransformer2DModel,
    PixArtTransformer2DModel,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    UniPCMultistepScheduler,
)
from peft import LoraConfig
from peft.utils import get_peft_model_state_dict
from helpers.training.ema import EMAModel
from helpers.training.exceptions import MultiDatasetExhausted
from diffusers.utils import (
    check_min_version,
    convert_state_dict_to_diffusers,
    is_wandb_available,
)
from diffusers.utils.import_utils import is_xformers_available
from transformers.utils import ContextManagers
from helpers.models.flux import (
    prepare_latent_image_ids,
    pack_latents,
    unpack_latents,
    get_mobius_guidance,
    apply_flow_schedule_shift,
)
is_optimi_available = False
try:
    from optimi import prepare_for_gradient_release
    is_optimi_available = True
except:
    pass
# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version(&quot;0.27.0.dev0&quot;)
SCHEDULER_NAME_MAP = {
    &quot;euler&quot;: EulerDiscreteScheduler,
    &quot;euler-a&quot;: EulerAncestralDiscreteScheduler,
    &quot;unipc&quot;: UniPCMultistepScheduler,
    &quot;ddim&quot;: DDIMScheduler,
    &quot;ddpm&quot;: DDPMScheduler,
}
logging.basicConfig(
    format=&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;,
    datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;,
    level=logging.INFO,
)
transformers.utils.logging.set_verbosity_warning()
diffusers.utils.logging.set_verbosity_warning()
class Trainer:
    def __init__(
        self,
        config: dict = None,
        disable_accelerator: bool = False,
        job_id: str = None,
        exit_on_error: bool = False,
    ):
        self.accelerator = None
        self.job_id = job_id
        StateTracker.set_job_id(job_id)
        self.parse_arguments(
            args=config,
            disable_accelerator=disable_accelerator,
            exit_on_error=exit_on_error,
        )
        self._misc_init()
        self.lycoris_wrapped_network = None
        self.lycoris_config = None
        self.lr_scheduler = None
        self.webhook_handler = None
        self.should_abort = False
        self.unet = None
        self.transformer = None
        self.vae = None
        self.text_encoder_1 = None
        self.text_encoder_2 = None
        self.text_encoder_3 = None
        self.controlnet = None
        self.ema_model = None
        self.validation = None
    def _config_to_obj(self, config):
        if not config:
            return None
        return type(&quot;Config&quot;, (object,), config)
    def parse_arguments(
        self, args=None, disable_accelerator: bool = False, exit_on_error: bool = False
    ):
        self.config = load_config(args, exit_on_error=exit_on_error)
        report_to = (
            None if self.config.report_to.lower() == &quot;none&quot; else self.config.report_to
        )
        if not disable_accelerator:
            self.accelerator = Accelerator(
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                mixed_precision=(
                    self.config.mixed_precision
                    if not torch.backends.mps.is_available()
                    else None
                ),
                log_with=report_to,
                project_config=self.config.accelerator_project_config,
                kwargs_handlers=[self.config.process_group_kwargs],
            )
        safety_check(args=self.config, accelerator=self.accelerator)
        if self.config.lr_scale:
            logger.info(
                f&quot;Scaling learning rate ({self.config.learning_rate}), due to --lr_scale&quot;
            )
            self.config.learning_rate = (
                self.config.learning_rate
                * self.config.gradient_accumulation_steps
                * self.config.train_batch_size
                * getattr(self.accelerator, &quot;num_processes&quot;, 1)
            )
        StateTracker.set_accelerator(self.accelerator)
        StateTracker.set_args(self.config)
        StateTracker.set_weight_dtype(self.config.weight_dtype)
        self.set_model_family()
        # this updates self.config further, so we will run it here.
        self.init_noise_schedule()
    def run(self):
        try:
            # Initialize essential configurations and schedules
            self.configure_webhook()
            self.init_noise_schedule()
            self.init_seed()
            self.init_huggingface_hub()
            # Core initialization steps with signal checks after each step
            self._initialize_components_with_signal_check(
                [
                    self.init_preprocessing_models,
                    self.init_data_backend,
                    self.init_validation_prompts,
                    self.init_unload_text_encoder,
                    self.init_unload_vae,
                    self.init_load_base_model,
                    self.init_precision,
                    self.init_controlnet_model,
                    self.init_freeze_models,
                    self.init_trainable_peft_adapter,
                    self.init_ema_model,
                ]
            )
            # Model movement and validation setup
            self.move_models(destination=&quot;accelerator&quot;)
            self._exit_on_signal()
            self.init_validations()
            self._exit_on_signal()
            self.init_benchmark_base_model()
            self._exit_on_signal()
            self.resume_and_prepare()
            self._exit_on_signal()
            self.init_trackers()
            # Start the training process
            self.train()
        except Exception as e:
            import traceback
            logger.error(
                f&quot;Failed to run training: {e}, traceback: {traceback.format_exc()}&quot;
            )
            self._send_webhook_msg(
                message=f&quot;Failed to run training: {e}&quot;,
            )
            self._send_webhook_raw(
                structured_data={
                    &quot;message&quot;: f&quot;Failed to run training: {e}&quot;,
                    &quot;status&quot;: &quot;error&quot;,
                },
                message_type=&quot;fatal_error&quot;,
            )
            raise e
    def _initialize_components_with_signal_check(self, initializers):
        &quot;&quot;&quot;
        Runs a list of initializer functions with signal checks after each.
        Args:
            initializers (list): A list of initializer functions to run sequentially.
        &quot;&quot;&quot;
        for initializer in initializers:
            initializer()
            self._exit_on_signal()
    def _get_noise_scheduler(self):
        _, _, noise_scheduler = load_scheduler_from_args(self.config)
        return noise_scheduler
    def init_noise_schedule(self):
        self.config, _flow_matching, self.noise_scheduler = load_scheduler_from_args(
            self.config
        )
        self.config.flow_matching = _flow_matching
        self.lr = 0.0
    def configure_webhook(self, send_startup_message: bool = True):
        self.webhook_handler = None
        if self.config.webhook_config is None:
            return
        from helpers.webhooks.handler import WebhookHandler
        self.webhook_handler = WebhookHandler(
            self.config.webhook_config,
            self.accelerator,
            f&quot;{self.config.tracker_project_name} {self.config.tracker_run_name}&quot;,
            send_video=True if self.config.model_family in [&quot;ltxvideo&quot;] else False,
            args=self.config,
        )
        StateTracker.set_webhook_handler(self.webhook_handler)
        if send_startup_message:
            self._send_webhook_msg(
                message=&quot;SimpleTuner has launched. Hold onto your butts!&quot;,
                store_response=True,
            )
        self._send_webhook_raw(
            structured_data={
                &quot;message&quot;: &quot;Training job has started, configuration has begun.&quot;
            },
            message_type=&quot;configure_webhook&quot;,
        )
    def _misc_init(self):
        &quot;&quot;&quot;things that do not really need an order.&quot;&quot;&quot;
        torch.set_num_threads(self.config.torch_num_threads)
        self.state = {}
        self.state[&quot;lr&quot;] = 0.0
        # Global step represents the most recently *completed* optimization step, which means it
        #  takes into account the number of gradient_accumulation_steps. If we use 1 gradient_accumulation_step,
        #  then global_step and step will be the same throughout training. However, if we use
        #  2 gradient_accumulation_steps, then global_step will be twice as large as step, and so on.
        self.state[&quot;global_step&quot;] = 0
        self.state[&quot;global_resume_step&quot;] = 0
        self.state[&quot;first_epoch&quot;] = 1
        self.timesteps_buffer = []
        self.guidance_values_list = []
        self.train_loss = 0.0
        self.bf = None
        self.grad_norm = None
        self.extra_lr_scheduler_kwargs = {}
        StateTracker.set_global_step(self.state[&quot;global_step&quot;])
        self.config.use_deepspeed_optimizer, self.config.use_deepspeed_scheduler = (
            prepare_model_for_deepspeed(self.accelerator, self.config)
        )
        self.config.base_weight_dtype = self.config.weight_dtype
        self.config.is_quanto = False
        self.config.is_torchao = False
        self.config.is_bnb = False
        if &quot;quanto&quot; in self.config.base_model_precision:
            self.config.is_quanto = True
        elif &quot;torchao&quot; in self.config.base_model_precision:
            self.config.is_torchao = True
        elif &quot;bnb&quot; in self.config.base_model_precision:
            self.config.is_bnb = True
        if self.config.is_quanto or self.config.is_torchao:
            from helpers.training.quantisation import quantise_model
            self.quantise_model = quantise_model
    def set_model_family(self, model_family: str = None):
        model_family = getattr(self.config, &quot;model_family&quot;, model_family)
        if model_family not in model_classes[&quot;full&quot;]:
            raise ValueError(f&quot;Invalid model family specified: {model_family}&quot;)
        self._set_model_paths()
        StateTracker.set_model_family(model_family)
        self.config.model_type_label = model_labels[model_family.lower()]
        if StateTracker.is_sdxl_refiner():
            self.config.model_type_label = &quot;SDXL Refiner&quot;
    def init_clear_backend_cache(self):
        if self.config.output_dir is not None:
            os.makedirs(self.config.output_dir, exist_ok=True)
        if self.config.preserve_data_backend_cache:
            return
        StateTracker.delete_cache_files(
            preserve_data_backend_cache=self.config.preserve_data_backend_cache
        )
    def init_seed(self):
        if self.config.seed is not None and self.config.seed != 0:
            set_seed(self.config.seed, self.config.seed_for_each_device)
    def init_huggingface_hub(self, access_token: str = None):
        # Handle the repository creation
        self.hub_manager = None
        if not self.accelerator.is_main_process or not self.config.push_to_hub:
            return
        if access_token:
            huggingface_hub.login(token=access_token)
        self.hub_manager = HubManager(config=self.config)
        try:
            StateTracker.set_hf_user(huggingface_hub.whoami())
            logger.info(
                f&quot;Logged into Hugging Face Hub as &apos;{StateTracker.get_hf_username()}&apos;&quot;
            )
        except Exception as e:
            logger.error(f&quot;Failed to log into Hugging Face Hub: {e}&quot;)
            raise e
    def _set_model_paths(self):
        self.config.vae_path = (
            self.config.pretrained_model_name_or_path
            if self.config.pretrained_vae_model_name_or_path is None
            else self.config.pretrained_vae_model_name_or_path
        )
        self.config.text_encoder_path, self.config.text_encoder_subfolder = (
            determine_te_path_subfolder(self.config)
        )
        self.config.text_encoder_path = get_model_config_path(
            self.config.model_family, self.config.text_encoder_path
        )
    def init_preprocessing_models(self, move_to_accelerator: bool = True):
        # image embeddings
        self.init_vae(move_to_accelerator=move_to_accelerator)
        # text embeds
        self.init_text_encoder(move_to_accelerator=move_to_accelerator)
    def init_vae(self, move_to_accelerator: bool = True):
        logger.info(f&quot;Load VAE: {self.config.vae_path}&quot;)
        self.config.vae_kwargs = {
            &quot;pretrained_model_name_or_path&quot;: self.config.vae_path,
            &quot;subfolder&quot;: &quot;vae&quot;,
            &quot;revision&quot;: self.config.revision,
            &quot;force_upcast&quot;: False,
            &quot;variant&quot;: self.config.variant,
        }
        if StateTracker.get_args().model_family == &quot;sana&quot;:
            from diffusers import AutoencoderDC as AutoencoderClass
        elif StateTracker.get_args().model_family == &quot;ltxvideo&quot;:
            from diffusers import AutoencoderKLLTXVideo as AutoencoderClass
        else:
            from diffusers import AutoencoderKL as AutoencoderClass
        self.vae_cls = AutoencoderClass
        with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
            try:
                self.vae = self.vae_cls.from_pretrained(**self.config.vae_kwargs)
            except Exception as e:
                logger.warning(
                    &quot;Couldn&apos;t load VAE with default path. Trying without a subfolder..&quot;
                )
                logger.error(e)
                self.config.vae_kwargs[&quot;subfolder&quot;] = None
                self.vae = self.vae_cls.from_pretrained(**self.config.vae_kwargs)
        if (
            self.vae is not None
            and self.config.vae_enable_tiling
            and hasattr(self.vae, &quot;enable_tiling&quot;)
        ):
            logger.warning(
                &quot;Enabling VAE tiling for greatly reduced memory consumption due to --vae_enable_tiling which may result in VAE tiling artifacts in encoded latents.&quot;
            )
            self.vae.enable_tiling()
        if not move_to_accelerator:
            logger.debug(&quot;Not moving VAE to accelerator.&quot;)
            return
        if self.vae is not None:
            # The VAE is in bfloat16 to avoid NaN losses.
            _vae_dtype = torch.bfloat16
            if hasattr(self.config, &quot;vae_dtype&quot;):
                # Let&apos;s use a case-switch for convenience: bf16, fp16, fp32, none/default
                if self.config.vae_dtype == &quot;bf16&quot;:
                    _vae_dtype = torch.bfloat16
                elif self.config.vae_dtype == &quot;fp16&quot;:
                    raise ValueError(
                        &quot;fp16 is not supported for SDXL&apos;s VAE. Please use bf16 or fp32.&quot;
                    )
                elif self.config.vae_dtype == &quot;fp32&quot;:
                    _vae_dtype = torch.float32
                elif (
                    self.config.vae_dtype == &quot;none&quot;
                    or self.config.vae_dtype == &quot;default&quot;
                ):
                    _vae_dtype = torch.bfloat16
            logger.info(
                f&quot;Loading VAE onto accelerator, converting from {self.vae.dtype} to {_vae_dtype}&quot;
            )
            self.vae.to(self.accelerator.device, dtype=_vae_dtype)
            StateTracker.set_vae_dtype(_vae_dtype)
            StateTracker.set_vae(self.vae)
    def init_text_tokenizer(self):
        logger.info(&quot;Load tokenizers&quot;)
        self.tokenizer_1, self.tokenizer_2, self.tokenizer_3 = get_tokenizers(
            self.config
        )
        self.tokenizers = [self.tokenizer_1, self.tokenizer_2, self.tokenizer_3]
    def init_text_encoder(self, move_to_accelerator: bool = True):
        self.init_text_tokenizer()
        self.text_encoder_1, self.text_encoder_2, self.text_encoder_3 = None, None, None
        self.text_encoder_cls_1, self.text_encoder_cls_2, self.text_encoder_cls_3 = (
            None,
            None,
            None,
        )
        with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
            if self.tokenizer_1 is not None:
                self.text_encoder_cls_1 = import_model_class_from_model_name_or_path(
                    self.config.text_encoder_path,
                    self.config.revision,
                    self.config,
                    subfolder=self.config.text_encoder_subfolder,
                )
            if self.tokenizer_2 is not None:
                self.text_encoder_cls_2 = import_model_class_from_model_name_or_path(
                    self.config.pretrained_model_name_or_path,
                    self.config.revision,
                    self.config,
                    subfolder=&quot;text_encoder_2&quot;,
                )
            if self.tokenizer_3 is not None and self.config.model_family == &quot;sd3&quot;:
                self.text_encoder_cls_3 = import_model_class_from_model_name_or_path(
                    self.config.pretrained_model_name_or_path,
                    self.config.revision,
                    self.config,
                    subfolder=&quot;text_encoder_3&quot;,
                )
            tokenizers = [self.tokenizer_1, self.tokenizer_2, self.tokenizer_3]
            text_encoder_classes = [
                self.text_encoder_cls_1,
                self.text_encoder_cls_2,
                self.text_encoder_cls_3,
            ]
            (
                text_encoder_variant,
                self.text_encoder_1,
                self.text_encoder_2,
                self.text_encoder_3,
            ) = load_tes(
                args=self.config,
                text_encoder_classes=text_encoder_classes,
                weight_dtype=self.config.weight_dtype,
                tokenizers=tokenizers,
                text_encoder_path=self.config.text_encoder_path,
                text_encoder_subfolder=self.config.text_encoder_subfolder,
            )
        if not move_to_accelerator:
            logger.debug(&quot;Not moving text encoders to accelerator.&quot;)
            return
        self.text_encoders = []
        self.tokenizers = []
        if self.tokenizer_1 is not None:
            logger.info(&quot;Moving text encoder to GPU.&quot;)
            self.text_encoder_1.to(
                self.accelerator.device, dtype=self.config.weight_dtype
            )
            self.tokenizers.append(self.tokenizer_1)
            self.text_encoders.append(self.text_encoder_1)
        if self.tokenizer_2 is not None:
            logger.info(&quot;Moving text encoder 2 to GPU.&quot;)
            self.text_encoder_2.to(
                self.accelerator.device, dtype=self.config.weight_dtype
            )
            self.tokenizers.append(self.tokenizer_2)
            self.text_encoders.append(self.text_encoder_2)
        if self.tokenizer_3 is not None:
            logger.info(&quot;Moving text encoder 3 to GPU.&quot;)
            self.text_encoder_3.to(
                self.accelerator.device, dtype=self.config.weight_dtype
            )
            self.tokenizers.append(self.tokenizer_3)
            self.text_encoders.append(self.text_encoder_3)
    def init_freeze_models(self):
        # Freeze vae and text_encoders
        if self.vae is not None:
            self.vae.requires_grad_(False)
        if self.text_encoder_1 is not None:
            self.text_encoder_1.requires_grad_(False)
        if self.text_encoder_2 is not None:
            self.text_encoder_2.requires_grad_(False)
        if self.text_encoder_3 is not None:
            self.text_encoder_3.requires_grad_(False)
        if &quot;lora&quot; in self.config.model_type or self.config.controlnet:
            if self.transformer is not None:
                self.transformer.requires_grad_(False)
            if self.unet is not None:
                self.unet.requires_grad_(False)
        self.accelerator.wait_for_everyone()
    def init_load_base_model(self):
        webhook_msg = f&quot;Loading model: `{self.config.pretrained_model_name_or_path}`...&quot;
        self._send_webhook_msg(message=webhook_msg)
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: webhook_msg},
            message_type=&quot;init_load_base_model_begin&quot;,
        )
        self.unet, self.transformer = load_diffusion_model(
            self.config, self.config.weight_dtype
        )
        self.accelerator.wait_for_everyone()
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: &quot;Base model has loaded.&quot;},
            message_type=&quot;init_load_base_model_completed&quot;,
        )
    def init_data_backend(self):
        try:
            self.init_clear_backend_cache()
            self._send_webhook_msg(
                message=&quot;Configuring data backends... (this may take a while!)&quot;
            )
            self._send_webhook_raw(
                structured_data={&quot;message&quot;: &quot;Configuring data backends.&quot;},
                message_type=&quot;init_data_backend_begin&quot;,
            )
            configure_multi_databackend(
                self.config,
                accelerator=self.accelerator,
                text_encoders=self.text_encoders,
                tokenizers=self.tokenizers,
            )
            self._send_webhook_raw(
                structured_data={&quot;message&quot;: &quot;Completed configuring data backends.&quot;},
                message_type=&quot;init_data_backend_completed&quot;,
            )
        except Exception as e:
            import traceback
            logger.error(f&quot;{e}, traceback: {traceback.format_exc()}&quot;)
            self._send_webhook_msg(
                message=f&quot;Failed to load data backends: {e}&quot;,
                message_level=&quot;critical&quot;,
            )
            self._send_webhook_raw(
                structured_data={
                    &quot;message&quot;: f&quot;Failed to load data backends: {e}&quot;,
                    &quot;status&quot;: &quot;error&quot;,
                },
                message_type=&quot;fatal_error&quot;,
            )
            raise e
        try:
            self.init_validation_prompts()
        except Exception as e:
            logger.error(&quot;Could not generate validation prompts.&quot;)
            logger.error(e)
            raise e
        # We calculate the number of steps per epoch by dividing the number of images by the effective batch divisor.
        # Gradient accumulation steps mean that we only update the model weights every /n/ steps.
        collected_data_backend_str = list(StateTracker.get_data_backends().keys())
        if self.config.push_to_hub and self.accelerator.is_main_process:
            self.hub_manager.collected_data_backend_str = collected_data_backend_str
            self.hub_manager.set_validation_prompts(
                self.validation_prompts, self.validation_shortnames
            )
            logger.debug(f&quot;Collected validation prompts: {self.validation_prompts}&quot;)
        self._recalculate_training_steps()
        logger.info(
            f&quot;Collected the following data backends: {collected_data_backend_str}&quot;
        )
        self._send_webhook_msg(
            message=f&quot;Collected the following data backends: {collected_data_backend_str}&quot;
        )
        self._send_webhook_raw(
            structured_data={
                &quot;message&quot;: f&quot;Collected the following data backends: {collected_data_backend_str}&quot;
            },
            message_type=&quot;init_data_backend&quot;,
        )
        self.accelerator.wait_for_everyone()
    def init_validation_prompts(self):
        if (
            hasattr(self.accelerator, &quot;state&quot;)
            and hasattr(self.accelerator.state, &quot;deepspeed_plugin&quot;)
            and getattr(self.accelerator.state.deepspeed_plugin, &quot;deepspeed_config&quot;, {})
            .get(&quot;zero_optimization&quot;, {})
            .get(&quot;stage&quot;)
            == 3
        ):
            logger.error(&quot;Cannot run validations with DeepSpeed ZeRO stage 3.&quot;)
            return
        if self.accelerator.is_main_process:
            if self.config.model_family == &quot;flux&quot;:
                (
                    self.validation_prompts,
                    self.validation_shortnames,
                    self.validation_negative_prompt_embeds,
                    self.validation_negative_pooled_embeds,
                    self.validation_negative_time_ids,
                ) = prepare_validation_prompt_list(
                    args=self.config,
                    embed_cache=StateTracker.get_default_text_embed_cache(),
                )
            else:
                (
                    self.validation_prompts,
                    self.validation_shortnames,
                    self.validation_negative_prompt_embeds,
                    self.validation_negative_pooled_embeds,
                ) = prepare_validation_prompt_list(
                    args=self.config,
                    embed_cache=StateTracker.get_default_text_embed_cache(),
                )
        else:
            self.validation_prompts = None
            self.validation_shortnames = None
            self.validation_negative_prompt_embeds = None
            self.validation_negative_pooled_embeds = None
        self.accelerator.wait_for_everyone()
    def stats_memory_used(self):
        # Grab GPU memory used:
        if torch.cuda.is_available():
            curent_memory_allocated = torch.cuda.memory_allocated() / 1024**3
        elif torch.backends.mps.is_available():
            curent_memory_allocated = torch.mps.current_allocated_memory() / 1024**3
        else:
            logger.warning(
                &quot;CUDA, ROCm, or Apple MPS not detected here. We cannot report VRAM reductions.&quot;
            )
            curent_memory_allocated = 0
        return curent_memory_allocated
    def init_unload_text_encoder(self):
        if self.config.model_type != &quot;full&quot; and self.config.train_text_encoder:
            return
        memory_before_unload = self.stats_memory_used()
        if self.accelerator.is_main_process:
            logger.info(&quot;Unloading text encoders, as they are not being trained.&quot;)
        if self.text_encoder_1 is not None:
            self.text_encoder_1 = self.text_encoder_1.to(&quot;cpu&quot;)
        if self.text_encoder_2 is not None:
            self.text_encoder_2 = self.text_encoder_2.to(&quot;cpu&quot;)
        if self.text_encoder_3 is not None:
            self.text_encoder_3 = self.text_encoder_3.to(&quot;cpu&quot;)
        del self.text_encoder_1, self.text_encoder_2, self.text_encoder_3
        self.text_encoder_1, self.text_encoder_2, self.text_encoder_3 = None, None, None
        self.text_encoders = []
        for backend_id, backend in StateTracker.get_data_backends().items():
            if &quot;text_embed_cache&quot; in backend:
                backend[&quot;text_embed_cache&quot;].text_encoders = None
                backend[&quot;text_embed_cache&quot;].pipeline = None
        reclaim_memory()
        memory_after_unload = self.stats_memory_used()
        memory_saved = memory_after_unload - memory_before_unload
        logger.info(
            f&quot;After nuking text encoders from orbit, we freed {abs(round(memory_saved, 2))} GB of VRAM.&quot;
            &quot; The real memories were the friends we trained a model on along the way.&quot;
        )
    def init_precision(
        self, preprocessing_models_only: bool = False, ema_only: bool = False
    ):
        self.config.enable_adamw_bf16 = (
            True if self.config.weight_dtype == torch.bfloat16 else False
        )
        quantization_device = (
            &quot;cpu&quot; if self.config.quantize_via == &quot;cpu&quot; else self.accelerator.device
        )
        if &quot;bnb&quot; in self.config.base_model_precision:
            # can&apos;t cast or move bitsandbytes models
            return
        if not self.config.disable_accelerator and self.config.is_quantized:
            if self.config.base_model_default_dtype == &quot;fp32&quot;:
                self.config.base_weight_dtype = torch.float32
                self.config.enable_adamw_bf16 = False
            elif self.config.base_model_default_dtype == &quot;bf16&quot;:
                self.config.base_weight_dtype = torch.bfloat16
                self.config.enable_adamw_bf16 = True
            if not preprocessing_models_only:
                if self.unet is not None:
                    logger.info(
                        f&quot;Moving U-net to dtype={self.config.base_weight_dtype}, device={quantization_device}&quot;
                    )
                    self.unet.to(
                        quantization_device, dtype=self.config.base_weight_dtype
                    )
                elif self.transformer is not None:
                    logger.info(
                        f&quot;Moving transformer to dtype={self.config.base_weight_dtype}, device={quantization_device}&quot;
                    )
                    self.transformer.to(
                        quantization_device, dtype=self.config.base_weight_dtype
                    )
        if self.config.is_quanto:
            with self.accelerator.local_main_process_first():
                if ema_only:
                    self.quantise_model(ema=self.ema_model, args=self.config)
                    return
                self.quantise_model(
                    unet=self.unet if not preprocessing_models_only else None,
                    transformer=(
                        self.transformer if not preprocessing_models_only else None
                    ),
                    text_encoder_1=self.text_encoder_1,
                    text_encoder_2=self.text_encoder_2,
                    text_encoder_3=self.text_encoder_3,
                    controlnet=None,
                    ema=self.ema_model,
                    args=self.config,
                )
        elif self.config.is_torchao:
            with self.accelerator.local_main_process_first():
                if ema_only:
                    self.ema_model = self.quantise_model(
                        ema=self.ema_model, args=self.config, return_dict=True
                    )[&quot;ema&quot;]
                    return
                (
                    self.unet,
                    self.transformer,
                    self.text_encoder_1,
                    self.text_encoder_2,
                    self.text_encoder_3,
                    self.controlnet,
                    self.ema_model,
                ) = self.quantise_model(
                    unet=self.unet if not preprocessing_models_only else None,
                    transformer=(
                        self.transformer if not preprocessing_models_only else None
                    ),
                    text_encoder_1=self.text_encoder_1,
                    text_encoder_2=self.text_encoder_2,
                    text_encoder_3=self.text_encoder_3,
                    controlnet=None,
                    ema=self.ema_model,
                    args=self.config,
                )
    def init_controlnet_model(self):
        if not self.config.controlnet:
            return
        logger.info(&quot;Creating the controlnet..&quot;)
        if self.config.controlnet_model_name_or_path:
            logger.info(&quot;Loading existing controlnet weights&quot;)
            self.controlnet = ControlNetModel.from_pretrained(
                self.config.controlnet_model_name_or_path
            )
        else:
            logger.info(&quot;Initializing controlnet weights from unet&quot;)
            self.controlnet = ControlNetModel.from_unet(self.unet)
        self.accelerator.wait_for_everyone()
    def init_trainable_peft_adapter(self):
        if &quot;lora&quot; not in self.config.model_type:
            return
        if self.config.controlnet:
            raise ValueError(&quot;Cannot train LoRA with ControlNet.&quot;)
        if &quot;standard&quot; == self.config.lora_type.lower():
            lora_info_msg = f&quot;Using LoRA training mode (rank={self.config.lora_rank})&quot;
            logger.info(lora_info_msg)
            self._send_webhook_msg(message=lora_info_msg)
            target_modules = determine_adapter_target_modules(
                self.config, self.unet, self.transformer
            )
            addkeys, misskeys = [], []
            if self.unet is not None:
                unet_lora_config = LoraConfig(
                    r=self.config.lora_rank,
                    lora_alpha=(
                        self.config.lora_alpha
                        if self.config.lora_alpha is not None
                        else self.config.lora_rank
                    ),
                    lora_dropout=self.config.lora_dropout,
                    init_lora_weights=self.config.lora_initialisation_style,
                    target_modules=target_modules,
                    use_dora=self.config.use_dora,
                )
                logger.info(&quot;Adding LoRA adapter to the unet model..&quot;)
                self.unet.add_adapter(unet_lora_config)
                if self.config.init_lora:
                    addkeys, misskeys = load_lora_weights(
                        {&quot;unet&quot;: self.unet},
                        self.config.init_lora,
                        use_dora=self.config.use_dora,
                    )
            elif self.transformer is not None:
                transformer_lora_config = LoraConfig(
                    r=self.config.lora_rank,
                    lora_alpha=(
                        self.config.lora_alpha
                        if self.config.lora_alpha is not None
                        else self.config.lora_rank
                    ),
                    init_lora_weights=self.config.lora_initialisation_style,
                    target_modules=target_modules,
                    use_dora=self.config.use_dora,
                )
                self.transformer.add_adapter(transformer_lora_config)
                if self.config.init_lora:
                    addkeys, misskeys = load_lora_weights(
                        {&quot;transformer&quot;: self.transformer},
                        self.config.init_lora,
                        use_dora=self.config.use_dora,
                    )
            if addkeys:
                logger.warning(
                    &quot;The following keys were found in %s, but are not part of the model and are ignored:\n %s.\nThis is most likely an error&quot;
                    % (self.config.init_lora, str(addkeys))
                )
            if misskeys:
                logger.warning(
                    &quot;The following keys were part of the model but not found in %s:\n %s.\nThese keys will be initialized according to the lora weight initialisation. This could be an error, or intended behaviour in case a lora is finetuned with additional keys.&quot;
                    % (self.config.init_lora, str(misskeys))
                )
        elif &quot;lycoris&quot; == self.config.lora_type.lower():
            from lycoris import create_lycoris
            with open(self.config.lycoris_config, &quot;r&quot;) as f:
                self.lycoris_config = json.load(f)
            multiplier = int(self.lycoris_config[&quot;multiplier&quot;])
            linear_dim = int(self.lycoris_config[&quot;linear_dim&quot;])
            linear_alpha = int(self.lycoris_config[&quot;linear_alpha&quot;])
            apply_preset = self.lycoris_config.get(&quot;apply_preset&quot;, None)
            if apply_preset is not None and apply_preset != {}:
                LycorisNetwork.apply_preset(apply_preset)
            # Remove the positional arguments we extracted.
            del self.lycoris_config[&quot;multiplier&quot;]
            del self.lycoris_config[&quot;linear_dim&quot;]
            del self.lycoris_config[&quot;linear_alpha&quot;]
            logger.info(&quot;Using lycoris training mode&quot;)
            self._send_webhook_msg(message=&quot;Using lycoris training mode.&quot;)
            model_for_lycoris_wrap = None
            if self.transformer is not None:
                model_for_lycoris_wrap = self.transformer
            if self.unet is not None:
                model_for_lycoris_wrap = self.unet
            if self.config.init_lora is not None:
                from lycoris import create_lycoris_from_weights
                self.lycoris_wrapped_network = create_lycoris_from_weights(
                    multiplier,
                    self.config.init_lora,
                    model_for_lycoris_wrap,
                    weights_sd=None,
                    **self.lycoris_config,
                )[0]
            else:
                self.lycoris_wrapped_network = create_lycoris(
                    model_for_lycoris_wrap,
                    multiplier,
                    linear_dim,
                    linear_alpha,
                    **self.lycoris_config,
                )
                if self.config.init_lokr_norm is not None:
                    init_lokr_network_with_perturbed_normal(
                        self.lycoris_wrapped_network,
                        scale=self.config.init_lokr_norm,
                    )
            self.lycoris_wrapped_network.apply_to()
            setattr(
                self.accelerator,
                &quot;_lycoris_wrapped_network&quot;,
                self.lycoris_wrapped_network,
            )
            lycoris_num_params = sum(
                p.numel() for p in self.lycoris_wrapped_network.parameters()
            )
            logger.info(
                f&quot;LyCORIS network has been initialized with {lycoris_num_params:,} parameters&quot;
            )
        self.accelerator.wait_for_everyone()
    def init_post_load_freeze(self):
        if self.config.layer_freeze_strategy == &quot;bitfit&quot;:
            from helpers.training.model_freeze import apply_bitfit_freezing
            if self.unet is not None:
                logger.info(&quot;Applying BitFit freezing strategy to the U-net.&quot;)
                self.unet = apply_bitfit_freezing(
                    unwrap_model(self.accelerator, self.unet), self.config
                )
            if self.transformer is not None:
                logger.warning(
                    &quot;Training DiT models with BitFit is not yet tested, and unexpected results may occur.&quot;
                )
                self.transformer = apply_bitfit_freezing(
                    unwrap_model(self.accelerator, self.transformer), self.config
                )
        self.enable_gradient_checkpointing()
    def enable_gradient_checkpointing(self):
        if self.config.gradient_checkpointing:
            logger.debug(&quot;Enabling gradient checkpointing.&quot;)
            if self.unet is not None:
                unwrap_model(
                    self.accelerator, self.unet
                ).enable_gradient_checkpointing()
            if self.transformer is not None and self.config.model_family != &quot;smoldit&quot;:
                unwrap_model(
                    self.accelerator, self.transformer
                ).enable_gradient_checkpointing()
            if self.config.controlnet:
                unwrap_model(
                    self.accelerator, self.controlnet
                ).enable_gradient_checkpointing()
            if (
                hasattr(self.config, &quot;train_text_encoder&quot;)
                and self.config.train_text_encoder
            ):
                unwrap_model(
                    self.accelerator, self.text_encoder_1
                ).gradient_checkpointing_enable()
                unwrap_model(
                    self.accelerator, self.text_encoder_2
                ).gradient_checkpointing_enable()
    def disable_gradient_checkpointing(self):
        if self.config.gradient_checkpointing:
            logger.debug(&quot;Disabling gradient checkpointing.&quot;)
            if self.unet is not None:
                unwrap_model(
                    self.accelerator, self.unet
                ).disable_gradient_checkpointing()
            if self.transformer is not None and self.config.model_family != &quot;smoldit&quot;:
                unwrap_model(
                    self.accelerator, self.transformer
                ).disable_gradient_checkpointing()
            if self.config.controlnet:
                unwrap_model(
                    self.accelerator, self.controlnet
                ).disable_gradient_checkpointing()
            if (
                hasattr(self.config, &quot;train_text_encoder&quot;)
                and self.config.train_text_encoder
            ):
                unwrap_model(
                    self.accelerator, self.text_encoder_1
                ).gradient_checkpointing_disable()
                unwrap_model(
                    self.accelerator, self.text_encoder_2
                ).gradient_checkpointing_disable()
    def _get_trainable_parameters(self):
        # Return just a list of the currently trainable parameters.
        if self.config.model_type == &quot;lora&quot;:
            if self.config.lora_type == &quot;lycoris&quot;:
                return self.lycoris_wrapped_network.parameters()
        if self.config.controlnet:
            return [
                param for param in self.controlnet.parameters() if param.requires_grad
            ]
        if self.unet is not None:
            return [param for param in self.unet.parameters() if param.requires_grad]
        if self.transformer is not None:
            return [
                param for param in self.transformer.parameters() if param.requires_grad
            ]
    def _recalculate_training_steps(self):
        # Scheduler and math around the number of training steps.
        if not hasattr(self.config, &quot;overrode_max_train_steps&quot;):
            self.config.overrode_max_train_steps = False
        self.config.total_num_batches = sum(
            [
                len(
                    backend[&quot;metadata_backend&quot;] if &quot;metadata_backend&quot; in backend else []
                )
                for _, backend in StateTracker.get_data_backends().items()
            ]
        )
        self.config.num_update_steps_per_epoch = math.ceil(
            self.config.total_num_batches / self.config.gradient_accumulation_steps
        )
        if getattr(self.config, &quot;overrode_max_train_steps&quot;, False):
            self.config.max_train_steps = (
                self.config.num_train_epochs * self.config.num_update_steps_per_epoch
            )
            # Afterwards we recalculate our number of training epochs
            self.config.num_train_epochs = math.ceil(
                self.config.max_train_steps / self.config.num_update_steps_per_epoch
            )
            logger.info(
                &quot;After removing any undesired samples and updating cache entries, we have settled on&quot;
                f&quot; {self.config.num_train_epochs} epochs and {self.config.num_update_steps_per_epoch} steps per epoch.&quot;
            )
        if self.config.max_train_steps is None or self.config.max_train_steps == 0:
            if (
                self.config.num_train_epochs is None
                or self.config.num_train_epochs == 0
            ):
                raise ValueError(
                    &quot;You must specify either --max_train_steps or --num_train_epochs with a value &gt; 0&quot;
                )
            self.config.max_train_steps = (
                self.config.num_train_epochs * self.config.num_update_steps_per_epoch
            )
            logger.info(
                f&quot;Calculated our maximum training steps at {self.config.max_train_steps} because we have&quot;
                f&quot; {self.config.num_train_epochs} epochs and {self.config.num_update_steps_per_epoch} steps per epoch.&quot;
            )
            self.config.overrode_max_train_steps = True
        elif self.config.num_train_epochs is None or self.config.num_train_epochs == 0:
            if self.config.max_train_steps is None or self.config.max_train_steps == 0:
                raise ValueError(
                    &quot;You must specify either --max_train_steps or --num_train_epochs with a value &gt; 0&quot;
                )
            self.config.num_train_epochs = math.ceil(
                self.config.max_train_steps
                / max(self.config.num_update_steps_per_epoch, 1)
            )
            logger.info(
                f&quot;Calculated our maximum training steps at {self.config.max_train_steps} because we have&quot;
                f&quot; {self.config.num_train_epochs} epochs and {self.config.num_update_steps_per_epoch} steps per epoch.&quot;
            )
        if self.lr_scheduler is not None and hasattr(
            self.lr_scheduler, &quot;num_update_steps_per_epoch&quot;
        ):
            self.lr_scheduler.num_update_steps_per_epoch = (
                self.config.num_update_steps_per_epoch
            )
        self.config.total_batch_size = (
            self.config.train_batch_size
            * self.accelerator.num_processes
            * self.config.gradient_accumulation_steps
        )
    def init_optimizer(self):
        logger.info(f&quot;Learning rate: {self.config.learning_rate}&quot;)
        extra_optimizer_args = {&quot;lr&quot;: self.config.learning_rate}
        # Initialize the optimizer
        optimizer_args_from_config, optimizer_class = (
            determine_optimizer_class_with_config(
                args=self.config,
                use_deepspeed_optimizer=self.config.use_deepspeed_optimizer,
                is_quantized=self.config.is_quantized,
                enable_adamw_bf16=self.config.enable_adamw_bf16,
            )
        )
        extra_optimizer_args.update(optimizer_args_from_config)
        self.params_to_optimize = determine_params_to_optimize(
            args=self.config,
            controlnet=self.controlnet,
            unet=self.unet,
            transformer=self.transformer,
            text_encoder_1=self.text_encoder_1,
            text_encoder_2=self.text_encoder_2,
            model_type_label=self.config.model_type_label,
            lycoris_wrapped_network=self.lycoris_wrapped_network,
        )
        if self.config.use_deepspeed_optimizer:
            logger.info(
                f&quot;DeepSpeed Optimizer arguments, weight_decay={self.config.adam_weight_decay} eps={self.config.adam_epsilon}, extra_arguments={extra_optimizer_args}&quot;
            )
            self.optimizer = optimizer_class(self.params_to_optimize)
        else:
            logger.info(f&quot;Optimizer arguments={extra_optimizer_args}&quot;)
            if self.config.train_text_encoder and self.config.text_encoder_lr:
                # changes the learning rate of text_encoder_parameters_one and text_encoder_parameters_two to be
                # --learning_rate
                self.params_to_optimize[1][&quot;lr&quot;] = float(self.config.learning_rate)
                if self.text_encoder_2 is not None:
                    self.params_to_optimize[2][&quot;lr&quot;] = float(self.config.learning_rate)
            self.optimizer = cpu_offload_optimizer(
                params_to_optimize=self.params_to_optimize,
                optimizer_cls=optimizer_class,
                optimizer_parameters=extra_optimizer_args,
                fused=self.config.fuse_optimizer,
                offload_gradients=self.config.optimizer_offload_gradients,
                offload_mechanism=self.config.optimizer_cpu_offload_method,
            )
        if (
            is_optimi_available
            and self.config.optimizer_release_gradients
            and &quot;optimi&quot; in self.config.optimizer
        ):
            logger.warning(
                &quot;Marking model for gradient release. This feature is experimental, and may use more VRAM or not work.&quot;
            )
            prepare_for_gradient_release(
                (
                    self.controlnet
                    if self.config.controlnet
                    else self.transformer if self.transformer is not None else self.unet
                ),
                self.optimizer,
            )
    def init_lr_scheduler(self):
        self.config.is_schedulefree = is_lr_schedulefree(self.config.optimizer)
        self.config.is_lr_scheduler_disabled = (
            is_lr_scheduler_disabled(self.config.optimizer)
            or self.config.use_deepspeed_scheduler
        )
        if self.config.is_schedulefree:
            logger.info(&quot;Using experimental ScheduleFree optimiser..&quot;)
        if self.config.is_lr_scheduler_disabled:
            # we don&apos;t use LR schedulers with schedulefree optimisers
            logger.info(&quot;Optimiser cannot use an LR scheduler, so we are disabling it.&quot;)
            lr_scheduler = None
            logger.info(f&quot;Using dummy learning rate scheduler&quot;)
            if torch.backends.mps.is_available():
                lr_scheduler = None
            else:
                lr_scheduler = accelerate.utils.DummyScheduler(
                    self.optimizer,
                    total_num_steps=self.config.max_train_steps,
                    warmup_num_steps=self.config.lr_warmup_steps,
                )
            return lr_scheduler
        logger.info(
            f&quot;Loading {self.config.lr_scheduler} learning rate scheduler with {self.config.lr_warmup_steps} warmup steps&quot;
        )
        lr_scheduler = get_lr_scheduler(
            self.config,
            self.optimizer,
            self.accelerator,
            logger,
            use_deepspeed_scheduler=False,
        )
        if hasattr(lr_scheduler, &quot;num_update_steps_per_epoch&quot;):
            lr_scheduler.num_update_steps_per_epoch = (
                self.config.num_update_steps_per_epoch
            )
        if hasattr(lr_scheduler, &quot;last_step&quot;):
            lr_scheduler.last_step = self.state.get(&quot;global_resume_step&quot;, 0)
        return lr_scheduler
    def init_ema_model(self):
        # Create EMA for the unet.
        self.ema_model = None
        if not self.config.use_ema:
            return
        if self.accelerator.is_main_process:
            logger.info(&quot;Using EMA. Creating EMAModel.&quot;)
            ema_model_cls = None
            ema_model_config = None
            if self.config.controlnet:
                ema_model_cls = self.controlnet.__class__
                ema_model_config = self.controlnet.config
            elif self.unet is not None:
                ema_model_cls = self.unet.__class__
                ema_model_config = self.unet.config
            elif self.transformer is not None:
                ema_model_cls = self.transformer.__class__
                ema_model_config = self.transformer.config
            else:
                raise ValueError(
                    f&quot;Please open a bug report or disable EMA. Unknown EMA model family: {self.config.model_family}&quot;
                )
            self.ema_model = EMAModel(
                self.config,
                self.accelerator,
                parameters=self._get_trainable_parameters(),
                model_cls=ema_model_cls,
                model_config=ema_model_config,
                decay=self.config.ema_decay,
                foreach=not self.config.ema_foreach_disable,
            )
            logger.info(
                f&quot;EMA model creation completed with {self.ema_model.parameter_count():,} parameters&quot;
            )
        self.accelerator.wait_for_everyone()
    def init_hooks(self):
        from helpers.training.save_hooks import SaveHookManager
        self.model_hooks = SaveHookManager(
            args=self.config,
            unet=self.unet,
            transformer=self.transformer,
            ema_model=self.ema_model,
            accelerator=self.accelerator,
            text_encoder_1=self.text_encoder_1,
            text_encoder_2=self.text_encoder_2,
            use_deepspeed_optimizer=self.config.use_deepspeed_optimizer,
        )
        self.accelerator.register_save_state_pre_hook(self.model_hooks.save_model_hook)
        self.accelerator.register_load_state_pre_hook(self.model_hooks.load_model_hook)
    def init_prepare_models(self, lr_scheduler):
        # Prepare everything with our `accelerator`.
        logger.info(&quot;Preparing models..&quot;)
        # TODO: Is this still needed? Seems like a hack job from January 2024.
        self.train_dataloaders = []
        for _, backend in StateTracker.get_data_backends().items():
            if &quot;train_dataloader&quot; not in backend:
                continue
            self.train_dataloaders.append(backend[&quot;train_dataloader&quot;])
            break
        if len(self.train_dataloaders) == 0:
            logger.error(&quot;For some reason, no dataloaders were configured.&quot;)
            sys.exit(0)
        if self.config.disable_accelerator:
            logger.warning(
                &quot;Because SIMPLETUNER_DISABLE_ACCELERATOR is set, we will not prepare the accelerator.&quot;
            )
            return
        logger.info(&quot;Loading our accelerator...&quot;)
        if torch.backends.mps.is_available():
            self.accelerator.native_amp = False
        self._send_webhook_msg(message=&quot;Moving weights to GPU...&quot;)
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: &quot;Moving weights to GPU&quot;},
            message_type=&quot;init_prepare_models_begin&quot;,
        )
        primary_model = self.unet if self.unet is not None else self.transformer
        if self.config.controlnet:
            primary_model = self.controlnet
        results = self.accelerator.prepare(
            primary_model, lr_scheduler, self.optimizer, self.train_dataloaders[0]
        )
        if self.config.controlnet:
            self.controlnet = results[0]
        elif self.unet is not None:
            self.unet = results[0]
        elif self.transformer is not None:
            self.transformer = results[0]
        if self.config.unet_attention_slice:
            if torch.backends.mps.is_available():
                logger.warning(
                    &quot;Using attention slicing when training SDXL on MPS can result in NaN errors on the first backward pass. If you run into issues, disable this option and reduce your batch size instead to reduce memory consumption.&quot;
                )
            if self.unet is not None:
                self.unet.set_attention_slice(&quot;auto&quot;)
            if self.transformer is not None:
                self.transformer.set_attention_slice(&quot;auto&quot;)
        self.lr_scheduler = results[1]
        self.optimizer = results[2]
        # The rest of the entries are dataloaders:
        self.train_dataloaders = [results[3:]]
        if self.config.use_ema and self.ema_model is not None:
            if self.config.ema_device == &quot;accelerator&quot;:
                logger.info(&quot;Moving EMA model weights to accelerator...&quot;)
            print(f&quot;EMA model: {self.ema_model}&quot;)
            self.ema_model.to(
                (
                    self.accelerator.device
                    if self.config.ema_device == &quot;accelerator&quot;
                    else &quot;cpu&quot;
                ),
                dtype=self.config.weight_dtype,
            )
            if self.config.ema_device == &quot;cpu&quot; and not self.config.ema_cpu_only:
                logger.info(&quot;Pinning EMA model weights to CPU...&quot;)
                try:
                    self.ema_model.pin_memory()
                except Exception as e:
                    self._send_webhook_raw(
                        structured_data={&quot;message&quot;: f&quot;Failed to pin EMA to CPU: {e}&quot;},
                        message_type=&quot;error&quot;,
                    )
                    logger.error(f&quot;Failed to pin EMA model to CPU: {e}&quot;)
        idx_count = 0
        for _, backend in StateTracker.get_data_backends().items():
            if idx_count == 0 or &quot;train_dataloader&quot; not in backend:
                continue
            self.train_dataloaders.append(
                self.accelerator.prepare(backend[&quot;train_dataloader&quot;])
            )
        idx_count = 0
        if &quot;lora&quot; in self.config.model_type and self.config.train_text_encoder:
            logger.info(&quot;Preparing text encoders for training.&quot;)
            if self.config.model_family == &quot;sd3&quot;:
                logger.info(&quot;NOTE: The third text encoder is not trained for SD3.&quot;)
            self.text_encoder_1, self.text_encoder_2 = self.accelerator.prepare(
                self.text_encoder_1, self.text_encoder_2
            )
        self._recalculate_training_steps()
        self.accelerator.wait_for_everyone()
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: &quot;Completed moving weights to GPU&quot;},
            message_type=&quot;init_prepare_models_completed&quot;,
        )
    def init_unload_vae(self):
        if self.config.keep_vae_loaded or self.config.vae_cache_ondemand:
            return
        memory_before_unload = self.stats_memory_used()
        self.vae = self.vae.to(&quot;cpu&quot;)
        del self.vae
        self.vae = None
        for _, backend in StateTracker.get_data_backends().items():
            if &quot;vaecache&quot; in backend:
                backend[&quot;vaecache&quot;].vae = None
        reclaim_memory()
        memory_after_unload = self.stats_memory_used()
        memory_saved = memory_after_unload - memory_before_unload
        logger.info(
            f&quot;After nuking the VAE from orbit, we freed {abs(round(memory_saved, 2)) * 1024} MB of VRAM.&quot;
        )
    def init_validations(self):
        if (
            hasattr(self.accelerator, &quot;state&quot;)
            and hasattr(self.accelerator.state, &quot;deepspeed_plugin&quot;)
            and getattr(self.accelerator.state.deepspeed_plugin, &quot;deepspeed_config&quot;, {})
            .get(&quot;zero_optimization&quot;, {})
            .get(&quot;stage&quot;)
            == 3
        ):
            logger.error(&quot;Cannot run validations with DeepSpeed ZeRO stage 3.&quot;)
            return
        self.evaluation = None
        if (
            self.config.eval_steps_interval is not None
            and self.config.eval_steps_interval &gt; 0
        ):
            from helpers.training.validation import Evaluation
            self.evaluation = Evaluation(accelerator=self.accelerator)
        model_evaluator = ModelEvaluator.from_config(args=self.config)
        self.validation = Validation(
            trainable_parameters=self._get_trainable_parameters,
            accelerator=self.accelerator,
            unet=self.unet,
            transformer=self.transformer,
            args=self.config,
            validation_prompts=self.validation_prompts,
            validation_shortnames=self.validation_shortnames,
            text_encoder_1=self.text_encoder_1,
            tokenizer=self.tokenizer_1,
            vae_path=self.config.vae_path,
            weight_dtype=self.config.weight_dtype,
            embed_cache=StateTracker.get_default_text_embed_cache(),
            validation_negative_pooled_embeds=self.validation_negative_pooled_embeds,
            validation_negative_prompt_embeds=self.validation_negative_prompt_embeds,
            text_encoder_2=self.text_encoder_2,
            tokenizer_2=self.tokenizer_2,
            text_encoder_3=self.text_encoder_3,
            tokenizer_3=self.tokenizer_3,
            ema_model=self.ema_model,
            vae=self.vae,
            controlnet=self.controlnet if self.config.controlnet else None,
            model_evaluator=model_evaluator,
            is_deepspeed=self.config.use_deepspeed_optimizer,
        )
        if not self.config.train_text_encoder and self.validation is not None:
            self.validation.clear_text_encoders()
        self.init_benchmark_base_model()
        self.accelerator.wait_for_everyone()
    def init_benchmark_base_model(self):
        if (
            self.config.disable_benchmark
            or self.validation is None
            or self.validation.benchmark_exists(&quot;base_model&quot;)
        ):
            # if we&apos;ve disabled it or the benchmark exists, we will not do it again.
            # deepspeed zero3 can&apos;t do validations at all.
            return
        if not self.accelerator.is_main_process:
            return
        logger.info(
            &quot;Benchmarking base model for comparison. Supply `--disable_benchmark: true` to disable this behaviour.&quot;
        )
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: &quot;Base model benchmark begins&quot;},
            message_type=&quot;init_benchmark_base_model_begin&quot;,
        )
        # we&apos;ll run validation on base model if it hasn&apos;t already.
        self.validation.run_validations(validation_type=&quot;base_model&quot;, step=0)
        self.validation.save_benchmark(&quot;base_model&quot;)
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: &quot;Base model benchmark completed&quot;},
            message_type=&quot;init_benchmark_base_model_completed&quot;,
        )
    def init_resume_checkpoint(self, lr_scheduler):
        # Potentially load in the weights and states from a previous save
        self.config.total_steps_remaining_at_start = self.config.max_train_steps
        self.state[&quot;current_epoch&quot;] = self.state[&quot;first_epoch&quot;]
        self.state[&quot;global_resume_step&quot;] = self.state[&quot;global_step&quot;] = (
            StateTracker.get_global_step()
        )
        StateTracker.set_global_resume_step(self.state[&quot;global_resume_step&quot;])
        if not self.config.resume_from_checkpoint:
            return lr_scheduler
        if self.config.resume_from_checkpoint != &quot;latest&quot;:
            path = os.path.basename(self.config.resume_from_checkpoint)
        else:
            # Get the most recent checkpoint
            dirs = os.listdir(self.config.output_dir)
            dirs = [d for d in dirs if d.startswith(&quot;checkpoint&quot;)]
            dirs = sorted(dirs, key=lambda x: int(x.split(&quot;-&quot;)[1]))
            path = dirs[-1] if len(dirs) &gt; 0 else None
        if path is None:
            logger.info(
                f&quot;Checkpoint &apos;{self.config.resume_from_checkpoint}&apos; does not exist. Starting a new training run.&quot;
            )
            self._send_webhook_raw(
                structured_data={
                    &quot;message&quot;: &quot;No model to resume. Beginning fresh training run.&quot;
                },
                message_type=&quot;init_resume_checkpoint&quot;,
            )
            self.config.resume_from_checkpoint = None
            return lr_scheduler
        logger.info(f&quot;Resuming from checkpoint {path}&quot;)
        self.accelerator.load_state(os.path.join(self.config.output_dir, path))
        try:
            if (
                &quot;constant&quot; == self.config.lr_scheduler
                and not self.config.is_schedulefree
            ):
                for g in self.optimizer.param_groups:
                    if &quot;lr&quot; in g:
                        g[&quot;lr&quot;] = self.config.learning_rate
                for k, v in lr_scheduler.state_dict().items():
                    if k in (&quot;base_lrs&quot;, &quot;_last_lr&quot;):
                        v[0] = self.config.learning_rate
        except Exception as e:
            self._send_webhook_raw(
                structured_data={
                    &quot;message&quot;: &quot;Could not update learning rate scheduler LR value.&quot;
                },
                message_type=&quot;warning&quot;,
            )
            logger.error(
                f&quot;Could not update lr_scheduler {self.config.lr_scheduler} learning rate to {self.config.learning_rate} upon resume: {e}&quot;
            )
        self._send_webhook_raw(
            structured_data={&quot;message&quot;: f&quot;Resuming model: {path}&quot;},
            message_type=&quot;init_resume_checkpoint&quot;,
        )
        training_state_filename = f&quot;training_state.json&quot;
        if get_rank() &gt; 0:
            training_state_filename = f&quot;training_state-{get_rank()}.json&quot;
        for _, backend in StateTracker.get_data_backends().items():
            if &quot;sampler&quot; in backend:
                backend[&quot;sampler&quot;].load_states(
                    state_path=os.path.join(
                        self.config.output_dir,
                        path,
                        training_state_filename,
                    ),
                )
        self.state[&quot;global_resume_step&quot;] = self.state[&quot;global_step&quot;] = (
            StateTracker.get_global_step()
        )
        StateTracker.set_global_resume_step(self.state[&quot;global_resume_step&quot;])
        training_state_in_ckpt = StateTracker.get_training_state()
        self._send_webhook_raw(
            structured_data=training_state_in_ckpt,
            message_type=&quot;init_resume_checkpoint_details&quot;,
        )
        logger.debug(f&quot;Training state inside checkpoint: {training_state_in_ckpt}&quot;)
        if hasattr(lr_scheduler, &quot;last_step&quot;):
            lr_scheduler.last_step = self.state[&quot;global_resume_step&quot;]
        logger.info(f&quot;Resuming from global_step {self.state[&apos;global_resume_step&apos;]}.&quot;)
        # Log the current state of each data backend.
        for _, backend in StateTracker.get_data_backends().items():
            if &quot;sampler&quot; in backend:
                backend[&quot;sampler&quot;].log_state()
        # We store the number of dataset resets that have occurred inside the checkpoint.
        self.state[&quot;first_epoch&quot;] = StateTracker.get_epoch()
        if self.state[&quot;first_epoch&quot;] &gt; 1 or self.state[&quot;global_resume_step&quot;] &gt; 1:
            self.config.total_steps_remaining_at_start -= self.state[
                &quot;global_resume_step&quot;
            ]
            logger.debug(
                f&quot;Resuming from epoch {self.state[&apos;first_epoch&apos;]}, which leaves us with {self.config.total_steps_remaining_at_start}.&quot;
            )
        self.state[&quot;current_epoch&quot;] = self.state[&quot;first_epoch&quot;]
        StateTracker.set_epoch(self.state[&quot;current_epoch&quot;])
        if hasattr(lr_scheduler, &quot;last_epoch&quot;):
            lr_scheduler.last_epoch = (
                training_state_in_ckpt.get(
                    &quot;epoch_step&quot;, self.state.get(&quot;global_resume_step&quot;, 1)
                )
                * self.accelerator.num_processes
            )
        if (
            self.state[&quot;current_epoch&quot;] &gt; self.config.num_train_epochs + 1
            and not self.config.ignore_final_epochs
        ):
            logger.info(
                f&quot;Reached the end ({self.state[&apos;current_epoch&apos;]} epochs) of our training run ({self.config.num_train_epochs} epochs). This run will do zero steps.&quot;
            )
        self.accelerator.wait_for_everyone()
        if self.optimizer is not None and self.config.optimizer == &quot;prodigy&quot;:
            # fix the device assignment for the prodigy optimizer parameters
            for group in (
                self.optimizer.param_groups
                if self.optimizer.optimizer.split_groups
                else self.optimizer.param_groups[:1]
            ):
                p = group[&quot;params&quot;][0]
                group[&quot;running_d_numerator&quot;] = group[&quot;running_d_numerator&quot;].to(p.device)
                group[&quot;running_d_denom&quot;] = group[&quot;running_d_denom&quot;].to(p.device)
        return lr_scheduler
    def init_trackers(self):
        # We need to initialize the trackers we use, and also store our configuration.
        # The trackers initializes automatically on the main process.
        self.guidance_values_table = None
        if self.accelerator.is_main_process:
            # Copy args into public_args:
            public_args = copy.deepcopy(self.config)
            delattr(public_args, &quot;accelerator_project_config&quot;)
            delattr(public_args, &quot;process_group_kwargs&quot;)
            delattr(public_args, &quot;weight_dtype&quot;)
            delattr(public_args, &quot;base_weight_dtype&quot;)
            delattr(public_args, &quot;vae_kwargs&quot;)
            delattr(public_args, &quot;sana_complex_human_instruction&quot;)
            # Hash the contents of public_args to reflect a deterministic ID for a single set of params:
            public_args_hash = hashlib.md5(
                json.dumps(vars(public_args), sort_keys=True).encode(&quot;utf-8&quot;)
            ).hexdigest()
            project_name = self.config.tracker_project_name or &quot;simpletuner-training&quot;
            tracker_run_name = (
                self.config.tracker_run_name or &quot;simpletuner-training-run&quot;
            )
            try:
                self.accelerator.init_trackers(
                    project_name,
                    config=vars(public_args),
                    init_kwargs={
                        &quot;wandb&quot;: {
                            &quot;name&quot;: tracker_run_name,
                            &quot;id&quot;: f&quot;{public_args_hash}&quot;,
                            &quot;resume&quot;: &quot;allow&quot;,
                            &quot;allow_val_change&quot;: True,
                        }
                    },
                )
            except Exception as e:
                if &quot;Object has no attribute &apos;disabled&apos;&quot; in repr(e):
                    logger.warning(
                        &quot;WandB is disabled, and Accelerate was not quite happy about it.&quot;
                    )
                else:
                    logger.error(f&quot;Could not initialize trackers: {e}&quot;)
                    self._send_webhook_raw(
                        structured_data={
                            &quot;message&quot;: f&quot;Could not initialize trackers. Continuing without. {e}&quot;
                        },
                        message_type=&quot;error&quot;,
                    )
            self._send_webhook_raw(
                structured_data=public_args.__dict__,
                message_type=&quot;training_config&quot;,
            )
    def resume_and_prepare(self):
        self.init_optimizer()
        lr_scheduler = self.init_lr_scheduler()
        self.init_hooks()
        self.init_prepare_models(lr_scheduler=lr_scheduler)
        lr_scheduler = self.init_resume_checkpoint(lr_scheduler=lr_scheduler)
        self.init_post_load_freeze()
    def enable_sageattention_inference(self):
        # if the sageattention is inference-only, we&apos;ll enable it.
        # if it&apos;s training only, we&apos;ll disable it.
        # if it&apos;s inference+training, we leave it alone.
        if (
            &quot;sageattention&quot; not in self.config.attention_mechanism
            or self.config.sageattention_usage == &quot;training+inference&quot;
        ):
            return
        if self.config.sageattention_usage == &quot;inference&quot;:
            self.enable_sageattention()
        if self.config.sageattention_usage == &quot;training&quot;:
            self.disable_sageattention()
    def disable_sageattention_inference(self):
        # if the sageattention is inference-only, we&apos;ll disable it.
        # if it&apos;s training only, we&apos;ll enable it.
        # if it&apos;s inference+training, we leave it alone.
        if (
            &quot;sageattention&quot; not in self.config.attention_mechanism
            or self.config.sageattention_usage == &quot;training+inference&quot;
        ):
            return
        if self.config.sageattention_usage == &quot;inference&quot;:
            self.disable_sageattention()
        if self.config.sageattention_usage == &quot;training&quot;:
            self.enable_sageattention()
    def disable_sageattention(self):
        if &quot;sageattention&quot; not in self.config.attention_mechanism:
            return
        if (
            hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;)
            and torch.nn.functional
            != torch.nn.functional.scaled_dot_product_attention_sdpa
        ):
            logger.info(&quot;Disabling SageAttention.&quot;)
            setattr(
                torch.nn.functional,
                &quot;scaled_dot_product_attention&quot;,
                torch.nn.functional.scaled_dot_product_attention_sdpa,
            )
    def enable_sageattention(self):
        if &quot;sageattention&quot; not in self.config.attention_mechanism:
            return
        # we&apos;ll try and load SageAttention and overload pytorch&apos;s sdpa function.
        try:
            logger.info(&quot;Enabling SageAttention.&quot;)
            from sageattention import (
                sageattn,
                sageattn_qk_int8_pv_fp16_triton,
                sageattn_qk_int8_pv_fp16_cuda,
                sageattn_qk_int8_pv_fp8_cuda,
            )
            sageattn_functions = {
                &quot;sageattention&quot;: sageattn,
                &quot;sageattention-int8-fp16-triton&quot;: sageattn_qk_int8_pv_fp16_triton,
                &quot;sageattention-int8-fp16-cuda&quot;: sageattn_qk_int8_pv_fp16_cuda,
                &quot;sageattention-int8-fp8-cuda&quot;: sageattn_qk_int8_pv_fp8_cuda,
            }
            # store the old SDPA for validations to use during VAE decode
            if not hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sdpa&quot;):
                setattr(
                    torch.nn.functional,
                    &quot;scaled_dot_product_attention_sdpa&quot;,
                    torch.nn.functional.scaled_dot_product_attention,
                )
            torch.nn.functional.scaled_dot_product_attention = sageattn_functions.get(
                self.config.attention_mechanism, &quot;sageattention&quot;
            )
            if not hasattr(torch.nn.functional, &quot;scaled_dot_product_attention_sage&quot;):
                setattr(
                    torch.nn.functional,
                    &quot;scaled_dot_product_attention_sage&quot;,
                    torch.nn.functional.scaled_dot_product_attention,
                )
            if &quot;training&quot; in self.config.sageattention_usage:
                logger.warning(
                    f&quot;Using {self.config.attention_mechanism} for attention calculations during training. Your attention layers will not be trained. To disable SageAttention, remove or set --attention_mechanism to a different value.&quot;
                )
        except ImportError as e:
            logger.error(
                &quot;Could not import SageAttention. Please install it to use this --attention_mechanism=sageattention.&quot;
            )
            logger.error(repr(e))
            sys.exit(1)
    def move_models(self, destination: str = &quot;accelerator&quot;):
        target_device = &quot;cpu&quot;
        if destination == &quot;accelerator&quot;:
            target_device = self.accelerator.device
        logger.info(
            f&quot;Moving the {&apos;U-net&apos; if self.unet is not None else &apos;diffusion transformer&apos;} to GPU in {self.config.weight_dtype if not self.config.is_quantized else self.config.base_model_precision} precision.&quot;
        )
        if self.unet is not None:
            if self.config.is_quantized:
                self.unet.to(target_device)
            else:
                self.unet.to(target_device, dtype=self.config.weight_dtype)
        if self.transformer is not None:
            if self.config.is_quantized:
                self.transformer.to(target_device)
            else:
                self.transformer.to(target_device, dtype=self.config.weight_dtype)
        if getattr(self.accelerator, &quot;_lycoris_wrapped_network&quot;, None) is not None:
            self.accelerator._lycoris_wrapped_network = (
                self.accelerator._lycoris_wrapped_network.to(
                    target_device, dtype=self.config.weight_dtype
                )
            )
        if (
            &quot;sageattention&quot; in self.config.attention_mechanism
            and &quot;training&quot; in self.config.sageattention_usage
        ):
            logger.info(
                &quot;Using SageAttention for training. This is an unsupported, experimental configuration.&quot;
            )
            self.enable_sageattention()
        elif (
            self.config.attention_mechanism == &quot;xformers&quot;
            and self.config.model_family
            not in [
                &quot;sd3&quot;,
                &quot;pixart_sigma&quot;,
                &quot;flux&quot;,
                &quot;smoldit&quot;,
                &quot;kolors&quot;,
            ]
        ):
            logger.info(&quot;Enabling xformers memory-efficient attention.&quot;)
            if is_xformers_available():
                import xformers  # type: ignore # noqa
                if self.unet is not None:
                    self.unet.enable_xformers_memory_efficient_attention()
                if self.transformer is not None:
                    self.transformer.enable_xformers_memory_efficient_attention()
                if self.config.controlnet:
                    self.controlnet.enable_xformers_memory_efficient_attention()
            else:
                raise ValueError(
                    &quot;xformers is not available. Make sure it is installed correctly&quot;
                )
        elif self.config.attention_mechanism == &quot;xformers&quot;:
            logger.warning(
                &quot;xformers is not enabled, as it is incompatible with this model type.&quot;
                &quot; Falling back to diffusers attention mechanism (Pytorch SDPA).&quot;
                &quot; Alternatively, provide --attention_mechanism=sageattention for a more efficient option on CUDA systems.&quot;
            )
            self.config.enable_xformers_memory_efficient_attention = False
            self.config.attention_mechanism = &quot;diffusers&quot;
        if self.config.controlnet:
            self.controlnet.train()
            logger.info(
                f&quot;Moving ControlNet to {target_device} in {self.config.weight_dtype} precision.&quot;
            )
            self.controlnet.to(device=target_device, dtype=self.config.weight_dtype)
            if self.config.train_text_encoder:
                logger.warning(
                    &quot;Unknown results will occur when finetuning the text encoder alongside ControlNet.&quot;
                )
    def mark_optimizer_train(self):
        if is_lr_schedulefree(self.config.optimizer) and hasattr(
            self.optimizer, &quot;train&quot;
        ):
            # we typically have to call train() on the optim for schedulefree.
            logger.debug(&quot;Setting optimiser into train() mode.&quot;)
            self.optimizer.train()
    def mark_optimizer_eval(self):
        if is_lr_schedulefree(self.config.optimizer) and hasattr(
            self.optimizer, &quot;eval&quot;
        ):
            # we typically have to call eval() on the optim for schedulefree before saving or running validations.
            logger.debug(&quot;Setting optimiser into eval() mode.&quot;)
            self.optimizer.eval()
    def _send_webhook_msg(
        self, message: str, message_level: str = &quot;info&quot;, store_response: bool = False
    ):
        if type(message) is not str:
            logger.error(
                f&quot;_send_webhook_msg received {type(message)} type message instead of str.&quot;
            )
            return False
        if self.webhook_handler is None or not self.webhook_handler:
            return
        self.webhook_handler.send(
            message=message, message_level=message_level, store_response=store_response
        )
    def _send_webhook_raw(
        self,
        structured_data: dict,
        message_type: str,
        message_level: str = &quot;info&quot;,
    ):
        if type(structured_data) is not dict:
            logger.error(
                f&quot;_send_webhook_msg received {type(structured_data)} type message instead of dict.&quot;
            )
            return False
        if not self.webhook_handler:
            return
        self.webhook_handler.send_raw(
            structured_data=structured_data,
            message_type=message_type,
            message_level=message_level,
            job_id=self.job_id,
        )
    def _train_initial_msg(self):
        initial_msg = &quot;\n***** Running training *****&quot;
        initial_msg += f&quot;\n-  Num batches = {self.config.total_num_batches}&quot;
        initial_msg += f&quot;\n-  Num Epochs = {self.config.num_train_epochs}&quot;
        initial_msg += f&quot;\n  - Current Epoch = {self.state[&apos;first_epoch&apos;]}&quot;
        initial_msg += f&quot;\n-  Total train batch size (w. parallel, distributed &amp; accumulation) = {self.config.total_batch_size}&quot;
        initial_msg += f&quot;\n  - Instantaneous batch size per device = {self.config.train_batch_size}&quot;
        initial_msg += f&quot;\n  - Gradient Accumulation steps = {self.config.gradient_accumulation_steps}&quot;
        initial_msg += f&quot;\n-  Total optimization steps = {self.config.max_train_steps}&quot;
        if self.state[&quot;global_step&quot;] &gt; 1:
            initial_msg += f&quot;\n  - Steps completed: {self.state[&apos;global_step&apos;]}&quot;
        initial_msg += f&quot;\n-  Total optimization steps remaining = {max(0, getattr(self.config, &apos;total_steps_remaining_at_start&apos;, self.config.max_train_steps))}&quot;
        logger.info(initial_msg)
        self._send_webhook_msg(message=initial_msg)
        structured_data = {
            &quot;total_num_batches&quot;: self.config.total_num_batches,
            &quot;total_num_epochs&quot;: self.config.num_train_epochs,
            &quot;total_num_steps&quot;: self.config.max_train_steps,
            &quot;current_epoch&quot;: self.state[&quot;first_epoch&quot;],
            &quot;total_batch_size&quot;: self.config.total_batch_size,
            &quot;micro_batch_size&quot;: self.config.train_batch_size,
            &quot;current_step&quot;: self.state[&quot;global_step&quot;],
            &quot;remaining_num_steps&quot;: max(
                0,
                getattr(
                    self.config,
                    &quot;total_steps_remaining_at_start&quot;,
                    self.config.max_train_steps,
                ),
            ),
        }
        self._send_webhook_raw(
            structured_data=structured_data, message_type=&quot;_train_initial_msg&quot;
        )
    def _epoch_rollover(self, epoch):
        if self.state[&quot;first_epoch&quot;] == epoch:
            return
        logger.debug(
            f&quot;Just completed epoch {self.state[&apos;current_epoch&apos;]}. Beginning epoch {epoch}. Starting epoch was {self.state[&apos;first_epoch&apos;]}. Final epoch will be {self.config.num_train_epochs}&quot;
        )
        for backend_id, backend in StateTracker.get_data_backends().items():
            backend_config = StateTracker.get_data_backend_config(backend_id)
            if (
                backend_config.get(&quot;crop&quot;)
                and backend_config.get(&quot;crop_aspect&quot;) == &quot;random&quot;
                and &quot;metadata_backend&quot; in backend
                and not self.config.aspect_bucket_disable_rebuild
            ) or (
                backend_config.get(&quot;vae_cache_clear_each_epoch&quot;)
                and &quot;vaecache&quot; in backend
            ):
                # when the aspect ratio is random, we need to shuffle the dataset on each epoch.
                if self.accelerator.is_main_process:
                    # we only compute the aspect ratio indices on the main process.
                    # we have to set read_only to False since we&apos;re generating a new, un-split list.
                    # otherwise, we can&apos;t actually save the new cache to disk.
                    backend[&quot;metadata_backend&quot;].read_only = False
                    # this will generate+save the new cache to the storage backend.
                    backend[&quot;metadata_backend&quot;].compute_aspect_ratio_bucket_indices(
                        ignore_existing_cache=True
                    )
                self.accelerator.wait_for_everyone()
                logger.info(f&quot;Reloading cache for backend {backend_id}&quot;)
                backend[&quot;metadata_backend&quot;].reload_cache(set_config=False)
                logger.info(&quot;Waiting for other threads to finish..&quot;)
                self.accelerator.wait_for_everyone()
                # we&apos;ll have to split the buckets between GPUs again now, so that the VAE cache distributes properly.
                logger.info(&quot;Splitting buckets across GPUs&quot;)
                backend[&quot;metadata_backend&quot;].split_buckets_between_processes(
                    gradient_accumulation_steps=self.config.gradient_accumulation_steps
                )
                # we have to rebuild the VAE cache if it exists.
                if &quot;vaecache&quot; in backend:
                    logger.info(&quot;Rebuilding VAE cache..&quot;)
                    backend[&quot;vaecache&quot;].rebuild_cache()
                # no need to manually call metadata_backend.save_cache() here.
        self.state[&quot;current_epoch&quot;] = epoch
        StateTracker.set_epoch(epoch)
        if self.config.lr_scheduler == &quot;cosine_with_restarts&quot;:
            self.extra_lr_scheduler_kwargs[&quot;epoch&quot;] = epoch
    def _exit_on_signal(self):
        if self.should_abort:
            self._send_webhook_raw(
                structured_data={&quot;message&quot;: &quot;Aborting training run.&quot;},
                message_type=&quot;exit&quot;,
            )
            raise StopIteration(&quot;Training run received abort signal.&quot;)
    def abort(self):
        logger.info(&quot;Aborting training run.&quot;)
        if self.bf is not None:
            self.bf.stop_fetching()
        # we should set should_abort = True on each data backend&apos;s vae cache, metadata, and text backend
        for _, backend in StateTracker.get_data_backends().items():
            if &quot;vaecache&quot; in backend:
                logger.debug(f&quot;Aborting VAE cache&quot;)
                backend[&quot;vaecache&quot;].should_abort = True
            if &quot;metadata_backend&quot; in backend:
                logger.debug(f&quot;Aborting metadata backend&quot;)
                backend[&quot;metadata_backend&quot;].should_abort = True
            if &quot;text_backend&quot; in backend:
                logger.debug(f&quot;Aborting text backend&quot;)
                backend[&quot;text_backend&quot;].should_abort = True
            if &quot;sampler&quot; in backend:
                logger.debug(f&quot;Aborting sampler&quot;)
                backend[&quot;sampler&quot;].should_abort = True
        self.should_abort = True
    def model_predict(
        self,
        prepared_batch,
        custom_timesteps: list = None,
    ):
        if self.config.controlnet:
            training_logger.debug(
                f&quot;Extra conditioning dtype: {prepared_batch[&apos;conditioning_pixel_values&apos;].dtype}&quot;
            )
        timesteps = prepared_batch.get(&quot;timesteps&quot;)
        noisy_latents = prepared_batch.get(&quot;noisy_latents&quot;)
        encoder_hidden_states = prepared_batch.get(&quot;encoder_hidden_states&quot;)
        added_cond_kwargs = prepared_batch.get(&quot;added_cond_kwargs&quot;)
        add_text_embeds = added_cond_kwargs.get(&quot;text_embeds&quot;)
        if custom_timesteps is not None:
            timesteps = custom_timesteps
        if not self.config.disable_accelerator:
            if self.config.controlnet:
                # ControlNet conditioning.
                controlnet_image = prepared_batch[&quot;conditioning_pixel_values&quot;].to(
                    dtype=self.config.weight_dtype
                )
                training_logger.debug(f&quot;Image shape: {controlnet_image.shape}&quot;)
                down_block_res_samples, mid_block_res_sample = self.controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    added_cond_kwargs=added_cond_kwargs,
                    controlnet_cond=controlnet_image,
                    return_dict=False,
                )
                # Predict the noise residual
                if self.unet is not None:
                    model_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states=encoder_hidden_states,
                        added_cond_kwargs=added_cond_kwargs,
                        down_block_additional_residuals=[
                            sample.to(dtype=self.config.weight_dtype)
                            for sample in down_block_res_samples
                        ],
                        mid_block_additional_residual=mid_block_res_sample.to(
                            dtype=self.config.weight_dtype
                        ),
                        return_dict=False,
                    )[0]
                if self.transformer is not None:
                    raise Exception(
                        &quot;ControlNet predictions for transformer models are not yet implemented.&quot;
                    )
            elif self.config.model_family == &quot;flux&quot;:
                # handle guidance
                packed_noisy_latents = pack_latents(
                    noisy_latents,
                    batch_size=prepared_batch[&quot;latents&quot;].shape[0],
                    num_channels_latents=prepared_batch[&quot;latents&quot;].shape[1],
                    height=prepared_batch[&quot;latents&quot;].shape[2],
                    width=prepared_batch[&quot;latents&quot;].shape[3],
                ).to(
                    dtype=self.config.base_weight_dtype,
                    device=self.accelerator.device,
                )
                if self.config.flux_guidance_mode == &quot;mobius&quot;:
                    guidance_scales = get_mobius_guidance(
                        self.config,
                        self.state[&quot;global_step&quot;],
                        self.config.num_update_steps_per_epoch,
                        prepared_batch[&quot;latents&quot;].shape[0],
                        self.accelerator.device,
                    )
                elif self.config.flux_guidance_mode == &quot;constant&quot;:
                    guidance_scales = [
                        float(self.config.flux_guidance_value)
                    ] * prepared_batch[&quot;latents&quot;].shape[0]
                elif self.config.flux_guidance_mode == &quot;random-range&quot;:
                    # Generate a list of random values within the specified range for each latent
                    guidance_scales = [
                        random.uniform(
                            self.config.flux_guidance_min,
                            self.config.flux_guidance_max,
                        )
                        for _ in range(prepared_batch[&quot;latents&quot;].shape[0])
                    ]
                self.guidance_values_list.append(guidance_scales)
                # Now `guidance` will have different values for each latent in `latents`.
                transformer_config = None
                if hasattr(self.transformer, &quot;module&quot;):
                    transformer_config = self.transformer.module.config
                elif hasattr(self.transformer, &quot;config&quot;):
                    transformer_config = self.transformer.config
                if transformer_config is not None and getattr(
                    transformer_config, &quot;guidance_embeds&quot;, False
                ):
                    guidance = torch.tensor(
                        guidance_scales, device=self.accelerator.device
                    )
                else:
                    guidance = None
                img_ids = prepare_latent_image_ids(
                    prepared_batch[&quot;latents&quot;].shape[0],
                    prepared_batch[&quot;latents&quot;].shape[2],
                    prepared_batch[&quot;latents&quot;].shape[3],
                    self.accelerator.device,
                    self.config.weight_dtype,
                )
                timesteps = (
                    torch.tensor(timesteps)
                    .expand(noisy_latents.shape[0])
                    .to(device=self.accelerator.device)
                    / 1000
                )
                text_ids = torch.zeros(
                    prepared_batch[&quot;prompt_embeds&quot;].shape[1],
                    3,
                ).to(
                    device=self.accelerator.device,
                    dtype=self.config.base_weight_dtype,
                )
                training_logger.debug(
                    &quot;DTypes:&quot;
                    f&quot;\n-&gt; Text IDs shape: {text_ids.shape if hasattr(text_ids, &apos;shape&apos;) else None}, dtype: {text_ids.dtype if hasattr(text_ids, &apos;dtype&apos;) else None}&quot;
                    f&quot;\n-&gt; Image IDs shape: {img_ids.shape if hasattr(img_ids, &apos;shape&apos;) else None}, dtype: {img_ids.dtype if hasattr(img_ids, &apos;dtype&apos;) else None}&quot;
                    f&quot;\n-&gt; Timesteps shape: {timesteps.shape if hasattr(timesteps, &apos;shape&apos;) else None}, dtype: {timesteps.dtype if hasattr(timesteps, &apos;dtype&apos;) else None}&quot;
                    f&quot;\n-&gt; Guidance: {guidance}&quot;
                    f&quot;\n-&gt; Packed Noisy Latents shape: {packed_noisy_latents.shape if hasattr(packed_noisy_latents, &apos;shape&apos;) else None}, dtype: {packed_noisy_latents.dtype if hasattr(packed_noisy_latents, &apos;dtype&apos;) else None}&quot;
                )
                flux_transformer_kwargs = {
                    &quot;hidden_states&quot;: packed_noisy_latents,
                    # YiYi notes: divide it by 1000 for now because we scale it by 1000 in the transforme rmodel (we should not keep it but I want to keep the inputs same for the model for testing)
                    &quot;timestep&quot;: timesteps,
                    &quot;guidance&quot;: guidance,
                    &quot;pooled_projections&quot;: prepared_batch[&quot;add_text_embeds&quot;].to(
                        device=self.accelerator.device,
                        dtype=self.config.base_weight_dtype,
                    ),
                    &quot;encoder_hidden_states&quot;: prepared_batch[&quot;prompt_embeds&quot;].to(
                        device=self.accelerator.device,
                        dtype=self.config.base_weight_dtype,
                    ),
                    &quot;txt_ids&quot;: text_ids.to(
                        device=self.accelerator.device,
                        dtype=self.config.base_weight_dtype,
                    ),
                    &quot;img_ids&quot;: img_ids,
                    &quot;joint_attention_kwargs&quot;: None,
                    &quot;return_dict&quot;: False,
                }
                if self.config.flux_attention_masked_training:
                    flux_transformer_kwargs[&quot;attention_mask&quot;] = prepared_batch[
                        &quot;encoder_attention_mask&quot;
                    ]
                    if flux_transformer_kwargs[&quot;attention_mask&quot;] is None:
                        raise ValueError(
                            &quot;No attention mask was discovered when attempting validation - this means you need to recreate your text embed cache.&quot;
                        )
                model_pred = self.transformer(**flux_transformer_kwargs)[0]
            elif self.config.model_family == &quot;sd3&quot;:
                # Stable Diffusion 3 uses a MM-DiT model where the VAE-produced
                #  image embeds are passed in with the TE-produced text embeds.
                model_pred = self.transformer(
                    hidden_states=noisy_latents.to(
                        device=self.accelerator.device,
                        dtype=self.config.base_weight_dtype,
                    ),
                    timestep=timesteps,
                    encoder_hidden_states=encoder_hidden_states.to(
                        device=self.accelerator.device,
                        dtype=self.config.base_weight_dtype,
                    ),
                    pooled_projections=add_text_embeds.to(
                        device=self.accelerator.device,
                        dtype=self.config.weight_dtype,
                    ),
                    return_dict=False,
                )[0]
            elif self.config.model_family == &quot;sana&quot;:
                model_pred = self.transformer(
                    noisy_latents.to(self.config.weight_dtype),
                    encoder_hidden_states=encoder_hidden_states.to(
                        self.config.weight_dtype
                    ),
                    encoder_attention_mask=prepared_batch[&quot;encoder_attention_mask&quot;],
                    timestep=timesteps,
                    return_dict=False,
                )[0]
            elif self.config.model_family == &quot;pixart_sigma&quot;:
                if noisy_latents.shape[1] != 4:
                    raise ValueError(
                        &quot;Pixart Sigma models require a latent size of 4 channels. Ensure you are using the correct VAE cache path.&quot;
                    )
                model_pred = self.transformer(
                    noisy_latents,
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=prepared_batch[&quot;encoder_attention_mask&quot;],
                    timestep=timesteps,
                    added_cond_kwargs=added_cond_kwargs,
                    return_dict=False,
                )[0]
                model_pred = model_pred.chunk(2, dim=1)[0]
            elif self.config.model_family == &quot;ltxvideo&quot;:
                if noisy_latents.shape[1] != 128:
                    raise ValueError(
                        &quot;LTX Video requires a latent size of 128 channels. Ensure you are using the correct VAE cache path.&quot;
                        f&quot; Shape received: {noisy_latents.shape}&quot;
                    )
                scale_value = 1
                height, width = (
                    noisy_latents.shape[3] * scale_value,
                    noisy_latents.shape[4] * scale_value,
                )
                training_logger.debug(
                    f&quot;Batch contents: {noisy_latents.shape} (h={height}, w={width})&quot;
                )
                # permute to (B, T, C, H, W)
                num_frames = noisy_latents.shape[2]
                if &quot;conditioning_mask&quot; in prepared_batch:
                    conditioning_mask = pack_ltx_latents(
                        prepared_batch[&quot;conditioning_mask&quot;]
                    ).squeeze(-1)
                packed_noisy_latents = pack_ltx_latents(noisy_latents, 1, 1).to(
                    self.config.weight_dtype
                )
                training_logger.debug(
                    f&quot;Packed batch shape: {packed_noisy_latents.shape}&quot;
                )
                training_logger.debug(
                    &quot;input dtypes:&quot;
                    f&quot;\n -&gt; noisy_latents: {noisy_latents.dtype}&quot;
                    f&quot;\n -&gt; encoder_hidden_states: {encoder_hidden_states.dtype}&quot;
                    f&quot;\n -&gt; timestep: {timesteps.dtype}&quot;
                )
                # Copied from a-r-r-o-w&apos;s script.
                latent_frame_rate = self.config.framerate / 8
                spatial_compression_ratio = 32
                # [0.32, 32, 32]
                rope_interpolation_scale = [
                    1 / latent_frame_rate,
                    spatial_compression_ratio,
                    spatial_compression_ratio,
                ]
                # rope_interpolation_scale = [1 / 25, 32, 32]
                model_pred = self.transformer(
                    packed_noisy_latents,
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=prepared_batch[&quot;encoder_attention_mask&quot;],
                    timestep=timesteps,
                    return_dict=False,
                    num_frames=num_frames,
                    rope_interpolation_scale=rope_interpolation_scale,
                    height=height,
                    width=width,
                )[0]
                training_logger.debug(
                    f&quot;Got to the end of prediction, {model_pred.shape}&quot;
                )
                # we need to unpack LTX video latents i think
                model_pred = unpack_ltx_latents(
                    model_pred,
                    num_frames=num_frames,
                    patch_size=1,
                    patch_size_t=1,
                    height=height,
                    width=width,
                )
            elif self.config.model_family == &quot;smoldit&quot;:
                first_latent_shape = noisy_latents.shape
                height = first_latent_shape[1] * 8
                width = first_latent_shape[2] * 8
                grid_height = height // 8 // self.transformer.config.patch_size
                grid_width = width // 8 // self.transformer.config.patch_size
                base_size = 512 // 8 // self.transformer.config.patch_size
                grid_crops_coords = get_resize_crop_region_for_grid(
                    (grid_height, grid_width), base_size
                )
                inputs = {
                    &quot;hidden_states&quot;: noisy_latents,
                    &quot;timestep&quot;: timesteps,
                    &quot;encoder_hidden_states&quot;: encoder_hidden_states,
                    &quot;encoder_attention_mask&quot;: prepared_batch[&quot;encoder_attention_mask&quot;],
                    &quot;image_rotary_emb&quot;: get_2d_rotary_pos_embed(
                        self.transformer.inner_dim
                        // self.transformer.config.num_attention_heads,
                        grid_crops_coords,
                        (grid_height, grid_width),
                    ),
                }
                model_pred = self.transformer(**inputs).sample
            elif self.unet is not None:
                if self.config.model_family == &quot;legacy&quot;:
                    # SD 1.5 or 2.x
                    model_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states,
                    ).sample
                else:
                    # SDXL, Kolors, other default unet prediction.
                    model_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states,
                        added_cond_kwargs=added_cond_kwargs,
                    ).sample
            else:
                raise Exception(&quot;Unknown error occurred, no prediction could be made.&quot;)
            if self.config.model_family == &quot;flux&quot;:
                model_pred = unpack_latents(
                    model_pred,
                    height=prepared_batch[&quot;latents&quot;].shape[2] * 8,
                    width=prepared_batch[&quot;latents&quot;].shape[3] * 8,
                    vae_scale_factor=16,
                )
        else:
            # Dummy model prediction for debugging.
            model_pred = torch.randn_like(noisy_latents)
        # x-prediction requires that we now subtract the noise residual from the prediction to get the target sample.
        if (
            hasattr(self.noise_scheduler, &quot;config&quot;)
            and hasattr(self.noise_scheduler.config, &quot;prediction_type&quot;)
            and self.noise_scheduler.config.prediction_type == &quot;sample&quot;
        ):
            model_pred = model_pred - prepared_batch[&quot;noise&quot;]
        return model_pred
    def _max_grad_value(self):
        max_grad_value = float(&quot;-inf&quot;)  # Start with a very small number
        for param in self._get_trainable_parameters():
            if param.grad is not None:
                max_grad_value = max(max_grad_value, param.grad.abs().max().item())
        return max_grad_value
    def prepare_batch(self, batch: list):
        &quot;&quot;&quot;
        Prepare a batch for the model prediction.
        Args:
            batch (list): Batch from iterator_fn.
        Returns:
            batch (list): Prepared batch.
        &quot;&quot;&quot;
        if not batch:
            training_logger.debug(
                f&quot;No batch was returned by the iterator_fn, returning {batch}&quot;
            )
            return batch
        target_device_kwargs = {
            &quot;device&quot;: self.accelerator.device,
            &quot;dtype&quot;: self.config.weight_dtype,
        }
        batch[&quot;batch_luminance&quot;] = batch.get(&quot;batch_luminance&quot;, -1.0)
        batch[&quot;encoder_hidden_states&quot;] = batch[&quot;prompt_embeds&quot;].to(
            **target_device_kwargs
        )
        pooled_embeds = batch.get(&quot;add_text_embeds&quot;)
        time_ids = batch[&quot;batch_time_ids&quot;]
        batch[&quot;added_cond_kwargs&quot;] = {}
        if pooled_embeds is not None and hasattr(pooled_embeds, &quot;to&quot;):
            batch[&quot;added_cond_kwargs&quot;][&quot;text_embeds&quot;] = pooled_embeds.to(
                **target_device_kwargs
            )
        if time_ids is not None and hasattr(time_ids, &quot;to&quot;):
            batch[&quot;added_cond_kwargs&quot;][&quot;time_ids&quot;] = time_ids.to(**target_device_kwargs)
        latents = batch.get(&quot;latent_batch&quot;)
        if not hasattr(latents, &quot;to&quot;):
            logger.error(&quot;Batch received:&quot;)
            logger.error(batch)
            raise ValueError(&quot;Received invalid value for latents.&quot;)
        batch[&quot;latents&quot;] = latents.to(**target_device_kwargs)
        encoder_attention_mask = batch.get(&quot;encoder_attention_mask&quot;)
        if encoder_attention_mask is not None and hasattr(encoder_attention_mask, &quot;to&quot;):
            batch[&quot;encoder_attention_mask&quot;] = encoder_attention_mask.to(
                **target_device_kwargs
            )
        if self.config.model_family in [&quot;sdxl&quot;, &quot;kolors&quot;]:
            batch[&quot;added_cond_kwargs&quot;] = batch.get(&quot;added_cond_kwargs&quot;)
        elif self.config.model_family in [&quot;pixart_sigma&quot;, &quot;smoldit&quot;]:
            batch[&quot;added_cond_kwargs&quot;] = batch.get(&quot;batch_time_ids&quot;)
        batch[&quot;is_regularisation_data&quot;] = batch.get(&quot;is_regularisation_data&quot;, False)
        # Sample noise that we&apos;ll add to the latents - self.config.noise_offset might need to be set to 0.1 by default.
        noise = torch.randn_like(batch[&quot;latents&quot;])
        bsz = batch[&quot;latents&quot;].shape[0]
        if self.config.input_perturbation != 0 and (
            not self.config.input_perturbation_steps
            or self.state[&quot;global_step&quot;] &lt; self.config.input_perturbation_steps
        ):
            input_perturbation = self.config.input_perturbation
            if self.config.input_perturbation_steps:
                input_perturbation *= 1.0 - (
                    self.state[&quot;global_step&quot;] / self.config.input_perturbation_steps
                )
            batch[&quot;noise&quot;] = noise + input_perturbation * torch.randn_like(
                batch[&quot;latents&quot;]
            )
        else:
            batch[&quot;noise&quot;] = noise
        if self.config.flow_matching:
            if not self.config.flux_fast_schedule and not any(
                [
                    self.config.flow_use_beta_schedule,
                    self.config.flow_use_uniform_schedule,
                ]
            ):
                # imported from cloneofsimo&apos;s minRF trainer: https://github.com/cloneofsimo/minRF
                # also used by: https://github.com/XLabs-AI/x-flux/tree/main
                # and: https://github.com/kohya-ss/sd-scripts/commit/8a0f12dde812994ec3facdcdb7c08b362dbceb0f
                batch[&quot;sigmas&quot;] = torch.sigmoid(
                    self.config.flow_sigmoid_scale
                    * torch.randn((bsz,), device=self.accelerator.device)
                )
                batch[&quot;sigmas&quot;] = apply_flow_schedule_shift(
                    self.config, self.noise_scheduler, batch[&quot;sigmas&quot;], batch[&quot;noise&quot;]
                )
            elif self.config.flow_use_uniform_schedule:
                batch[&quot;sigmas&quot;] = torch.rand((bsz,), device=self.accelerator.device)
                batch[&quot;sigmas&quot;] = apply_flow_schedule_shift(
                    self.config, self.noise_scheduler, batch[&quot;sigmas&quot;], batch[&quot;noise&quot;]
                )
            elif self.config.flow_use_beta_schedule:
                alpha = self.config.flow_beta_schedule_alpha
                beta = self.config.flow_beta_schedule_beta
                # Create a Beta distribution instance
                beta_dist = Beta(alpha, beta)
                # Sample from the Beta distribution
                batch[&quot;sigmas&quot;] = beta_dist.sample((bsz,)).to(
                    device=self.accelerator.device
                )
                batch[&quot;sigmas&quot;] = apply_flow_schedule_shift(
                    self.config, self.noise_scheduler, batch[&quot;sigmas&quot;], noise
                )
            else:
                # fast schedule can only use these sigmas, and they can be sampled up to batch size times
                available_sigmas = [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    0.25,
                ]
                batch[&quot;sigmas&quot;] = torch.tensor(
                    random.choices(available_sigmas, k=bsz),
                    device=self.accelerator.device,
                )
            batch[&quot;timesteps&quot;] = batch[&quot;sigmas&quot;] * 1000.0
            batch[&quot;sigmas&quot;] = batch[&quot;sigmas&quot;].view(-1, 1, 1, 1)
        else:
            if self.config.offset_noise:
                if (
                    self.config.noise_offset_probability == 1.0
                    or random.random() &lt; self.config.noise_offset_probability
                ):
                    noise = noise + self.config.noise_offset * torch.randn(
                        batch[&quot;latents&quot;].shape[0],
                        batch[&quot;latents&quot;].shape[1],
                        1,
                        1,
                        device=batch[&quot;latents&quot;].device,
                    )
            # Sample a random timestep for each image, potentially biased by the timestep weights.
            # Biasing the timestep weights allows us to spend less time training irrelevant timesteps.
            weights = generate_timestep_weights(
                self.config, self.noise_scheduler.config.num_train_timesteps
            ).to(self.accelerator.device)
            # Instead of uniformly sampling the timestep range, we&apos;ll split our weights and schedule into bsz number of segments.
            # This enables more broad sampling and potentially more effective training.
            if bsz &gt; 1 and not self.config.disable_segmented_timestep_sampling:
                batch[&quot;timesteps&quot;] = segmented_timestep_selection(
                    actual_num_timesteps=self.noise_scheduler.config.num_train_timesteps,
                    bsz=bsz,
                    weights=weights,
                    use_refiner_range=StateTracker.is_sdxl_refiner()
                    and not StateTracker.get_args().sdxl_refiner_uses_full_range,
                ).to(self.accelerator.device)
            else:
                batch[&quot;timesteps&quot;] = torch.multinomial(
                    weights, bsz, replacement=True
                ).long()
        if self.config.flow_matching:
            if len(batch[&quot;latents&quot;].shape) == 5:
                # ltxvideo and others with 5D tensors need expansion to match dims here i think
                training_logger.debug(
                    f&quot;Latents shape vs sigmas, timesteps: {batch[&apos;latents&apos;].shape}, {batch[&apos;sigmas&apos;].shape}, {batch[&apos;timesteps&apos;].shape}&quot;
                )
                batch[&quot;sigmas&quot;] = batch[&quot;sigmas&quot;].reshape(bsz, 1, 1, 1, 1)
                num_frame_latents = batch[&quot;latents&quot;].shape[2]
                if num_frame_latents &gt; 1 and batch[&quot;is_i2v_data&quot;] is True:
                    # the theory is that if you have a single-frame latent, we expand it to num_frames and then do less destructive denoising.
                    single_frame_latents = batch[&quot;latents&quot;]
                    if num_frame_latents &gt; 1:
                        # for an actual video though, we&apos;ll grab one frame using the worst syntax we can think of:
                        single_frame_latents = batch[&quot;latents&quot;][
                            :, :, 0, :, :
                        ].unsqueeze(dim=2)
                        training_logger.debug(
                            f&quot;All latents shape: {batch[&apos;latents&apos;].shape}&quot;
                        )
                        training_logger.debug(
                            f&quot;Single frame latents shape: {single_frame_latents.shape}&quot;
                        )
                    batch[&quot;i2v_conditioning_mask&quot;] = make_i2v_conditioning_mask(
                        batch[&quot;latents&quot;], protect_frame_index=0
                    )
                    batch[&quot;timesteps&quot;], batch[&quot;noise&quot;], new_sigmas = (
                        apply_first_frame_protection(
                            batch[&quot;latents&quot;],
                            batch[&quot;timesteps&quot;],
                            batch[&quot;noise&quot;],
                            batch[&quot;i2v_conditioning_mask&quot;],
                            protect_first_frame=self.config.ltx_protect_first_frame,
                            first_frame_probability=self.config.ltx_i2v_prob,
                            partial_noise_fraction=self.config.ltx_partial_noise_fraction,
                        )
                    )
                    if new_sigmas is not None:
                        batch[&quot;sigmas&quot;] = new_sigmas
                    training_logger.debug(
                        f&quot;Applied mask {batch[&apos;i2v_conditioning_mask&apos;].shape} to timestep {batch[&apos;timesteps&apos;].shape}&quot;
                    )
            batch[&quot;noisy_latents&quot;] = (1 - batch[&quot;sigmas&quot;]) * batch[&quot;latents&quot;] + batch[
                &quot;sigmas&quot;
            ] * batch[&quot;noise&quot;]
        else:
            # Add noise to the latents according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            batch[&quot;noisy_latents&quot;] = self.noise_scheduler.add_noise(
                batch[&quot;latents&quot;].float(), batch[&quot;noise&quot;].float(), batch[&quot;timesteps&quot;]
            ).to(
                device=self.accelerator.device,
                dtype=self.config.weight_dtype,
            )
        return batch
    def get_prediction_target(self, prepared_batch: dict):
        if self.config.flow_matching:
            # This is the flow-matching target for vanilla SD3.
            # If self.config.flow_matching_loss == &quot;diffusion&quot;, we will instead use v_prediction (see below)
            if self.config.flow_matching_loss == &quot;diffusers&quot;:
                target = prepared_batch[&quot;latents&quot;]
            elif self.config.flow_matching_loss == &quot;compatible&quot;:
                target = prepared_batch[&quot;noise&quot;] - prepared_batch[&quot;latents&quot;]
            elif self.config.flow_matching_loss == &quot;sd35&quot;:
                sigma_reshaped = prepared_batch[&quot;sigmas&quot;].view(
                    -1, 1, 1, 1
                )  # Ensure sigma has the correct shape
                target = (
                    prepared_batch[&quot;noisy_latents&quot;] - prepared_batch[&quot;latents&quot;]
                ) / sigma_reshaped
        elif self.noise_scheduler.config.prediction_type == &quot;epsilon&quot;:
            target = prepared_batch[&quot;noise&quot;]
        elif self.noise_scheduler.config.prediction_type == &quot;v_prediction&quot; or (
            self.config.flow_matching and self.config.flow_matching_loss == &quot;diffusion&quot;
        ):
            # When not using flow-matching, train on velocity prediction objective.
            target = self.noise_scheduler.get_velocity(
                prepared_batch[&quot;latents&quot;],
                prepared_batch[&quot;noise&quot;],
                prepared_batch[&quot;timesteps&quot;],
            )
        elif self.noise_scheduler.config.prediction_type == &quot;sample&quot;:
            # We set the target to latents here, but the model_pred will return the noise sample prediction.
            # We will have to subtract the noise residual from the prediction to get the target sample.
            target = prepared_batch[&quot;latents&quot;]
        else:
            raise ValueError(
                f&quot;Unknown prediction type {self.noise_scheduler.config.prediction_type}&quot;
                &quot;Supported types are &apos;epsilon&apos;, `sample`, and &apos;v_prediction&apos;.&quot;
            )
        return target
    def _calculate_loss(
        self,
        prepared_batch: dict,
        model_pred,
        target,
        apply_conditioning_mask: bool = True,
    ):
        # Compute the per-pixel loss without reducing over spatial dimensions
        if self.config.flow_matching:
            # For flow matching, compute the per-pixel squared differences
            loss = (
                model_pred.float() - target.float()
            ) ** 2  # Shape: (batch_size, C, H, W)
        elif self.config.snr_gamma is None or self.config.snr_gamma == 0:
            training_logger.debug(&quot;Calculating loss&quot;)
            loss = self.config.snr_weight * F.mse_loss(
                model_pred.float(), target.float(), reduction=&quot;none&quot;
            )  # Shape: (batch_size, C, H, W)
        else:
            # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
            # Since we predict the noise instead of x_0, the original formulation is slightly changed.
            # This is discussed in Section 4.2 of the same paper.
            training_logger.debug(&quot;Using min-SNR loss&quot;)
            snr = compute_snr(prepared_batch[&quot;timesteps&quot;], self.noise_scheduler)
            snr_divisor = snr
            if self.noise_scheduler.config.prediction_type == &quot;v_prediction&quot; or (
                self.config.flow_matching
                and self.config.flow_matching_loss == &quot;diffusion&quot;
            ):
                snr_divisor = snr + 1
            training_logger.debug(&quot;Calculating MSE loss weights using SNR as divisor&quot;)
            mse_loss_weights = (
                torch.stack(
                    [
                        snr,
                        self.config.snr_gamma
                        * torch.ones_like(prepared_batch[&quot;timesteps&quot;]),
                    ],
                    dim=1,
                ).min(dim=1)[0]
                / snr_divisor
            )  # Shape: (batch_size,)
            # Compute the per-pixel MSE loss without reduction
            loss = F.mse_loss(
                model_pred.float(), target.float(), reduction=&quot;none&quot;
            )  # Shape: (batch_size, C, H, W)
            # Reshape mse_loss_weights for broadcasting and apply to loss
            mse_loss_weights = mse_loss_weights.view(
                -1, 1, 1, 1
            )  # Shape: (batch_size, 1, 1, 1)
            loss = loss * mse_loss_weights  # Shape: (batch_size, C, H, W)
        # Mask the loss using any conditioning data
        conditioning_type = prepared_batch.get(&quot;conditioning_type&quot;)
        if conditioning_type == &quot;mask&quot; and apply_conditioning_mask:
            # Adapted from:
            # https://github.com/kohya-ss/sd-scripts/blob/main/library/custom_train_functions.py#L482
            mask_image = (
                prepared_batch[&quot;conditioning_pixel_values&quot;]
                .to(dtype=loss.dtype, device=loss.device)[:, 0]
                .unsqueeze(1)
            )  # Shape: (batch_size, 1, H&apos;, W&apos;)
            mask_image = torch.nn.functional.interpolate(
                mask_image, size=loss.shape[2:], mode=&quot;area&quot;
            )  # Resize to match loss spatial dimensions
            mask_image = mask_image / 2 + 0.5  # Normalize to [0,1]
            loss = loss * mask_image  # Element-wise multiplication
        # Reduce the loss by averaging over channels and spatial dimensions
        loss = loss.mean(dim=list(range(1, len(loss.shape))))  # Shape: (batch_size,)
        # Further reduce the loss by averaging over the batch dimension
        loss = loss.mean()  # Scalar value
        return loss
    def train(self):
        self.init_trackers()
        self._train_initial_msg()
        self.mark_optimizer_train()
        # Only show the progress bar once on each machine.
        show_progress_bar = True
        if not self.accelerator.is_local_main_process:
            show_progress_bar = False
        progress_bar = tqdm(
            range(0, self.config.max_train_steps),
            disable=not show_progress_bar,
            initial=self.state[&quot;global_step&quot;],
            desc=f&quot;Epoch {self.state[&apos;first_epoch&apos;]}/{self.config.num_train_epochs} Steps&quot;,
            ncols=125,
        )
        self.accelerator.wait_for_everyone()
        # Some values that are required to be initialised later.
        step = self.state[&quot;global_step&quot;]
        training_luminance_values = []
        current_epoch_step = None
        self.bf, fetch_thread = None, None
        iterator_fn = random_dataloader_iterator
        num_epochs_to_track = self.config.num_train_epochs + 1
        if self.config.ignore_final_epochs:
            num_epochs_to_track += 1000000
        for epoch in range(self.state[&quot;first_epoch&quot;], num_epochs_to_track):
            if (
                self.state[&quot;current_epoch&quot;] &gt; self.config.num_train_epochs + 1
                and not self.config.ignore_final_epochs
            ):
                # This might immediately end training, but that&apos;s useful for simply exporting the model.
                logger.info(
                    f&quot;Training run is complete ({self.config.num_train_epochs}/{self.config.num_train_epochs} epochs, {self.state[&apos;global_step&apos;]}/{self.config.max_train_steps} steps).&quot;
                )
                break
            self._epoch_rollover(epoch)
            if self.config.controlnet:
                self.controlnet.train()
                training_models = [self.controlnet]
            else:
                if self.unet is not None:
                    self.unet.train()
                    training_models = [self.unet]
                if self.transformer is not None:
                    self.transformer.train()
                    training_models = [self.transformer]
            if (
                &quot;lora&quot; in self.config.model_type
                and self.config.train_text_encoder
                and &quot;standard&quot; in self.config.lora_type.lower()
            ):
                self.text_encoder_1.train()
                self.text_encoder_2.train()
                training_models.append(self.text_encoder_1)
                training_models.append(self.text_encoder_2)
            if current_epoch_step is not None:
                # We are resetting to the next epoch, if it is not none.
                current_epoch_step = 0
            else:
                # If it&apos;s None, we need to calculate the current epoch step based on the current global step.
                current_epoch_step = (
                    self.state[&quot;global_step&quot;] % self.config.num_update_steps_per_epoch
                )
            train_backends = {}
            for backend_id, backend in StateTracker.get_data_backends().items():
                if (
                    StateTracker.backend_status(backend_id)
                    or &quot;train_dataloader&quot; not in backend
                ):
                    # Exclude exhausted backends.
                    logger.debug(
                        f&quot;Excluding backend: {backend_id}, as it is exhausted? {StateTracker.backend_status(backend_id)} or not found {(&apos;train_dataloader&apos; not in backend)}&quot;
                    )
                    continue
                train_backends[backend_id] = backend[&quot;train_dataloader&quot;]
            # Begin dataloader prefetch, if enabled.
            iterator_args = [train_backends]
            if self.config.dataloader_prefetch:
                iterator_args = []
                if self.bf is not None:
                    self.bf.stop_fetching()
                self.bf = BatchFetcher(
                    datasets=train_backends,
                    max_size=self.config.dataloader_prefetch_qlen,
                    step=step,
                )
                if fetch_thread is not None:
                    fetch_thread.join()
                fetch_thread = self.bf.start_fetching()
                iterator_fn = self.bf.next_response
            while True:
                self._exit_on_signal()
                step += 1
                prepared_batch = self.prepare_batch(iterator_fn(step, *iterator_args))
                training_logger.debug(f&quot;Iterator: {iterator_fn}&quot;)
                if self.config.lr_scheduler == &quot;cosine_with_restarts&quot;:
                    self.extra_lr_scheduler_kwargs[&quot;step&quot;] = self.state[&quot;global_step&quot;]
                if self.accelerator.is_main_process:
                    progress_bar.set_description(
                        f&quot;Epoch {self.state[&apos;current_epoch&apos;]}/{self.config.num_train_epochs}, Steps&quot;
                    )
                # If we receive a False from the enumerator, we know we reached the next epoch.
                if prepared_batch is False:
                    logger.debug(f&quot;Reached the end of epoch {epoch}&quot;)
                    break
                if prepared_batch is None:
                    import traceback
                    raise ValueError(
                        f&quot;Received a None batch, which is not a good thing. Traceback: {traceback.format_exc()}&quot;
                    )
                # Add the current batch of training data&apos;s avg luminance to a list.
                if &quot;batch_luminance&quot; in prepared_batch:
                    training_luminance_values.append(prepared_batch[&quot;batch_luminance&quot;])
                with self.accelerator.accumulate(training_models):
                    bsz = prepared_batch[&quot;latents&quot;].shape[0]
                    training_logger.debug(&quot;Sending latent batch to GPU.&quot;)
                    if int(bsz) != int(self.config.train_batch_size):
                        logger.error(
                            f&quot;Received {bsz} latents, but expected {self.config.train_batch_size}. Processing short batch.&quot;
                        )
                    training_logger.debug(f&quot;Working on batch size: {bsz}&quot;)
                    # Prepare the data for the scatter plot
                    for timestep in prepared_batch[&quot;timesteps&quot;].tolist():
                        self.timesteps_buffer.append(
                            (self.state[&quot;global_step&quot;], timestep)
                        )
                    encoder_hidden_states = prepared_batch[&quot;encoder_hidden_states&quot;]
                    training_logger.debug(
                        f&quot;Encoder hidden states: {encoder_hidden_states.shape}&quot;
                    )
                    add_text_embeds = prepared_batch[&quot;add_text_embeds&quot;]
                    training_logger.debug(
                        f&quot;Pooled embeds: {add_text_embeds.shape if add_text_embeds is not None else None}&quot;
                    )
                    # Get the target for loss depending on the prediction type
                    target = self.get_prediction_target(prepared_batch)
                    added_cond_kwargs = prepared_batch.get(&quot;added_cond_kwargs&quot;)
                    # Predict the noise residual and compute loss
                    is_regularisation_data = prepared_batch.get(
                        &quot;is_regularisation_data&quot;, False
                    )
                    if is_regularisation_data and self.config.model_type == &quot;lora&quot;:
                        training_logger.debug(&quot;Predicting parent model residual.&quot;)
                        with torch.no_grad():
                            if self.config.lora_type.lower() == &quot;lycoris&quot;:
                                training_logger.debug(
                                    &quot;Detaching LyCORIS adapter for parent prediction.&quot;
                                )
                                self.accelerator._lycoris_wrapped_network.set_multiplier(
                                    0.0
                                )
                            else:
                                raise ValueError(
                                    f&quot;Cannot train parent-student networks on {self.config.lora_type} model. Only LyCORIS is supported.&quot;
                                )
                            target = self.model_predict(
                                prepared_batch=prepared_batch,
                            )
                            if self.config.lora_type.lower() == &quot;lycoris&quot;:
                                training_logger.debug(
                                    &quot;Attaching LyCORIS adapter for student prediction.&quot;
                                )
                                self.accelerator._lycoris_wrapped_network.set_multiplier(
                                    1.0
                                )
                    training_logger.debug(&quot;Predicting noise residual.&quot;)
                    model_pred = self.model_predict(
                        prepared_batch=prepared_batch,
                    )
                    loss = self._calculate_loss(
                        prepared_batch, model_pred, target, apply_conditioning_mask=True
                    )
                    parent_loss = None
                    if is_regularisation_data:
                        parent_loss = loss
                    # Gather the losses across all processes for logging (if using distributed training)
                    avg_loss = self.accelerator.gather(
                        loss.repeat(self.config.train_batch_size)
                    ).mean()
                    self.train_loss += (
                        avg_loss.item() / self.config.gradient_accumulation_steps
                    )
                    # Backpropagate
                    self.grad_norm = None
                    if not self.config.disable_accelerator:
                        training_logger.debug(&quot;Backwards pass.&quot;)
                        self.accelerator.backward(loss)
                        if (
                            self.config.optimizer != &quot;adam_bfloat16&quot;
                            and self.config.gradient_precision == &quot;fp32&quot;
                        ):
                            # After backward, convert gradients to fp32 for stable accumulation
                            for param in self.params_to_optimize:
                                if param.grad is not None:
                                    param.grad.data = param.grad.data.to(torch.float32)
                        self.grad_norm = self._max_grad_value()
                        if (
                            self.accelerator.sync_gradients
                            and self.config.optimizer
                            not in [&quot;optimi-stableadamw&quot;, &quot;prodigy&quot;]
                            and self.config.max_grad_norm &gt; 0
                        ):
                            # StableAdamW/Prodigy do not need clipping, similar to Adafactor.
                            if self.config.grad_clip_method == &quot;norm&quot;:
                                self.grad_norm = self.accelerator.clip_grad_norm_(
                                    self._get_trainable_parameters(),
                                    self.config.max_grad_norm,
                                )
                            elif self.config.use_deepspeed_optimizer:
                                # deepspeed can only do norm clipping (internally)
                                pass
                            elif self.config.grad_clip_method == &quot;value&quot;:
                                self.accelerator.clip_grad_value_(
                                    self._get_trainable_parameters(),
                                    self.config.max_grad_norm,
                                )
                            else:
                                raise ValueError(
                                    f&quot;Unknown grad clip method: {self.config.grad_clip_method}. Supported methods: value, norm&quot;
                                )
                        training_logger.debug(&quot;Stepping components forward.&quot;)
                        if self.config.optimizer_release_gradients:
                            step_offset = 0  # simpletuner indexes steps from 1.
                            should_not_release_gradients = (
                                step + step_offset
                            ) % self.config.gradient_accumulation_steps != 0
                            training_logger.debug(
                                f&quot;step: {step}, should_not_release_gradients: {should_not_release_gradients}, self.config.optimizer_release_gradients: {self.config.optimizer_release_gradients}&quot;
                            )
                            self.optimizer.optimizer_accumulation = (
                                should_not_release_gradients
                            )
                        else:
                            self.optimizer.step()
                        self.optimizer.zero_grad(
                            set_to_none=self.config.set_grads_to_none
                        )
                # Checks if the accelerator has performed an optimization step behind the scenes
                wandb_logs = {}
                if self.accelerator.sync_gradients:
                    try:
                        if &quot;prodigy&quot; in self.config.optimizer:
                            self.lr_scheduler.step(**self.extra_lr_scheduler_kwargs)
                            self.lr = self.optimizer.param_groups[0][&quot;d&quot;]
                        elif self.config.is_lr_scheduler_disabled:
                            # hackjob method of retrieving LR from accelerated optims
                            self.lr = StateTracker.get_last_lr()
                        else:
                            self.lr_scheduler.step(**self.extra_lr_scheduler_kwargs)
                            self.lr = self.lr_scheduler.get_last_lr()[0]
                    except Exception as e:
                        logger.error(
                            f&quot;Failed to get the last learning rate from the scheduler. Error: {e}&quot;
                        )
                    wandb_logs.update(
                        {
                            &quot;train_loss&quot;: self.train_loss,
                            &quot;optimization_loss&quot;: loss,
                            &quot;learning_rate&quot;: self.lr,
                            &quot;epoch&quot;: epoch,
                        }
                    )
                    if parent_loss is not None:
                        wandb_logs[&quot;regularisation_loss&quot;] = parent_loss
                    if self.config.model_family == &quot;flux&quot; and self.guidance_values_list:
                        # avg the values
                        guidance_values = torch.tensor(self.guidance_values_list).mean()
                        wandb_logs[&quot;mean_cfg&quot;] = guidance_values.item()
                        self.guidance_values_list = []
                    if self.grad_norm is not None:
                        if self.config.grad_clip_method == &quot;norm&quot;:
                            wandb_logs[&quot;grad_norm&quot;] = self.grad_norm
                        else:
                            wandb_logs[&quot;grad_absmax&quot;] = self.grad_norm
                    if self.validation is not None and hasattr(
                        self.validation, &quot;evaluation_result&quot;
                    ):
                        eval_result = self.validation.get_eval_result()
                        if eval_result is not None and type(eval_result) == dict:
                            # add the dict to wandb_logs
                            self.validation.clear_eval_result()
                            wandb_logs.update(eval_result)
                    progress_bar.update(1)
                    self.state[&quot;global_step&quot;] += 1
                    current_epoch_step += 1
                    StateTracker.set_global_step(self.state[&quot;global_step&quot;])
                    ema_decay_value = &quot;None (EMA not in use)&quot;
                    if self.config.use_ema:
                        if self.ema_model is not None:
                            self.ema_model.step(
                                parameters=self._get_trainable_parameters(),
                                global_step=self.state[&quot;global_step&quot;],
                            )
                            wandb_logs[&quot;ema_decay_value&quot;] = self.ema_model.get_decay()
                            ema_decay_value = wandb_logs[&quot;ema_decay_value&quot;]
                        self.accelerator.wait_for_everyone()
                    # Log scatter plot to wandb
                    if (
                        self.config.report_to == &quot;wandb&quot;
                        and self.accelerator.is_main_process
                    ):
                        # Prepare the data for the scatter plot
                        data = [
                            [iteration, timestep]
                            for iteration, timestep in self.timesteps_buffer
                        ]
                        table = wandb.Table(
                            data=data, columns=[&quot;global_step&quot;, &quot;timestep&quot;]
                        )
                        wandb_logs[&quot;timesteps_scatter&quot;] = wandb.plot.scatter(
                            table,
                            &quot;global_step&quot;,
                            &quot;timestep&quot;,
                            title=&quot;Timestep distribution by step&quot;,
                        )
                    # Clear buffers
                    self.timesteps_buffer = []
                    # Average out the luminance values of each batch, so that we can store that in this step.
                    avg_training_data_luminance = sum(training_luminance_values) / len(
                        training_luminance_values
                    )
                    wandb_logs[&quot;train_luminance&quot;] = avg_training_data_luminance
                    logger.debug(
                        f&quot;Step {self.state[&apos;global_step&apos;]} of {self.config.max_train_steps}: loss {loss.item()}, lr {self.lr}, epoch {epoch}/{self.config.num_train_epochs}, ema_decay_value {ema_decay_value}, train_loss {self.train_loss}&quot;
                    )
                    webhook_pending_msg = f&quot;Step {self.state[&apos;global_step&apos;]} of {self.config.max_train_steps}: loss {round(loss.item(), 4)}, lr {self.lr}, epoch {epoch}/{self.config.num_train_epochs}, ema_decay_value {ema_decay_value}, train_loss {round(self.train_loss, 4)}&quot;
                    # Reset some values for the next go.
                    training_luminance_values = []
                    self.train_loss = 0.0
                    if (
                        self.config.webhook_reporting_interval is not None
                        and self.state[&quot;global_step&quot;]
                        % self.config.webhook_reporting_interval
                        == 0
                    ):
                        structured_data = {
                            &quot;state&quot;: self.state,
                            &quot;loss&quot;: round(self.train_loss, 4),
                            &quot;parent_loss&quot;: parent_loss,
                            &quot;learning_rate&quot;: self.lr,
                            &quot;epoch&quot;: epoch,
                            &quot;final_epoch&quot;: self.config.num_train_epochs,
                        }
                        self._send_webhook_raw(
                            structured_data=structured_data, message_type=&quot;train&quot;
                        )
                    if self.state[&quot;global_step&quot;] % self.config.checkpointing_steps == 0:
                        self._send_webhook_msg(
                            message=f&quot;Checkpoint: `{webhook_pending_msg}`&quot;,
                            message_level=&quot;info&quot;,
                        )
                        if self.accelerator.is_main_process:
                            # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                            if self.config.checkpoints_total_limit is not None:
                                checkpoints = os.listdir(self.config.output_dir)
                                checkpoints = [
                                    d for d in checkpoints if d.startswith(&quot;checkpoint&quot;)
                                ]
                                checkpoints = sorted(
                                    checkpoints, key=lambda x: int(x.split(&quot;-&quot;)[1])
                                )
                                # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
                                if (
                                    len(checkpoints)
                                    &gt;= self.config.checkpoints_total_limit
                                ):
                                    num_to_remove = (
                                        len(checkpoints)
                                        - self.config.checkpoints_total_limit
                                        + 1
                                    )
                                    removing_checkpoints = checkpoints[0:num_to_remove]
                                    logger.debug(
                                        f&quot;{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints&quot;
                                    )
                                    logger.debug(
                                        f&quot;removing checkpoints: {&apos;, &apos;.join(removing_checkpoints)}&quot;
                                    )
                                    for removing_checkpoint in removing_checkpoints:
                                        removing_checkpoint = os.path.join(
                                            self.config.output_dir, removing_checkpoint
                                        )
                                        try:
                                            shutil.rmtree(
                                                removing_checkpoint, ignore_errors=True
                                            )
                                        except Exception as e:
                                            logger.error(
                                                f&quot;Failed to remove directory: {removing_checkpoint}&quot;
                                            )
                                            print(e)
                        if (
                            self.accelerator.is_main_process
                            or self.config.use_deepspeed_optimizer
                        ):
                            save_path = os.path.join(
                                self.config.output_dir,
                                f&quot;checkpoint-{self.state[&apos;global_step&apos;]}&quot;,
                            )
                            print(&quot;\n&quot;)
                            # schedulefree optim needs the optimizer to be in eval mode to save the state (and then back to train after)
                            self.mark_optimizer_eval()
                            self.accelerator.save_state(save_path)
                            self.mark_optimizer_train()
                            for _, backend in StateTracker.get_data_backends().items():
                                if &quot;sampler&quot; in backend:
                                    logger.debug(f&quot;Backend: {backend}&quot;)
                                    backend[&quot;sampler&quot;].save_state(
                                        state_path=os.path.join(
                                            save_path,
                                            self.model_hooks.training_state_path,
                                        ),
                                    )
                    if (
                        self.config.accelerator_cache_clear_interval is not None
                        and self.state[&quot;global_step&quot;]
                        % self.config.accelerator_cache_clear_interval
                        == 0
                    ):
                        reclaim_memory()
                    # here we might run eval loss calculations.
                    if self.evaluation is not None and self.evaluation.would_evaluate(
                        self.state
                    ):
                        self.mark_optimizer_eval()
                        all_accumulated_losses = self.evaluation.execute_eval(
                            prepare_batch=self.prepare_batch,
                            model_predict=self.model_predict,
                            calculate_loss=self._calculate_loss,
                            get_prediction_target=self.get_prediction_target,
                            noise_scheduler=self._get_noise_scheduler(),
                        )
                        tracker_table = self.evaluation.generate_tracker_table(
                            all_accumulated_losses=all_accumulated_losses
                        )
                        print(f&quot;Tracking information: {tracker_table}&quot;)
                        wandb_logs.update(tracker_table)
                        self.mark_optimizer_train()
                    self.accelerator.log(
                        wandb_logs,
                        step=self.state[&quot;global_step&quot;],
                    )
                logs = {
                    &quot;step_loss&quot;: loss.detach().item(),
                    &quot;lr&quot;: float(self.lr),
                }
                if self.grad_norm is not None:
                    if self.config.grad_clip_method == &quot;norm&quot;:
                        logs[&quot;grad_norm&quot;] = float(self.grad_norm.clone().detach())
                    elif self.config.grad_clip_method == &quot;value&quot;:
                        logs[&quot;grad_absmax&quot;] = self.grad_norm
                progress_bar.set_postfix(**logs)
                if self.validation is not None:
                    if self.validation.would_validate():
                        self.mark_optimizer_eval()
                        self.enable_sageattention_inference()
                        self.disable_gradient_checkpointing()
                    self.validation.run_validations(
                        validation_type=&quot;intermediary&quot;, step=step
                    )
                    if self.validation.would_validate():
                        self.disable_sageattention_inference()
                        self.enable_gradient_checkpointing()
                        self.mark_optimizer_train()
                if (
                    self.config.push_to_hub
                    and self.config.push_checkpoints_to_hub
                    and self.state[&quot;global_step&quot;] % self.config.checkpointing_steps == 0
                    and step % self.config.gradient_accumulation_steps == 0
                    and self.state[&quot;global_step&quot;] &gt; self.state[&quot;global_resume_step&quot;]
                ):
                    if self.accelerator.is_main_process:
                        try:
                            self.hub_manager.upload_latest_checkpoint(
                                validation_images=(
                                    getattr(self.validation, &quot;validation_images&quot;)
                                    if self.validation is not None
                                    else None
                                ),
                                webhook_handler=self.webhook_handler,
                            )
                        except Exception as e:
                            logger.error(
                                f&quot;Error uploading to hub: {e}, continuing training.&quot;
                            )
                self.accelerator.wait_for_everyone()
                if self.state[&quot;global_step&quot;] &gt;= self.config.max_train_steps or (
                    epoch &gt; self.config.num_train_epochs
                    and not self.config.ignore_final_epochs
                ):
                    logger.info(
                        f&quot;Training has completed.&quot;
                        f&quot;\n -&gt; global_step = {self.state[&apos;global_step&apos;]}, max_train_steps = {self.config.max_train_steps}, epoch = {epoch}, num_train_epochs = {self.config.num_train_epochs}&quot;,
                    )
                    break
            if self.state[&quot;global_step&quot;] &gt;= self.config.max_train_steps or (
                epoch &gt; self.config.num_train_epochs
                and not self.config.ignore_final_epochs
            ):
                logger.info(
                    f&quot;Exiting training loop. Beginning model unwind at epoch {epoch}, step {self.state[&apos;global_step&apos;]}&quot;
                )
                break
        # Create the pipeline using the trained modules and save it.
        self.accelerator.wait_for_everyone()
        validation_images = None
        if self.accelerator.is_main_process:
            self.mark_optimizer_eval()
            if self.validation is not None:
                self.enable_sageattention_inference()
                self.disable_gradient_checkpointing()
                validation_images = self.validation.run_validations(
                    validation_type=&quot;final&quot;,
                    step=self.state[&quot;global_step&quot;],
                    force_evaluation=True,
                    skip_execution=True,
                ).validation_images
                # we don&apos;t have to do this but we will anyway.
                self.disable_sageattention_inference()
            if self.unet is not None:
                self.unet = unwrap_model(self.accelerator, self.unet)
            if self.transformer is not None:
                self.transformer = unwrap_model(self.accelerator, self.transformer)
            if (
                &quot;lora&quot; in self.config.model_type
                and &quot;standard&quot; == self.config.lora_type.lower()
            ):
                if self.transformer is not None:
                    transformer_lora_layers = get_peft_model_state_dict(
                        self.transformer
                    )
                elif self.unet is not None:
                    unet_lora_layers = convert_state_dict_to_diffusers(
                        get_peft_model_state_dict(self.unet)
                    )
                else:
                    raise Exception(
                        &quot;Couldn&apos;t locate the unet or transformer model for export.&quot;
                    )
                if self.config.train_text_encoder:
                    self.text_encoder_1 = self.accelerator.unwrap_model(
                        self.text_encoder_1
                    )
                    self.text_encoder_lora_layers = convert_state_dict_to_diffusers(
                        get_peft_model_state_dict(self.text_encoder_1)
                    )
                    if self.text_encoder_2 is not None:
                        self.text_encoder_2 = self.accelerator.unwrap_model(
                            self.text_encoder_2
                        )
                        text_encoder_2_lora_layers = convert_state_dict_to_diffusers(
                            get_peft_model_state_dict(self.text_encoder_2)
                        )
                        if self.text_encoder_3 is not None:
                            text_encoder_3 = self.accelerator.unwrap_model(
                                self.text_encoder_3
                            )
                else:
                    text_encoder_lora_layers = None
                    text_encoder_2_lora_layers = None
                if self.config.model_family == &quot;flux&quot;:
                    from diffusers.pipelines import FluxPipeline
                    FluxPipeline.save_lora_weights(
                        save_directory=self.config.output_dir,
                        transformer_lora_layers=transformer_lora_layers,
                        text_encoder_lora_layers=text_encoder_lora_layers,
                    )
                elif self.config.model_family == &quot;ltxvideo&quot;:
                    from diffusers.pipelines import LTXPipeline
                    LTXPipeline.save_lora_weights(
                        save_directory=self.config.output_dir,
                        transformer_lora_layers=transformer_lora_layers,
                    )
                elif self.config.model_family == &quot;sd3&quot;:
                    StableDiffusion3Pipeline.save_lora_weights(
                        save_directory=self.config.output_dir,
                        transformer_lora_layers=transformer_lora_layers,
                        text_encoder_lora_layers=text_encoder_lora_layers,
                        text_encoder_2_lora_layers=text_encoder_2_lora_layers,
                    )
                else:
                    StableDiffusionXLPipeline.save_lora_weights(
                        save_directory=self.config.output_dir,
                        unet_lora_layers=unet_lora_layers,
                        text_encoder_lora_layers=text_encoder_lora_layers,
                        text_encoder_2_lora_layers=text_encoder_2_lora_layers,
                    )
                del self.unet
                del self.transformer
                del text_encoder_lora_layers
                del text_encoder_2_lora_layers
                reclaim_memory()
            elif (
                &quot;lora&quot; in self.config.model_type
                and &quot;lycoris&quot; == self.config.lora_type.lower()
            ):
                if (
                    self.accelerator.is_main_process
                    or self.config.use_deepspeed_optimizer
                ):
                    logger.info(
                        f&quot;Saving final LyCORIS checkpoint to {self.config.output_dir}&quot;
                    )
                    # Save final LyCORIS checkpoint.
                    if (
                        getattr(self.accelerator, &quot;_lycoris_wrapped_network&quot;, None)
                        is not None
                    ):
                        from helpers.publishing.huggingface import (
                            LORA_SAFETENSORS_FILENAME,
                        )
                        self.accelerator._lycoris_wrapped_network.save_weights(
                            os.path.join(
                                self.config.output_dir, LORA_SAFETENSORS_FILENAME
                            ),
                            list(
                                self.accelerator._lycoris_wrapped_network.parameters()
                            )[0].dtype,
                            {
                                &quot;lycoris_config&quot;: json.dumps(self.lycoris_config)
                            },  # metadata
                        )
                        shutil.copy2(
                            self.config.lycoris_config,
                            os.path.join(self.config.output_dir, &quot;lycoris_config.json&quot;),
                        )
            elif self.config.use_ema:
                if self.unet is not None:
                    self.ema_model.copy_to(self.unet.parameters())
                if self.transformer is not None:
                    self.ema_model.copy_to(self.transformer.parameters())
            if self.config.model_type == &quot;full&quot;:
                # Now we build a full SDXL Pipeline to export the model with.
                if self.config.model_family == &quot;sd3&quot;:
                    self.pipeline = StableDiffusion3Pipeline.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        text_encoder_2=self.text_encoder_2
                        or (
                            self.text_encoder_cls_2.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder_2&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer_2=self.tokenizer_2,
                        text_encoder_3=self.text_encoder_3
                        or (
                            self.text_encoder_cls_3.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder_3&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer_3=self.tokenizer_3,
                        vae=self.vae
                        or (
                            self.vae_cls.from_pretrained(
                                self.config.vae_path,
                                subfolder=(
                                    &quot;vae&quot;
                                    if self.config.pretrained_vae_model_name_or_path
                                    is None
                                    else None
                                ),
                                revision=self.config.revision,
                                variant=self.config.variant,
                                force_upcast=False,
                            )
                        ),
                        transformer=self.transformer,
                    )
                    if (
                        self.config.flow_matching
                        and self.config.flow_matching_loss == &quot;diffusion&quot;
                    ):
                        # Diffusion-based SD3 is currently fixed to a Euler v-prediction schedule.
                        self.pipeline.scheduler = SCHEDULER_NAME_MAP[
                            &quot;euler&quot;
                        ].from_pretrained(
                            self.config.pretrained_model_name_or_path,
                            revision=self.config.revision,
                            subfolder=&quot;scheduler&quot;,
                            prediction_type=&quot;v_prediction&quot;,
                            timestep_spacing=self.config.training_scheduler_timestep_spacing,
                            rescale_betas_zero_snr=self.config.rescale_betas_zero_snr,
                        )
                        logger.debug(
                            f&quot;Setting scheduler to Euler for SD3. Config: {self.pipeline.scheduler.config}&quot;
                        )
                elif self.config.model_family == &quot;flux&quot;:
                    from diffusers.pipelines import FluxPipeline
                    self.pipeline = FluxPipeline.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        transformer=self.transformer,
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        vae=self.vae,
                    )
                elif self.config.model_family == &quot;legacy&quot;:
                    from diffusers import StableDiffusionPipeline
                    self.pipeline = StableDiffusionPipeline.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        vae=self.vae
                        or (
                            self.vae_cls.from_pretrained(
                                self.config.vae_path,
                                subfolder=(
                                    &quot;vae&quot;
                                    if self.config.pretrained_vae_model_name_or_path
                                    is None
                                    else None
                                ),
                                revision=self.config.revision,
                                variant=self.config.variant,
                                force_upcast=False,
                            )
                        ),
                        unet=self.unet,
                        torch_dtype=self.config.weight_dtype,
                    )
                elif self.config.model_family == &quot;smoldit&quot;:
                    from helpers.models.smoldit import SmolDiTPipeline
                    self.pipeline = SmolDiTPipeline(
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        vae=self.vae
                        or (
                            self.vae_cls.from_pretrained(
                                self.config.vae_path,
                                subfolder=(
                                    &quot;vae&quot;
                                    if self.config.pretrained_vae_model_name_or_path
                                    is None
                                    else None
                                ),
                                revision=self.config.revision,
                                variant=self.config.variant,
                                force_upcast=False,
                            )
                        ),
                        transformer=self.transformer,
                        scheduler=None,
                    )
                elif self.config.model_family == &quot;ltxvideo&quot;:
                    from diffusers import LTXPipeline
                    self.pipeline = LTXPipeline.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        vae=self.vae,
                        transformer=self.transformer,
                    )
                elif self.config.model_family == &quot;sana&quot;:
                    from diffusers import SanaPipeline
                    self.pipeline = SanaPipeline.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        text_encoder=self.text_encoder_1
                        or (
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        vae=self.vae,
                        transformer=self.transformer,
                    )
                else:
                    sdxl_pipeline_cls = StableDiffusionXLPipeline
                    if self.config.model_family == &quot;kolors&quot;:
                        from helpers.kolors.pipeline import KolorsPipeline
                        sdxl_pipeline_cls = KolorsPipeline
                    self.pipeline = sdxl_pipeline_cls.from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        text_encoder=(
                            self.text_encoder_cls_1.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        text_encoder_2=(
                            self.text_encoder_cls_2.from_pretrained(
                                self.config.pretrained_model_name_or_path,
                                subfolder=&quot;text_encoder_2&quot;,
                                revision=self.config.revision,
                                variant=self.config.variant,
                            )
                            if self.config.save_text_encoder
                            else None
                        ),
                        tokenizer=self.tokenizer_1,
                        tokenizer_2=self.tokenizer_2,
                        vae=StateTracker.get_vae()
                        or self.vae_cls.from_pretrained(
                            self.config.vae_path,
                            subfolder=(
                                &quot;vae&quot;
                                if self.config.pretrained_vae_model_name_or_path is None
                                else None
                            ),
                            revision=self.config.revision,
                            variant=self.config.variant,
                            force_upcast=False,
                        ),
                        unet=self.unet,
                        revision=self.config.revision,
                        add_watermarker=self.config.enable_watermark,
                        torch_dtype=self.config.weight_dtype,
                    )
                if (
                    not self.config.flow_matching
                    and self.config.validation_noise_scheduler is not None
                ):
                    self.pipeline.scheduler = SCHEDULER_NAME_MAP[
                        self.config.validation_noise_scheduler
                    ].from_pretrained(
                        self.config.pretrained_model_name_or_path,
                        revision=self.config.revision,
                        subfolder=&quot;scheduler&quot;,
                        prediction_type=self.config.prediction_type,
                        timestep_spacing=self.config.training_scheduler_timestep_spacing,
                        rescale_betas_zero_snr=self.config.rescale_betas_zero_snr,
                    )
                self.pipeline.save_pretrained(
                    os.path.join(self.config.output_dir, &quot;pipeline&quot;),
                    safe_serialization=True,
                )
            if self.config.push_to_hub and self.accelerator.is_main_process:
                self.hub_manager.upload_model(validation_images, self.webhook_handler)
        self.accelerator.end_training()</file><file path="helpers/training/validation.py">import torch
import diffusers
import os
import wandb
import logging
import sys
import numpy as np
from tqdm import tqdm
from helpers.training.wrappers import unwrap_model
from PIL import Image
from helpers.training.state_tracker import StateTracker
from helpers.training.exceptions import MultiDatasetExhausted
from helpers.models.sdxl.pipeline import (
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
)
from helpers.legacy.pipeline import StableDiffusionPipeline
from diffusers.schedulers import (
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    FlowMatchEulerDiscreteScheduler,
    UniPCMultistepScheduler,
    DDIMScheduler,
    DDPMScheduler,
)
from diffusers.utils.torch_utils import is_compiled_module
from helpers.multiaspect.image import MultiaspectImage
from helpers.image_manipulation.brightness import calculate_luminance
from PIL import Image, ImageDraw, ImageFont
from diffusers import SanaPipeline
from helpers.training.deepspeed import (
    deepspeed_zero_init_disabled_context_manager,
    prepare_model_for_deepspeed,
)
from transformers.utils import ContextManagers
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;) or &quot;INFO&quot;)
try:
    from helpers.models.sd3.pipeline import (
        StableDiffusion3Pipeline,
        StableDiffusion3Img2ImgPipeline,
    )
except ImportError:
    logger.error(
        &quot;Stable Diffusion 3 not available in this release of Diffusers. Please upgrade.&quot;
    )
    raise ImportError()
SCHEDULER_NAME_MAP = {
    &quot;euler&quot;: EulerDiscreteScheduler,
    &quot;euler-a&quot;: EulerAncestralDiscreteScheduler,
    &quot;flow-match&quot;: FlowMatchEulerDiscreteScheduler,
    &quot;unipc&quot;: UniPCMultistepScheduler,
    &quot;ddim&quot;: DDIMScheduler,
    &quot;ddpm&quot;: DDPMScheduler,
    &quot;sana&quot;: FlowMatchEulerDiscreteScheduler,
}
import logging
import os
import time
from diffusers.utils import is_wandb_available
from helpers.prompts import PromptHandler
from diffusers import (
    AutoencoderKL,
    DDIMScheduler,
)
if is_wandb_available():
    import wandb
logger = logging.getLogger(&quot;validation&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;) or &quot;INFO&quot;)
def resize_validation_images(validation_images, edge_length):
    # we have to scale all the inputs to a stage4 image down to 64px smaller edge.
    resized_validation_samples = []
    for _sample in validation_images:
        validation_shortname, validation_prompt, training_sample_image = _sample
        resize_to, crop_to, new_aspect_ratio = (
            MultiaspectImage.calculate_new_size_by_pixel_edge(
                aspect_ratio=MultiaspectImage.calculate_image_aspect_ratio(
                    training_sample_image
                ),
                resolution=int(edge_length),
                original_size=training_sample_image.size,
            )
        )
        # we can be less precise here
        training_sample_image = training_sample_image.resize(crop_to)
        resized_validation_samples.append(
            (validation_shortname, validation_prompt, training_sample_image)
        )
    return resized_validation_samples
def reset_eval_datasets():
    eval_datasets = StateTracker.get_data_backends(_type=&quot;eval&quot;)
    for dataset_name, dataset in eval_datasets.items():
        if &quot;train_dataset&quot; not in dataset:
            logger.debug(
                f&quot;Skipping eval set {dataset_name} because it lacks a dataloader.&quot;
            )
        try:
            dataset[&quot;sampler&quot;]._reset_buckets(raise_exhaustion_signal=False)
        except MultiDatasetExhausted as e:
            pass
def retrieve_eval_images(dataset_name=None):
    &quot;&quot;&quot;
    If `dataset_name` is provided, only fetch samples from that specific dataset.
    Otherwise, we iterate over *all* eval datasets until we find a valid sample.
    Returns:
        A collated batch from the eval dataset(s), or raises MultiDatasetExhausted.
    &quot;&quot;&quot;
    eval_datasets = StateTracker.get_data_backends(_type=&quot;eval&quot;)
    output = {}
    eval_samples = None
    from helpers.training.collate import collate_fn
    new_sample = None
    # We loop until we successfully retrieve one collated batch or exhaust the data.
    while new_sample is None:
        # Decide which dataset(s) to pull from
        if dataset_name is not None:
            # Only attempt to pull from the requested dataset
            dataset_keys = [dataset_name]
        else:
            # Fallback: iterate over *all* eval datasets
            dataset_keys = list(eval_datasets.keys())
        for ds_name in dataset_keys:
            dataset = eval_datasets.get(ds_name)
            if not dataset or &quot;train_dataset&quot; not in dataset:
                logger.debug(
                    f&quot;Skipping eval set {ds_name} because it lacks a dataloader.&quot;
                )
                continue
            try:
                new_sample = next(dataset[&quot;sampler&quot;].__iter__())
                data_loaded = dataset[&quot;train_dataset&quot;].__getitem__(new_sample)
                if data_loaded:
                    output = collate_fn([data_loaded])
                # Indicate that we&apos;ve found a batch and can break out of the loop
                new_sample = False
                break
            except MultiDatasetExhausted as e:
                logger.debug(
                    f&quot;Ran out of evaluation samples for dataset {ds_name}. Resetting buckets.&quot;
                )
                dataset[&quot;sampler&quot;]._reset_buckets(raise_exhaustion_signal=False)
                # We re-raise if we&apos;ve exhausted this dataset. If `dataset_name` is set,
                # we effectively stop; if it&apos;s None, we move to the next dataset.
                if dataset_name is not None:
                    raise e
    return output
def retrieve_validation_images():
    &quot;&quot;&quot;
    From each data backend, collect the top 5 images for validation, such that
    we select the same images on each startup, unless the dataset changes.
    Returns:
        dict: A dictionary of shortname to image paths.
    &quot;&quot;&quot;
    args = StateTracker.get_args()
    data_backends = StateTracker.get_data_backends(
        _type=&quot;conditioning&quot; if args.controlnet else &quot;image&quot;
    )
    validation_data_backend_id = args.eval_dataset_id
    validation_set = []
    logger.info(&quot;Collecting validation images&quot;)
    for _data_backend in data_backends:
        data_backend = StateTracker.get_data_backend(_data_backend)
        data_backend_config = data_backend.get(&quot;config&quot;, {})
        should_skip_dataset = data_backend_config.get(&quot;disable_validation&quot;, False)
        logger.debug(f&quot;Backend {_data_backend}: {data_backend}&quot;)
        if &quot;id&quot; not in data_backend or (
            args.controlnet and data_backend.get(&quot;dataset_type&quot;, None) != &quot;conditioning&quot;
        ):
            logger.debug(
                f&quot;Skipping data backend: {_data_backend} dataset_type {data_backend.get(&apos;dataset_type&apos;, None)}&quot;
            )
            continue
        logger.debug(f&quot;Checking data backend: {data_backend[&apos;id&apos;]}&quot;)
        if (
            validation_data_backend_id is not None
            and data_backend[&quot;id&quot;] != validation_data_backend_id
        ) or should_skip_dataset:
            logger.warning(f&quot;Not collecting images from {data_backend[&apos;id&apos;]}&quot;)
            continue
        if &quot;sampler&quot; in data_backend:
            validation_samples_from_sampler = data_backend[
                &quot;sampler&quot;
            ].retrieve_validation_set(batch_size=args.num_eval_images)
            if &quot;stage2&quot; in args.model_type:
                validation_samples_from_sampler = resize_validation_images(
                    validation_samples_from_sampler, edge_length=64
                )
            validation_set.extend(validation_samples_from_sampler)
        else:
            logger.warning(
                f&quot;Data backend {data_backend[&apos;id&apos;]} does not have a sampler. Skipping.&quot;
            )
    return validation_set
def prepare_validation_prompt_list(args, embed_cache):
    validation_negative_prompt_embeds = None
    validation_negative_pooled_embeds = None
    validation_prompts = (
        [&quot;&quot;] if not StateTracker.get_args().validation_disable_unconditional else []
    )
    validation_shortnames = (
        [&quot;unconditional&quot;]
        if not StateTracker.get_args().validation_disable_unconditional
        else []
    )
    if not hasattr(embed_cache, &quot;model_type&quot;):
        raise ValueError(
            f&quot;The default text embed cache backend was not found. You must specify &apos;default: true&apos; on your text embed data backend via {StateTracker.get_args().data_backend_config}.&quot;
        )
    model_type = embed_cache.model_type
    validation_sample_images = None
    if (
        &quot;deepfloyd-stage2&quot; in args.model_type
        or args.controlnet
        or args.validation_using_datasets
    ):
        # Now, we prepare the DeepFloyd upscaler image inputs so that we can calculate their prompts.
        # If we don&apos;t do it here, they won&apos;t be available at inference time.
        validation_sample_images = retrieve_validation_images()
        if len(validation_sample_images) &gt; 0:
            StateTracker.set_validation_sample_images(validation_sample_images)
            # Collect the prompts for the validation images.
            for _validation_sample in tqdm(
                validation_sample_images,
                ncols=100,
                desc=&quot;Precomputing validation image embeds&quot;,
            ):
                _, validation_prompt, _ = _validation_sample
                embed_cache.compute_embeddings_for_prompts(
                    [validation_prompt], load_from_cache=False
                )
            time.sleep(5)
    if args.validation_prompt_library:
        # Use the SimpleTuner prompts library for validation prompts.
        from helpers.prompts import prompts as prompt_library
        # Iterate through the prompts with a progress bar
        for shortname, prompt in tqdm(
            prompt_library.items(),
            leave=False,
            ncols=100,
            desc=&quot;Precomputing validation prompt embeddings&quot;,
        ):
            embed_cache.compute_embeddings_for_prompts(
                [prompt], is_validation=True, load_from_cache=False
            )
            validation_prompts.append(prompt)
            validation_shortnames.append(shortname)
    if args.user_prompt_library is not None:
        user_prompt_library = PromptHandler.load_user_prompts(args.user_prompt_library)
        for shortname, prompt in tqdm(
            user_prompt_library.items(),
            leave=False,
            ncols=100,
            desc=&quot;Precomputing user prompt library embeddings&quot;,
        ):
            embed_cache.compute_embeddings_for_prompts(
                [prompt], is_validation=True, load_from_cache=False
            )
            validation_prompts.append(prompt)
            validation_shortnames.append(shortname)
    if args.validation_prompt is not None:
        # Use a single prompt for validation.
        # This will add a single prompt to the prompt library, if in use.
        validation_prompts = validation_prompts + [args.validation_prompt]
        validation_shortnames = validation_shortnames + [&quot;validation&quot;]
        embed_cache.compute_embeddings_for_prompts(
            [args.validation_prompt], is_validation=True, load_from_cache=False
        )
    # Compute negative embed for validation prompts, if any are set.
    if validation_prompts:
        logger.info(&quot;Precomputing the negative prompt embed for validations.&quot;)
        if model_type == &quot;sdxl&quot; or model_type == &quot;sd3&quot; or model_type == &quot;kolors&quot;:
            (
                validation_negative_prompt_embeds,
                validation_negative_pooled_embeds,
            ) = embed_cache.compute_embeddings_for_prompts(
                [StateTracker.get_args().validation_negative_prompt],
                is_validation=True,
                load_from_cache=False,
            )
            return (
                validation_prompts,
                validation_shortnames,
                validation_negative_prompt_embeds,
                validation_negative_pooled_embeds,
            )
        elif model_type == &quot;legacy&quot;:
            validation_negative_prompt_embeds = (
                embed_cache.compute_embeddings_for_prompts(
                    [StateTracker.get_args().validation_negative_prompt],
                    load_from_cache=False,
                )
            )
            return (
                validation_prompts,
                validation_shortnames,
                validation_negative_prompt_embeds,
                None,
            )
        elif model_type in [&quot;pixart_sigma&quot;, &quot;smoldit&quot;, &quot;sana&quot;, &quot;ltxvideo&quot;]:
            # we use the legacy encoder but we return no pooled embeds.
            validation_negative_prompt_embeds = embed_cache.compute_embeddings_for_prompts(
                [StateTracker.get_args().validation_negative_prompt],
                load_from_cache=False,
                is_negative_prompt=True,  # sana needs this to disable Complex Human Instruction on negative embed generation
            )
            return (
                validation_prompts,
                validation_shortnames,
                validation_negative_prompt_embeds,
                None,
            )
        elif model_type == &quot;flux&quot;:
            (
                validation_negative_prompt_embeds,
                validation_negative_pooled_embeds,
                validation_negative_time_ids,
                _,
            ) = embed_cache.compute_embeddings_for_prompts(
                [StateTracker.get_args().validation_negative_prompt],
                load_from_cache=False,
            )
            return (
                validation_prompts,
                validation_shortnames,
                validation_negative_prompt_embeds,
                validation_negative_pooled_embeds,
                validation_negative_time_ids,
            )
        else:
            raise ValueError(f&quot;Unknown model type &apos;{model_type}&apos;&quot;)
def parse_validation_resolution(input_str: str) -&gt; tuple:
    &quot;&quot;&quot;
    If the args.validation_resolution:
     - is an int, we&apos;ll treat it as height and width square aspect
     - if it has an x in it, we will split and treat as WIDTHxHEIGHT
     - if it has comma, we will split and treat each value as above
    &quot;&quot;&quot;
    if isinstance(input_str, int) or input_str.isdigit():
        if (
            &quot;deepfloyd-stage2&quot; in StateTracker.get_args().model_type
            and int(input_str) &lt; 256
        ):
            raise ValueError(
                &quot;Cannot use less than 256 resolution for DeepFloyd stage 2.&quot;
            )
        return (input_str, input_str)
    if &quot;x&quot; in input_str:
        pieces = input_str.split(&quot;x&quot;)
        if &quot;deepfloyd-stage2&quot; in StateTracker.get_args().model_type and (
            int(pieces[0]) &lt; 256 or int(pieces[1]) &lt; 256
        ):
            raise ValueError(
                &quot;Cannot use less than 256 resolution for DeepFloyd stage 2.&quot;
            )
        return (int(pieces[0]), int(pieces[1]))
def get_validation_resolutions():
    &quot;&quot;&quot;
    If the args.validation_resolution:
     - is an int, we&apos;ll treat it as height and width square aspect
     - if it has an x in it, we will split and treat as WIDTHxHEIGHT
     - if it has comma, we will split and treat each value as above
    &quot;&quot;&quot;
    validation_resolution_parameter = StateTracker.get_args().validation_resolution
    if (
        type(validation_resolution_parameter) is str
        and &quot;,&quot; in validation_resolution_parameter
    ):
        return [
            parse_validation_resolution(res)
            for res in validation_resolution_parameter.split(&quot;,&quot;)
        ]
    return [parse_validation_resolution(validation_resolution_parameter)]
def get_validation_resolutions():
    &quot;&quot;&quot;
    If the args.validation_resolution:
     - is an int, we&apos;ll treat it as height and width square aspect
     - if it has an x in it, we will split and treat as WIDTHxHEIGHT
     - if it has comma, we will split and treat each value as above
    &quot;&quot;&quot;
    validation_resolution_parameter = StateTracker.get_args().validation_resolution
    if (
        type(validation_resolution_parameter) is str
        and &quot;,&quot; in validation_resolution_parameter
    ):
        return [
            parse_validation_resolution(res)
            for res in validation_resolution_parameter.split(&quot;,&quot;)
        ]
    return [parse_validation_resolution(validation_resolution_parameter)]
def parse_validation_resolution(input_str: str) -&gt; tuple:
    &quot;&quot;&quot;
    If the args.validation_resolution:
     - is an int, we&apos;ll treat it as height and width square aspect
     - if it has an x in it, we will split and treat as WIDTHxHEIGHT
     - if it has comma, we will split and treat each value as above
    &quot;&quot;&quot;
    is_df_ii = (
        True if &quot;deepfloyd-stage2&quot; in StateTracker.get_args().model_type else False
    )
    if isinstance(input_str, int) or input_str.isdigit():
        if is_df_ii and int(input_str) &lt; 256:
            raise ValueError(
                &quot;Cannot use less than 256 resolution for DeepFloyd stage 2.&quot;
            )
        return (input_str, input_str)
    if &quot;x&quot; in input_str:
        pieces = input_str.split(&quot;x&quot;)
        if is_df_ii and (int(pieces[0]) &lt; 256 or int(pieces[1]) &lt; 256):
            raise ValueError(
                &quot;Cannot use less than 256 resolution for DeepFloyd stage 2.&quot;
            )
        return (int(pieces[0]), int(pieces[1]))
class Validation:
    def __init__(
        self,
        accelerator,
        unet,
        transformer,
        args,
        validation_prompts,
        validation_shortnames,
        text_encoder_1,
        tokenizer,
        vae_path,
        weight_dtype,
        embed_cache,
        validation_negative_pooled_embeds,
        validation_negative_prompt_embeds,
        text_encoder_2,
        tokenizer_2,
        ema_model,
        vae,
        controlnet=None,
        text_encoder_3=None,
        tokenizer_3=None,
        is_deepspeed: bool = False,
        model_evaluator=None,
        trainable_parameters=None,
    ):
        self.trainable_parameters = trainable_parameters
        self.accelerator = accelerator
        self.prompt_handler = None
        self.unet = unet
        self.transformer = transformer
        self.controlnet = controlnet
        self.args = args
        self.save_dir = os.path.join(args.output_dir, &quot;validation_images&quot;)
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir, exist_ok=True)
        self.global_step = None
        self.global_resume_step = None
        self.text_encoder_1 = text_encoder_1
        self.tokenizer_1 = tokenizer
        self.text_encoder_2 = text_encoder_2
        self.tokenizer_2 = tokenizer_2
        self.vae_path = vae_path
        self.validation_prompts = validation_prompts
        self.validation_shortnames = validation_shortnames
        self.validation_images = None
        self.weight_dtype = weight_dtype
        self.embed_cache = embed_cache
        self.validation_negative_prompt_mask = None
        self.validation_negative_pooled_embeds = validation_negative_pooled_embeds
        self.validation_negative_prompt_embeds = (
            validation_negative_prompt_embeds
            if (
                type(validation_negative_prompt_embeds) is not list
                and type(validation_negative_prompt_embeds) is not tuple
            )
            else validation_negative_prompt_embeds[0]
        )
        self.ema_model = ema_model
        self.ema_enabled = False
        self.vae = vae
        self.pipeline = None
        self.deepfloyd = True if &quot;deepfloyd&quot; in self.args.model_type else False
        self.deepfloyd_stage2 = (
            True if &quot;deepfloyd-stage2&quot; in self.args.model_type else False
        )
        self._discover_validation_input_samples()
        self.validation_resolutions = (
            get_validation_resolutions() if not self.deepfloyd_stage2 else [&quot;base-256&quot;]
        )
        self.text_encoder_3 = text_encoder_3
        self.tokenizer_3 = tokenizer_3
        self.flow_matching = (
            self.args.model_family == &quot;sd3&quot;
            and self.args.flow_matching_loss != &quot;diffusion&quot;
        ) or self.args.model_family == &quot;flux&quot;
        self.deepspeed = is_deepspeed
        if is_deepspeed:
            if args.use_ema:
                if args.ema_validation != &quot;none&quot;:
                    logger.error(
                        &quot;EMA validation is not supported via DeepSpeed.&quot;
                        &quot; Please use --ema_validation=none or disable DeepSpeed.&quot;
                    )
                    sys.exit(1)
        self.inference_device = (
            accelerator.device
            if not is_deepspeed
            else &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
        )
        self.model_evaluator = model_evaluator
        if self.model_evaluator is not None:
            logger.info(f&quot;Using model evaluator: {self.model_evaluator}&quot;)
        self._update_state()
        self.eval_scores = {}
    def _validation_seed_source(self):
        if self.args.validation_seed_source == &quot;gpu&quot;:
            return self.inference_device
        elif self.args.validation_seed_source == &quot;cpu&quot;:
            return &quot;cpu&quot;
        else:
            raise Exception(&quot;Unknown validation seed source. Options: cpu, gpu&quot;)
    def _get_generator(self):
        _validation_seed_source = self._validation_seed_source()
        _generator = torch.Generator(device=_validation_seed_source).manual_seed(
            self.args.validation_seed or self.args.seed or 0
        )
        return _generator
    def clear_text_encoders(self):
        &quot;&quot;&quot;
        Sets all text encoders to None.
        Returns:
            None
        &quot;&quot;&quot;
        self.text_encoder_1 = None
        self.text_encoder_2 = None
        self.text_encoder_3 = None
    def init_vae(self):
        args = StateTracker.get_args()
        vae_path = (
            args.pretrained_model_name_or_path
            if args.pretrained_vae_model_name_or_path is None
            else args.pretrained_vae_model_name_or_path
        )
        precached_vae = StateTracker.get_vae()
        logger.debug(
            f&quot;Was the VAE loaded? {precached_vae if precached_vae is None else &apos;Yes&apos;}&quot;
        )
        if self.args.model_family == &quot;sana&quot;:
            from diffusers import AutoencoderDC as AutoencoderClass
        else:
            from diffusers import AutoencoderKL as AutoencoderClass
        self.vae = precached_vae
        if self.vae is None:
            logger.info(f&quot;Initialising {AutoencoderClass}&quot;)
            with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
                self.vae = AutoencoderClass.from_pretrained(
                    vae_path,
                    subfolder=(
                        &quot;vae&quot;
                        if args.pretrained_vae_model_name_or_path is None
                        else None
                    ),
                    revision=args.revision,
                    force_upcast=False,
                ).to(self.inference_device)
        StateTracker.set_vae(self.vae)
        return self.vae
    def _discover_validation_input_samples(self):
        &quot;&quot;&quot;
        If we have some workflow that requires image inputs for validation, we&apos;ll bind those now.
        Returns:
            Validation object (self)
        &quot;&quot;&quot;
        self.validation_image_inputs = None
        if (
            self.deepfloyd_stage2
            or self.args.validation_using_datasets
            or self.args.controlnet
        ):
            self.validation_image_inputs = retrieve_validation_images()
            # Validation inputs are in the format of a list of tuples:
            # [(shortname, prompt, image), ...]
            logger.debug(
                f&quot;Image inputs discovered for validation: {self.validation_image_inputs}&quot;
            )
    def _pipeline_cls(self):
        model_type = StateTracker.get_model_family()
        if model_type == &quot;sdxl&quot;:
            if self.args.controlnet:
                from diffusers.pipelines import StableDiffusionXLControlNetPipeline
                return StableDiffusionXLControlNetPipeline
            if self.args.validation_using_datasets:
                return StableDiffusionXLImg2ImgPipeline
            return StableDiffusionXLPipeline
        elif model_type == &quot;flux&quot;:
            from helpers.models.flux import FluxPipeline
            if self.args.controlnet:
                raise NotImplementedError(&quot;Flux ControlNet is not yet supported.&quot;)
            if self.args.validation_using_datasets:
                raise NotImplementedError(
                    &quot;Flux inference validation using img2img is not yet supported. Please remove --validation_using_datasets.&quot;
                )
            return FluxPipeline
        elif model_type == &quot;kolors&quot;:
            if self.args.controlnet:
                raise NotImplementedError(&quot;Kolors ControlNet is not yet supported.&quot;)
            if self.args.validation_using_datasets:
                try:
                    from helpers.kolors.pipeline import KolorsImg2ImgPipeline
                except:
                    logger.error(
                        &quot;Kolors pipeline requires the latest version of Diffusers.&quot;
                    )
                return KolorsImg2ImgPipeline
            try:
                from helpers.kolors.pipeline import KolorsPipeline
            except Exception:
                logger.error(
                    &quot;Kolors pipeline requires the latest version of Diffusers.&quot;
                )
            return KolorsPipeline
        elif model_type == &quot;legacy&quot;:
            if self.deepfloyd_stage2:
                from diffusers.pipelines import IFSuperResolutionPipeline
                return IFSuperResolutionPipeline
            return StableDiffusionPipeline
        elif model_type == &quot;sd3&quot;:
            if self.args.controlnet:
                raise Exception(&quot;SD3 ControlNet is not yet supported.&quot;)
            if self.args.validation_using_datasets:
                return StableDiffusion3Img2ImgPipeline
            return StableDiffusion3Pipeline
        elif model_type == &quot;pixart_sigma&quot;:
            if self.args.controlnet:
                raise Exception(
                    &quot;PixArt Sigma ControlNet inference validation is not yet supported.&quot;
                )
            if self.args.validation_using_datasets:
                raise Exception(
                    &quot;PixArt Sigma inference validation using img2img is not yet supported. Please remove --validation_using_datasets.&quot;
                )
            from helpers.models.pixart.pipeline import PixArtSigmaPipeline
            return PixArtSigmaPipeline
        elif model_type == &quot;smoldit&quot;:
            from helpers.models.smoldit import SmolDiTPipeline
            return SmolDiTPipeline
        elif model_type == &quot;sana&quot;:
            from diffusers import SanaPipeline
            return SanaPipeline
        elif model_type == &quot;ltxvideo&quot;:
            from diffusers import LTXPipeline
            return LTXPipeline
        else:
            raise NotImplementedError(
                f&quot;Model type {model_type} not implemented for validation.&quot;
            )
    def _gather_prompt_embeds(self, validation_prompt: str):
        prompt_embeds = {}
        current_validation_prompt_mask = None
        if (
            StateTracker.get_model_family() == &quot;sdxl&quot;
            or StateTracker.get_model_family() == &quot;sd3&quot;
            or StateTracker.get_model_family() == &quot;kolors&quot;
            or StateTracker.get_model_family() == &quot;flux&quot;
        ):
            _embed = self.embed_cache.compute_embeddings_for_prompts(
                [validation_prompt]
            )
            current_validation_time_ids = None
            if len(_embed) == 2:
                (
                    current_validation_prompt_embeds,
                    current_validation_pooled_embeds,
                ) = _embed
            elif len(_embed) == 3:
                (
                    current_validation_prompt_embeds,
                    current_validation_pooled_embeds,
                    current_validation_time_ids,
                ) = _embed
            elif len(_embed) == 4:
                (
                    current_validation_prompt_embeds,
                    current_validation_pooled_embeds,
                    current_validation_time_ids,
                    current_validation_prompt_mask,
                ) = _embed
            else:
                raise ValueError(
                    f&quot;Unexpected number of embeddings returned from cache: {_embed}&quot;
                )
            current_validation_pooled_embeds = current_validation_pooled_embeds.to(
                device=self.inference_device, dtype=self.weight_dtype
            )
            if current_validation_time_ids is not None:
                current_validation_time_ids = current_validation_time_ids.to(
                    device=self.inference_device, dtype=self.weight_dtype
                )
            self.validation_negative_pooled_embeds = (
                self.validation_negative_pooled_embeds.to(
                    device=self.inference_device, dtype=self.weight_dtype
                )
            )
            prompt_embeds[&quot;pooled_prompt_embeds&quot;] = current_validation_pooled_embeds.to(
                device=self.inference_device, dtype=self.weight_dtype
            )
            prompt_embeds[&quot;negative_pooled_prompt_embeds&quot;] = (
                self.validation_negative_pooled_embeds
            )
            # if current_validation_time_ids is not None:
            #     prompt_embeds[&quot;time_ids&quot;] = current_validation_time_ids
        elif (
            StateTracker.get_model_family() == &quot;legacy&quot;
            or StateTracker.get_model_family() == &quot;pixart_sigma&quot;
            or StateTracker.get_model_family() == &quot;smoldit&quot;
            or StateTracker.get_model_family() == &quot;sana&quot;
            or StateTracker.get_model_family() == &quot;ltxvideo&quot;
        ):
            self.validation_negative_pooled_embeds = None
            current_validation_pooled_embeds = None
            current_validation_prompt_embeds = (
                self.embed_cache.compute_embeddings_for_prompts([validation_prompt])
            )
            if StateTracker.get_model_family() in [
                &quot;pixart_sigma&quot;,
                &quot;smoldit&quot;,
                &quot;sana&quot;,
                &quot;ltxvideo&quot;,
            ]:
                current_validation_prompt_embeds, current_validation_prompt_mask = (
                    current_validation_prompt_embeds
                )
                current_validation_prompt_embeds = current_validation_prompt_embeds[
                    0
                ].to(device=self.inference_device, dtype=self.weight_dtype)
                if (
                    type(self.validation_negative_prompt_embeds) is tuple
                    or type(self.validation_negative_prompt_embeds) is list
                ):
                    (
                        self.validation_negative_prompt_embeds,
                        self.validation_negative_prompt_mask,
                    ) = self.validation_negative_prompt_embeds[0]
            else:
                current_validation_prompt_embeds = current_validation_prompt_embeds[
                    0
                ].to(device=self.inference_device, dtype=self.weight_dtype)
            # logger.debug(
            #     f&quot;Validations received the prompt embed: ({type(current_validation_prompt_embeds)}) positive={current_validation_prompt_embeds.shape if type(current_validation_prompt_embeds) is not list else current_validation_prompt_embeds[0].shape},&quot;
            #     f&quot; ({type(self.validation_negative_prompt_embeds)}) negative={self.validation_negative_prompt_embeds.shape if type(self.validation_negative_prompt_embeds) is not list else self.validation_negative_prompt_embeds[0].shape}&quot;
            # )
            # logger.debug(
            #     f&quot;Dtypes: {current_validation_prompt_embeds.dtype}, {self.validation_negative_prompt_embeds.dtype}&quot;
            # )
        else:
            raise NotImplementedError(
                f&quot;Model type {StateTracker.get_model_family()} not implemented for validation.&quot;
            )
        current_validation_prompt_embeds = current_validation_prompt_embeds.to(
            device=self.inference_device, dtype=self.weight_dtype
        )
        self.validation_negative_prompt_embeds = (
            self.validation_negative_prompt_embeds.to(
                device=self.inference_device, dtype=self.weight_dtype
            )
        )
        # when sampling unconditional guidance, you should only zero one or the other prompt, and not both.
        # we&apos;ll assume that the user has a negative prompt, so that the unconditional sampling works.
        # the positive prompt embed is zeroed out for SDXL at the time of it being placed into the cache.
        # the embeds are not zeroed out for any other model, including Stable Diffusion 3.
        prompt_embeds[&quot;prompt_embeds&quot;] = current_validation_prompt_embeds
        prompt_embeds[&quot;negative_prompt_embeds&quot;] = self.validation_negative_prompt_embeds
        if StateTracker.get_model_family() in [
            &quot;pixart_sigma&quot;,
            &quot;smoldit&quot;,
            &quot;sana&quot;,
            &quot;ltxvideo&quot;,
        ] or (
            StateTracker.get_model_family() == &quot;flux&quot;
            and StateTracker.get_args().flux_attention_masked_training
        ):
            logger.debug(
                f&quot;mask: {current_validation_prompt_mask.shape if type(current_validation_prompt_mask) is torch.Tensor else None}&quot;
            )
            assert current_validation_prompt_mask is not None
            prompt_embeds[&quot;prompt_mask&quot;] = current_validation_prompt_mask
            prompt_embeds[&quot;negative_mask&quot;] = self.validation_negative_prompt_mask
        return prompt_embeds
    def _benchmark_path(self, benchmark: str = &quot;base_model&quot;):
        # does the benchmark directory exist?
        if not os.path.exists(os.path.join(self.args.output_dir, &quot;benchmarks&quot;)):
            os.makedirs(os.path.join(self.args.output_dir, &quot;benchmarks&quot;), exist_ok=True)
        return os.path.join(self.args.output_dir, &quot;benchmarks&quot;, benchmark)
    def stitch_benchmark_image(
        self,
        validation_image_result,
        benchmark_image,
        separator_width=5,
        labels=[&quot;base model&quot;, &quot;checkpoint&quot;],
    ):
        &quot;&quot;&quot;
        For each image, make a new canvas and place it side by side with its equivalent from {self.validation_image_inputs}
        Add &quot;base model&quot; text to the left image and &quot;checkpoint&quot; text to the right image
        Include a separator between the images
        &quot;&quot;&quot;
        # Calculate new dimensions
        new_width = (
            benchmark_image.size[0] + validation_image_result.size[0] + separator_width
        )
        new_height = benchmark_image.size[1]
        # Create a new image with a white background
        new_image = Image.new(&quot;RGB&quot;, (new_width, new_height), color=&quot;white&quot;)
        # Paste the images with a gap between them
        new_image.paste(benchmark_image, (0, 0))
        new_image.paste(
            validation_image_result, (benchmark_image.size[0] + separator_width, 0)
        )
        # Create a drawing object
        draw = ImageDraw.Draw(new_image)
        # Use a default font
        try:
            font = ImageFont.truetype(&quot;arial.ttf&quot;, 36)
        except IOError:
            font = ImageFont.load_default()
        # Add text to the left image
        if labels[0] is not None:
            draw.text(
                (10, 10),
                labels[0],
                fill=(255, 255, 255),
                font=font,
                stroke_width=2,
                stroke_fill=(0, 0, 0),
            )
        if labels[1] is not None:
            # Add text to the right image
            draw.text(
                (benchmark_image.size[0] + separator_width + 10, 10),
                labels[1],
                fill=(255, 255, 255),
                font=font,
                stroke_width=2,
                stroke_fill=(0, 0, 0),
            )
        # Draw a vertical line as a separator
        line_color = (200, 200, 200)  # Light gray
        for i in range(separator_width):
            x = validation_image_result.size[0] + i
            draw.line([(x, 0), (x, new_height)], fill=line_color)
        return new_image
    def _benchmark_image(self, shortname, resolution):
        &quot;&quot;&quot;
        We will retrieve the benchmark image for the shortname.
        &quot;&quot;&quot;
        if not self.benchmark_exists():
            return None
        base_model_benchmark = self._benchmark_path(&quot;base_model&quot;)
        benchmark_image = None
        _test_filename = f&quot;{shortname}_{resolution[0]}x{resolution[1]}.png&quot;
        for _benchmark_image in os.listdir(base_model_benchmark):
            _basename = os.path.basename(_benchmark_image)
            if _basename == _test_filename:
                benchmark_image = Image.open(
                    os.path.join(base_model_benchmark, _benchmark_image)
                )
                break
        return benchmark_image
    def _benchmark_images(self):
        &quot;&quot;&quot;
        We will retrieve the benchmark images so they can be stitched to the validation outputs.
        &quot;&quot;&quot;
        if not self.benchmark_exists():
            return None
        benchmark_images = []
        base_model_benchmark = self._benchmark_path(&quot;base_model&quot;)
        for _benchmark_image in os.listdir(base_model_benchmark):
            if _benchmark_image.endswith(&quot;.png&quot;):
                benchmark_images.append(
                    (
                        _benchmark_image.replace(&quot;.png&quot;, &quot;&quot;),
                        f&quot;Base model benchmark image {_benchmark_image}&quot;,
                        Image.open(
                            os.path.join(base_model_benchmark, _benchmark_image)
                        ),
                    )
                )
        return benchmark_images
    def benchmark_exists(self, benchmark: str = &quot;base_model&quot;):
        &quot;&quot;&quot;
        Determines whether the base model benchmark outputs already exist.
        &quot;&quot;&quot;
        base_model_benchmark = self._benchmark_path()
        return os.path.exists(base_model_benchmark)
    def save_benchmark(self, benchmark: str = &quot;base_model&quot;):
        &quot;&quot;&quot;
        Saves the benchmark outputs for the base model.
        &quot;&quot;&quot;
        base_model_benchmark = self._benchmark_path(benchmark=benchmark)
        if not os.path.exists(base_model_benchmark):
            os.makedirs(base_model_benchmark, exist_ok=True)
        if self.validation_images is None:
            return
        for shortname, image_list in self.validation_images.items():
            for idx, image in enumerate(image_list):
                if hasattr(image, &quot;size&quot;):
                    width, height = image.size
                    image.save(
                        os.path.join(
                            base_model_benchmark, f&quot;{shortname}_{width}x{height}.png&quot;
                        )
                    )
                elif type(image) is list:
                    # maybe video
                    print(f&quot;video? {image}&quot;)
                    if self.args.model_family in [&quot;ltxvideo&quot;]:
                        from diffusers.utils.export_utils import export_to_video
                        export_to_video(
                            image,
                            os.path.join(
                                base_model_benchmark, f&quot;{shortname}_{idx}.mp4&quot;
                            ),
                            fps=self.args.framerate,
                        )
    def _update_state(self):
        &quot;&quot;&quot;Updates internal state with the latest from StateTracker.&quot;&quot;&quot;
        self.global_step = StateTracker.get_global_step()
        self.global_resume_step = StateTracker.get_global_resume_step() or 1
    def would_validate(
        self,
        step: int = 0,
        validation_type=&quot;intermediary&quot;,
        force_evaluation: bool = False,
    ):
        # a wrapper for should_perform_intermediary_validation that can run in the training loop
        self._update_state()
        return self.should_perform_intermediary_validation(
            step, self.validation_prompts, validation_type
        ) or (step == 0 and validation_type == &quot;base_model&quot;)
    def run_validations(
        self,
        step: int = 0,
        validation_type=&quot;intermediary&quot;,
        force_evaluation: bool = False,
        skip_execution: bool = False,
    ):
        self._update_state()
        would_do_intermediary_validation = self.should_perform_intermediary_validation(
            step, self.validation_prompts, validation_type
        ) or (step == 0 and validation_type == &quot;base_model&quot;)
        logger.debug(
            f&quot;Should evaluate: {would_do_intermediary_validation}, force evaluation: {force_evaluation}, skip execution: {skip_execution}&quot;
        )
        if not would_do_intermediary_validation and not force_evaluation:
            return self
        if would_do_intermediary_validation and validation_type == &quot;final&quot;:
            # If the validation would have fired off, we&apos;ll skip it.
            # This is useful at the end of training so we don&apos;t validate 2x.
            logger.debug(
                &quot;Not running validation because intermediary might have already fired off.&quot;
            )
            return self
        if StateTracker.get_webhook_handler() is not None:
            StateTracker.get_webhook_handler().send(
                message=&quot;Validations are generating.. this might take a minute! 🖼️&quot;,
                message_level=&quot;info&quot;,
            )
        if self.accelerator.is_main_process or self.deepspeed:
            logger.debug(&quot;Starting validation process...&quot;)
            diffusers.utils.logging._tqdm_active = False
            self.setup_pipeline(validation_type)
            if self.pipeline is None:
                logger.error(
                    &quot;Not able to run validations, we did not obtain a valid pipeline.&quot;
                )
                self.validation_images = None
                return self
            self.setup_scheduler()
            self.process_prompts()
            self.finalize_validation(validation_type)
            if self.evaluation_result is not None:
                logger.info(f&quot;Evaluation result: {self.evaluation_result}&quot;)
            logger.debug(&quot;Validation process completed.&quot;)
            self.clean_pipeline()
        return self
    def should_perform_intermediary_validation(
        self, step, validation_prompts, validation_type
    ):
        should_do_intermediary_validation = (
            validation_prompts
            and self.global_step % self.args.validation_steps == 0
            and step % self.args.gradient_accumulation_steps == 0
            and self.global_step &gt; self.global_resume_step
        )
        return should_do_intermediary_validation and (
            self.accelerator.is_main_process or self.deepseed
        )
    def setup_scheduler(self):
        if self.flow_matching and not self.args.model_family == &quot;sana&quot;:
            # NO TOUCHIE FOR FLOW-MATCHING.
            # Touchie for sana though. It needs a new scheduler on every inference.
            return
        elif self.args.model_family == &quot;sana&quot;:
            self.args.validation_noise_scheduler = &quot;sana&quot;
        if self.args.validation_noise_scheduler is None:
            return
        scheduler_args = {}
        if (
            self.pipeline is not None
            and &quot;variance_type&quot; in self.pipeline.scheduler.config
        ):
            variance_type = self.pipeline.scheduler.config.variance_type
            if variance_type in [&quot;learned&quot;, &quot;learned_range&quot;]:
                variance_type = &quot;fixed_small&quot;
            scheduler_args[&quot;variance_type&quot;] = variance_type
        if self.deepfloyd:
            self.args.validation_noise_scheduler = &quot;ddpm&quot;
        scheduler = SCHEDULER_NAME_MAP[
            self.args.validation_noise_scheduler
        ].from_pretrained(
            self.args.pretrained_model_name_or_path,
            subfolder=&quot;scheduler&quot;,
            revision=self.args.revision,
            prediction_type=self.args.prediction_type,
            timestep_spacing=self.args.inference_scheduler_timestep_spacing,
            rescale_betas_zero_snr=self.args.rescale_betas_zero_snr,
            **scheduler_args,
        )
        if self.pipeline is not None:
            self.pipeline.scheduler = scheduler
        return scheduler
    def setup_pipeline(self, validation_type):
        if hasattr(self.accelerator, &quot;_lycoris_wrapped_network&quot;):
            self.accelerator._lycoris_wrapped_network.set_multiplier(
                float(getattr(self.args, &quot;validation_lycoris_strength&quot;, 1.0))
            )
        if self.pipeline is None:
            pipeline_cls = self._pipeline_cls()
            extra_pipeline_kwargs = {
                &quot;text_encoder&quot;: self.text_encoder_1,
                &quot;tokenizer&quot;: self.tokenizer_1,
                &quot;vae&quot;: self.vae,
            }
            if self.args.model_family in [&quot;legacy&quot;]:
                extra_pipeline_kwargs[&quot;safety_checker&quot;] = None
            if self.args.model_family in [&quot;sd3&quot;, &quot;sdxl&quot;, &quot;flux&quot;]:
                extra_pipeline_kwargs[&quot;text_encoder_2&quot;] = None
            if self.args.model_family in [&quot;sd3&quot;]:
                extra_pipeline_kwargs[&quot;text_encoder_3&quot;] = None
            if type(pipeline_cls) is StableDiffusionXLPipeline:
                del extra_pipeline_kwargs[&quot;text_encoder&quot;]
                del extra_pipeline_kwargs[&quot;tokenizer&quot;]
                if validation_type == &quot;final&quot;:
                    if self.text_encoder_1 is not None:
                        extra_pipeline_kwargs[&quot;text_encoder_1&quot;] = unwrap_model(
                            self.accelerator, self.text_encoder_1
                        )
                        extra_pipeline_kwargs[&quot;tokenizer_1&quot;] = self.tokenizer_1
                        if self.text_encoder_2 is not None:
                            extra_pipeline_kwargs[&quot;text_encoder_2&quot;] = unwrap_model(
                                self.accelerator, self.text_encoder_2
                            )
                            extra_pipeline_kwargs[&quot;tokenizer_2&quot;] = self.tokenizer_2
                else:
                    extra_pipeline_kwargs[&quot;text_encoder_1&quot;] = None
                    extra_pipeline_kwargs[&quot;tokenizer_1&quot;] = None
                    extra_pipeline_kwargs[&quot;text_encoder_2&quot;] = None
                    extra_pipeline_kwargs[&quot;tokenizer_2&quot;] = None
            if self.args.model_family == &quot;smoldit&quot;:
                extra_pipeline_kwargs[&quot;transformer&quot;] = unwrap_model(
                    self.accelerator, self.transformer
                )
                extra_pipeline_kwargs[&quot;tokenizer&quot;] = self.tokenizer_1
                extra_pipeline_kwargs[&quot;text_encoder&quot;] = self.text_encoder_1
                extra_pipeline_kwargs[&quot;scheduler&quot;] = self.setup_scheduler()
            if self.args.controlnet:
                # ControlNet training has an additional adapter thingy.
                extra_pipeline_kwargs[&quot;controlnet&quot;] = unwrap_model(
                    self.accelerator, self.controlnet
                )
            if self.unet is not None:
                extra_pipeline_kwargs[&quot;unet&quot;] = unwrap_model(
                    self.accelerator, self.unet
                )
            if self.transformer is not None:
                extra_pipeline_kwargs[&quot;transformer&quot;] = unwrap_model(
                    self.accelerator, self.transformer
                )
            if self.args.model_family == &quot;sd3&quot; and self.args.train_text_encoder:
                if self.text_encoder_1 is not None:
                    extra_pipeline_kwargs[&quot;text_encoder&quot;] = unwrap_model(
                        self.accelerator, self.text_encoder_1
                    )
                    extra_pipeline_kwargs[&quot;tokenizer&quot;] = self.tokenizer_1
                if self.text_encoder_2 is not None:
                    extra_pipeline_kwargs[&quot;text_encoder_2&quot;] = unwrap_model(
                        self.accelerator, self.text_encoder_2
                    )
                    extra_pipeline_kwargs[&quot;tokenizer_2&quot;] = self.tokenizer_2
                if self.text_encoder_3 is not None:
                    extra_pipeline_kwargs[&quot;text_encoder_3&quot;] = unwrap_model(
                        self.accelerator, self.text_encoder_3
                    )
                    extra_pipeline_kwargs[&quot;tokenizer_3&quot;] = self.tokenizer_3
            if self.vae is None or not hasattr(self.vae, &quot;device&quot;):
                extra_pipeline_kwargs[&quot;vae&quot;] = self.init_vae()
            else:
                logger.info(f&quot;Found VAE: {self.vae.config}&quot;)
            if (
                &quot;vae&quot; in extra_pipeline_kwargs
                and extra_pipeline_kwargs.get(&quot;vae&quot;) is not None
                and extra_pipeline_kwargs[&quot;vae&quot;].device != self.inference_device
            ):
                extra_pipeline_kwargs[&quot;vae&quot;] = extra_pipeline_kwargs[&quot;vae&quot;].to(
                    self.inference_device
                )
            pipeline_kwargs = {
                &quot;pretrained_model_name_or_path&quot;: self.args.pretrained_model_name_or_path,
                &quot;revision&quot;: self.args.revision,
                &quot;variant&quot;: self.args.variant,
                &quot;torch_dtype&quot;: self.weight_dtype,
                **extra_pipeline_kwargs,
            }
            logger.debug(f&quot;Initialising pipeline with kwargs: {pipeline_kwargs}&quot;)
            attempt = 0
            while attempt &lt; 3:
                attempt += 1
                try:
                    if self.args.model_family == &quot;smoldit&quot;:
                        self.pipeline = pipeline_cls(
                            vae=self.vae,
                            transformer=unwrap_model(
                                self.accelerator, self.transformer
                            ),
                            tokenizer=self.tokenizer_1,
                            text_encoder=self.text_encoder_1,
                            scheduler=self.setup_scheduler(),
                        )
                    else:
                        self.pipeline = pipeline_cls.from_pretrained(**pipeline_kwargs)
                except Exception as e:
                    import traceback
                    logger.error(e)
                    logger.error(traceback.format_exc())
                    continue
                break
            if self.args.validation_torch_compile:
                if self.deepspeed:
                    logger.warning(
                        &quot;DeepSpeed does not support torch compile. Disabling. Set --validation_torch_compile=False to suppress this warning.&quot;
                    )
                elif self.args.lora_type.lower() == &quot;lycoris&quot;:
                    logger.warning(
                        &quot;LyCORIS does not support torch compile for validation due to graph compile breaks. Disabling. Set --validation_torch_compile=False to suppress this warning.&quot;
                    )
                else:
                    if self.unet is not None and not is_compiled_module(self.unet):
                        logger.warning(
                            f&quot;Compiling the UNet for validation ({self.args.validation_torch_compile})&quot;
                        )
                        self.pipeline.unet = torch.compile(
                            self.pipeline.unet,
                            mode=self.args.validation_torch_compile_mode,
                            fullgraph=False,
                        )
                    if self.transformer is not None and not is_compiled_module(
                        self.transformer
                    ):
                        logger.warning(
                            f&quot;Compiling the transformer for validation ({self.args.validation_torch_compile})&quot;
                        )
                        self.pipeline.transformer = torch.compile(
                            self.pipeline.transformer,
                            mode=self.args.validation_torch_compile_mode,
                            fullgraph=False,
                        )
        self.pipeline = self.pipeline.to(self.inference_device)
        self.pipeline.set_progress_bar_config(disable=True)
    def clean_pipeline(self):
        &quot;&quot;&quot;Remove the pipeline.&quot;&quot;&quot;
        if hasattr(self.accelerator, &quot;_lycoris_wrapped_network&quot;):
            self.accelerator._lycoris_wrapped_network.set_multiplier(1.0)
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None
    def process_prompts(self):
        &quot;&quot;&quot;Processes each validation prompt and logs the result.&quot;&quot;&quot;
        self.validation_prompt_dict = {}
        self.evaluation_result = None
        validation_images = {}
        _content = zip(self.validation_shortnames, self.validation_prompts)
        total_samples = (
            len(self.validation_shortnames)
            if self.validation_shortnames is not None
            else 0
        )
        self.eval_scores = {}
        if self.validation_image_inputs:
            # Override the pipeline inputs to be entirely based upon the validation image inputs.
            _content = self.validation_image_inputs
            total_samples = len(_content) if _content is not None else 0
        for content in tqdm(
            _content if _content else [],
            desc=&quot;Processing validation prompts&quot;,
            total=total_samples,
            leave=False,
            position=1,
        ):
            validation_input_image = None
            logger.debug(f&quot;content: {content}&quot;)
            if len(content) == 3:
                shortname, prompt, validation_input_image = content
            elif len(content) == 2:
                shortname, prompt = content
            else:
                raise ValueError(
                    f&quot;Validation content is not in the correct format: {content}&quot;
                )
            self.validation_prompt_dict[shortname] = prompt
            logger.debug(f&quot;Processing validation for prompt: {prompt}&quot;)
            (
                stitched_validation_images,
                checkpoint_validation_images,
                ema_validation_images,
            ) = self.validate_prompt(prompt, shortname, validation_input_image)
            validation_images.update(stitched_validation_images)
            if self.args.model_family in [&quot;ltxvideo&quot;]:
                self._save_videos(validation_images, shortname, prompt)
            else:
                self._save_images(validation_images, shortname, prompt)
            logger.debug(f&quot;Completed generating image: {prompt}&quot;)
            self.validation_images = validation_images
            self.evaluation_result = self.evaluate_images(checkpoint_validation_images)
            self._log_validations_to_webhook(validation_images, shortname, prompt)
        try:
            self._log_validations_to_trackers(validation_images)
        except Exception as e:
            logger.error(f&quot;Error logging validation images: {e}&quot;)
    def get_eval_result(self):
        return self.evaluation_result or {}
    def clear_eval_result(self):
        self.evaluation_result = None
    def stitch_conditioning_images(self, validation_image_results, conditioning_image):
        &quot;&quot;&quot;
        For each image, make a new canvas and place it side by side with its equivalent from {self.validation_image_inputs}
        &quot;&quot;&quot;
        stitched_validation_images = []
        for idx, image in enumerate(validation_image_results):
            new_width = image.size[0] * 2
            new_height = image.size[1]
            new_image = Image.new(&quot;RGB&quot;, (new_width, new_height))
            new_image.paste(image, (0, 0))
            new_image.paste(conditioning_image, (image.size[0], 0))
            stitched_validation_images.append(new_image)
        return stitched_validation_images
    def _validation_types(self):
        types = [&quot;checkpoint&quot;]
        if self.args.use_ema:
            # ema has different validations we can add or overwrite.
            if self.args.ema_validation == &quot;ema_only&quot;:
                # then we do not sample the base ckpt being trained, only the EMA weights.
                types = [&quot;ema&quot;]
            if self.args.ema_validation == &quot;comparison&quot;:
                # then we sample both.
                types.append(&quot;ema&quot;)
        return types
    def validate_prompt(
        self, prompt, validation_shortname, validation_input_image=None
    ):
        &quot;&quot;&quot;Generate validation images for a single prompt.&quot;&quot;&quot;
        # Placeholder for actual image generation and logging
        logger.debug(f&quot;Validating prompt: {prompt}&quot;)
        # benchmarked / stitched validation images
        stitched_validation_images = {}
        # untouched / un-stitched validation images
        checkpoint_validation_images = {}
        ema_validation_images = {}
        for resolution in self.validation_resolutions:
            extra_validation_kwargs = {}
            if validation_input_image is not None:
                extra_validation_kwargs[&quot;image&quot;] = validation_input_image
                if self.deepfloyd_stage2:
                    validation_resolution_width, validation_resolution_height = (
                        val * 4 for val in extra_validation_kwargs[&quot;image&quot;].size
                    )
                elif self.args.controlnet or self.args.validation_using_datasets:
                    validation_resolution_width, validation_resolution_height = (
                        extra_validation_kwargs[&quot;image&quot;].size
                    )
                else:
                    raise ValueError(
                        &quot;Validation input images are not supported for this model type.&quot;
                    )
            else:
                validation_resolution_width, validation_resolution_height = resolution
            if (
                self.args.model_family == &quot;sd3&quot;
                and type(self.args.validation_guidance_skip_layers) is list
            ):
                extra_validation_kwargs[&quot;skip_layer_guidance_start&quot;] = float(
                    self.args.validation_guidance_skip_layers_start
                )
                extra_validation_kwargs[&quot;skip_layer_guidance_stop&quot;] = float(
                    self.args.validation_guidance_skip_layers_stop
                )
                extra_validation_kwargs[&quot;skip_layer_guidance_scale&quot;] = float(
                    self.args.validation_guidance_skip_scale
                )
                extra_validation_kwargs[&quot;skip_guidance_layers&quot;] = list(
                    self.args.validation_guidance_skip_layers
                )
            if not self.flow_matching and self.args.model_family not in [
                &quot;deepfloyd&quot;,
                &quot;pixart_sigma&quot;,
                &quot;kolors&quot;,
                &quot;flux&quot;,
                &quot;sd3&quot;,
                &quot;sana&quot;,
                &quot;ltxvideo&quot;,
            ]:
                extra_validation_kwargs[&quot;guidance_rescale&quot;] = (
                    self.args.validation_guidance_rescale
                )
            if StateTracker.get_args().validation_using_datasets:
                extra_validation_kwargs[&quot;strength&quot;] = getattr(
                    self.args, &quot;validation_strength&quot;, 0.2
                )
                logger.debug(
                    f&quot;Set validation image denoise strength to {extra_validation_kwargs[&apos;strength&apos;]}&quot;
                )
            logger.debug(
                f&quot;Processing width/height: {validation_resolution_width}x{validation_resolution_height}&quot;
            )
            if validation_shortname not in stitched_validation_images:
                stitched_validation_images[validation_shortname] = []
                checkpoint_validation_images[validation_shortname] = []
                ema_validation_images[validation_shortname] = []
            try:
                extra_validation_kwargs.update(self._gather_prompt_embeds(prompt))
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error gathering text embed for validation prompt {prompt}: {e}, traceback: {traceback.format_exc()}&quot;
                )
                continue
            try:
                pipeline_kwargs = {
                    &quot;prompt&quot;: None,
                    &quot;negative_prompt&quot;: None,
                    &quot;num_images_per_prompt&quot;: self.args.num_validation_images,
                    &quot;num_inference_steps&quot;: self.args.validation_num_inference_steps,
                    &quot;guidance_scale&quot;: self.args.validation_guidance,
                    &quot;height&quot;: MultiaspectImage._round_to_nearest_multiple(
                        int(validation_resolution_height)
                    ),
                    &quot;width&quot;: MultiaspectImage._round_to_nearest_multiple(
                        int(validation_resolution_width)
                    ),
                    **extra_validation_kwargs,
                }
                if self.args.validation_guidance_real &gt; 1.0:
                    pipeline_kwargs[&quot;guidance_scale_real&quot;] = float(
                        self.args.validation_guidance_real
                    )
                if (
                    isinstance(self.args.validation_no_cfg_until_timestep, int)
                    and self.args.model_family == &quot;flux&quot;
                ):
                    pipeline_kwargs[&quot;no_cfg_until_timestep&quot;] = (
                        self.args.validation_no_cfg_until_timestep
                    )
                logger.debug(
                    f&quot;Image being generated with parameters: {pipeline_kwargs}&quot;
                )
                # Print the device attr of any parameters that have one
                for key, value in pipeline_kwargs.items():
                    if hasattr(value, &quot;device&quot;):
                        logger.debug(f&quot;Device for {key}: {value.device}&quot;)
                for key, value in self.pipeline.components.items():
                    if hasattr(value, &quot;device&quot;):
                        logger.debug(f&quot;Device for {key}: {value.device}&quot;)
                if StateTracker.get_model_family() == &quot;flux&quot;:
                    if &quot;negative_prompt&quot; in pipeline_kwargs:
                        del pipeline_kwargs[&quot;negative_prompt&quot;]
                if StateTracker.get_model_family() in [&quot;ltxvideo&quot;]:
                    del pipeline_kwargs[&quot;num_images_per_prompt&quot;]
                if self.args.model_family == &quot;sana&quot;:
                    pipeline_kwargs[&quot;complex_human_instruction&quot;] = (
                        self.args.sana_complex_human_instruction
                    )
                if StateTracker.get_model_family() in [
                    &quot;pixart_sigma&quot;,
                    &quot;smoldit&quot;,
                    &quot;sana&quot;,
                    &quot;ltxvideo&quot;,
                ]:
                    if pipeline_kwargs.get(&quot;negative_prompt&quot;) is not None:
                        del pipeline_kwargs[&quot;negative_prompt&quot;]
                    if pipeline_kwargs.get(&quot;prompt&quot;) is not None:
                        del pipeline_kwargs[&quot;prompt&quot;]
                    pipeline_kwargs[&quot;prompt_attention_mask&quot;] = pipeline_kwargs.pop(
                        &quot;prompt_mask&quot;
                    )[0].to(device=self.inference_device, dtype=self.weight_dtype)
                    pipeline_kwargs[&quot;negative_prompt_attention_mask&quot;] = torch.unsqueeze(
                        pipeline_kwargs.pop(&quot;negative_mask&quot;)[0], dim=0
                    ).to(device=self.inference_device, dtype=self.weight_dtype)
                validation_types = self._validation_types()
                all_validation_type_results = {}
                for current_validation_type in validation_types:
                    if not self.args.validation_randomize:
                        pipeline_kwargs[&quot;generator&quot;] = self._get_generator()
                        logger.debug(
                            f&quot;Using a generator? {pipeline_kwargs[&apos;generator&apos;]}&quot;
                        )
                    if current_validation_type == &quot;ema&quot;:
                        self.enable_ema_for_inference()
                    if self.args.model_family in [&quot;ltxvideo&quot;]:
                        all_validation_type_results[current_validation_type] = (
                            self.pipeline(**pipeline_kwargs).frames
                        )
                    else:
                        all_validation_type_results[current_validation_type] = (
                            self.pipeline(**pipeline_kwargs).images
                        )
                    if current_validation_type == &quot;ema&quot;:
                        self.disable_ema_for_inference()
                # retrieve the default image result for stitching to controlnet inputs.
                ema_image_results = all_validation_type_results.get(&quot;ema&quot;)
                validation_image_results = all_validation_type_results.get(
                    &quot;checkpoint&quot;, ema_image_results
                )
                original_validation_image_results = validation_image_results
                benchmark_image = None
                if self.args.controlnet:
                    validation_image_results = self.stitch_conditioning_images(
                        original_validation_image_results,
                        extra_validation_kwargs[&quot;image&quot;],
                    )
                elif not self.args.disable_benchmark and self.benchmark_exists(
                    &quot;base_model&quot;
                ):
                    benchmark_image = self._benchmark_image(
                        validation_shortname, resolution
                    )
                    if benchmark_image is not None:
                        for idx, validation_image in enumerate(
                            validation_image_results
                        ):
                            validation_image_results[idx] = self.stitch_benchmark_image(
                                validation_image_result=validation_image,
                                benchmark_image=benchmark_image,
                            )
                checkpoint_validation_images[validation_shortname].extend(
                    original_validation_image_results
                )
                stitched_validation_images[validation_shortname].extend(
                    validation_image_results
                )
                if self.args.use_ema:
                    ema_validation_images[validation_shortname].extend(
                        ema_image_results
                    )
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error generating validation image: {e}, {traceback.format_exc()}&quot;
                )
                continue
        if (
            self.args.use_ema
            and self.args.ema_validation == &quot;comparison&quot;
            and benchmark_image is not None
        ):
            for idx, validation_image in enumerate(
                stitched_validation_images[validation_shortname]
            ):
                stitched_validation_images[validation_shortname][idx] = (
                    self.stitch_benchmark_image(
                        validation_image_result=ema_validation_images[
                            validation_shortname
                        ][idx],
                        benchmark_image=stitched_validation_images[
                            validation_shortname
                        ][idx],
                        labels=[None, &quot;EMA&quot;],
                    )
                )
        return (
            stitched_validation_images,
            checkpoint_validation_images,
            ema_validation_images,
        )
    def _save_videos(self, validation_images, validation_shortname, validation_prompt):
        validation_img_idx = 0
        from diffusers.utils.export_utils import export_to_video
        for validation_image in validation_images[validation_shortname]:
            size_x, size_y = validation_image[0].size
            res_label = f&quot;{size_x}x{size_y}&quot;
            export_to_video(
                validation_image,
                os.path.join(
                    self.save_dir,
                    f&quot;step_{StateTracker.get_global_step()}_{validation_shortname}_{validation_img_idx}_{res_label}.mp4&quot;,
                ),
                fps=self.args.framerate,
            )
            validation_img_idx += 1
    def _save_images(self, validation_images, validation_shortname, validation_prompt):
        validation_img_idx = 0
        for validation_image in validation_images[validation_shortname]:
            res = self.validation_resolutions[validation_img_idx]
            if &quot;x&quot; in res:
                res_label = str(res)
            elif type(res) is tuple:
                res_label = f&quot;{res[0]}x{res[1]}&quot;
            else:
                res_label = f&quot;{res}x{res}&quot;
            validation_image.save(
                os.path.join(
                    self.save_dir,
                    f&quot;step_{StateTracker.get_global_step()}_{validation_shortname}_{res_label}.png&quot;,
                )
            )
            validation_img_idx += 1
    def _log_validations_to_webhook(
        self, validation_images, validation_shortname, validation_prompt
    ):
        if StateTracker.get_webhook_handler() is not None:
            StateTracker.get_webhook_handler().send(
                (
                    f&quot;Validation {&apos;image&apos; if StateTracker.get_webhook_handler().send_video is False else &apos;video&apos;} for `{validation_shortname if validation_shortname != &apos;&apos; else &apos;(blank shortname)&apos;}`&quot;
                    f&quot;\nValidation prompt: `{validation_prompt if validation_prompt != &apos;&apos; else &apos;(blank prompt)&apos;}`&quot;
                    f&quot;\nEvaluation score: {self.eval_scores.get(validation_shortname, &apos;N/A&apos;)}&quot;
                ),
                images=validation_images[validation_shortname],
            )
    def _log_validations_to_trackers(self, validation_images):
        for tracker in self.accelerator.trackers:
            if tracker.name == &quot;comet_ml&quot;:
                experiment = self.accelerator.get_tracker(&quot;comet_ml&quot;).tracker
                for shortname, image_list in validation_images.items():
                    for idx, image in enumerate(image_list):
                        experiment.log_image(
                            image,
                            name=f&quot;{shortname} - {self.validation_resolutions[idx]}&quot;,
                        )
            elif tracker.name == &quot;tensorboard&quot;:
                tracker = self.accelerator.get_tracker(&quot;tensorboard&quot;)
                for shortname, image_list in validation_images.items():
                    tracker.log_images(
                        {
                            f&quot;{shortname} - {self.validation_resolutions[idx]}&quot;: np.moveaxis(
                                np.array(image), -1, 0
                            )[
                                np.newaxis, ...
                            ]
                            for idx, image in enumerate(image_list)
                        },
                        step=StateTracker.get_global_step(),
                    )
            elif tracker.name == &quot;wandb&quot;:
                resolution_list = [
                    f&quot;{res[0]}x{res[1]}&quot; for res in get_validation_resolutions()
                ]
                if self.args.tracker_image_layout == &quot;table&quot;:
                    columns = [
                        &quot;Prompt&quot;,
                        *resolution_list,
                        &quot;Mean Luminance&quot;,
                    ]
                    table = wandb.Table(columns=columns)
                    # Process each prompt and its associated images
                    for prompt_shortname, image_list in validation_images.items():
                        wandb_images = []
                        luminance_values = []
                        logger.debug(
                            f&quot;Prompt {prompt_shortname} has {len(image_list)} images&quot;
                        )
                        for image in image_list:
                            logger.debug(f&quot;Adding to table: {image}&quot;)
                            wandb_image = wandb.Image(image)
                            wandb_images.append(wandb_image)
                            luminance = calculate_luminance(image)
                            luminance_values.append(luminance)
                        mean_luminance = torch.tensor(luminance_values).mean().item()
                        while len(wandb_images) &lt; len(resolution_list):
                            # any missing images will crash it. use None so they are indexed.
                            logger.debug(&quot;Found a missing image - masking with a None&quot;)
                            wandb_images.append(None)
                        table.add_data(prompt_shortname, *wandb_images, mean_luminance)
                    # Log the table to Weights &amp; Biases
                    tracker.log(
                        {&quot;Validation Gallery&quot;: table},
                        step=StateTracker.get_global_step(),
                    )
                elif self.args.tracker_image_layout == &quot;gallery&quot;:
                    gallery_images = {}
                    for prompt_shortname, image_list in validation_images.items():
                        logger.debug(
                            f&quot;Prompt {prompt_shortname} has {len(image_list)} images&quot;
                        )
                        for idx, image in enumerate(image_list):
                            wandb_image = wandb.Image(
                                image,
                                caption=f&quot;{prompt_shortname} - {resolution_list[idx]}&quot;,
                            )
                            gallery_images[
                                f&quot;{prompt_shortname} - {resolution_list[idx]}&quot;
                            ] = wandb_image
                    # Log all images in one call to prevent the global step from ticking
                    tracker.log(gallery_images, step=StateTracker.get_global_step())
    def _primary_model(self):
        if self.args.controlnet:
            return self.controlnet
        if self.unet is not None:
            return self.unet
        if self.transformer is not None:
            return self.transformer
    def enable_ema_for_inference(self, pipeline=None):
        if self.ema_enabled:
            logger.debug(&quot;EMA already enabled. Not enabling EMA.&quot;)
            return
        if self.args.use_ema:
            logger.debug(&quot;Enabling EMA.&quot;)
            self.ema_enabled = True
            if self.args.model_type == &quot;lora&quot;:
                if self.args.lora_type.lower() == &quot;lycoris&quot;:
                    logger.debug(&quot;Setting Lycoris multiplier to 1.0&quot;)
                    self.accelerator._lycoris_wrapped_network.set_multiplier(1.0)
                    logger.debug(&quot;Storing Lycoris weights for later recovery.&quot;)
                    self.ema_model.store(
                        self.accelerator._lycoris_wrapped_network.parameters()
                    )
                    logger.debug(
                        &quot;Storing the EMA weights into the Lycoris adapter for inference.&quot;
                    )
                    self.ema_model.copy_to(
                        self.accelerator._lycoris_wrapped_network.parameters()
                    )
                elif self.args.lora_type.lower() == &quot;standard&quot;:
                    _trainable_parameters = [
                        x for x in self._primary_model().parameters() if x.requires_grad
                    ]
                    self.ema_model.store(_trainable_parameters)
                    self.ema_model.copy_to(_trainable_parameters)
            else:
                # if self.args.ema_device != &quot;accelerator&quot;:
                #     logger.info(&quot;Moving checkpoint to CPU for storage.&quot;)
                #     self._primary_model().to(&quot;cpu&quot;)
                logger.debug(&quot;Storing EMA weights for later recovery.&quot;)
                self.ema_model.store(self.trainable_parameters())
                logger.debug(&quot;Storing the EMA weights into the model for inference.&quot;)
                self.ema_model.copy_to(self.trainable_parameters())
            # if self.args.ema_device != &quot;accelerator&quot;:
            #     logger.debug(&quot;Moving checkpoint to CPU for storage.&quot;)
            #     self._primary_model().to(&quot;cpu&quot;)
            #     logger.debug(&quot;Moving EMA weights to GPU for inference.&quot;)
            #     self.ema_model.to(self.inference_device)
        else:
            logger.debug(
                &quot;Skipping EMA model setup for validation, as we are not using EMA.&quot;
            )
    def disable_ema_for_inference(self):
        if not self.ema_enabled:
            logger.debug(&quot;EMA was not enabled. Not disabling EMA.&quot;)
            return
        if self.args.use_ema:
            logger.debug(&quot;Disabling EMA.&quot;)
            self.ema_enabled = False
            if (
                self.args.model_type == &quot;lora&quot;
                and self.args.lora_type.lower() == &quot;lycoris&quot;
            ):
                logger.debug(&quot;Setting Lycoris network multiplier to 1.0.&quot;)
                self.accelerator._lycoris_wrapped_network.set_multiplier(1.0)
                logger.debug(&quot;Restoring Lycoris weights.&quot;)
                self.ema_model.restore(
                    self.accelerator._lycoris_wrapped_network.parameters()
                )
            else:
                logger.debug(&quot;Restoring trainable parameters.&quot;)
                self.ema_model.restore(self.trainable_parameters())
            if self.args.ema_device != &quot;accelerator&quot;:
                logger.debug(&quot;Moving EMA weights to CPU for storage.&quot;)
                self.ema_model.to(self.args.ema_device)
                self._primary_model().to(self.inference_device)
        else:
            logger.debug(
                &quot;Skipping EMA model restoration for validation, as we are not using EMA.&quot;
            )
    def finalize_validation(self, validation_type):
        &quot;&quot;&quot;Cleans up and restores original state if necessary.&quot;&quot;&quot;
        if not self.args.keep_vae_loaded and not self.args.vae_cache_ondemand:
            self.vae = self.vae.to(&quot;cpu&quot;)
            self.vae = None
        self.pipeline = None
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    def evaluate_images(self, images: list = None):
        if self.model_evaluator is None:
            return None
        for shortname, image_list in images.items():
            if shortname in self.eval_scores:
                continue
            prompt = self.validation_prompt_dict.get(shortname, &quot;&quot;)
            for image in image_list:
                evaluation_score = self.model_evaluator.evaluate([image], [prompt])
                self.eval_scores[shortname] = round(float(evaluation_score), 4)
        # Log the scores into dict: {&quot;min&quot;, &quot;max&quot;, &quot;mean&quot;, &quot;std&quot;}
        result = {
            &quot;clip/min&quot;: min(self.eval_scores.values()),
            &quot;clip/max&quot;: max(self.eval_scores.values()),
            &quot;clip/mean&quot;: np.mean(list(self.eval_scores.values())),
            &quot;clip/std&quot;: np.std(list(self.eval_scores.values())),
        }
        return result
class Evaluation:
    &quot;&quot;&quot;
    A class for running eval loss calculations on prepared batches..
    &quot;&quot;&quot;
    def __init__(self, accelerator):
        self.config = StateTracker.get_args()
        self.accelerator = accelerator
    def would_evaluate(self, training_state: dict):
        if not self.accelerator.is_main_process:
            return
        if self.config.eval_steps_interval is None:
            return False
        if self.config.eval_steps_interval == 0:
            return False
        if (
            training_state[&quot;global_step&quot;] % self.config.eval_steps_interval == 0
            and training_state[&quot;global_step&quot;] &gt; training_state[&quot;global_resume_step&quot;]
        ):
            return True
        return False
    def total_eval_batches(self, dataset_name=None):
        &quot;&quot;&quot;
        Return the total number of eval batches across:
          - all eval datasets if dataset_name is None
          - the specific dataset if dataset_name is given
        &quot;&quot;&quot;
        eval_datasets = StateTracker.get_data_backends(_type=&quot;eval&quot;)
        if dataset_name is not None:
            ds = eval_datasets.get(dataset_name)
            return len(ds[&quot;sampler&quot;]) if ds else 0
        return sum(len(x[&quot;sampler&quot;]) for _, x in eval_datasets.items())
    def get_timestep_schedule(self, noise_scheduler):
        noise_scheduler.set_timesteps(self.config.eval_timesteps)
        timesteps = noise_scheduler.timesteps
        return timesteps
    def _evaluate_dataset_pass(
        self,
        dataset_name,
        prepare_batch,
        model_predict,
        calculate_loss,
        get_prediction_target,
        noise_scheduler,
    ):
        &quot;&quot;&quot;
        Evaluate on exactly one dataset (if dataset_name is not None),
        or across *all* eval datasets (if dataset_name is None).
        Returns a dictionary: {
            timestep_value -&gt; [list of losses at that timestep]
        }
        &quot;&quot;&quot;
        if not self.accelerator.is_main_process:
            return {}
        accumulated_eval_losses = {}
        eval_batch = True
        evaluated_sample_count = 0
        # Figure out how many total batches for this pass
        total_batches = self.total_eval_batches(dataset_name=dataset_name)
        if self.config.num_eval_images is not None:
            total_batches = min(self.config.num_eval_images, total_batches)
        main_progress_bar = tqdm(
            total=total_batches,
            desc=f&quot;Calculate validation loss ({dataset_name or &apos;ALL&apos;})&quot;,
            position=0,
            leave=True,
        )
        # Save and restore RNG states so that eval doesn&apos;t disturb training RNG
        cpu_rng_state = torch.get_rng_state()
        if torch.cuda.is_available():
            cuda_rng_state = torch.cuda.get_rng_state()
        eval_timestep_list = self.get_timestep_schedule(noise_scheduler)
        logger.debug(f&quot;Evaluation timesteps: {eval_timestep_list}&quot;)
        while eval_batch is not False and evaluated_sample_count &lt; total_batches:
            try:
                evaluated_sample_count += 1
                if (
                    self.config.num_eval_images is not None
                    and evaluated_sample_count &gt; self.config.num_eval_images
                ):
                    reset_eval_datasets()
                    raise MultiDatasetExhausted(
                        &quot;Max eval samples reached, resetting evaluations.&quot;
                    )
                # Pass the dataset_name so we fetch from the correct place
                eval_batch = retrieve_eval_images(dataset_name=dataset_name)
            except MultiDatasetExhausted:
                logger.info(
                    f&quot;Evaluation loss calculation completed for dataset: {dataset_name}&quot;
                )
                eval_batch = False
            if eval_batch is not None and eval_batch is not False:
                # Fix a known seed so noise is consistent across eval
                torch.manual_seed(0)
                prepared_eval_batch = prepare_batch(eval_batch)
                if &quot;latents&quot; not in prepared_eval_batch:
                    raise ValueError(
                        &quot;Error calculating eval batch: no &apos;latents&apos; found.&quot;
                    )
                bsz = prepared_eval_batch[&quot;latents&quot;].shape[0]
                sample_text_str = &quot;samples&quot; if bsz &gt; 1 else &quot;sample&quot;
                with torch.no_grad():
                    for eval_timestep in tqdm(
                        eval_timestep_list,
                        total=len(eval_timestep_list),
                        desc=f&quot;Evaluating batch of {bsz} {sample_text_str}&quot;,
                        position=1,
                        leave=False,
                    ):
                        if eval_timestep not in accumulated_eval_losses:
                            accumulated_eval_losses[eval_timestep] = []
                        torch.manual_seed(0)
                        current_eval_timestep_tensor = (
                            torch.Tensor([eval_timestep])
                            .expand(prepared_eval_batch[&quot;noisy_latents&quot;].shape[0])
                            .to(
                                dtype=self.config.weight_dtype,
                                device=prepared_eval_batch[&quot;noisy_latents&quot;].device,
                            )
                        )
                        eval_prediction = model_predict(
                            prepared_batch=prepared_eval_batch,
                            custom_timesteps=current_eval_timestep_tensor,
                        )
                        eval_loss = calculate_loss(
                            prepared_batch=prepared_eval_batch,
                            model_pred=eval_prediction,
                            target=get_prediction_target(prepared_eval_batch),
                            apply_conditioning_mask=False,
                        )
                        accumulated_eval_losses[eval_timestep].append(eval_loss)
                    main_progress_bar.update(1)
        try:
            reset_eval_datasets()
        except:
            pass
        # Restore RNG
        torch.set_rng_state(cpu_rng_state)
        if torch.cuda.is_available():
            torch.cuda.set_rng_state(cuda_rng_state)
        return accumulated_eval_losses
    def execute_eval(
        self,
        prepare_batch,
        model_predict,
        calculate_loss,
        get_prediction_target,
        noise_scheduler,
    ):
        &quot;&quot;&quot;
        Either run a pooled pass (all eval datasets at once) or
        run each dataset individually (and also produce a pooled result).
        &quot;&quot;&quot;
        if not self.accelerator.is_main_process:
            return {}
        # Decide if we pool or do separate passes
        pooling = getattr(self.config, &quot;eval_dataset_pooling&quot;)
        eval_datasets = StateTracker.get_data_backends(
            _type=&quot;eval&quot;
        )  # dict of {name: ...}
        if pooling:
            # Single pass across ALL eval datasets
            logger.info(&quot;Running a single pooled eval pass across all datasets.&quot;)
            pooled_losses = self._evaluate_dataset_pass(
                dataset_name=None,
                prepare_batch=prepare_batch,
                model_predict=model_predict,
                calculate_loss=calculate_loss,
                get_prediction_target=get_prediction_target,
                noise_scheduler=noise_scheduler,
            )
            # We&apos;ll store everything under a &quot;pooled&quot; key for consistency
            accumulated = {&quot;pooled&quot;: pooled_losses}
        else:
            # Multiple passes: one per dataset
            logger.info(
                &quot;Running separate eval passes for each dataset + pooled results.&quot;
            )
            accumulated = {}
            # We&apos;ll also keep an aggregator for the final &apos;pooled&apos; pass
            from collections import defaultdict
            pooled_collector = defaultdict(list)
            for ds_name in eval_datasets.keys():
                ds_losses = self._evaluate_dataset_pass(
                    dataset_name=ds_name,
                    prepare_batch=prepare_batch,
                    model_predict=model_predict,
                    calculate_loss=calculate_loss,
                    get_prediction_target=get_prediction_target,
                    noise_scheduler=noise_scheduler,
                )
                accumulated[ds_name] = ds_losses
                # Collect them into the global &quot;pooled&quot; aggregator
                for tstep, losses in ds_losses.items():
                    pooled_collector[tstep].extend(losses)
        return accumulated
    def generate_tracker_table(self, all_accumulated_losses: dict):
        &quot;&quot;&quot;
        all_accumulated_losses is expected to be:
        {
          dataset_name_1: { timestep_1: [losses], timestep_2: [losses], ... },
          dataset_name_2: { ... },
          ...
          &quot;pooled&quot;: { timestep_x: [losses], ... }
        }
        If config.eval_dataset_pooling = True, then typically you&apos;ll only see a &quot;pooled&quot; key.
        If config.eval_dataset_pooling = False, you&apos;ll see multiple datasets plus &quot;pooled&quot;.
        &quot;&quot;&quot;
        if not self.accelerator.is_main_process:
            return {}
        results = {}
        # Helper to flatten timesteps-&gt;loss arrays into a single (ts, loss) table
        def flatten_timestep_losses(timestep_dict):
            data_rows = []
            for ts, loss_list in timestep_dict.items():
                for loss_val in loss_list:
                    data_rows.append((ts, loss_val))
            return data_rows
        logger.info(&quot;Generating evaluation tracker tables...&quot;)
        for ds_name, timestep_dict in all_accumulated_losses.items():
            data_rows = flatten_timestep_losses(timestep_dict)
            if not data_rows:
                continue
            total_loss = sum(x[1] for x in data_rows)
            num_items = len(data_rows)
            mean_loss = total_loss / num_items
            # By default, store a minimal result
            results_key = f&quot;loss/val/{ds_name}&quot;
            results[results_key] = mean_loss
            if self.config.report_to == &quot;wandb&quot;:
                # Create a small wandb table for these data
                table = wandb.Table(data=data_rows, columns=[&quot;timestep&quot;, &quot;eval_loss&quot;])
                chart = wandb.plot.line(
                    table,
                    x=&quot;timestep&quot;,
                    y=&quot;eval_loss&quot;,
                    title=f&quot;{results_key} by timestep&quot;,
                )
                results[f&quot;chart/{results_key}&quot;] = chart
        return results</file><file path="helpers/training/wrappers.py">from diffusers.utils.torch_utils import is_compiled_module
def unwrap_model(accelerator, model):
    model = accelerator.unwrap_model(model)
    model = model._orig_mod if is_compiled_module(model) else model
    return model</file><file path="helpers/webhooks/config.py">from json import load
supported_webhooks = [&quot;discord&quot;, &quot;raw&quot;]
def check_discord_webhook_config(config: dict) -&gt; bool:
    if &quot;webhook_type&quot; not in config or config[&quot;webhook_type&quot;] != &quot;discord&quot;:
        return False
    if &quot;webhook_url&quot; not in config:
        raise ValueError(&quot;Discord webhook config is missing &apos;webhook_url&apos; value.&quot;)
    return True
def check_raw_webhook_config(config: dict) -&gt; bool:
    if config.get(&quot;webhook_type&quot;) != &quot;raw&quot;:
        return False
    missing_fields = []
    required_fields = [&quot;callback_url&quot;]
    for config_field in required_fields:
        if not config.get(config_field):
            missing_fields.append(config_field)
    if missing_fields:
        raise ValueError(f&quot;Missing fields on webhook config: {missing_fields}&quot;)
    return False
class WebhookConfig:
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.values = self.load_config()
        if (
            &quot;webhook_type&quot; not in self.values
            or self.values[&quot;webhook_type&quot;] not in supported_webhooks
        ):
            raise ValueError(
                f&quot;Invalid webhook type specified in config. Supported values: {supported_webhooks}&quot;
            )
        if check_discord_webhook_config(self.values):
            self.webhook_type = &quot;discord&quot;
        elif check_raw_webhook_config(self.values):
            self.webhook_type = &quot;raw&quot;
    def load_config(self):
        with open(self.config_path, &quot;r&quot;) as f:
            return load(f)
    def get_config(self):
        return self.values
    def __getattr__(self, name):
        return self.values.get(name, None)</file><file path="helpers/webhooks/handler.py">from helpers.webhooks.config import WebhookConfig
from pathlib import Path
import requests
import os
import json
import logging
import numpy as np
import time
from io import BytesIO
import base64
import imageio  # &lt;-- for in-memory video encoding (MP4 or GIF)
# Define log levels
log_levels = {&quot;critical&quot;: 0, &quot;error&quot;: 1, &quot;warning&quot;: 2, &quot;info&quot;: 3, &quot;debug&quot;: 4}
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class WebhookHandler:
    def __init__(
        self,
        config_path: str,
        accelerator,
        project_name: str,
        args,
        mock_webhook_config: WebhookConfig = None,
        send_video: bool = False,
    ):
        self.accelerator = accelerator
        self.config = mock_webhook_config or WebhookConfig(config_path)
        self.webhook_url = self.config.values.get(
            &quot;webhook_url&quot;, self.config.values.get(&quot;callback_url&quot;, None)
        )
        self.webhook_type = self.config.webhook_type  # &quot;discord&quot; or &quot;raw&quot;
        self.message_prefix = (
            f&quot;`({self.config.message_prefix})` &quot;
            if self.config.message_prefix is not None
            else f&quot;`({project_name})` &quot;
        )
        self.log_level = log_levels.get(
            self.config.log_level or &quot;info&quot;, log_levels[&quot;info&quot;]
        )
        self.stored_response = None
        self.send_video = send_video
        self.video_framerate = args.framerate
    def _check_level(self, level: str) -&gt; bool:
        &quot;&quot;&quot;Check if the message level meets the configured log level.&quot;&quot;&quot;
        return log_levels.get(level, &quot;info&quot;) &lt;= self.log_level
    def _send_request(
        self,
        message: str,
        images: list = None,
        store_response: bool = False,
        raw_request: bool = False,
    ):
        &quot;&quot;&quot;Send the webhook request based on the webhook type.&quot;&quot;&quot;
        if self.webhook_type == &quot;discord&quot;:
            # Prepare Discord-style payload
            data = {&quot;content&quot;: f&quot;{self.message_prefix}{message}&quot;}
            if self.send_video:
                # images is actually a list of &quot;videos&quot; in this usage
                files = self._prepare_videos(images)
            else:
                # images is a list of PIL Images
                files = self._prepare_images(images)
            request_args = {&quot;data&quot;: data, &quot;files&quot;: files}
        elif self.webhook_type == &quot;raw&quot;:
            # Prepare raw data payload for direct POST
            if raw_request:
                # If already fully formed JSON or dict, just send raw
                data = message
                files = None
            else:
                # Convert images to base64 for a generic &quot;raw&quot; JSON
                data = {
                    &quot;message&quot;: message,
                    &quot;images&quot;: (
                        [self._convert_image_to_base64(img) for img in images]
                        if images
                        else []
                    ),
                }
                files = None
            request_args = {&quot;json&quot;: data, &quot;files&quot;: files}
        else:
            logger.error(f&quot;Unsupported webhook type: {self.webhook_type}&quot;)
            return
        # Send request
        try:
            logger.debug(f&quot;Sending webhook request: {request_args}&quot;)
            post_result = requests.post(self.webhook_url, **request_args)
            post_result.raise_for_status()
        except Exception as e:
            logger.error(f&quot;Could not send webhook request: {e}&quot;)
            return
        if store_response:
            self.stored_response = post_result.headers
    def _prepare_videos(self, videos: list):
        &quot;&quot;&quot;
        Convert in-memory video frames to file objects for Discord uploads.
        We assume each item in `videos` is either:
        1) A list of frames (PIL Images or NumPy arrays) for a single video, or
        2) Already an in-memory BytesIO with encoded MP4 data.
        &quot;&quot;&quot;
        files = {}
        if not videos:
            return files
        if not isinstance(videos, list):
            raise ValueError(f&quot;Videos must be a list. Received: {type(videos)}&quot;)
        for index, vid_data in enumerate(videos):
            # If vid_data is a BytesIO, assume it&apos;s already an mp4
            if isinstance(vid_data, BytesIO):
                vid_data.seek(0)
                files[f&quot;file{index}&quot;] = (f&quot;video{index}.mp4&quot;, vid_data, &quot;video/mp4&quot;)
            else:
                # Otherwise, assume vid_data is a list of frames (PIL or np.ndarray).
                # We&apos;ll convert each PIL Image to a NumPy array, then pass to mimwrite.
                frames = []
                for frame in vid_data:
                    if hasattr(frame, &quot;convert&quot;):  # i.e. PIL image
                        frames.append(np.asarray(frame))
                    else:
                        # assume it&apos;s already a NumPy array
                        frames.append(frame)
                video_byte_array = BytesIO()
                imageio.v3.imwrite(
                    video_byte_array,
                    frames,  # a list of NumPy arrays
                    plugin=&quot;pyav&quot;,  # or &quot;ffmpeg&quot;
                    fps=self.video_framerate,
                    extension=&quot;.mp4&quot;,
                    codec=&quot;libx264&quot;,
                )
                video_byte_array.seek(0)
                files[f&quot;file{index}&quot;] = (
                    f&quot;video{index}.mp4&quot;,
                    video_byte_array,
                    &quot;video/mp4&quot;,
                )
        return files
    def _prepare_images(self, images: list):
        &quot;&quot;&quot;Convert PIL images to file objects for Discord uploads.&quot;&quot;&quot;
        files = {}
        if not images:
            return files
        if not isinstance(images, list):
            raise ValueError(f&quot;Images must be a list of PIL images. Received: {images}&quot;)
        for index, img in enumerate(images):
            img_byte_array = BytesIO()
            img.save(img_byte_array, format=&quot;PNG&quot;)
            img_byte_array.seek(0)
            files[f&quot;file{index}&quot;] = (
                f&quot;image{index}.png&quot;,
                img_byte_array,
                &quot;image/png&quot;,
            )
        return files
    def _convert_image_to_base64(self, image):
        &quot;&quot;&quot;Convert PIL image to a base64 string (for &apos;raw&apos; webhook type).&quot;&quot;&quot;
        img_byte_array = BytesIO()
        image.save(img_byte_array, format=&quot;PNG&quot;)
        img_byte_array.seek(0)
        return base64.b64encode(img_byte_array.read()).decode(&quot;utf-8&quot;)
    def send(
        self,
        message: str,
        images: list = None,
        message_level: str = &quot;info&quot;,
        store_response: bool = False,
    ):
        &quot;&quot;&quot;
        Send a message through the webhook with optional images/videos.
        If self.send_video is True, `images` is interpreted as `videos`.
        &quot;&quot;&quot;
        # Only send from main process if it&apos;s Discord (to avoid duplicates).
        if not self.accelerator.is_main_process or self.webhook_type != &quot;discord&quot;:
            return
        if not self._check_level(message_level):
            return
        if images is not None and not isinstance(images, list):
            images = [images]
        # Discord limits: max 10 attachments
        max_attachments = 10
        if images and len(images) &gt; max_attachments:
            for i in range(0, len(images), max_attachments):
                self._send_request(
                    message,
                    images[i : i + max_attachments],
                    store_response=store_response,
                )
        else:
            self._send_request(message, images, store_response=store_response)
    def send_raw(
        self,
        structured_data: dict,
        message_type: str,
        message_level: str = &quot;info&quot;,
        job_id: str = None,
    ):
        &quot;&quot;&quot;
        Send structured data to a &quot;raw&quot; webhook, e.g. for step progress.
        Ignores &apos;images&apos; entirely, uses JSON payload only.
        &quot;&quot;&quot;
        if (
            self.webhook_type != &quot;raw&quot;
            or not self.accelerator.is_main_process
            or not self._check_level(message_level)
        ):
            return
        structured_data[&quot;message_type&quot;] = message_type
        structured_data[&quot;job_id&quot;] = job_id
        structured_data[&quot;timestamp&quot;] = int(time.time())
        self._send_request(
            message=structured_data, images=None, store_response=False, raw_request=True
        )</file><file path="helpers/webhooks/mixin.py">from helpers.webhooks.handler import WebhookHandler
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import _get_rank as get_rank
current_rank = get_rank()
class WebhookMixin:
    webhook_handler: WebhookHandler = None
    def set_webhook_handler(self, webhook_handler: WebhookHandler):
        self.webhook_handler = webhook_handler
    def send_progress_update(self, type: str, progress: int, total: int, current: int):
        if total == 1:
            return
        if int(current_rank) != 0:
            return
        progress = {
            &quot;message_type&quot;: &quot;progress_update&quot;,
            &quot;message&quot;: {
                &quot;progress_type&quot;: type,
                &quot;progress&quot;: progress,
                &quot;total_elements&quot;: total,
                &quot;current_estimated_index&quot;: current,
            },
        }
        self.webhook_handler.send_raw(
            progress, &quot;progress_update&quot;, job_id=StateTracker.get_job_id()
        )</file><file path="helpers/log_format.py">import logging
import os
from colorama import Fore, Back, Style, init
class ColorizedFormatter(logging.Formatter):
    level_colors = {
        logging.DEBUG: Fore.CYAN,
        logging.INFO: Fore.GREEN,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Back.WHITE + Style.BRIGHT,
    }
    def format(self, record):
        level_color = self.level_colors.get(record.levelno, &quot;&quot;)
        reset_color = Style.RESET_ALL
        message = super().format(record)
        return f&quot;{level_color}{message}{reset_color}&quot;
# Initialize colorama
init(autoreset=True)
# Create a logger
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)  # Set lowest level to capture everything
# Create handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(
    logging.INFO
)  # Change to ERROR if you want to suppress INFO messages too
console_handler.setFormatter(
    ColorizedFormatter(&quot;%(asctime)s [%(levelname)s] %(message)s&quot;)
)
# blank out the existing debug.log, if exists
if os.path.exists(&quot;debug.log&quot;):
    with open(&quot;debug.log&quot;, &quot;w&quot;):
        pass
# Create a file handler
file_handler = logging.FileHandler(&quot;debug.log&quot;)
file_handler.setLevel(logging.DEBUG)  # Capture debug and above
file_handler.setFormatter(
    logging.Formatter(&quot;%(asctime)s [%(levelname)s] (%(name)s) %(message)s&quot;)
)
# Remove existing handlers
for handler in logger.handlers[:]:
    logger.removeHandler(handler)
# Add handlers to the logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)
forward_logger = logging.getLogger(&quot;diffusers.models.unet_2d_condition&quot;)
forward_logger.setLevel(logging.WARNING)
pil_logger = logging.getLogger(&quot;PIL&quot;)
pil_logger.setLevel(logging.INFO)
pil_logger = logging.getLogger(&quot;PIL.Image&quot;)
pil_logger.setLevel(&quot;ERROR&quot;)
pil_logger = logging.getLogger(&quot;PIL.PngImagePlugin&quot;)
pil_logger.setLevel(&quot;ERROR&quot;)
transformers_logger = logging.getLogger(&quot;transformers.configuration_utils&quot;)
transformers_logger.setLevel(&quot;ERROR&quot;)
diffusers_logger = logging.getLogger(&quot;diffusers.configuration_utils&quot;)
diffusers_logger.setLevel(&quot;ERROR&quot;)
diffusers_utils_logger = logging.getLogger(&quot;diffusers.pipelines.pipeline_utils&quot;)
diffusers_utils_logger.setLevel(&quot;ERROR&quot;)
torchdistlogger = logging.getLogger(&quot;torch.distributed.nn.jit.instantiator&quot;)
torchdistlogger.setLevel(&quot;WARNING&quot;)
torch_utils_logger = logging.getLogger(&quot;diffusers.utils.torch_utils&quot;)
torch_utils_logger.setLevel(&quot;ERROR&quot;)
import warnings
# Suppress specific PIL warning
warnings.filterwarnings(
    &quot;ignore&quot;,
    category=UserWarning,
    module=&quot;PIL&quot;,
    message=&quot;Palette images with Transparency expressed in bytes should be converted to RGBA images&quot;,
)
warnings.filterwarnings(
    &quot;ignore&quot;,
    category=FutureWarning,
    module=&quot;transformers.deepspeed&quot;,
    message=&quot;transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations&quot;,
)
# Ignore torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
warnings.filterwarnings(
    &quot;ignore&quot;,
    category=DeprecationWarning,
    module=&quot;torch.utils._pytree&quot;,
    message=&quot;torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.&quot;,
)
warnings.filterwarnings(
    &quot;ignore&quot;,
)
warnings.filterwarnings(
    &quot;ignore&quot;,
    message=&quot;.*is deprecated.*&quot;,
)</file><file path="helpers/prompts.py">import json
import regex as re
from pathlib import Path
from helpers.training.state_tracker import StateTracker
from helpers.training.multi_process import _get_rank as get_rank
from helpers.training import image_file_extensions
import numpy
try:
    import pandas as pd
except ImportError:
    raise ImportError(&quot;Pandas is required for the ParquetMetadataBackend.&quot;)
prompts = {
    &quot;alien_landscape&quot;: &quot;Alien planet, strange rock formations, glowing plants, bizarre creatures, surreal atmosphere&quot;,
    &quot;alien_market&quot;: &quot;Alien marketplace, bizarre creatures, exotic goods, vibrant colors, otherworldly atmosphere&quot;,
    &quot;child_balloon&quot;: &quot;Child holding a balloon, happy expression, colorful balloons, sunny day, high detail&quot;,
    &quot;comic_strip&quot;: &quot;a 4-panel comic strip showing an orange cat saying the words &apos;HELP&apos; and &apos;LASAGNA&apos;&quot;,
    &quot;comic_book&quot;: &quot;a hand is holding a comic book with a cover that reads &apos;The Adventures of Superhero&apos;&quot;,
    &quot;crystal_cave&quot;: &quot;Underground cave filled with crystals, glowing lights, reflective surfaces, fantasy environment, high detail&quot;,
    &quot;cyberpunk_bazaar&quot;: &quot;Bustling cyberpunk bazaar, vendors, neon signs, advanced tech, crowded, high detail&quot;,
    &quot;cyberpunk_hacker&quot;: &quot;Cyberpunk hacker in a dark room, neon glow, multiple screens, intense focus, high detail&quot;,
    &quot;cybernetic_anne&quot;: &quot;a cybernetic anne of green gables with neural implant and bio mech augmentations&quot;,
    &quot;dystopian_city&quot;: &quot;Post-apocalyptic cityscape, ruined buildings, overgrown vegetation, dark and gritty, high detail&quot;,
    &quot;enchanted_castle&quot;: &quot;Magical castle in a lush forest, glowing windows, fantasy architecture, high resolution, detailed textures&quot;,
    &quot;enchanted_forest_ruins&quot;: &quot;Ruins of an ancient temple in an enchanted forest, glowing runes, mystical creatures, high detail&quot;,
    &quot;enchanted_forest&quot;: &quot;Mystical forest, glowing plants, fairies, magical creatures, fantasy art, high detail&quot;,
    &quot;enchanted_garden&quot;: &quot;Magical garden with glowing flowers, fairies, serene atmosphere, detailed plants, high resolution&quot;,
    &quot;fairy_garden&quot;: &quot;Whimsical garden filled with fairies, magical plants, sparkling lights, serene atmosphere, high detail&quot;,
    &quot;fantasy_dragon&quot;: &quot;Majestic dragon soaring through the sky, detailed scales, dynamic pose, fantasy art, high resolution&quot;,
    &quot;floating_islands&quot;: &quot;Fantasy world, floating islands in the sky, waterfalls, lush vegetation, detailed landscape, high resolution&quot;,
    &quot;futuristic_cityscape&quot;: &quot;Futuristic city skyline at night, neon lights, cyberpunk style, high contrast, sharp focus&quot;,
    &quot;galactic_battle&quot;: &quot;Space battle scene, starships fighting, laser beams, explosions, cosmic background&quot;,
    &quot;haunted_fairground&quot;: &quot;Abandoned fairground at night, eerie rides, ghostly figures, fog, dark atmosphere, high detail&quot;,
    &quot;haunted_mansion&quot;: &quot;Spooky haunted mansion on a hill, dark and eerie, glowing windows, ghostly atmosphere, high detail&quot;,
    &quot;hardcover_textbook&quot;: &quot;a hardcover physics textbook that is called PHYSICS FOR DUMMIES&quot;,
    &quot;medieval_battle&quot;: &quot;Epic medieval battle, knights in armor, dynamic action, detailed landscape, high resolution&quot;,
    &quot;medieval_market&quot;: &quot;Bustling medieval market with merchants, knights, and jesters, vibrant colors, detailed&quot;,
    &quot;medieval_tavern&quot;: &quot;Cozy medieval tavern, warm firelight, adventurers drinking, detailed interior, rustic atmosphere&quot;,
    &quot;neon_cityscape&quot;: &quot;Futuristic city skyline at night, neon lights, cyberpunk style, high contrast, sharp focus&quot;,
    &quot;neon_forest&quot;: &quot;Forest with neon-lit trees, glowing plants, bioluminescence, surreal atmosphere, high detail&quot;,
    &quot;neon_sign&quot;: &quot;Bright neon sign in a busy city street, &apos;Open 24 Hours&apos;, bold typography, glowing lights&quot;,
    &quot;neon_typography&quot;: &quot;Vibrant neon sign, &apos;Bar&apos;, bold typography, dark background, glowing lights, detailed design&quot;,
    &quot;pirate_ship&quot;: &quot;Pirate ship on the high seas, stormy weather, detailed sails, dramatic waves, photorealistic&quot;,
    &quot;pirate_treasure&quot;: &quot;Pirate discovering a treasure chest, detailed gold coins, tropical island, dramatic lighting&quot;,
    &quot;psychedelic&quot;: &quot;a photograph of a woman experiencing a psychedelic trip. trippy, 8k, uhd, fractal&quot;,
    &quot;rainy_cafe&quot;: &quot;Cozy cafe on a rainy day, people sipping coffee, warm lights, reflections on wet pavement, photorealistic&quot;,
    &quot;retro_arcade&quot;: &quot;1980s arcade, neon lights, vintage game machines, kids playing, vibrant colors, nostalgic atmosphere&quot;,
    &quot;retro_game_room&quot;: &quot;1980s game room with vintage arcade machines, neon lights, vibrant colors, nostalgic feel&quot;,
    &quot;robot_blacksmith&quot;: &quot;Robot blacksmith forging metal, sparks flying, detailed workshop, futuristic and medieval blend&quot;,
    &quot;robot_dancer&quot;: &quot;Sleek robot performing a dance, futuristic theater, holographic effects, detailed, high resolution&quot;,
    &quot;robot_factory&quot;: &quot;High-tech factory where robots are assembled, detailed machinery, futuristic setting, high detail&quot;,
    &quot;robotic_garden&quot;: &quot;Garden tended by robots, mechanical plants, colorful flowers, futuristic setting, high detail&quot;,
    &quot;robotic_pet&quot;: &quot;Cute robotic pet, futuristic home, sleek design, detailed features, friendly and animated&quot;,
    &quot;security_footage&quot;: &quot;cctv trail camera night time security picture of a wendigo in the woods&quot;,
    &quot;space_explorer&quot;: &quot;Astronaut exploring an alien planet, detailed landscape, futuristic suit, cosmic background&quot;,
    &quot;space_station&quot;: &quot;Futuristic space station orbiting a distant exoplanet, sleek design, detailed structures, cosmic backdrop&quot;,
    &quot;soon&quot;: &quot;a person holding a sign that reads &apos;SOON&apos;&quot;,
    &quot;steampunk_airship&quot;: &quot;Steampunk airship in the sky, intricate design, Victorian aesthetics, dynamic scene, high detail&quot;,
    &quot;steampunk_inventor&quot;: &quot;Steampunk inventor in a workshop, intricate gadgets, Victorian attire, mechanical arm, goggles&quot;,
    &quot;stormy_ocean&quot;: &quot;Stormy ocean with towering waves, dramatic skies, detailed water, intense atmosphere, high resolution&quot;,
    &quot;stormy_sea&quot;: &quot;Dramatic stormy sea, lighthouse in the distance, lightning striking, dark clouds, high detail&quot;,
    &quot;urban_art&quot;: &quot;Graffiti artist creating a mural, vibrant colors, urban setting, dynamic action, high resolution&quot;,
    &quot;urban_graffiti&quot;: &quot;Urban alleyway filled with vibrant graffiti art, tags and murals, realistic textures&quot;,
    &quot;urban_street_sign&quot;: &quot;Urban street sign, &apos;Main Street&apos;, bold typography, realistic textures, weathered look&quot;,
    &quot;vintage_car_show&quot;: &quot;Classic car show with vintage vehicles, vibrant colors, nostalgic atmosphere, high detail&quot;,
    &quot;vintage_diner_sign&quot;: &quot;Retro diner sign, &apos;Joe&apos;s Diner&apos;, classic 1950s design, neon lights, weathered look&quot;,
    &quot;vintage_store_sign&quot;: &quot;Vintage store sign with elaborate typography, &apos;Antique Shop&apos;, hand-painted, weathered look&quot;,
}
def prompt_library_injection(new_prompts: dict) -&gt; dict:
    &quot;&quot;&quot;
    Add more prompts to the built-in SimpleTuner Prompt library.
    Args:
        new_prompts (dict): A dict of shortnames matching the existing prompt library format:
        {
            &quot;nickname_here&quot;: &quot;prompt goes here&quot;,
            ...
        }
    Returns:
        dict: Completed prompt library.
    &quot;&quot;&quot;
    # Unpack the new prompts into the library.
    global prompts
    return {**prompts, **new_prompts}
import logging
from helpers.data_backend.base import BaseDataBackend
from tqdm import tqdm
import os
logger = logging.getLogger(&quot;PromptHandler&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
class CaptionNotFoundError(Exception):
    pass
class PromptHandler:
    def __init__(
        self,
        args: dict,
        text_encoders: list,
        tokenizers: list,
        accelerator,
        model_type: str = &quot;sdxl&quot;,
    ):
        if args.disable_compel:
            raise Exception(
                &quot;--disable_compel was provided, but the Compel engine was still attempted to be initialised.&quot;
            )
        from compel import Compel, ReturnedEmbeddingsType
        self.accelerator = accelerator
        self.encoder_style = model_type
        self.compel = None
        if model_type in [&quot;sdxl&quot;, &quot;legacy&quot;]:
            if (
                len(text_encoders) == 2
                and text_encoders[1] is not None
                and text_encoders[0] is not None
            ):
                # SDXL Refiner and Base can both use the 2nd tokenizer/encoder.
                logger.debug(
                    &quot;Initialising Compel prompt manager with dual text encoders.&quot;
                )
                self.compel = Compel(
                    tokenizer=tokenizers,
                    text_encoder=text_encoders,
                    truncate_long_prompts=False,
                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,
                    requires_pooled=[
                        False,  # CLIP-L does not produce pooled embeds.
                        True,  # CLIP-G produces pooled embeds.
                    ],
                    device=accelerator.device,
                )
            elif len(text_encoders) == 2 and text_encoders[0] is None:
                # SDXL Refiner has ONLY the 2nd tokenizer/encoder, which needs to be the only one in Compel.
                logger.debug(
                    &quot;Initialising Compel prompt manager with just the 2nd text encoder.&quot;
                )
                self.compel = Compel(
                    tokenizer=tokenizers[1],
                    text_encoder=text_encoders[1],
                    truncate_long_prompts=False,
                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,
                    requires_pooled=True,
                    device=accelerator.device,
                )
                self.encoder_style = &quot;sdxl-refiner&quot;
            elif model_type == &quot;legacy&quot;:
                # Any other pipeline uses the first tokenizer/encoder.
                logger.debug(
                    &quot;Initialising the Compel prompt manager with a single text encoder.&quot;
                )
                pipe_tokenizer = tokenizers[0]
                pipe_text_encoder = text_encoders[0]
                self.compel = Compel(
                    tokenizer=pipe_tokenizer,
                    text_encoder=pipe_text_encoder,
                    truncate_long_prompts=False,
                    returned_embeddings_type=ReturnedEmbeddingsType.LAST_HIDDEN_STATES_NORMALIZED,
                    device=accelerator.device,
                )
                self.encoder_style = &quot;legacy&quot;
        self.text_encoders = text_encoders
        self.tokenizers = tokenizers
    @staticmethod
    def retrieve_prompt_column_from_parquet(
        sampler_backend_id: str,
    ) -&gt; str:
        parquetdb = StateTracker.get_parquet_database(sampler_backend_id)
        dataframe = parquetdb[0]
        if dataframe is None:
            raise ValueError(
                f&quot;Parquet database not found for sampler {sampler_backend_id}.&quot;
            )
        caption_column = (
            StateTracker.get_data_backend_config(sampler_backend_id)
            .get(&quot;parquet&quot;, {})
            .get(&quot;caption_column&quot;, None)
        )
        if not caption_column:
            raise ValueError(
                f&quot;Caption column not found for sampler {sampler_backend_id}. Config: {StateTracker.get_data_backend_config(sampler_backend_id)}&quot;
            )
        # Return just that column
        all_captions = dataframe[caption_column].values
        fallback_caption_column = (
            StateTracker.get_data_backend_config(sampler_backend_id)
            .get(&quot;parquet&quot;, {})
            .get(&quot;fallback_caption_column&quot;)
        )
        if fallback_caption_column is not None and all_captions is not None:
            # Combine the lists
            fallback_captions = dataframe[fallback_caption_column].values
            all_captions = [
                x if x else y for x, y in zip(all_captions, fallback_captions)
            ]
        return all_captions
    @staticmethod
    def prepare_instance_prompt_from_parquet(
        image_path: str,
        use_captions: bool,
        prepend_instance_prompt: bool,
        data_backend: BaseDataBackend,
        instance_prompt: str = None,
        sampler_backend_id: str = None,
    ) -&gt; str:
        if sampler_backend_id is None:
            raise ValueError(&quot;Sampler backend ID is required.&quot;)
        if not use_captions:
            if not instance_prompt:
                raise ValueError(
                    &quot;Instance prompt is required when instance_prompt_only is enabled.&quot;
                )
            return instance_prompt
        metadata_backend = StateTracker.get_data_backend(sampler_backend_id)[
            &quot;metadata_backend&quot;
        ]
        if metadata_backend is None:
            raise ValueError(
                f&quot;Could not find metadata backend for sampler {sampler_backend_id}: {StateTracker.get_data_backend(sampler_backend_id)}&quot;
            )
        (
            parquet_db,
            filename_column,
            caption_column,
            fallback_caption_column,
            identifier_includes_extension,
        ) = StateTracker.get_parquet_database(sampler_backend_id)
        backend_config = StateTracker.get_data_backend_config(
            data_backend_id=data_backend.id
        )
        instance_data_dir = backend_config.get(&quot;instance_data_dir&quot;)
        image_filename_stem = image_path
        if instance_data_dir is not None and instance_data_dir in image_filename_stem:
            image_filename_stem = image_filename_stem.replace(instance_data_dir, &quot;&quot;)
            if image_filename_stem.startswith(&quot;/&quot;):
                image_filename_stem = image_filename_stem[1:]
        if not identifier_includes_extension:
            image_filename_stem = os.path.splitext(image_filename_stem)[0]
        image_caption = metadata_backend.caption_cache_entry(image_filename_stem)
        if instance_prompt is None and fallback_caption_column and not image_caption:
            raise CaptionNotFoundError(
                f&quot;Could not locate caption for image {image_path} in sampler_backend {sampler_backend_id} with filename column {filename_column}, caption column {caption_column}, and a parquet database with {len(parquet_db)} entries.&quot;
            )
        elif (
            instance_prompt is None
            and not fallback_caption_column
            and not image_caption
        ):
            raise CaptionNotFoundError(
                f&quot;Could not locate caption for image {image_path} in sampler_backend {sampler_backend_id} with filename column {filename_column}, caption column {caption_column}, and a parquet database with {len(parquet_db)} entries.&quot;
            )
        if type(image_caption) == bytes:
            image_caption = image_caption.decode(&quot;utf-8&quot;)
        if type(image_caption) == str:
            image_caption = image_caption.strip()
        if type(image_caption) in (list, tuple, numpy.ndarray, pd.Series):
            image_caption = [
                str(item).strip() for item in image_caption if item is not None
            ]
        if prepend_instance_prompt:
            if type(image_caption) == list:
                image_caption = [instance_prompt + &quot; &quot; + x for x in image_caption]
            else:
                image_caption = instance_prompt + &quot; &quot; + image_caption
        return image_caption
    @staticmethod
    def prepare_instance_prompt_from_filename(
        image_path: str,
        use_captions: bool,
        prepend_instance_prompt: bool,
        instance_prompt: str = None,
    ) -&gt; str:
        if not use_captions:
            if not instance_prompt:
                raise ValueError(
                    &quot;Instance prompt is required when instance_prompt_only is enabled.&quot;
                )
            return instance_prompt
        image_caption = Path(image_path).stem
        # Underscores to spaces.
        image_caption = image_caption.replace(&quot;_&quot;, &quot; &quot;)
        if prepend_instance_prompt:
            image_caption = instance_prompt + &quot; &quot; + image_caption
        return image_caption
    @staticmethod
    def prepare_instance_prompt_from_textfile(
        image_path: str,
        use_captions: bool,
        prepend_instance_prompt: bool,
        data_backend: BaseDataBackend,
        instance_prompt: str = None,
    ) -&gt; str:
        if not use_captions:
            if not instance_prompt:
                raise ValueError(
                    &quot;Instance prompt is required when instance_prompt_only is enabled.&quot;
                )
            return instance_prompt
        caption_file = os.path.splitext(image_path)[0] + &quot;.txt&quot;
        if not data_backend.exists(caption_file):
            raise FileNotFoundError(f&quot;Caption file {caption_file} not found.&quot;)
        try:
            image_caption = data_backend.read(caption_file)
            # Convert from bytes to str:
            if type(image_caption) == bytes:
                image_caption = image_caption.decode(&quot;utf-8&quot;)
            # any newlines? split into array
            if &quot;\n&quot; in image_caption:
                image_caption = image_caption.split(&quot;\n&quot;)
                # Remove any empty strings
                image_caption = [x for x in image_caption if x]
            if prepend_instance_prompt:
                if type(image_caption) is list:
                    image_caption = [instance_prompt + &quot; &quot; + x for x in image_caption]
                else:
                    image_caption = instance_prompt + &quot; &quot; + image_caption
            return image_caption
        except Exception as e:
            logger.error(f&quot;Could not read caption file {caption_file}: {e}&quot;)
    @staticmethod
    def magic_prompt(
        image_path: str,
        use_captions: bool,
        caption_strategy: str,
        prepend_instance_prompt: bool,
        data_backend: BaseDataBackend,
        instance_prompt: str = None,
        sampler_backend_id: str = None,
    ) -&gt; str:
        &quot;&quot;&quot;Pull a prompt for an image file like magic, using one of the available caption strategies.
        Args:
            image_path (str): The image path.
            caption_strategy (str): Currently, &apos;filename&apos; or &apos;textfile&apos;.
            use_captions (bool): If false, the folder containing the image is used as an instance prompt.
            prepend_instance_prompt (bool): If true, the folder name of the image is prepended to the caption.
        Raises:
            ValueError: _description_
        Returns:
            _type_: _description_
        &quot;&quot;&quot;
        if caption_strategy == &quot;filename&quot;:
            instance_prompt = PromptHandler.prepare_instance_prompt_from_filename(
                image_path=image_path,
                use_captions=use_captions,
                prepend_instance_prompt=prepend_instance_prompt,
                instance_prompt=instance_prompt,
            )
        elif caption_strategy == &quot;textfile&quot;:
            # Can return multiple captions, if the file has newlines.
            instance_prompt = PromptHandler.prepare_instance_prompt_from_textfile(
                image_path,
                use_captions=use_captions,
                prepend_instance_prompt=prepend_instance_prompt,
                instance_prompt=instance_prompt,
                data_backend=data_backend,
            )
        elif caption_strategy == &quot;parquet&quot;:
            # Can return multiple captions, if the field is a list.
            instance_prompt = PromptHandler.prepare_instance_prompt_from_parquet(
                image_path,
                use_captions=use_captions,
                prepend_instance_prompt=prepend_instance_prompt,
                instance_prompt=instance_prompt,
                data_backend=data_backend,
                sampler_backend_id=sampler_backend_id,
            )
        elif caption_strategy == &quot;instanceprompt&quot;:
            return instance_prompt
        elif caption_strategy == &quot;csv&quot;:
            return data_backend.get_caption(image_path)
        else:
            raise ValueError(
                f&quot;Unsupported caption strategy: {caption_strategy}. Supported: &apos;filename&apos;, &apos;textfile&apos;, &apos;parquet&apos;, &apos;instanceprompt&apos;&quot;
            )
        return instance_prompt
    @staticmethod
    def get_all_captions(
        instance_data_dir: str,
        use_captions: bool,
        prepend_instance_prompt: bool,
        data_backend: BaseDataBackend,
        caption_strategy: str,
        instance_prompt: str = None,
    ) -&gt; list:
        captions = []
        images_missing_captions = []
        all_image_files = StateTracker.get_image_files(
            data_backend_id=data_backend.id
        ) or data_backend.list_files(
            instance_data_dir=instance_data_dir, file_extensions=image_file_extensions
        )
        backend_config = StateTracker.get_data_backend_config(
            data_backend_id=data_backend.id
        )
        if type(all_image_files) == list and type(all_image_files[0]) == tuple:
            all_image_files = all_image_files[0][2]
        from tqdm import tqdm
        # if caption_strategy == &quot;parquet&quot;:
        #     return PromptHandler.retrieve_prompt_column_from_parquet(
        #         sampler_backend_id=data_backend.id
        #     )
        for image_path in tqdm(
            all_image_files,
            desc=&quot;Loading captions&quot;,
            total=len(all_image_files),
            disable=True if get_rank() &gt; 0 else False,
            leave=False,
            ncols=125,
        ):
            try:
                if caption_strategy == &quot;filename&quot;:
                    caption = PromptHandler.prepare_instance_prompt_from_filename(
                        image_path=str(image_path),
                        use_captions=use_captions,
                        prepend_instance_prompt=prepend_instance_prompt,
                        instance_prompt=instance_prompt,
                    )
                elif caption_strategy == &quot;textfile&quot;:
                    caption = PromptHandler.prepare_instance_prompt_from_textfile(
                        image_path,
                        use_captions=use_captions,
                        prepend_instance_prompt=prepend_instance_prompt,
                        instance_prompt=instance_prompt,
                        data_backend=data_backend,
                    )
                elif caption_strategy == &quot;parquet&quot;:
                    caption = PromptHandler.prepare_instance_prompt_from_parquet(
                        image_path,
                        use_captions=use_captions,
                        prepend_instance_prompt=prepend_instance_prompt,
                        instance_prompt=instance_prompt,
                        data_backend=data_backend,
                        sampler_backend_id=data_backend.id,
                    )
                elif caption_strategy == &quot;instanceprompt&quot;:
                    return [instance_prompt], []
                elif caption_strategy == &quot;csv&quot;:
                    caption = data_backend.get_caption(image_path)
                else:
                    raise ValueError(
                        f&quot;Unsupported caption strategy: {caption_strategy}. Supported: &apos;filename&apos;, &apos;textfile&apos;, &apos;parquet&apos;, &apos;instanceprompt&apos;&quot;
                    )
            except CaptionNotFoundError as e:
                logger.error(f&quot;Could not load caption for image {image_path}: {e}&quot;)
                images_missing_captions.append(image_path)
                continue
            if type(caption) not in [tuple, list, dict]:
                captions.append(caption)
            else:
                # allow caching of multiple captions, if returned by the backend.
                captions.extend(caption)
        # Deduplicate captions
        # TODO: Investigate why this prevents captions from processing on multigpu systems.
        # captions = list(set(captions))
        return captions, images_missing_captions
    @staticmethod
    def filter_caption(data_backend: BaseDataBackend, caption: str) -&gt; str:
        &quot;&quot;&quot;Just filter a single caption.
        Args:
            data_backend (BaseDataBackend): The data backend for the instance.
            caption (str): The caption to filter.
        Raises:
            e: If caption filter list can not be loaded.
            ValueError: If we have an invalid filter list.
            FileNotFoundError: If the filter list can not be found.
        Returns:
            str: The filtered caption.
        &quot;&quot;&quot;
        return PromptHandler.filter_captions(data_backend, [caption])[0]
    @staticmethod
    def filter_captions(data_backend: BaseDataBackend, captions: list) -&gt; list:
        &quot;&quot;&quot;
        If the data backend config contains the entry &quot;caption_filter_list&quot;, this function will filter the captions.
        The caption_filter file contains strings or regular expressions, one per line.
        If a line doesn&apos;t have any regex control characters in it, we&apos;ll treat it as a string.
        &quot;&quot;&quot;
        data_backend_config = StateTracker.get_data_backend_config(
            data_backend_id=data_backend.id
        )
        caption_filter_list = data_backend_config.get(&quot;caption_filter_list&quot;, None)
        if not caption_filter_list or caption_filter_list == &quot;&quot;:
            return captions
        if (
            type(caption_filter_list) == str
            and os.path.splitext(caption_filter_list)[1] == &quot;.json&quot;
        ):
            # It&apos;s a path to a filter list. Load it in JSON format.
            caption_filter_list_path = Path(caption_filter_list)
            try:
                with open(caption_filter_list_path, &quot;r&quot;) as caption_filter_list:
                    caption_filter_list = json.load(caption_filter_list)
            except Exception as e:
                logger.error(
                    f&quot;Caption filter list for data backend &apos;{data_backend.id}&apos; could not be loaded: {e}&quot;
                )
                raise e
        elif (
            type(caption_filter_list) == str
            and os.path.splitext(caption_filter_list)[1] == &quot;.txt&quot;
        ):
            # We have a plain text list of filter strings/regex. Load them into an array:
            caption_filter_list_path = Path(caption_filter_list)
            try:
                with open(caption_filter_list_path, &quot;r&quot;) as caption_filter_list:
                    caption_filter_list = caption_filter_list.readlines()
                    # Strip newlines from the ends:
                    caption_filter_list = [x.strip(&quot;\n&quot;) for x in caption_filter_list]
            except Exception as e:
                logger.error(
                    f&quot;Caption filter list for data backend &apos;{data_backend.id}&apos; could not be loaded: {e}&quot;
                )
                raise e
        # We have the filter list. Is it valid and non-empty?
        if type(caption_filter_list) != list or len(caption_filter_list) == 0:
            logger.debug(
                f&quot;Data backend &apos;{data_backend.id}&apos; has an invalid or empty caption filter list.&quot;
            )
            return captions
        elif type(caption_filter_list) is not list:
            raise ValueError(
                f&quot;Data backend &apos;{data_backend.id}&apos; has an invalid caption filter list: {caption_filter_list}&quot;
            )
        # Iterate through each caption
        filtered_captions = []
        for caption in tqdm(
            captions,
            desc=&quot;Filtering captions&quot;,
            total=len(captions),
            ncols=125,
            disable=True if len(captions) &lt; 10 else False,
        ):
            if type(caption) is list:
                caption = caption[0]
            modified_caption = caption
            # Apply each filter to the caption
            logger.debug(f&quot;Filtering caption: {modified_caption}&quot;)
            if modified_caption is None:
                logger.error(
                    f&quot;Encountered a None caption in the list, data backend: {data_backend.id}&quot;
                )
                continue
            for filter_item in caption_filter_list:
                # Check for special replace pattern &apos;s/replace/entry/&apos;
                if filter_item.startswith(&quot;s/&quot;) and filter_item.count(&quot;/&quot;) == 2:
                    _, search, replace = filter_item.split(&quot;/&quot;)
                    regex_modified_caption = re.sub(search, replace, modified_caption)
                    if regex_modified_caption != modified_caption:
                        # logger.debug(
                        #     f&quot;Applying regex SEARCH {filter_item} to caption: {modified_caption}&quot;
                        # )
                        modified_caption = regex_modified_caption
                else:
                    # Treat as plain string and remove occurrences
                    if modified_caption is not None:
                        modified_caption = str(modified_caption).replace(
                            filter_item, &quot;&quot;
                        )
                try:
                    # Assume all filters as regex patterns for flexibility
                    pattern = re.compile(filter_item)
                    try:
                        regex_modified_caption = pattern.sub(&quot;&quot;, modified_caption)
                    except:
                        regex_modified_caption = modified_caption
                    if regex_modified_caption != modified_caption:
                        # logger.debug(
                        #     f&quot;Applying regex FILTER {filter_item} to caption: {modified_caption}&quot;
                        # )
                        modified_caption = regex_modified_caption
                except re.error as e:
                    logger.error(f&quot;Regex error with pattern {filter_item}: {e}&quot;)
            # Add the modified caption to the filtered list
            # if caption != modified_caption:
            #     logger.debug(
            #         f&quot;After all filters have finished, here is the modified caption: {modified_caption}&quot;
            #     )
            filtered_captions.append(modified_caption)
        # Return the list of modified captions
        return filtered_captions
    @staticmethod
    def load_user_prompts(user_prompt_path: str = None):
        if not user_prompt_path:
            return {}
        # Does the file exist?
        user_prompt_path = Path(user_prompt_path)
        if not user_prompt_path.exists():
            raise FileNotFoundError(f&quot;User prompt file {user_prompt_path} not found.&quot;)
        # Load the file.
        try:
            with user_prompt_path.open(&quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                user_prompts = json.load(f)
                # logger.debug(f&quot;Loaded user prompts: {user_prompts}&quot;)
            return user_prompts
        except Exception as e:
            logger.error(f&quot;Could not read user prompt file {user_prompt_path}: {e}&quot;)
            return {}</file><file path="install/apple/pyproject.toml">[tool.poetry]
name = &quot;simpletuner&quot;
version = &quot;1.1.0&quot;
description = &quot;Stable Diffusion 2.x and XL tuner.&quot;
authors = [&quot;bghira&quot;]
license = &quot;AGPLv3&quot;
readme = &quot;README.md&quot;
package-mode = false

[tool.poetry.dependencies]
python = &quot;&gt;=3.10,&lt;3.13&quot;
torch = &quot;^2.6.0&quot;
torchvision = &quot;^0.21.0&quot;
diffusers = &quot;^0.32.2&quot;
transformers = &quot;^4.49.0&quot;
datasets = &quot;^3.0.0&quot;
wandb = &quot;^0.19.1&quot;
requests = &quot;^2.32.3&quot;
pillow = &quot;^11.0.0&quot;
opencv-python = &quot;^4.10.0.84&quot;
accelerate = &quot;^1.2.1&quot;
safetensors = &quot;^0.4.5&quot;
compel = &quot;^2.0.1&quot;
clip-interrogator = &quot;^0.6.0&quot;
open-clip-torch = &quot;^2.26.1&quot;
iterutils = &quot;^0.1.6&quot;
scipy = &quot;^1.11.1&quot;
boto3 = &quot;^1.35.83&quot;
pandas = &quot;^2.2.3&quot;
botocore = &quot;^1.35.83&quot;
urllib3 = &quot;&lt;1.27&quot;
torchsde = &quot;^0.2.6&quot;
torchmetrics = &quot;^1.1.1&quot;
colorama = &quot;^0.4.6&quot;
numpy = &quot;^2.2.0&quot;
peft = &quot;^0.14.0&quot;
tensorboard = &quot;^2.18.0&quot;
regex = &quot;^2023.12.25&quot;
huggingface-hub = &quot;^0.29.1&quot;
optimum-quanto = {git = &quot;https://github.com/huggingface/optimum-quanto&quot;}
torch-optimi = &quot;^0.2.1&quot;
lycoris-lora = {git = &quot;https://github.com/kohakublueleaf/lycoris&quot;, rev = &quot;dev&quot;}
fastapi = {extras = [&quot;standard&quot;], version = &quot;^0.115.0&quot;}
deepspeed = &quot;^0.16.1&quot;
sentencepiece = &quot;^0.2.0&quot;
torchao = &quot;^0.7.0&quot;
torchaudio = &quot;^2.5.0&quot;
atomicwrites = &quot;^1.4.1&quot;
beautifulsoup4 = &quot;^4.12.3&quot;
prodigy-plus-schedule-free = &quot;^1.9.0&quot;
imageio-ffmpeg = &quot;^0.6.0&quot;
imageio = {extras = [&quot;pyav&quot;], version = &quot;^2.37.0&quot;}


[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[[tool.poetry.source]]
priority = &quot;supplemental&quot;
name = &quot;pytorch-nightly&quot;
url = &quot;https://download.pytorch.org/whl/nightly/cpu&quot;</file><file path="install/github/pyproject.toml">[tool.poetry]
name = &quot;simpletuner&quot;
version = &quot;1.1.0&quot;
description = &quot;Stable Diffusion 2.x and XL tuner.&quot;
authors = [&quot;bghira&quot;]
license = &quot;AGPLv3&quot;
readme = &quot;README.md&quot;
package-mode = false

[tool.poetry.dependencies]
python = &quot;&gt;=3.10,&lt;3.13&quot;
torch = { &quot;version&quot; = &quot;&gt;=2.4.1&quot;, &quot;source&quot; = &quot;pytorch&quot; }
torchvision = &quot;^0.19.0&quot;
diffusers = &quot;^0.30.3&quot;
transformers = &quot;^4.44.2&quot;
datasets = &quot;^3.0.0&quot;
wandb = &quot;^0.18.1&quot;
requests = &quot;^2.32.3&quot;
pillow = &quot;^10.4.0&quot;
opencv-python = &quot;^4.10.0.84&quot;
accelerate = &quot;^0.34.2&quot;
safetensors = &quot;^0.4.5&quot;
compel = &quot;^2.0.1&quot;
clip-interrogator = &quot;^0.6.0&quot;
open-clip-torch = &quot;^2.26.1&quot;
iterutils = &quot;^0.1.6&quot;
scipy = &quot;^1.11.1&quot;
boto3 = &quot;^1.35.24&quot;
pandas = &quot;^2.2.3&quot;
botocore = &quot;^1.35.24&quot;
urllib3 = &quot;&lt;1.27&quot;
torchsde = &quot;^0.2.5&quot;
torchmetrics = &quot;^1.1.1&quot;
colorama = &quot;^0.4.6&quot;
numpy = &quot;1.26&quot;
peft = &quot;^0.12.0&quot;
tensorboard = &quot;^2.17.1&quot;
regex = &quot;^2023.12.25&quot;
huggingface-hub = &quot;^0.23.3&quot;
optimum-quanto = {git = &quot;https://github.com/huggingface/optimum-quanto&quot;}
torch-optimi = &quot;^0.2.1&quot;
lycoris-lora = {git = &quot;https://github.com/kohakublueleaf/lycoris&quot;, rev = &quot;dev&quot;}
fastapi = {extras = [&quot;standard&quot;], version = &quot;^0.115.0&quot;}
deepspeed = &quot;^0.15.1&quot;
sentencepiece = &quot;^0.2.0&quot;
torchao = &quot;^0.5.0&quot;


[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[[tool.poetry.source]]
priority = &quot;supplemental&quot;
name = &quot;pytorch&quot;
url = &quot;https://download.pytorch.org/whl/cpu&quot;

[[tool.poetry.source]]
priority = &quot;supplemental&quot;
name = &quot;pytorch-nightly&quot;
url = &quot;https://download.pytorch.org/whl/nightly/cpu&quot;</file><file path="install/rocm/pyproject.toml">[tool.poetry]
name = &quot;simpletuner&quot;
version = &quot;1.1.0&quot;
description = &quot;Stable Diffusion 2.x and XL tuner.&quot;
authors = [&quot;bghira&quot;]
license = &quot;AGPLv3&quot;
readme = &quot;README.md&quot;
package-mode = false

[tool.poetry.dependencies]
python = &quot;&gt;=3.10,&lt;3.13&quot;
torch = {version = &quot;^2.4.1+rocm6.0&quot;, source = &quot;pytorch-rocm&quot;}
torchaudio = {version = &quot;^2.4.1+rocm6.0&quot;, source = &quot;pytorch-rocm&quot;}
torchmetrics = {version = &quot;^1&quot;, source = &quot;pytorch-rocm&quot;}
torchvision = {version = &quot;^0.19.1+rocm6.0&quot;, source = &quot;pytorch-rocm&quot;}
pytorch-triton-rocm = {version = &quot;*&quot;, source = &quot;pytorch-rocm&quot;}
accelerate = &quot;^1.2.1&quot;
boto3 = &quot;^1.35.83&quot;
botocore = &quot;^1.35.83&quot;
clip-interrogator = &quot;^0.6.0&quot;
colorama = &quot;^0.4.6&quot;
compel = &quot;^2&quot;
datasets = &quot;^3.0.0&quot;
deepspeed = &quot;^0.16.1&quot;
diffusers = &quot;^0.32.2&quot;
iterutils = &quot;^0.1.6&quot;
numpy = &quot;^2.2.0&quot;
open-clip-torch = &quot;^2.26.1&quot;
opencv-python = &quot;^4.10.0.84&quot;
pandas = &quot;^2.2.3&quot;
peft = &quot;^0.14.0&quot;
pillow = &quot;^11.0.0&quot;
requests = &quot;^2.32.3&quot;
safetensors = &quot;^0.4.5&quot;
scipy = &quot;^1&quot;
tensorboard = &quot;^2.18.0&quot;
torchsde = &quot;^0.2.6&quot;
transformers = &quot;^4.49.0&quot;
urllib3 = &quot;&lt;1.27&quot;
wandb = &quot;^0.19.1&quot;
sentencepiece = &quot;^0.2.0&quot;
optimum-quanto = {git = &quot;https://github.com/huggingface/optimum-quanto&quot;}
lycoris-lora = {git = &quot;https://github.com/kohakublueleaf/lycoris&quot;, rev = &quot;dev&quot;}
torch-optimi = &quot;^0.2.1&quot;
fastapi = {extras = [&quot;standard&quot;], version = &quot;^0.115.0&quot;}
bitsandbytes = &quot;^0.44.1&quot;
atomicwrites = &quot;^1.4.1&quot;
torchao = &quot;^0.7.0&quot;
beautifulsoup4 = &quot;^4.12.3&quot;
prodigy-plus-schedule-free = &quot;^1.9.0&quot;
huggingface-hub = &quot;^0.29.1&quot;
imageio-ffmpeg = &quot;^0.6.0&quot;
imageio = {extras = [&quot;pyav&quot;], version = &quot;^2.37.0&quot;}


[build-system]
requires = [&quot;poetry-core&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[[tool.poetry.source]]
priority = &quot;explicit&quot;
name = &quot;pytorch-rocm&quot;
url = &quot;https://download.pytorch.org/whl/rocm6.0&quot;</file><file path="simpletuner_sdk/thread_keeper/__init__.py">from concurrent.futures import ThreadPoolExecutor, Future
from typing import Dict
import threading
# We can only really have one thread going at a time anyway.
executor = ThreadPoolExecutor(max_workers=1)
# But, we&apos;ve designed this for a future where multiple background threads might be managed.
thread_registry: Dict[str, Future] = {}
# So we don&apos;t zig while we zag.
lock = threading.Lock()
def submit_job(job_id: str, func, *args, **kwargs):
    with lock:
        if (
            job_id in thread_registry
            and get_thread_status(job_id, with_lock=False).lower() == &quot;running&quot;
        ):
            raise Exception(f&quot;Job with ID {job_id} is already running or pending.&quot;)
        # Remove the completed or cancelled future from the registry
        thread_registry.pop(job_id, None)
        # Submit the new job
        future = executor.submit(func, *args, **kwargs)
        thread_registry[job_id] = future
def get_thread_status(job_id: str, with_lock: bool = True) -&gt; str:
    if with_lock:
        with lock:
            future = thread_registry.get(job_id)
            if not future:
                return &quot;No such job.&quot;
            if future.running():
                return &quot;Running&quot;
            elif future.done():
                if future.exception():
                    return f&quot;Failed: {future.exception()}&quot;
                return &quot;Completed&quot;
            return &quot;Pending&quot;
    else:
        future = thread_registry.get(job_id)
        if not future:
            return &quot;No such job.&quot;
        if future.running():
            return &quot;Running&quot;
        elif future.done():
            if future.exception():
                return f&quot;Failed: {future.exception()}&quot;
            return &quot;Completed&quot;
        return &quot;Pending&quot;
def terminate_thread(job_id: str) -&gt; bool:
    with lock:
        future = thread_registry.get(job_id)
        if not future:
            print(f&quot;Thread {job_id} not found&quot;)
            return False
        # Attempt to cancel the future if it hasn&apos;t started running
        cancelled = future.cancel()
        if cancelled:
            del thread_registry[job_id]
        return cancelled
def list_threads():
    return {job_id: get_thread_status(job_id) for job_id in thread_registry}</file><file path="simpletuner_sdk/api_state.py">&quot;&quot;&quot;
API State tracker to persist and resume tracker states and configurations.
If a server were to crash during a training job, we can immediately reload the system state
and continue training from the last checkpoint.
&quot;&quot;&quot;
import os
import json
import logging
logger = logging.getLogger(&quot;SimpleTunerSDK&quot;)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;WARNING&quot;))
class APIState:
    state = {}
    state_file = &quot;api_state.json&quot;
    trainer = None
    @classmethod
    def load_state(cls):
        if os.path.exists(cls.state_file):
            with open(cls.state_file, &quot;r&quot;) as f:
                cls.state = json.load(f)
                logger.info(f&quot;Loaded state from {cls.state_file}: {cls.state}&quot;)
        else:
            logger.info(f&quot;No state file found at {cls.state_file}&quot;)
    @classmethod
    def save_state(cls):
        with open(cls.state_file, &quot;w&quot;) as f:
            json.dump(cls.state, f)
            logger.info(f&quot;Saved state to {cls.state_file}: {cls.state}&quot;)
    @classmethod
    def get_state(cls, key=None):
        if not key:
            return cls.state
        return cls.state.get(key)
    @classmethod
    def set_state(cls, key, value):
        cls.state[key] = value
        cls.save_state()
    @classmethod
    def delete_state(cls, key):
        if key in cls.state:
            del cls.state[key]
            cls.save_state()
    @classmethod
    def clear_state(cls):
        cls.state = {}
        cls.save_state()
    @classmethod
    def set_job(cls, job_id, job: dict):
        cls.set_state(&quot;current_job&quot;, job)
        cls.set_state(&quot;current_job_id&quot;, job_id)
        cls.set_state(&quot;status&quot;, &quot;running&quot;)
    @classmethod
    def get_job(cls):
        return {
            &quot;job_id&quot;: cls.get_state(&quot;current_job_id&quot;),
            &quot;job&quot;: cls.get_state(&quot;job&quot;),
        }
    @classmethod
    def cancel_job(cls):
        cls.delete_state(&quot;current_job&quot;)
        cls.delete_state(&quot;current_job_id&quot;)
        cls.set_state(&quot;status&quot;, &quot;cancelled&quot;)
    @classmethod
    def set_trainer(cls, trainer):
        cls.trainer = trainer
    @classmethod
    def get_trainer(cls):
        return cls.trainer</file><file path="simpletuner_sdk/configuration.py">import json, logging, os
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&apos;SIMPLETUNER_LOG_LEVEL&apos;, &apos;INFO&apos;))
from fastapi import APIRouter, HTTPException, status, BackgroundTasks
from pydantic import BaseModel
from helpers.training.trainer import Trainer
from simpletuner_sdk.api_state import APIState
from helpers.configuration.cmd_args import get_default_config
from helpers.configuration.json_file import normalize_args
from simpletuner_sdk.thread_keeper import submit_job, get_thread_status
# Define a Pydantic model for input validation
class ConfigModel(BaseModel):
    job_id: str
    # the actual Trainer config
    trainer_config: dict
    # what we will write as config/multidatabackend.json
    dataloader_config: list
    # what we will write as config/webhooks.json
    webhooks_config: dict
    # optional lycoris_config
    lycoris_config: dict = None
    # optional user_prompt_library
    user_prompt_library: dict = None
class Configuration:
    def __init__(self):
        self.router = APIRouter(prefix=&quot;/training/configuration&quot;)
        self.router.add_api_route(&quot;/preload&quot;, self.preload, methods=[&quot;POST&quot;])
        self.router.add_api_route(&quot;/check&quot;, self.check, methods=[&quot;POST&quot;])
        self.router.add_api_route(&quot;/default&quot;, self.default, methods=[&quot;GET&quot;])
        self.router.add_api_route(&quot;/run&quot;, self.run, methods=[&quot;POST&quot;])
    async def preload(self):
        &quot;&quot;&quot;
        Download models for a given configuration
        &quot;&quot;&quot;
        training_config = None
        trainer = Trainer(config=training_config)
        trainer.init_preprocessing_models(move_to_accelerator=False)
        trainer.init_unload_vae()
        trainer.init_unload_text_encoder()
        trainer.init_load_base_model(move_to_accelerator=False)
        return {&quot;status&quot;: &quot;successfully downloaded models&quot;}
    def _config_clear(self):
        # clear prev configs from disk first
        for file in [&quot;config/multidatabackend.json&quot;, &quot;config/webhooks.json&quot;, &quot;config/lycoris_config.json&quot;, &quot;config/user_prompt_library.json&quot;]:
            if os.path.exists(file):
                os.remove(file)
    def _config_save(self, job_config: ConfigModel):
        with open(&quot;config/multidatabackend.json&quot;, mode=&quot;w&quot;) as file_handler:
            json.dump(job_config.dataloader_config, file_handler, indent=4)
            job_config.trainer_config[&quot;data_backend_config&quot;] = (
                &quot;config/multidatabackend.json&quot;
            )
        with open(&quot;config/webhooks.json&quot;, mode=&quot;w&quot;) as file_handler:
            json.dump(job_config.webhooks_config, file_handler, indent=4)
            job_config.trainer_config[&quot;webhook_config&quot;] = &quot;config/webhooks.json&quot;
        if hasattr(job_config, &quot;lycoris_config&quot;):
            print(f&quot;LyCORIS config present: {job_config.lycoris_config}&quot;)
            with open(&quot;config/lycoris_config.json&quot;, &quot;w&quot;) as f:
                f.write(json.dumps(job_config.lycoris_config, indent=4))
                job_config.trainer_config[&quot;lycoris_config&quot;] = &quot;config/lycoris_config.json&quot;
        user_prompt_library_path = job_config.trainer_config.get(&apos;--user_prompt_library&apos;, None)
        print(f&quot;User prompt library path: {user_prompt_library_path}&quot;)
        if user_prompt_library_path and hasattr(job_config, &quot;user_prompt_library&quot;):
            print(f&quot;User prompt library present: {job_config.user_prompt_library}&quot;)
            with open(user_prompt_library_path, &quot;w&quot;) as f:
                f.write(json.dumps(job_config.user_prompt_library, indent=4))
                job_config.trainer_config[&quot;user_prompt_library&quot;] = &quot;config/user_prompt_library.json&quot;
    async def check(self, job_config: ConfigModel):
        &quot;&quot;&quot;
        Check for problems with a given configuration
        &quot;&quot;&quot;
        try:
            trainer = APIState.get_trainer()
            if trainer:
                return {
                    &quot;status&quot;: False,
                    &quot;result&quot;: &quot;Could not test configuration, a previous configuration was already loaded.&quot;,
                }
            self._config_clear()
            self._config_save(job_config)
            trainer = Trainer(config=normalize_args(job_config.trainer_config))
            return {
                &quot;status&quot;: True,
                &quot;result&quot;: f&quot;Configuration validated successfully&quot;,
            }
        except Exception as e:
            import traceback
            print(traceback.format_exc())
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f&quot;Could not validate configuration: {str(e)}&quot;,
            )
    async def default(self) -&gt; dict:
        &quot;&quot;&quot;
        Get default configuration
        &quot;&quot;&quot;
        return get_default_config()
    async def run(self, job_config: ConfigModel) -&gt; dict:
        &quot;&quot;&quot;
        Run the training job in a separate thread.
        &quot;&quot;&quot;
        print(&quot;Received call&quot;)
        trainer = APIState.get_trainer()
        current_job_id = APIState.get_state(&quot;current_job_id&quot;)
        job_id = job_config.job_id
        current_job_status = get_thread_status(current_job_id)
        if trainer and current_job_status.lower() == &quot;running&quot;:
            return {
                &quot;status&quot;: False,
                &quot;result&quot;: f&quot;Could not run job, &apos;{current_job_id}&apos; is already running.&quot;,
            }
        self._config_clear()
        self._config_save(job_config)
        try:
            logger.info(&quot;Creating new Trainer instance..&quot;)
            trainer = Trainer(config=normalize_args(job_config.trainer_config), job_id=job_id)
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f&quot;Error loading configuration: {str(e)}&quot;,
            )
        APIState.set_trainer(trainer)
        APIState.set_job(job_config.job_id, job_config.__dict__)
        APIState.set_state(&quot;status&quot;, &quot;pending&quot;)
        if not trainer:
            return {
                &quot;status&quot;: &quot;error&quot;,
                &quot;result&quot;: &quot;No training job has been configured yet. Trainer was unavailable.&quot;,
            }
        if current_job_status.lower() in [&quot;running&quot;, &quot;pending&quot;]:
            return {
                &quot;status&quot;: &quot;error&quot;,
                &quot;result&quot;: f&quot;The training job &apos;{job_config.job_id}&apos; is already {current_job_status}.&quot;,
            }
        try:
            # Submit the job to the thread manager
            print(&quot;Submitting job to thread..&quot;)
            submit_job(job_id, trainer.run)
            APIState.set_state(&quot;status&quot;, &quot;Running&quot;)
            return {
                &quot;status&quot;: &quot;success&quot;,
                &quot;result&quot;: f&quot;Started training run with job ID {job_id}.&quot;,
            }
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f&quot;Error starting training run: {str(e)}&quot;,
            )</file><file path="simpletuner_sdk/interface.py">from fastapi import APIRouter, Request
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
import os
class WebInterface:
    def __init__(self):
        self.router = APIRouter(prefix=&quot;/web&quot;)
        self.router.add_api_route(
            &quot;/&quot;, self.get_page, methods=[&quot;GET&quot;], response_class=HTMLResponse
        )
        self.templates = Jinja2Templates(
            directory=&quot;templates&quot;
        )  # Define the directory where your templates are stored
        self.template_file = &quot;ui.template&quot;  # This should correspond to an HTML file in the templates directory
    async def get_page(self, request: Request):
        &quot;&quot;&quot;
        Retrieve ui.template from disk and display it to the user.
        If the template file does not exist, display a default message.
        &quot;&quot;&quot;
        if os.path.exists(f&quot;templates/{self.template_file}&quot;):
            # Serve the template if it exists
            return self.templates.TemplateResponse(
                self.template_file, {&quot;request&quot;: request}
            )
        else:
            # Default HTML if template is missing
            return HTMLResponse(
                content=&quot;&quot;&quot;
            &lt;!DOCTYPE html&gt;
            &lt;html&gt;
            &lt;head&gt;
                &lt;title&gt;SimpleTuner&lt;/title&gt;
            &lt;/head&gt;
            &lt;body&gt;
                &lt;h1&gt;SimpleTuner&lt;/h1&gt;
                &lt;p&gt;Welcome to SimpleTuner. This installation does not include a compatible web interface. Sorry.&lt;/p&gt;
            &lt;/body&gt;
            &lt;/html&gt;
            &quot;&quot;&quot;,
                status_code=200,
            )</file><file path="simpletuner_sdk/training_host.py">from fastapi import APIRouter
from pydantic import BaseModel
from simpletuner_sdk.api_state import APIState
from simpletuner_sdk.thread_keeper import list_threads, terminate_thread
class TrainingHost:
    def __init__(self):
        self.router = APIRouter(prefix=&quot;/training&quot;)
        self.router.add_api_route(&quot;/&quot;, self.get_job, methods=[&quot;GET&quot;])
        self.router.add_api_route(&quot;/&quot;, self.get_job, methods=[&quot;GET&quot;])
        self.router.add_api_route(&quot;/state&quot;, self.get_host_state, methods=[&quot;GET&quot;])
        self.router.add_api_route(&quot;/cancel&quot;, self.cancel_job, methods=[&quot;POST&quot;])
    def get_host_state(self):
        &quot;&quot;&quot;
        Get the current host status using APIState
        &quot;&quot;&quot;
        return {&quot;result&quot;: APIState.get_state(), &quot;job_list&quot;: list_threads()}
    def get_job(self):
        &quot;&quot;&quot;
        Returns just the currently active job from APIState
        &quot;&quot;&quot;
        return {&quot;result&quot;: APIState.get_job()}
    def cancel_job(self):
        &quot;&quot;&quot;
        Cancel the currently active job
        &quot;&quot;&quot;
        trainer = APIState.get_trainer()
        if not trainer:
            return {&quot;status&quot;: False, &quot;result&quot;: &quot;No job to cancel&quot;}
        trainer.abort()
        is_terminated = terminate_thread(job_id=APIState.get_state(&quot;current_job_id&quot;))
        APIState.set_trainer(None)
        APIState.cancel_job()
        return {&quot;result&quot;: &quot;Job marked for cancellation.&quot;}</file><file path="tests/helpers/data.py"># Mock data_backend for unit testing
class MockDataBackend:
    @staticmethod
    def read(image_path_str):
        # Dummy read method for testing
        return b&quot;fake_image_data&quot;</file><file path="tests/test_collate.py">import unittest
from unittest.mock import patch, MagicMock
import numpy as np
import torch
from helpers.training.collate import (
    collate_fn,
)  # Adjust this import according to your project structure
from helpers.training.state_tracker import StateTracker  # Adjust imports as needed
class TestCollateFn(unittest.TestCase):
    def setUp(self):
        # Set up any common variables or mocks used in multiple tests
        self.mock_batch = [
            {
                &quot;training_samples&quot;: [
                    {
                        &quot;image_path&quot;: &quot;fake_path_1.png&quot;,
                        &quot;instance_prompt_text&quot;: &quot;caption 1&quot;,
                        &quot;luminance&quot;: 0.5,
                        &quot;original_size&quot;: (100, 100),
                        &quot;image_data&quot;: MagicMock(),
                        &quot;crop_coordinates&quot;: [0, 0, 100, 100],
                        &quot;data_backend_id&quot;: &quot;foo&quot;,
                        &quot;aspect_ratio&quot;: 1.0,
                    }
                ],
                &quot;conditioning_samples&quot;: [],
            },
            # Add more examples as needed
        ]
        # Mock StateTracker.get_args() to return a mock object with required attributes
        StateTracker.set_args(
            MagicMock(caption_dropout_probability=0.5, controlnet=False, flux=False)
        )
        fake_accelerator = MagicMock(device=&quot;cpu&quot;)
        StateTracker.set_accelerator(fake_accelerator)
    @patch(&quot;helpers.training.collate.compute_latents&quot;)
    @patch(&quot;helpers.training.collate.compute_prompt_embeddings&quot;)
    @patch(&quot;helpers.training.collate.gather_conditional_sdxl_size_features&quot;)
    def test_collate_fn(self, mock_gather, mock_compute_embeds, mock_compute_latents):
        # Mock the responses from the compute functions
        mock_compute_latents.return_value = torch.randn(
            2, 512
        )  # Adjust dimensions as needed
        mock_compute_embeds.return_value = (
            torch.randn(2, 768),
            torch.randn(2, 768),
        )  # Example embeddings
        mock_gather.return_value = torch.tensor(
            [[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]]
        )
        mock_compute_latents.to = MagicMock(return_value=mock_compute_latents)
        # Call collate_fn with a mock batch
        with patch(&quot;helpers.training.state_tracker.StateTracker.get_data_backend&quot;):
            # Mock get_data_backend() to return a mock object with required attributes
            StateTracker.get_data_backend.return_value = MagicMock(
                compute_embeddings_for_legacy_prompts=MagicMock()
            )
            result = collate_fn(self.mock_batch)
        # Assert that the results are as expected
        self.assertIn(&quot;latent_batch&quot;, result)
        self.assertIn(&quot;prompt_embeds&quot;, result)
        self.assertIn(&quot;add_text_embeds&quot;, result)
        self.assertIn(&quot;batch_time_ids&quot;, result)
        self.assertIn(&quot;batch_luminance&quot;, result)
        # Check that the conditioning dropout was correctly applied (random elements should be zeros)
        # This can be tricky since the dropout is random; you may want to set a fixed random seed or test the structure more than values
    # You can add more test methods to cover different aspects like different dropout probabilities, edge cases, etc.
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_cropping.py">import unittest
from PIL import Image
from helpers.multiaspect.image import (
    MultiaspectImage,
)  # Adjust import according to your project structure
class TestCropping(unittest.TestCase):
    def setUp(self):
        # Creating a sample image for testing
        self.sample_image = Image.new(&quot;RGB&quot;, (500, 300), &quot;white&quot;)
    def test_crop_corner(self):
        target_width, target_height = 300, 200
        from helpers.image_manipulation.cropping import CornerCropping
        cropper = CornerCropping(self.sample_image)
        cropped_image, (top, left) = cropper.set_intermediary_size(
            target_width + 10, target_height + 10
        ).crop(target_width, target_height)
        # Check if cropped coordinates are within original image bounds
        self.assertTrue(0 &lt;= left &lt; self.sample_image.width)
        self.assertTrue(0 &lt;= top &lt; self.sample_image.height)
        self.assertTrue(left + target_width &lt;= self.sample_image.width)
        self.assertTrue(top + target_height &lt;= self.sample_image.height)
        self.assertEqual(cropped_image.size, (target_width, target_height))
    def test_crop_center(self):
        from helpers.image_manipulation.cropping import CenterCropping
        cropper = CenterCropping(self.sample_image)
        target_width, target_height = 300, 200
        cropper.set_intermediary_size(target_width + 10, target_height + 10)
        cropped_image, (left, top) = cropper.crop(target_width, target_height)
        # Similar checks as above
        self.assertTrue(0 &lt;= left &lt; self.sample_image.width)
        self.assertTrue(0 &lt;= top &lt; self.sample_image.height)
        self.assertTrue(left + target_width &lt;= self.sample_image.width)
        self.assertTrue(top + target_height &lt;= self.sample_image.height)
        self.assertEqual(cropped_image.size, (target_width, target_height))
    def test_crop_random(self):
        from helpers.image_manipulation.cropping import RandomCropping
        target_width, target_height = 300, 200
        cropped_image, (top, left) = (
            RandomCropping(self.sample_image)
            .set_intermediary_size(target_width + 10, target_height + 10)
            .crop(target_width, target_height)
        )
        # Similar checks as above
        self.assertTrue(0 &lt;= left &lt; self.sample_image.width)
        self.assertTrue(0 &lt;= top &lt; self.sample_image.height)
        self.assertTrue(left + target_width &lt;= self.sample_image.width)
        self.assertTrue(top + target_height &lt;= self.sample_image.height)
        self.assertEqual(cropped_image.size, (target_width, target_height))
    # Add additional tests for other methods as necessary
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_custom_schedules.py">import unittest
from unittest.mock import patch, MagicMock
import torch
import torch.optim as optim
from helpers.training.custom_schedule import (
    get_polynomial_decay_schedule_with_warmup,
    enforce_zero_terminal_snr,
    patch_scheduler_betas,
    segmented_timestep_selection,
)
class TestPolynomialDecayWithWarmup(unittest.TestCase):
    def test_polynomial_decay_schedule_with_warmup(self):
        optimizer = optim.SGD([torch.randn(2, 2, requires_grad=True)], lr=0.1)
        scheduler = get_polynomial_decay_schedule_with_warmup(
            optimizer, num_warmup_steps=10, num_training_steps=100
        )
        # Test warmup
        ranges = [0, 0.01, 0.02, 0.03, 0.04, 0.05]
        first_lr = round(scheduler.get_last_lr()[0], 2)
        for step in range(len(ranges)):
            last_lr = round(scheduler.get_last_lr()[0], 2)
            optimizer.step()
            scheduler.step()
        self.assertAlmostEqual(last_lr, ranges[-1], places=3)
        # Test decay
        for step in range(len(ranges), 100):
            optimizer.step()
            scheduler.step()
        # Implement your decay formula here to check
        expected_lr = 1e-7
        self.assertAlmostEqual(scheduler.get_last_lr()[0], expected_lr, places=4)
    def test_enforce_zero_terminal_snr(self):
        betas = torch.tensor([0.9, 0.8, 0.7])
        new_betas = enforce_zero_terminal_snr(betas)
        final_beta = new_betas[-1]
        self.assertEqual(final_beta, 1.0)
    def test_patch_scheduler_betas(self):
        # Create a dummy scheduler with betas attribute
        class DummyScheduler:
            def __init__(self):
                self.betas = torch.tensor([0.9, 0.8, 0.7])
        scheduler = DummyScheduler()
        # Check value before.
        final_beta = scheduler.betas[-1]
        self.assertEqual(final_beta, 0.7)
        patch_scheduler_betas(scheduler)
        final_beta = scheduler.betas[-1]
        self.assertEqual(final_beta, 1.0)
    def test_inverted_schedule(self):
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_args&quot;,
            return_value=MagicMock(
                refiner_training=True,
                refiner_training_invert_schedule=True,
                refiner_training_strength=0.35,
            ),
        ):
            weights = torch.ones(1000)  # Uniform weights
            selected_timesteps = segmented_timestep_selection(
                1000, 10, weights, use_refiner_range=False
            )
            self.assertTrue(
                all(350 &lt;= t &lt;= 999 for t in selected_timesteps),
                f&quot;Selected timesteps: {selected_timesteps}&quot;,
            )
    def test_normal_schedule(self):
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_args&quot;,
            return_value=MagicMock(
                refiner_training=True,
                refiner_training_invert_schedule=False,
                refiner_training_strength=0.35,
            ),
        ):
            weights = torch.ones(1000)  # Uniform weights
            selected_timesteps = segmented_timestep_selection(
                1000, 10, weights, use_refiner_range=False
            )
            self.assertTrue(all(0 &lt;= t &lt; 350 for t in selected_timesteps))
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_dataset.py">import unittest
import pandas as pd
from unittest.mock import patch, Mock, MagicMock
from PIL import Image
from pathlib import Path
from helpers.multiaspect.dataset import MultiAspectDataset
from helpers.metadata.backends.discovery import DiscoveryMetadataBackend
from helpers.data_backend.base import BaseDataBackend
from helpers.data_backend.factory import check_column_values
class TestMultiAspectDataset(unittest.TestCase):
    def setUp(self):
        self.instance_data_dir = &quot;/some/fake/path&quot;
        self.accelerator = Mock()
        self.metadata_backend = Mock(spec=DiscoveryMetadataBackend)
        self.metadata_backend.__len__ = Mock(return_value=10)
        self.image_metadata = {
            &quot;image_path&quot;: &quot;fake_image_path&quot;,
            &quot;original_size&quot;: (16, 8),
            &quot;crop_coordinates&quot;: (0, 0),
            &quot;target_size&quot;: (16, 8),
            &quot;aspect_ratio&quot;: 1.0,
            &quot;luminance&quot;: 0.5,
        }
        self.metadata_backend.get_metadata_by_filepath = Mock(
            return_value=self.image_metadata
        )
        self.data_backend = Mock(spec=BaseDataBackend)
        self.image_path = &quot;fake_image_path&quot;
        # Mock the Path.exists method to return True
        with patch(&quot;pathlib.Path.exists&quot;, return_value=True):
            self.dataset = MultiAspectDataset(
                id=&quot;foo&quot;,
                datasets=[range(10)],
            )
    def test_init_invalid_instance_data_dir(self):
        MultiAspectDataset(
            id=&quot;foo&quot;,
            datasets=[range(10)],
        )
    def test_len(self):
        self.metadata_backend.__len__.return_value = 10
        self.assertEqual(len(self.dataset), 10)
    def test_getitem_valid_image(self):
        mock_image_data = b&quot;fake_image_data&quot;
        self.data_backend.read.return_value = mock_image_data
        with patch(&quot;PIL.Image.open&quot;) as mock_image_open:
            # Create a blank canvas:
            mock_image = Image.new(mode=&quot;RGB&quot;, size=(16, 8))
            mock_image_open.return_value = mock_image
            target = tuple(
                [
                    {
                        &quot;image_path&quot;: self.image_path,
                        &quot;image_data&quot;: mock_image,
                        &quot;instance_prompt_text&quot;: &quot;fake_prompt_text&quot;,
                        &quot;original_size&quot;: (16, 8),
                        &quot;target_size&quot;: (16, 8),
                        &quot;aspect_ratio&quot;: 1.0,
                        &quot;luminance&quot;: 0.5,
                    }
                ]
            )
            examples = self.dataset.__getitem__(target)
        # Grab the size of the first image:
        examples = examples[&quot;training_samples&quot;]
        first_size = examples[0][&quot;original_size&quot;]
        # Are all sizes the same?
        for example in examples:
            self.assertIsNotNone(example)
            self.assertEqual(example[&quot;original_size&quot;], first_size)
            self.assertEqual(example[&quot;image_path&quot;], self.image_path)
    def test_getitem_invalid_image(self):
        self.data_backend.read.side_effect = Exception(&quot;Some error&quot;)
        with self.assertRaises(Exception):
            with self.assertLogs(&quot;MultiAspectDataset&quot;, level=&quot;ERROR&quot;) as cm:
                self.dataset.__getitem__(self.image_metadata)
class TestDataBackendFactory(unittest.TestCase):
    def test_all_null(self):
        column_data = pd.Series([None, None, None])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains only null values&quot;, str(context.exception))
    def test_arrays_with_nulls(self):
        column_data = pd.Series([[1, 2], None, [3, 4]])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains null arrays&quot;, str(context.exception))
    def test_empty_arrays(self):
        column_data = pd.Series([[1, 2], [], [3, 4]])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains empty arrays&quot;, str(context.exception))
    def test_null_elements_in_arrays(self):
        column_data = pd.Series([[1, None], [2, 3], [3, 4]])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains null values within arrays&quot;, str(context.exception))
    def test_empty_strings_in_arrays(self):
        column_data = pd.Series([[&quot;&quot;, &quot;&quot;], [&quot;&quot;, &quot;&quot;], [&quot;&quot;, &quot;&quot;]])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(
            &quot;contains only empty strings within arrays&quot;, str(context.exception)
        )
    def test_scalar_strings_with_nulls(self):
        column_data = pd.Series([&quot;a&quot;, None, &quot;b&quot;])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains null values&quot;, str(context.exception))
    def test_scalar_strings_with_empty(self):
        column_data = pd.Series([&quot;a&quot;, &quot;&quot;, &quot;b&quot;])
        with self.assertRaises(ValueError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;contains empty strings&quot;, str(context.exception))
    def test_with_fallback_caption(self):
        column_data = pd.Series([None, &quot;&quot;, [None], [&quot;&quot;]])
        try:
            check_column_values(
                column_data,
                &quot;test_column&quot;,
                &quot;test_file.parquet&quot;,
                fallback_caption_column=True,
            )
        except ValueError:
            self.fail(
                &quot;check_column_values() raised ValueError unexpectedly with fallback_caption_column=True&quot;
            )
    def test_invalid_data_type(self):
        column_data = pd.Series([1, 2, 3])
        with self.assertRaises(TypeError) as context:
            check_column_values(column_data, &quot;test_column&quot;, &quot;test_file.parquet&quot;)
        self.assertIn(&quot;Unsupported data type in column&quot;, str(context.exception))
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_ema.py">import unittest
import torch
import tempfile
import os
from helpers.training.ema import EMAModel
class TestEMAModel(unittest.TestCase):
    def setUp(self):
        # Set up a simple model and its parameters
        self.model = torch.nn.Linear(10, 5)  # Simple linear model
        self.args = type(
            &quot;Args&quot;,
            (),
            {&quot;ema_update_interval&quot;: None, &quot;ema_device&quot;: &quot;cpu&quot;, &quot;ema_cpu_only&quot;: True},
        )
        self.accelerator = None  # For simplicity, assuming no accelerator in tests
        self.ema_model = EMAModel(
            args=self.args,
            accelerator=self.accelerator,
            parameters=self.model.parameters(),
            decay=0.999,
            min_decay=0.999,  # Force decay to be 0.999
            update_after_step=-1,  # Ensure decay is used from step 1
            use_ema_warmup=False,  # Disable EMA warmup
            foreach=False,
        )
    def test_ema_initialization(self):
        &quot;&quot;&quot;Test that the EMA model initializes correctly.&quot;&quot;&quot;
        self.assertEqual(
            len(self.ema_model.shadow_params), len(list(self.model.parameters()))
        )
        for shadow_param, model_param in zip(
            self.ema_model.shadow_params, self.model.parameters()
        ):
            self.assertTrue(torch.equal(shadow_param, model_param))
    def test_ema_step(self):
        &quot;&quot;&quot;Test that the EMA model updates correctly after a step.&quot;&quot;&quot;
        # Perform a model parameter update
        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)
        dummy_input = torch.randn(1, 10)  # Adjust to match input size
        dummy_output = self.model(dummy_input)
        loss = dummy_output.sum()  # A dummy loss function
        loss.backward()
        optimizer.step()
        # Save a copy of the model parameters after the update but before the EMA update.
        model_params = [p.clone() for p in self.model.parameters()]
        # Save a copy of the shadow parameters before the EMA update.
        shadow_params_before = [p.clone() for p in self.ema_model.shadow_params]
        # Perform an EMA update
        self.ema_model.step(self.model.parameters(), global_step=1)
        decay = self.ema_model.cur_decay_value  # This should be 0.999
        # Verify that the decay used is as expected
        self.assertAlmostEqual(
            decay, 0.999, places=6, msg=&quot;Decay value is not as expected.&quot;
        )
        # Verify shadow parameters have changed
        for shadow_param, shadow_param_before in zip(
            self.ema_model.shadow_params, shadow_params_before
        ):
            self.assertFalse(
                torch.equal(shadow_param, shadow_param_before),
                &quot;Shadow parameters did not update correctly.&quot;,
            )
        # Compute and check expected shadow parameter values
        for shadow_param, shadow_param_before, model_param in zip(
            self.ema_model.shadow_params, shadow_params_before, self.model.parameters()
        ):
            expected_shadow = decay * shadow_param_before + (1 - decay) * model_param
            self.assertTrue(
                torch.allclose(shadow_param, expected_shadow, atol=1e-6),
                f&quot;Shadow parameter does not match expected value.&quot;,
            )
    def test_save_and_load_state_dict(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = os.path.join(temp_dir, &quot;ema_model_state.pth&quot;)
            # Save the state
            self.ema_model.save_state_dict(temp_path)
            # Create a new EMA model and load the state
            new_ema_model = EMAModel(
                args=self.args,
                accelerator=self.accelerator,
                parameters=self.model.parameters(),
                decay=0.999,
            )
            new_ema_model.load_state_dict(temp_path)
            # Check that the new EMA model&apos;s shadow parameters match the saved state
            for shadow_param, new_shadow_param in zip(
                self.ema_model.shadow_params, new_ema_model.shadow_params
            ):
                self.assertTrue(torch.equal(shadow_param, new_shadow_param))
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_image.py">import unittest, random, logging, os
logger = logging.getLogger(__name__)
logger.setLevel(os.environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, logging.INFO))
from unittest.mock import patch
from unittest.mock import Mock, MagicMock
from PIL import Image
from io import BytesIO
from helpers.multiaspect.image import MultiaspectImage
from helpers.training.state_tracker import StateTracker
from tests.helpers.data import MockDataBackend
class TestMultiaspectImage(unittest.TestCase):
    def setUp(self):
        self.resolution = 128
        self.test_image = Image.new(&quot;RGB&quot;, (512, 256), color=&quot;red&quot;)
        self.data_backend = MagicMock()
        self.metadata_backend = MagicMock()
        self.metadata_backend.resolution = 1.0  # Example resolution
        self.metadata_backend.resolution_type = &quot;dimension&quot;  # Example resolution type
        self.metadata_backend.meets_resolution_requirements.return_value = True
        # Mock image data to simulate reading from the backend
        self.image_path_str = &quot;test_image.jpg&quot;
        # Convert the test image to bytes
        image_bytes = BytesIO()
        self.test_image.save(image_bytes, format=&quot;JPEG&quot;)
        image_bytes.seek(0)  # Important: move back to the start of the BytesIO object
        self.mock_image_data = image_bytes.getvalue()
        self.data_backend.read.return_value = self.mock_image_data
    def test_aspect_ratio_calculation(self):
        &quot;&quot;&quot;
        Test that the aspect ratio calculation returns expected results.
        &quot;&quot;&quot;
        StateTracker.set_args(MagicMock(aspect_bucket_rounding=2))
        self.assertEqual(
            MultiaspectImage.calculate_image_aspect_ratio((1920, 1080)), 1.78
        )
        self.assertEqual(
            MultiaspectImage.calculate_image_aspect_ratio((1080, 1920)), 0.56
        )
    def test_calculate_new_size_by_pixel_edge(self):
        # Define test cases for 1.0 and 0.5 megapixels
        test_edge_lengths = [1024, 512, 256, 64]
        # Number of random tests to perform
        num_random_tests = 1000
        with patch(&quot;helpers.training.state_tracker.StateTracker.get_args&quot;) as mock_args:
            for edge_length in test_edge_lengths:
                mock_args.return_value = Mock(
                    resolution_type=&quot;pixel&quot;,
                    resolution=self.resolution,
                    crop_style=&quot;random&quot;,
                    aspect_bucket_rounding=2,
                    aspect_bucket_alignment=64 if edge_length &gt; 64 else 8,
                )
                for _ in range(num_random_tests):
                    # Generate a random original width and height
                    original_width = random.randint(edge_length, edge_length * 50)
                    original_height = random.randint(edge_length, edge_length * 50)
                    original_aspect_ratio = original_width / original_height
                    # Calculate new size
                    reformed_size, intermediary_size, new_aspect_ratio = (
                        MultiaspectImage.calculate_new_size_by_pixel_edge(
                            original_aspect_ratio,
                            edge_length,
                            (original_width, original_height),
                        )
                    )
                    # Calculate the resulting megapixels
                    new_width, new_height = reformed_size
                    # Check that the resulting image size is not below the specified minimum edge length.
                    self.assertTrue(
                        min(new_width, new_height) &gt;= edge_length,
                        f&quot;Final target size {new_width}x{new_height} = {min(new_width, new_height)} px is below the specified {edge_length} px from original size {original_width}x{original_height}, alignment {64 if edge_length &gt; 256 else 8}&quot;,
                    )
                    # Check that the resulting image size is not below the specified minimum edge length.
                    new_width, new_height = intermediary_size
                    self.assertTrue(
                        min(new_width, new_height) &gt;= edge_length,
                        f&quot;Intermediary size {new_width}x{new_height} = {min(new_width, new_height)} px is below the specified {edge_length} px from original size {original_width}x{original_height}, alignment {64 if edge_length &gt; 256 else 8}&quot;,
                    )
                    # Check that the intermediary size is larger than the target size.
                    self.assertTrue(
                        intermediary_size &gt;= reformed_size,
                        f&quot;Intermediary size is less than reformed size: {intermediary_size} &lt; {reformed_size} (original size: {original_width}x{original_height})&quot;,
                    )
    def test_calculate_batch_size_by_pixel_edge(self):
        test_edge_lengths = [1024, 768, 512, 256, 64]
        num_images_per_batch = 100
        aspect_ratios = [
            1.5,
            1.0,
            0.67,
            0.76,
            1.33,
            2.0,
            8.0,
            1.78,
        ]  # Example fixed aspect ratio for all test cases
        with patch(&quot;helpers.training.state_tracker.StateTracker.get_args&quot;) as mock_args:
            for edge_length in test_edge_lengths:
                for fixed_aspect_ratio in aspect_ratios:
                    aspect_bucket_alignment = 64 if edge_length &gt; 64 else 8
                    edge_length += 512
                    logger.debug(
                        f&quot;Using aspect bucket alignment: {aspect_bucket_alignment} for {edge_length}px edge length, test aspect ratio: {fixed_aspect_ratio}&quot;
                    )
                    mock_args.return_value = Mock(
                        resolution_type=&quot;pixel&quot;,
                        resolution=edge_length,
                        crop_style=&quot;random&quot;,
                        aspect_bucket_rounding=2,
                        aspect_bucket_alignment=aspect_bucket_alignment,
                    )
                    # Generate a batch of original sizes correctly adhering to the fixed aspect ratio
                    original_sizes = []
                    for _ in range(num_images_per_batch):
                        # Generate a random height and calculate the corresponding width
                        height = max(
                            edge_length, random.randint(edge_length, edge_length * 50)
                        )
                        if fixed_aspect_ratio &gt;= 1:
                            height = random.randint(edge_length, edge_length * 10)
                            width = int(height * fixed_aspect_ratio)
                        else:
                            width = random.randint(edge_length, edge_length * 10)
                            height = int(width / fixed_aspect_ratio)
                        # Check if size is large enough once pixel-adjusted
                        self.assertTrue(
                            min(width, height) &gt;= edge_length,
                            f&quot;Original size {width}x{height} is below the specified {edge_length} px&quot;,
                        )
                        original_sizes.append((width, height))
                    # Ensure aspect ratios are correctly calculated
                    for width, height in original_sizes:
                        calculated_aspect_ratio = (
                            MultiaspectImage.calculate_image_aspect_ratio(
                                width / height
                            )
                        )
                        self.assertEqual(
                            calculated_aspect_ratio,
                            fixed_aspect_ratio,
                            msg=f&quot;Generated size {width}x{height} has aspect ratio {calculated_aspect_ratio}, expected {fixed_aspect_ratio}&quot;,
                        )
                    first_aspect_ratio = None
                    first_transformed_aspect_ratio = None
                    first_reformed_size = None
                    for original_width, original_height in original_sizes:
                        reformed_size, intermediary_size, new_aspect_ratio = (
                            MultiaspectImage.calculate_new_size_by_pixel_edge(
                                fixed_aspect_ratio,
                                edge_length,
                                (original_width, original_height),
                            )
                        )
                        if first_reformed_size is None:
                            first_reformed_size = reformed_size
                        if first_aspect_ratio is None:
                            first_aspect_ratio = (
                                MultiaspectImage.calculate_image_aspect_ratio(
                                    intermediary_size
                                )
                            )
                        if first_transformed_aspect_ratio is None:
                            first_transformed_aspect_ratio = new_aspect_ratio
                        if (
                            new_aspect_ratio != first_transformed_aspect_ratio
                            or MultiaspectImage.calculate_image_aspect_ratio(
                                intermediary_size
                            )
                            != fixed_aspect_ratio
                        ):
                            logger.debug(&quot;####&quot;)
                            logger.debug(
                                f&quot;-&gt; First aspect ratio: {first_aspect_ratio}, first transformed aspect ratio: {first_transformed_aspect_ratio}, first reformed size: {first_reformed_size}&quot;
                            )
                            logger.debug(
                                f&quot;-&gt; Original size: {original_width}x{original_height} ({MultiaspectImage.calculate_image_aspect_ratio((original_width, original_height))})&quot;
                            )
                            logger.debug(
                                f&quot;-&gt; Reformed size: {reformed_size} ({MultiaspectImage.calculate_image_aspect_ratio(reformed_size)})&quot;
                            )
                            logger.debug(
                                f&quot;-&gt; {&apos;*&apos; if MultiaspectImage.calculate_image_aspect_ratio(intermediary_size) != fixed_aspect_ratio else &apos;&apos;}Intermediary size: {intermediary_size} ({MultiaspectImage.calculate_image_aspect_ratio(intermediary_size)} vs {fixed_aspect_ratio})&quot;
                            )
                            logger.debug(
                                f&quot;-&gt; {&apos;*&apos; if new_aspect_ratio != first_transformed_aspect_ratio else &apos;&apos;}New aspect ratio: {new_aspect_ratio} vs {first_transformed_aspect_ratio}&quot;
                            )
                            logger.debug(&quot;####&quot;)
                        self.assertEqual(
                            MultiaspectImage.calculate_image_aspect_ratio(
                                intermediary_size
                            ),
                            fixed_aspect_ratio,
                        )
                        self.assertEqual(first_reformed_size, reformed_size)
                        self.assertEqual(
                            MultiaspectImage.calculate_image_aspect_ratio(
                                reformed_size
                            ),
                            first_transformed_aspect_ratio,
                        )
    def test_calculate_new_size_by_pixel_area(self):
        # Define test cases for 1.0 and 0.5 megapixels
        test_megapixels = [1.0, 0.5]
        # Number of random tests to perform
        num_random_tests = 100
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_args&quot;
        ) as mock_args, patch(
            &quot;helpers.training.state_tracker.StateTracker._load_from_disk&quot;
        ) as load_from_disk_mock, patch(
            &quot;helpers.training.state_tracker.StateTracker._save_to_disk&quot;
        ) as save_to_disk_mock:
            load_from_disk_mock.return_value = {}
            save_to_disk_mock.return_value = True
            mock_args.return_value = Mock(
                resolution_type=&quot;pixel&quot;,
                resolution=self.resolution,
                crop_style=&quot;random&quot;,
                aspect_bucket_rounding=2,
                aspect_bucket_alignment=64,
            )
            for mp in test_megapixels:
                for _ in range(num_random_tests):
                    # Generate a random original width and height
                    original_width = random.randint(512, 2500)
                    original_height = random.randint(512, 2500)
                    original_aspect_ratio = original_width / original_height
                    # Calculate new size
                    target_size, intermediary_size, new_aspect_ratio = (
                        MultiaspectImage.calculate_new_size_by_pixel_area(
                            original_aspect_ratio, mp, (original_width, original_height)
                        )
                    )
                    # Calculate the resulting megapixels
                    target_width, target_height = target_size
                    intermediary_width, intermediary_height = intermediary_size
                    self.assertGreaterEqual(
                        intermediary_width,
                        target_width,
                        (
                            f&quot;Final width {target_width} is greater than the intermediary {intermediary_size}&quot;
                            f&quot;, original size {original_width}x{original_height} and target megapixels {mp}&quot;
                            f&quot; for aspect ratio {original_aspect_ratio}&quot;
                        ),
                    )
                    self.assertGreaterEqual(
                        intermediary_height,
                        target_height,
                        f&quot;Final height {target_height} is greater than the intermediary {intermediary_size}&quot;,
                    )
                    self.assertAlmostEqual(
                        MultiaspectImage.calculate_image_aspect_ratio(
                            (original_width, original_height)
                        ),
                        MultiaspectImage.calculate_image_aspect_ratio(
                            intermediary_size
                        ),
                        delta=0.02,
                    )
    def test_calculate_new_size_by_pixel_area_uniformity(self):
        # Example input resolutions and expected output
        test_cases = [
            (
                3911,
                5476,
                1.0,
            ),  # Original resolution and target megapixels, ar=0.714
            (
                4539,
                6527,
                1.0,
            ),  # Original resolution and target megapixels, ar=0.695
            (
                832,
                1216,
                1.0,
            ),
        ]
        expected_size = (
            832,
            1216,
        )  # Expected final size for all test cases based on a fixed aspect ratio
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_args&quot;
        ) as mock_args, patch(
            &quot;helpers.training.state_tracker.StateTracker._load_from_disk&quot;
        ) as load_from_disk_mock, patch(
            &quot;helpers.training.state_tracker.StateTracker._save_to_disk&quot;
        ) as save_to_disk_mock:
            load_from_disk_mock.return_value = {}
            save_to_disk_mock.return_value = True
            mock_args.return_value = Mock(
                resolution_type=&quot;pixel&quot;,
                resolution=self.resolution,
                crop_style=&quot;random&quot;,
                aspect_bucket_rounding=2,
                aspect_bucket_alignment=64,
            )
            for W, H, megapixels in test_cases:
                final_size, intermediary_size, new_aspect_ratio = (
                    MultiaspectImage.calculate_new_size_by_pixel_area(
                        MultiaspectImage.calculate_image_aspect_ratio(W / H),
                        megapixels,
                        (W, H),
                    )
                )
                W_final, H_final = final_size
                self.assertEqual(
                    (W_final, H_final),
                    expected_size,
                    f&quot;Failed for original size {W}x{H}&quot;,
                )
                self.assertNotEqual(
                    new_aspect_ratio,
                    (W / H),
                    f&quot;Failed for original size {W}x{H}&quot;,
                )
    def test_calculate_new_size_by_pixel_area_squares(self):
        # Example input resolutions and expected output
        test_cases = [
            (
                4000,
                4000,
                1.0,
            ),  # Original resolution and target megapixels, ar=0.714
            (
                2000,
                2000,
                1.0,
            ),  # Original resolution and target megapixels, ar=0.695
        ]
        expected_size = (
            1024,
            1024,
        )  # Expected final size for all test cases based on a fixed aspect ratio
        with patch(&quot;helpers.training.state_tracker.StateTracker.get_args&quot;) as mock_args:
            mock_args.return_value = Mock(
                resolution_type=&quot;pixel&quot;,
                resolution=self.resolution,
                crop_style=&quot;random&quot;,
                aspect_bucket_rounding=2,
                aspect_bucket_alignment=64,
            )
            for W, H, megapixels in test_cases:
                final_size, intermediary_size, _ = (
                    MultiaspectImage.calculate_new_size_by_pixel_area(
                        MultiaspectImage.calculate_image_aspect_ratio((W, H)),
                        megapixels,
                        (W, H),
                    )
                )
                W_final, H_final = final_size
                self.assertEqual(
                    (W_final, H_final),
                    expected_size,
                    f&quot;Failed for original size {W}x{H}&quot;,
                )
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_metadata_backend.py">import unittest, json
from PIL import Image
from unittest.mock import Mock, patch, MagicMock
from helpers.metadata.backends.discovery import DiscoveryMetadataBackend
from helpers.training.state_tracker import StateTracker
from tests.helpers.data import MockDataBackend
class TestMetadataBackend(unittest.TestCase):
    def setUp(self):
        self.data_backend = MockDataBackend()
        self.data_backend.id = &quot;foo&quot;
        self.test_image = Image.new(&quot;RGB&quot;, (512, 256), color=&quot;red&quot;)
        self.accelerator = Mock()
        self.data_backend.exists = Mock(return_value=True)
        self.data_backend.write = Mock(return_value=True)
        self.data_backend.list_files = Mock(
            return_value=[(&quot;subdir&quot;, &quot;&quot;, &quot;image_path.png&quot;)]
        )
        self.data_backend.read = Mock(return_value=self.test_image.tobytes())
        # Mock image data to simulate reading from the backend
        self.image_path_str = &quot;test_image.jpg&quot;
        self.instance_data_dir = &quot;/some/fake/path&quot;
        self.cache_file = &quot;/some/fake/cache&quot;
        self.metadata_file = &quot;/some/fake/metadata.json&quot;
        StateTracker.set_args(MagicMock())
        # Overload cache file with json:
        with patch(
            &quot;helpers.training.state_tracker.StateTracker._save_to_disk&quot;,
            return_value=True,
        ), patch(&quot;pathlib.Path.exists&quot;, return_value=True):
            with self.assertLogs(&quot;DiscoveryMetadataBackend&quot;, level=&quot;WARNING&quot;):
                self.metadata_backend = DiscoveryMetadataBackend(
                    id=&quot;foo&quot;,
                    instance_data_dir=self.instance_data_dir,
                    cache_file=self.cache_file,
                    metadata_file=self.metadata_file,
                    batch_size=1,
                    data_backend=self.data_backend,
                    resolution=1,
                    resolution_type=&quot;area&quot;,
                    accelerator=self.accelerator,
                    repeats=0,
                )
    def test_len(self):
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;],
            &quot;1.5&quot;: [&quot;image3&quot;],
        }
        self.assertEqual(len(self.metadata_backend), 3)
    def test_discover_new_files(self):
        # Assuming that StateTracker.get_image_files returns known files
        # and list_files should return both known and potentially new files
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_image_files&quot;,
            return_value=[&quot;image1.jpg&quot;, &quot;image2.png&quot;, &quot;image3.jpg&quot;, &quot;image4.png&quot;],
        ), patch(
            &quot;helpers.training.state_tracker.StateTracker.set_image_files&quot;,
            return_value=None,
        ), patch.object(
            self.data_backend,
            &quot;list_files&quot;,
            return_value=[&quot;image1.jpg&quot;, &quot;image2.png&quot;, &quot;image3.jpg&quot;, &quot;image4.png&quot;],
        ):
            self.metadata_backend.aspect_ratio_bucket_indices = {
                &quot;1.0&quot;: [&quot;image1.jpg&quot;, &quot;image2.png&quot;]
            }
            new_files = self.metadata_backend._discover_new_files(for_metadata=False)
            # Assuming the method&apos;s logic excludes files known ([&quot;image1.jpg&quot;, &quot;image2.png&quot;])
            # The expectation is that only [&quot;image3.jpg&quot;, &quot;image4.png&quot;] are returned as new
            self.assertEqual(sorted(new_files), sorted([&quot;image3.jpg&quot;, &quot;image4.png&quot;]))
    def test_load_cache_valid(self):
        valid_cache_data = {
            &quot;aspect_ratio_bucket_indices&quot;: {&quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;]},
        }
        with patch.object(
            self.data_backend, &quot;read&quot;, return_value=json.dumps(valid_cache_data)
        ):
            self.metadata_backend.reload_cache()
        self.assertEqual(
            self.metadata_backend.aspect_ratio_bucket_indices,
            {&quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;]},
        )
    def test_load_cache_invalid(self):
        invalid_cache_data = &quot;this is not valid json&quot;
        with patch.object(self.data_backend, &quot;read&quot;, return_value=invalid_cache_data):
            with self.assertLogs(&quot;DiscoveryMetadataBackend&quot;, level=&quot;WARNING&quot;):
                self.metadata_backend.reload_cache()
    def test_save_cache(self):
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;]
        }
        with patch.object(self.data_backend, &quot;write&quot;) as mock_write:
            self.metadata_backend.save_cache()
        mock_write.assert_called_once()
    def test_minimum_aspect_size(self):
        # when metadata_backend.minimum_aspect_ratio is not None and &gt; 0.0 it will remove buckets from the list.
        # this test ensures that the bucket is removed when the value is set correctly.
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;],
            &quot;1.5&quot;: [&quot;image3&quot;],
        }
        self.metadata_backend.minimum_aspect_ratio = 1.25
        self.metadata_backend._enforce_min_aspect_ratio()
        self.assertEqual(
            self.metadata_backend.aspect_ratio_bucket_indices, {&quot;1.5&quot;: [&quot;image3&quot;]}
        )
    def test_maximum_aspect_size(self):
        # when metadata_backend.maximum_aspect_ratio is not None and &gt; 0.0 it will remove buckets from the list.
        # this test ensures that the bucket is removed when the value is set correctly.
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;],
            &quot;1.5&quot;: [&quot;image3&quot;],
        }
        self.metadata_backend.maximum_aspect_ratio = 1.25
        self.metadata_backend._enforce_max_aspect_ratio()
        self.assertEqual(
            self.metadata_backend.aspect_ratio_bucket_indices, {&quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;]}
        )
    def test_unbound_aspect_list(self):
        # when metadata_backend.maximum_aspect_ratio is None and metadata_backend.minimum_aspect_ratio is None
        # the aspect_ratio_bucket_indices should not be modified.
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;],
            &quot;1.5&quot;: [&quot;image3&quot;],
        }
        self.metadata_backend._enforce_min_aspect_ratio()
        self.metadata_backend._enforce_max_aspect_ratio()
        self.assertEqual(
            self.metadata_backend.aspect_ratio_bucket_indices,
            {&quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;], &quot;1.5&quot;: [&quot;image3&quot;]},
        )
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_model_card.py">import unittest
from unittest.mock import MagicMock, patch
import os
import json
from helpers.publishing.metadata import (
    _negative_prompt,
    _torch_device,
    _model_imports,
    _model_load,
    _validation_resolution,
    _skip_layers,
    _guidance_rescale,
)
from helpers.publishing.metadata import *
class TestMetadataFunctions(unittest.TestCase):
    def setUp(self):
        # Mock the args object
        self.args = MagicMock()
        self.args.lora_type = &quot;standard&quot;
        self.args.model_type = &quot;lora&quot;
        self.args.model_family = &quot;sdxl&quot;
        self.args.validation_prompt = &quot;A test prompt&quot;
        self.args.validation_negative_prompt = &quot;A negative prompt&quot;
        self.args.validation_num_inference_steps = 50
        self.args.validation_guidance = 7.5
        self.args.validation_guidance_rescale = 0.7
        self.args.validation_resolution = &quot;512x512&quot;
        self.args.pretrained_model_name_or_path = &quot;test-model&quot;
        self.args.output_dir = &quot;test-output&quot;
        self.args.lora_rank = 4
        self.args.lora_alpha = 1.0
        self.args.lora_dropout = 0.0
        self.args.lora_init_type = &quot;kaiming_uniform&quot;
        self.args.model_card_note = &quot;Test note&quot;
        self.args.validation_using_datasets = False
        self.args.flow_matching_loss = &quot;compatible&quot;
        self.args.flux_fast_schedule = False
        self.args.flow_schedule_auto_shift = False
        self.args.flow_schedule_shift = None
        self.args.flux_guidance_value = None
        self.args.flux_guidance_min = None
        self.args.flux_guidance_max = None
        self.args.flow_use_beta_schedule = False
        self.args.flow_beta_schedule_alpha = None
        self.args.flow_beta_schedule_beta = None
        self.args.flux_attention_masked_training = False
        self.args.flow_use_uniform_schedule = False
        self.args.flux_lora_target = None
        self.args.validation_guidance_skip_layers = None
        self.args.validation_seed = 1234
        self.args.validation_noise_scheduler = &quot;ddim&quot;
        self.args.model_card_safe_for_work = True
        self.args.learning_rate = 1e-4
        self.args.max_grad_norm = 1.0
        self.args.train_batch_size = 4
        self.args.gradient_accumulation_steps = 1
        self.args.optimizer = &quot;AdamW&quot;
        self.args.optimizer_config = &quot;&quot;
        self.args.mixed_precision = &quot;fp16&quot;
        self.args.base_model_precision = &quot;no_change&quot;
        self.args.flux_guidance_mode = &quot;constant&quot;
        self.args.flux_guidance_value = 1.0
        self.args.t5_padding = &quot;unmodified&quot;
        self.args.enable_xformers_memory_efficient_attention = False
        self.args.attention_mechanism = &quot;diffusers&quot;
    def test_model_imports(self):
        self.args.lora_type = &quot;standard&quot;
        self.args.model_type = &quot;lora&quot;
        expected_output = &quot;import torch\nfrom diffusers import DiffusionPipeline&quot;
        output = _model_imports(self.args)
        self.assertEqual(output.strip(), expected_output.strip())
        self.args.lora_type = &quot;lycoris&quot;
        output = _model_imports(self.args)
        self.assertIn(&quot;from lycoris import create_lycoris_from_weights&quot;, output)
    def test_model_load(self):
        self.args.pretrained_model_name_or_path = &quot;pretrained-model&quot;
        self.args.output_dir = &quot;output-dir&quot;
        self.args.lora_type = &quot;standard&quot;
        self.args.model_type = &quot;lora&quot;
        with patch(
            &quot;helpers.publishing.metadata.StateTracker.get_hf_username&quot;,
            return_value=&quot;testuser&quot;,
        ):
            output = _model_load(self.args, repo_id=&quot;repo-id&quot;)
            self.assertIn(&quot;pipeline.load_lora_weights&quot;, output)
            self.assertIn(&quot;adapter_id = &apos;testuser/repo-id&apos;&quot;, output)
        self.args.lora_type = &quot;lycoris&quot;
        output = _model_load(self.args)
        self.assertIn(&quot;pytorch_lora_weights.safetensors&quot;, output)
    def test_torch_device(self):
        output = _torch_device()
        expected_output = &quot;&apos;cuda&apos; if torch.cuda.is_available() else &apos;mps&apos; if torch.backends.mps.is_available() else &apos;cpu&apos;&quot;
        self.assertEqual(output.strip(), expected_output.strip())
    def test_negative_prompt(self):
        self.args.model_family = &quot;sdxl&quot;
        output = _negative_prompt(self.args)
        expected_output = &quot;negative_prompt = &apos;A negative prompt&apos;&quot;
        self.assertEqual(output.strip(), expected_output.strip())
        output_in_call = _negative_prompt(self.args, in_call=True)
        self.assertIn(&quot;negative_prompt=negative_prompt&quot;, output_in_call)
    def test_guidance_rescale(self):
        self.args.model_family = &quot;sdxl&quot;
        output = _guidance_rescale(self.args)
        expected_output = &quot;\n    guidance_rescale=0.7,&quot;
        self.assertEqual(output.strip(), expected_output.strip())
        self.args.model_family = &quot;flux&quot;
        output = _guidance_rescale(self.args)
        self.assertEqual(output.strip(), &quot;&quot;)
    def test_skip_layers(self):
        self.args.model_family = &quot;sd3&quot;
        self.args.validation_guidance_skip_layers = 2
        output = _skip_layers(self.args)
        expected_output = &quot;\n    skip_guidance_layers=2,&quot;
        self.assertEqual(output.strip(), expected_output.strip())
        self.args.model_family = &quot;sdxl&quot;
        output = _skip_layers(self.args)
        self.assertEqual(output.strip(), &quot;&quot;)
    def test_validation_resolution(self):
        self.args.validation_resolution = &quot;512x512&quot;
        output = _validation_resolution(self.args)
        expected_output = &quot;width=512,\n    height=512,&quot;
        self.assertEqual(output.strip(), expected_output.strip())
        self.args.validation_resolution = &quot;&quot;
        output = _validation_resolution(self.args)
        expected_output = &quot;width=1024,\n    height=1024,&quot;
        self.assertEqual(output.strip(), expected_output.strip())
    def test_code_example(self):
        with patch(
            &quot;helpers.publishing.metadata._model_imports&quot;,
            return_value=&quot;import torch\nfrom diffusers import DiffusionPipeline&quot;,
        ):
            with patch(
                &quot;helpers.publishing.metadata._model_load&quot;, return_value=&quot;pipeline = ...&quot;
            ):
                with patch(
                    &quot;helpers.publishing.metadata._torch_device&quot;, return_value=&quot;&apos;cuda&apos;&quot;
                ):
                    with patch(
                        &quot;helpers.publishing.metadata._negative_prompt&quot;,
                        return_value=&quot;negative_prompt = &apos;A negative prompt&apos;&quot;,
                    ):
                        with patch(
                            &quot;helpers.publishing.metadata._validation_resolution&quot;,
                            return_value=&quot;width=512,\n    height=512,&quot;,
                        ):
                            output = code_example(self.args)
                            self.assertIn(&quot;import torch&quot;, output)
                            self.assertIn(&quot;pipeline = ...&quot;, output)
                            self.assertIn(&quot;pipeline.to(&apos;cuda&apos;)&quot;, output)
    def test_model_type(self):
        self.args.model_type = &quot;lora&quot;
        self.args.lora_type = &quot;standard&quot;
        output = model_type(self.args)
        self.assertEqual(output, &quot;standard PEFT LoRA&quot;)
        self.args.lora_type = &quot;lycoris&quot;
        output = model_type(self.args)
        self.assertEqual(output, &quot;LyCORIS adapter&quot;)
        self.args.model_type = &quot;full&quot;
        output = model_type(self.args)
        self.assertEqual(output, &quot;full rank finetune&quot;)
    def test_lora_info(self):
        self.args.model_type = &quot;lora&quot;
        self.args.lora_type = &quot;standard&quot;
        output = lora_info(self.args)
        self.assertIn(&quot;LoRA Rank: 4&quot;, output)
        self.args.lora_type = &quot;lycoris&quot;
        # Mocking the file reading
        lycoris_config = {&quot;key&quot;: &quot;value&quot;}
        with patch(
            &quot;builtins.open&quot;,
            unittest.mock.mock_open(read_data=json.dumps(lycoris_config)),
        ):
            output = lora_info(self.args)
            self.assertIn(&apos;&quot;key&quot;: &quot;value&quot;&apos;, output)
    def test_model_card_note(self):
        output = model_card_note(self.args)
        self.assertIn(&quot;Test note&quot;, output)
        self.args.model_card_note = &quot;&quot;
        output = model_card_note(self.args)
        self.assertEqual(output.strip(), &quot;&quot;)
    def test_flux_schedule_info(self):
        self.args.model_family = &quot;flux&quot;
        output = flux_schedule_info(self.args)
        self.assertEqual(
            &quot; (extra parameters=[&apos;flux_guidance_mode=constant&apos;, &apos;flux_guidance_value=1.0&apos;, &apos;flow_matching_loss=compatible&apos;])&quot;,
            output,
        )
        self.args.flux_fast_schedule = True
        output = flux_schedule_info(self.args)
        self.assertIn(&quot;flux_fast_schedule&quot;, output)
    def test_sd3_schedule_info(self):
        self.args.model_family = &quot;sd3&quot;
        output = sd3_schedule_info(self.args)
        self.assertIn(&quot;(no special parameters set)&quot;, output)
        self.args.flow_schedule_auto_shift = True
        output = sd3_schedule_info(self.args)
        self.assertIn(&quot;flow_schedule_auto_shift&quot;, output)
    def test_model_schedule_info(self):
        with patch(
            &quot;helpers.publishing.metadata.flux_schedule_info&quot;, return_value=&quot;flux info&quot;
        ):
            with patch(
                &quot;helpers.publishing.metadata.sd3_schedule_info&quot;, return_value=&quot;sd3 info&quot;
            ):
                self.args.model_family = &quot;flux&quot;
                output = model_schedule_info(self.args)
                self.assertEqual(output, &quot;flux info&quot;)
                self.args.model_family = &quot;sd3&quot;
                output = model_schedule_info(self.args)
                self.assertEqual(output, &quot;sd3 info&quot;)
    def test_save_model_card(self):
        # Mocking StateTracker methods
        self.args.model_family = &quot;flux&quot;
        self.args.model_type = &quot;lora&quot;
        self.args.lora_type = &quot;lycoris&quot;
        self.args.base_model_precision = &quot;int8-quanto&quot;
        with patch(
            &quot;helpers.publishing.metadata.StateTracker.get_model_family&quot;,
            return_value=&quot;sdxl&quot;,
        ):
            with patch(
                &quot;helpers.publishing.metadata.StateTracker.get_data_backends&quot;,
                return_value={},
            ):
                with patch(
                    &quot;helpers.publishing.metadata.StateTracker.get_epoch&quot;, return_value=1
                ):
                    with patch(
                        &quot;helpers.publishing.metadata.StateTracker.get_global_step&quot;,
                        return_value=1000,
                    ):
                        with patch(
                            &quot;helpers.publishing.metadata.StateTracker.get_weight_dtype&quot;,
                            return_value=torch.bfloat16,
                        ):
                            with patch(
                                &quot;helpers.publishing.metadata.StateTracker.get_accelerator&quot;,
                                return_value=MagicMock(num_processes=1),
                            ):
                                with patch(
                                    &quot;helpers.training.state_tracker.StateTracker.get_args&quot;,
                                    return_value=self.args,
                                ):
                                    with patch(
                                        &quot;builtins.open&quot;, unittest.mock.mock_open()
                                    ) as mock_file:
                                        save_model_card(
                                            repo_id=&quot;test-repo&quot;,
                                            images=None,
                                            base_model=&quot;test-base-model&quot;,
                                            train_text_encoder=True,
                                            prompt=&quot;Test prompt&quot;,
                                            validation_prompts=[&quot;Test prompt&quot;],
                                            validation_shortnames=[&quot;shortname&quot;],
                                            repo_folder=&quot;test-folder&quot;,
                                        )
                                        # Ensure the README.md was written
                                        mock_file.assert_called_with(
                                            os.path.join(&quot;test-folder&quot;, &quot;README.md&quot;),
                                            &quot;w&quot;,
                                            encoding=&quot;utf-8&quot;,
                                        )
    def test_adapter_download_fn(self):
        with patch(&quot;huggingface_hub.hf_hub_download&quot;, return_value=&quot;path/to/adapter&quot;):
            from helpers.publishing.metadata import lycoris_download_info
            output = lycoris_download_info()
            self.assertIn(&quot;hf_hub_download&quot;, output)
    def test_pipeline_move_full_bf16(self):
        from helpers.publishing.metadata import _pipeline_move_to
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_weight_dtype&quot;,
            return_value=torch.bfloat16,
        ):
            output = _pipeline_move_to(args=self.args)
        self.assertNotIn(&quot;torch.bfloat16&quot;, output)
    def test_pipeline_move_lycoris_bf16(self):
        from helpers.publishing.metadata import _pipeline_move_to
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_weight_dtype&quot;,
            return_value=torch.bfloat16,
        ):
            self.args.model_type = &quot;lora&quot;
            self.args.lora_type = &quot;lycoris&quot;
            self.args.base_model_precision = &quot;no_change&quot;
            output = _pipeline_move_to(args=self.args)
        self.assertNotIn(&quot;torch.bfloat16&quot;, output)
    def test_pipeline_move_lycoris_int8(self):
        from helpers.publishing.metadata import _pipeline_move_to
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_weight_dtype&quot;,
            return_value=torch.bfloat16,
        ):
            self.args.model_type = &quot;lora&quot;
            self.args.lora_type = &quot;lycoris&quot;
            self.args.base_model_precision = &quot;int8-quanto&quot;
            output = _pipeline_move_to(args=self.args)
        self.assertNotIn(&quot;torch.bfloat16&quot;, output)
    def test_pipeline_quanto_hint_unet(self):
        from helpers.publishing.metadata import _pipeline_quanto
        output = _pipeline_quanto(args=self.args)
        self.assertIn(&quot;quantize&quot;, output)
        self.assertIn(&quot;optimum.quanto&quot;, output)
        self.assertIn(&quot;pipeline.unet&quot;, output)
    def test_pipeline_quanto_hint_transformer(self):
        from helpers.publishing.metadata import _pipeline_quanto
        self.args.model_family = &quot;flux&quot;
        output = _pipeline_quanto(args=self.args)
        self.assertIn(&quot;quantize&quot;, output)
        self.assertIn(&quot;optimum.quanto&quot;, output)
        self.assertIn(&quot;pipeline.transformer&quot;, output)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_prompthandler.py">import unittest
import pandas as pd
from unittest.mock import patch, MagicMock
from helpers.prompts import (
    PromptHandler,
)
class TestPromptHandler(unittest.TestCase):
    def setUp(self):
        self.args = MagicMock()
        self.args.disable_compel = False
        self.text_encoders = [MagicMock(), MagicMock()]
        self.tokenizers = [MagicMock(), MagicMock()]
        self.accelerator = MagicMock()
        self.model_type = &quot;sdxl&quot;
        self.data_backend = MagicMock()
    @patch(&quot;helpers.training.state_tracker.StateTracker.get_parquet_database&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.get_data_backend&quot;)
    def test_prepare_instance_prompt_from_parquet(
        self, mock_get_data_backend, mock_get_parquet_database
    ):
        # Setup
        image_path = &quot;image_3.jpg&quot;
        use_captions = True
        prepend_instance_prompt = True
        data_backend = MagicMock()
        instance_prompt = &quot;Instance Prompt&quot;
        sampler_backend_id = &quot;sampler1&quot;
        filename_column = &quot;filename&quot;
        caption_column = &quot;caption&quot;
        mock_metadata_backend = MagicMock()
        mock_metadata_backend.caption_cache_entry = MagicMock()
        mock_metadata_backend.caption_cache_entry.return_value = (
            &quot;a giant arcade game type claw...&quot;
        )
        mock_get_data_backend.return_value = {
            &quot;metadata_backend&quot;: mock_metadata_backend,
        }
        # Simulate the DataFrame structure and the expected row
        fallback_caption_column = &quot;tags&quot;
        mock_df = pd.DataFrame(
            [
                {
                    filename_column: &quot;image_3&quot;,
                    caption_column: &quot;a giant arcade game type claw...&quot;,
                    fallback_caption_column: &quot;tags for image_3&quot;,
                }
            ]
        )
        # Configure the mock to return the simulated DataFrame
        mock_get_parquet_database.return_value = (
            mock_df,
            filename_column,
            caption_column,
            fallback_caption_column,
            False,
        )
        # Execute
        result_caption = PromptHandler.prepare_instance_prompt_from_parquet(
            image_path=image_path,
            use_captions=use_captions,
            prepend_instance_prompt=prepend_instance_prompt,
            data_backend=data_backend,
            instance_prompt=instance_prompt,
            sampler_backend_id=sampler_backend_id,
        )
        # Verify
        expected_caption = f&quot;{instance_prompt} a giant arcade game type claw...&quot;
        self.assertEqual(result_caption, expected_caption)
        mock_get_parquet_database.assert_called_once_with(sampler_backend_id)
    def test_raises_value_error_on_missing_sampler_backend_id(self):
        with self.assertRaises(ValueError):
            PromptHandler.prepare_instance_prompt_from_parquet(
                image_path=&quot;path/to/image.jpg&quot;,
                use_captions=True,
                prepend_instance_prompt=True,
                data_backend=MagicMock(),
                instance_prompt=&quot;Instance Prompt&quot;,
                sampler_backend_id=None,  # This should cause a ValueError
            )
    @patch(&quot;builtins.open&quot;)
    @patch(&quot;helpers.prompts.BaseDataBackend&quot;)
    def test_instance_prompt_prepended_textfile(self, mock_backend, open_mock):
        # Setup
        open_mock.return_value.__enter__.return_value.read.return_value = (
            &quot;Caption from filename&quot;
        )
        instance_prompt = &quot;Test Instance Prompt&quot;
        caption_from_file = &quot;Caption from file&quot;
        mock_backend.exists.return_value = True
        mock_backend.read.return_value = caption_from_file
        # Instantiate PromptHandler with mocked backend and check the result
        handler = PromptHandler(
            args=self.args,
            text_encoders=[&quot;LameTest&quot;],
            tokenizers=[&quot;LameTest&quot;],
            accelerator=MagicMock(),
        )
        result_caption = handler.magic_prompt(
            &quot;path/to/image.png&quot;,
            use_captions=True,
            caption_strategy=&quot;textfile&quot;,
            prepend_instance_prompt=True,
            data_backend=mock_backend,
            instance_prompt=instance_prompt,
        )
        # Verify
        expected_caption = f&quot;{instance_prompt} {caption_from_file}&quot;
        self.assertEqual(result_caption, expected_caption)
    def test_instance_prompt_prepended_filename(self):
        # Setup
        instance_prompt = &quot;Test Instance Prompt&quot;
        image_filename = &quot;image&quot;
        # Execute
        handler = PromptHandler(
            args=self.args,
            text_encoders=[&quot;LameTest&quot;],
            tokenizers=[&quot;LameTest&quot;],
            accelerator=MagicMock(),
        )
        result_caption = handler.magic_prompt(
            f&quot;path/to/{image_filename}.png&quot;,
            use_captions=True,
            caption_strategy=&quot;filename&quot;,
            prepend_instance_prompt=True,
            instance_prompt=instance_prompt,
            data_backend=MagicMock(),
        )
        # Verify
        expected_caption = f&quot;{instance_prompt} {image_filename}&quot;
        self.assertEqual(result_caption, expected_caption)
    @patch(&quot;helpers.prompts.PromptHandler.prepare_instance_prompt_from_filename&quot;)
    def test_prepare_instance_prompt_from_filename_called(self, mock_prepare):
        &quot;&quot;&quot;Ensure that prepare_instance_prompt_from_filename is called with correct arguments.&quot;&quot;&quot;
        # Setup
        image_path = &quot;path/to/image.png&quot;
        use_captions = True
        prepend_instance_prompt = True
        instance_prompt = &quot;test prompt&quot;
        # Execute
        prompt_handler = PromptHandler(
            self.args,
            self.text_encoders,
            self.tokenizers,
            self.accelerator,
            self.model_type,
        )
        result = prompt_handler.magic_prompt(
            image_path=image_path,
            use_captions=use_captions,
            caption_strategy=&quot;filename&quot;,
            prepend_instance_prompt=prepend_instance_prompt,
            data_backend=self.data_backend,
            instance_prompt=instance_prompt,
        )
        # Verify
        mock_prepare.assert_called_once_with(
            image_path=image_path,
            use_captions=use_captions,
            prepend_instance_prompt=prepend_instance_prompt,
            instance_prompt=instance_prompt,
        )
    @patch(&quot;helpers.prompts.PromptHandler.prepare_instance_prompt_from_textfile&quot;)
    def test_prepare_instance_prompt_from_textfile_called(self, mock_prepare):
        &quot;&quot;&quot;Ensure that prepare_instance_prompt_from_textfile is called when the caption_strategy is &apos;textfile&apos;.&quot;&quot;&quot;
        # Setup
        image_path = &quot;path/to/image.png&quot;
        use_captions = True
        prepend_instance_prompt = False
        instance_prompt = None
        caption_strategy = &quot;textfile&quot;
        # Execute
        prompt_handler = PromptHandler(
            self.args,
            self.text_encoders,
            self.tokenizers,
            self.accelerator,
            self.model_type,
        )
        result = prompt_handler.magic_prompt(
            image_path=image_path,
            use_captions=use_captions,
            caption_strategy=caption_strategy,
            prepend_instance_prompt=prepend_instance_prompt,
            instance_prompt=instance_prompt,
            data_backend=self.data_backend,
        )
        # Verify
        mock_prepare.assert_called_once_with(
            image_path,
            use_captions=use_captions,
            prepend_instance_prompt=prepend_instance_prompt,
            instance_prompt=instance_prompt,
            data_backend=self.data_backend,
        )
    def test_magic_prompt_raises_error_with_invalid_strategy(self):
        &quot;&quot;&quot;Ensure magic_prompt raises ValueError with an unsupported caption strategy.&quot;&quot;&quot;
        # Setup
        image_path = &quot;path/to/image.png&quot;
        use_captions = True
        prepend_instance_prompt = True
        caption_strategy = &quot;invalid_strategy&quot;
        prompt_handler = PromptHandler(
            self.args,
            self.text_encoders,
            self.tokenizers,
            self.accelerator,
            self.model_type,
        )
        # Verify
        with self.assertRaises(ValueError):
            prompt_handler.magic_prompt(
                image_path,
                use_captions,
                caption_strategy,
                prepend_instance_prompt,
                self.data_backend,
            )
    @patch(&quot;helpers.prompts.PromptHandler.filter_captions&quot;)
    def test_filter_captions_called(self, mock_filter):
        &quot;&quot;&quot;Ensure that filter_captions is called with the correct arguments.&quot;&quot;&quot;
        captions = [&quot;caption 1&quot;, &quot;caption 2&quot;]
        prompt_handler = PromptHandler(
            self.args,
            self.text_encoders,
            self.tokenizers,
            self.accelerator,
            self.model_type,
        )
        prompt_handler.filter_captions(self.data_backend, captions)
        # Verify
        mock_filter.assert_called_once_with(self.data_backend, captions)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_sampler.py">import unittest, os, logging
from math import ceil
from PIL import Image
from unittest import skip
from unittest.mock import Mock, MagicMock, patch
from helpers.multiaspect.sampler import MultiAspectSampler
from helpers.metadata.backends.discovery import DiscoveryMetadataBackend
from helpers.multiaspect.state import BucketStateManager
from tests.helpers.data import MockDataBackend
from accelerate import PartialState
from PIL import Image
class TestMultiAspectSampler(unittest.TestCase):
    def setUp(self):
        self.process_state = PartialState()
        self.accelerator = MagicMock()
        self.accelerator.log = MagicMock()
        self.metadata_backend = Mock(spec=DiscoveryMetadataBackend)
        self.metadata_backend.id = &quot;foo&quot;
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;, &quot;image3&quot;, &quot;image4&quot;],
        }
        self.metadata_backend.seen_images = {}
        self.data_backend = MockDataBackend()
        self.data_backend.id = &quot;foo&quot;
        self.batch_size = 2
        self.seen_images_path = &quot;/some/fake/seen_images.json&quot;
        self.state_path = &quot;/some/fake/state.json&quot;
        self.sampler = MultiAspectSampler(
            id=&quot;foo&quot;,
            metadata_backend=self.metadata_backend,
            data_backend=self.data_backend,
            accelerator=self.accelerator,
            batch_size=self.batch_size,
            minimum_image_size=0,
        )
        self.sampler.state_manager = Mock(spec=BucketStateManager)
        self.sampler.state_manager.load_state.return_value = {}
    def test_len(self):
        self.assertEqual(len(self.sampler), 2)
    def test_save_state(self):
        with patch.object(self.sampler.state_manager, &quot;save_state&quot;) as mock_save_state:
            self.sampler.save_state(self.state_path)
        mock_save_state.assert_called_once()
    def test_load_buckets(self):
        buckets = self.sampler.load_buckets()
        self.assertEqual(buckets, [&quot;1.0&quot;])
    def test_change_bucket(self):
        self.sampler.buckets = [&quot;1.5&quot;]
        self.sampler.exhausted_buckets = [&quot;1.0&quot;]
        self.sampler.change_bucket()
        self.assertEqual(self.sampler.current_bucket, 0)  # Should now point to &apos;1.5&apos;
    def test_move_to_exhausted(self):
        self.sampler.current_bucket = 0  # Pointing to &apos;1.0&apos;
        self.sampler.buckets = [&quot;1.0&quot;]
        self.sampler.change_bucket()
        self.sampler.move_to_exhausted()
        self.assertEqual(self.sampler.exhausted_buckets, [&quot;1.0&quot;])
        self.assertEqual(self.sampler.buckets, [])
    @skip(&quot;Infinite Loop Boulevard&quot;)
    def test_iter_yields_correct_batches(self):
        # Add about 100 images to the metadata_backend
        all_images = [&quot;image&quot; + str(i) for i in range(100)]
        self.metadata_backend.aspect_ratio_bucket_indices = {&quot;1.0&quot;: all_images}
        self.metadata_backend.buckets = [&quot;1.0&quot;]
        self.sampler._get_image_files = MagicMock(return_value=all_images)
        self.sampler._get_unseen_images = MagicMock(return_value=all_images)
        self.data_backend.exists = MagicMock(return_value=True)
        # Loop over __iter__ about 100 times:
        batches = []
        batch_size = 4
        for _ in range(ceil(len(all_images) / batch_size)):
            # extract batch_item from generator:
            with patch(
                &quot;PIL.Image.open&quot;, return_value=MagicMock(spec=Image.Image)
            ) as mock_image:
                logging.warning(&quot;mock_image: %s&quot;, mock_image)
                batch_item = next(self.sampler.__iter__())
            self.assertIn(batch_item, all_images)
            batches.append(batch_item)
        self.assertEqual(len(batches), len(all_images))
    @skip(&quot;Infinite Loop Boulevard&quot;)
    def test_iter_handles_small_images(self):
        # Mocking the _validate_and_yield_images_from_samples method to simulate small images
        def mock_validate_and_yield_images_from_samples(samples, bucket):
            # Simulate that &apos;image2&apos; is too small and thus not returned
            return [img for img in samples if img != &quot;image2&quot;]
        self.metadata_backend.aspect_ratio_bucket_indices = {
            &quot;1.0&quot;: [&quot;image1&quot;, &quot;image2&quot;, &quot;image3&quot;, &quot;image4&quot;]
        }
        self.sampler._validate_and_yield_images_from_samples = (
            mock_validate_and_yield_images_from_samples
        )
        batches = list(self.sampler)
        self.assertEqual(len(batches), 2)
        self.assertEqual(batches, [[&quot;image1&quot;, &quot;image3&quot;], [&quot;image4&quot;]])
    @skip(&quot;Currently broken test.&quot;)
    def test_iter_handles_incorrect_aspect_ratios_with_real_logic(self):
        # Create mock image files with different sizes using PIL
        img_paths = [
            &quot;/tmp/image1.jpg&quot;,
            &quot;/tmp/image2.jpg&quot;,
            &quot;/tmp/incorrect_image.jpg&quot;,
            &quot;/tmp/image4.jpg&quot;,
        ]
        img1 = Image.new(&quot;RGB&quot;, (100, 100), color=&quot;red&quot;)
        img1.save(img_paths[0])
        img2 = Image.new(&quot;RGB&quot;, (100, 100), color=&quot;green&quot;)
        img2.save(img_paths[1])
        img3 = Image.new(
            &quot;RGB&quot;, (50, 100), color=&quot;blue&quot;
        )  # This image has a different size
        img3.save(img_paths[2])
        img4 = Image.new(&quot;RGB&quot;, (100, 100), color=&quot;yellow&quot;)
        img4.save(img_paths[3])
        self.metadata_backend.aspect_ratio_bucket_indices = {&quot;1.0&quot;: img_paths}
        # Collect batches by iterating over the generator
        batches = [next(self.sampler.__iter__()) for _ in range(len(img_paths))]
        # Ensure that all batches have consistent image sizes
        # We retrieve the size using PIL for validation
        first_img_size = Image.open(batches[0]).size
        self.assertNotIn(img_paths[2], batches)
        self.assertTrue(
            all(Image.open(img_path).size == first_img_size for img_path in batches)
        )
        # Clean up the mock images
        for img_path in img_paths:
            os.remove(img_path)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_save_hooks.py">import unittest
from helpers.training.save_hooks import SaveHookManager
from accelerate import Accelerator
from transformers import CLIPTextModel, CLIPTextModelWithProjection
from diffusers import (
    UNet2DConditionModel,
    SD3Transformer2DModel,
    FluxTransformer2DModel,
)
from argparse import Namespace
from helpers.training.state_tracker import StateTracker
import logging
hf_logger = logging.getLogger(&quot;diffusers.models.modeling_utils&quot;)
hf_logger.disabled = True
class TestSaveHookManager(unittest.TestCase):
    def setUp(self):
        self.accelerator = Accelerator(device_placement=False)
    def _initialize_models(
        self, model_type, ckpt_id, unet=None, transformer=None, text_encoder_2=None
    ):
        self.ckpt_id = ckpt_id
        self.unet = unet
        self.transformer = transformer
        self.text_encoder_one = CLIPTextModel.from_pretrained(
            self.ckpt_id, subfolder=&quot;text_encoder&quot;
        )
        self.text_encoder_two = text_encoder_2
        args = Namespace(
            controlnet=None,
            sd3=False,
            flux=False,
            pixart_sigma=False,
            flux_attention_masked_training=False,
        )
        if model_type == &quot;sd3&quot;:
            args.model_family = &quot;sd3&quot;
        elif model_type == &quot;flux&quot;:
            args.model_family = &quot;flux&quot;
        self.args = args
        StateTracker.set_model_family(model_type)
    def _test_hook_manager(self, expected_denoiser_class, expected_pipeline_class):
        model_hooks = SaveHookManager(
            args=self.args,
            unet=self.unet,
            transformer=self.transformer,
            ema_model=None,
            accelerator=self.accelerator,
            text_encoder_1=self.text_encoder_one,
            text_encoder_2=self.text_encoder_two,
            use_deepspeed_optimizer=False,
        )
        self.assertIsNotNone(model_hooks)
        self.assertEqual(model_hooks.denoiser_class.__name__, expected_denoiser_class)
        self.assertEqual(model_hooks.pipeline_class.__name__, expected_pipeline_class)
    def test_hook_manager_sd(self):
        self._initialize_models(
            model_type=&quot;legacy&quot;,
            ckpt_id=&quot;hf-internal-testing/tiny-stable-diffusion-pipe&quot;,
            unet=UNet2DConditionModel.from_pretrained(
                &quot;hf-internal-testing/tiny-stable-diffusion-pipe&quot;, subfolder=&quot;unet&quot;
            ),
        )
        self._test_hook_manager(&quot;UNet2DConditionModel&quot;, &quot;StableDiffusionPipeline&quot;)
    def test_hook_manager_sdxl(self):
        self._initialize_models(
            model_type=&quot;sdxl&quot;,
            ckpt_id=&quot;hf-internal-testing/tiny-stable-diffusion-xl-pipe&quot;,
            unet=UNet2DConditionModel.from_pretrained(
                &quot;hf-internal-testing/tiny-stable-diffusion-xl-pipe&quot;, subfolder=&quot;unet&quot;
            ),
            text_encoder_2=CLIPTextModelWithProjection.from_pretrained(
                &quot;hf-internal-testing/tiny-stable-diffusion-xl-pipe&quot;,
                subfolder=&quot;text_encoder_2&quot;,
            ),
        )
        self._test_hook_manager(&quot;UNet2DConditionModel&quot;, &quot;StableDiffusionXLPipeline&quot;)
    def test_hook_manager_sd3(self):
        self._initialize_models(
            model_type=&quot;sd3&quot;,
            ckpt_id=&quot;hf-internal-testing/tiny-sd3-pipe&quot;,
            transformer=SD3Transformer2DModel.from_pretrained(
                &quot;hf-internal-testing/tiny-sd3-pipe&quot;, subfolder=&quot;transformer&quot;
            ),
            text_encoder_2=CLIPTextModelWithProjection.from_pretrained(
                &quot;hf-internal-testing/tiny-sd3-pipe&quot;, subfolder=&quot;text_encoder_2&quot;
            ),
        )
        self._test_hook_manager(&quot;SD3Transformer2DModel&quot;, &quot;StableDiffusion3Pipeline&quot;)
    def test_hook_manager_flux(self):
        self._initialize_models(
            model_type=&quot;flux&quot;,
            ckpt_id=&quot;hf-internal-testing/tiny-flux-pipe&quot;,
            transformer=FluxTransformer2DModel.from_pretrained(
                &quot;hf-internal-testing/tiny-flux-pipe&quot;, subfolder=&quot;transformer&quot;
            ),
        )
        self._test_hook_manager(&quot;FluxTransformer2DModel&quot;, &quot;FluxPipeline&quot;)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_state.py">import unittest
from helpers.multiaspect.state import BucketStateManager
class TestBucketStateManager(unittest.TestCase):
    def setUp(self):
        pass  # TODO: Add setup code if needed
    def test_example(self):
        # TODO: Write test cases
        self.assertEqual(True, True)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_trainer.py"># test_trainer.py
import unittest
from unittest.mock import Mock, patch, MagicMock
import torch, os
os.environ[&quot;SIMPLETUNER_LOG_LEVEL&quot;] = &quot;CRITICAL&quot;
from helpers.training.trainer import Trainer
class TestTrainer(unittest.TestCase):
    @patch(&quot;helpers.training.trainer.load_config&quot;)
    @patch(&quot;helpers.training.trainer.safety_check&quot;)
    @patch(
        &quot;helpers.training.trainer.load_scheduler_from_args&quot;,
        return_value=(Mock(), None, Mock()),
    )
    @patch(&quot;helpers.training.state_tracker.StateTracker&quot;)
    @patch(
        &quot;helpers.training.state_tracker.StateTracker.set_model_family&quot;,
        return_value=True,
    )
    @patch(&quot;torch.set_num_threads&quot;)
    @patch(&quot;helpers.training.trainer.Accelerator&quot;)
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    def test_config_to_obj(
        self,
        mock_misc_init,
        mock_parse_args,
        mock_accelerator,
        mock_set_num_threads,
        mock_set_model_family,
        mock_state_tracker,
        mock_load_scheduler_from_args,
        mock_safety_check,
        mock_load_config,
    ):
        trainer = Trainer()
        config_dict = {&quot;a&quot;: 1, &quot;b&quot;: 2}
        config_obj = trainer._config_to_obj(config_dict)
        self.assertEqual(config_obj.a, 1)
        self.assertEqual(config_obj.b, 2)
        config_none = trainer._config_to_obj(None)
        self.assertIsNone(config_none)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.set_seed&quot;)
    def test_init_seed_with_value(self, mock_set_seed, mock_parse_args, mock_misc_init):
        trainer = Trainer()
        trainer.config = Mock(seed=42, seed_for_each_device=False)
        trainer.init_seed()
        mock_set_seed.assert_called_with(42, False)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.set_seed&quot;)
    def test_init_seed_none(self, mock_set_seed, mock_parse_args, mock_misc_init):
        trainer = Trainer()
        trainer.config = Mock(seed=None, seed_for_each_device=False)
        trainer.init_seed()
        mock_set_seed.assert_not_called()
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;torch.cuda.is_available&quot;, return_value=True)
    @patch(&quot;torch.cuda.memory_allocated&quot;, return_value=1024**3)
    def test_stats_memory_used_cuda(
        self, mock_memory_allocated, mock_is_available, mock_parse_args, mock_misc_init
    ):
        trainer = Trainer()
        memory_used = trainer.stats_memory_used()
        self.assertEqual(memory_used, 1.0)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;torch.cuda.is_available&quot;, return_value=False)
    @patch(&quot;torch.backends.mps.is_available&quot;, return_value=True)
    @patch(&quot;torch.mps.current_allocated_memory&quot;, return_value=1024**3)
    def test_stats_memory_used_mps(
        self,
        mock_current_allocated_memory,
        mock_mps_is_available,
        mock_cuda_is_available,
        mock_parse_args,
        mock_misc_init,
    ):
        trainer = Trainer()
        memory_used = trainer.stats_memory_used()
        self.assertEqual(memory_used, 1.0)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;torch.cuda.is_available&quot;, return_value=False)
    @patch(&quot;torch.backends.mps.is_available&quot;, return_value=False)
    @patch(&quot;helpers.training.trainer.logger&quot;)
    def test_stats_memory_used_none(
        self,
        mock_logger,
        mock_mps_is_available,
        mock_cuda_is_available,
        mock_parse_args,
        mock_misc_init,
    ):
        trainer = Trainer()
        memory_used = trainer.stats_memory_used()
        self.assertEqual(memory_used, 0)
        mock_logger.warning.assert_called_with(
            &quot;CUDA, ROCm, or Apple MPS not detected here. We cannot report VRAM reductions.&quot;
        )
    @patch(&quot;torch.set_num_threads&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.set_global_step&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.set_args&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.set_weight_dtype&quot;)
    @patch(&quot;helpers.training.trainer.Trainer.set_model_family&quot;)
    @patch(&quot;helpers.training.trainer.Trainer.init_noise_schedule&quot;)
    @patch(
        &quot;accelerate.accelerator.Accelerator&quot;,
        return_value=Mock(device=Mock(type=&quot;cuda&quot;)),
    )
    @patch(&quot;accelerate.state.AcceleratorState&quot;, Mock())
    @patch(
        &quot;argparse.ArgumentParser.parse_args&quot;,
        return_value=Mock(
            torch_num_threads=2,
            train_batch_size=1,
            weight_dtype=torch.float32,
            model_type=&quot;full&quot;,
            optimizer=&quot;adamw_bf16&quot;,
            optimizer_config=None,
            max_train_steps=2,
            num_train_epochs=0,
            timestep_bias_portion=0,
            metadata_update_interval=100,
            gradient_accumulation_steps=1,
            validation_resolution=1024,
            mixed_precision=&quot;bf16&quot;,
            report_to=&quot;none&quot;,
            output_dir=&quot;output_dir&quot;,
            logging_dir=&quot;logging_dir&quot;,
            learning_rate=1,
            flow_schedule_shift=3,
            user_prompt_library=None,
            flow_schedule_auto_shift=False,
            validation_guidance_skip_layers=None,
            pretrained_model_name_or_path=&quot;some/path&quot;,
            base_model_precision=&quot;no_change&quot;,
            gradient_checkpointing_interval=None,
            # deprecated options
            flux_beta_schedule_alpha=None,
            flux_beta_schedule_beta=None,
            flux_use_beta_schedule=None,
            flux_use_uniform_schedule=None,
            flux_schedule_shift=None,
            flux_schedule_auto_shift=None,
            flow_matching_sigmoid_scale=None,
            eval_steps_interval=None,
            controlnet=False,
        ),
    )
    def test_misc_init(
        self,
        mock_argparse,
        # mock_accelerator_state,
        mock_accelerator,
        mock_init_noise_schedule,
        mock_set_model_family,
        mock_set_weight_dtype,
        mock_set_args,
        mock_set_global_step,
        mock_set_num_threads,
    ):
        trainer = Trainer(disable_accelerator=True)
        trainer._misc_init()
        mock_set_num_threads.assert_called_with(2)
        self.assertEqual(
            trainer.state,
            {&quot;lr&quot;: 0.0, &quot;global_step&quot;: 0, &quot;global_resume_step&quot;: 0, &quot;first_epoch&quot;: 1},
        )
        self.assertEqual(trainer.timesteps_buffer, [])
        self.assertEqual(trainer.guidance_values_list, [])
        self.assertEqual(trainer.train_loss, 0.0)
        self.assertIsNone(trainer.bf)
        self.assertIsNone(trainer.grad_norm)
        self.assertEqual(trainer.extra_lr_scheduler_kwargs, {})
        mock_set_global_step.assert_called_with(0)
        mock_set_args.assert_called_with(trainer.config)
        mock_set_weight_dtype.assert_called_with(trainer.config.weight_dtype)
        mock_set_model_family.assert_called()
        mock_init_noise_schedule.assert_called()
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(
        &quot;helpers.training.trainer.load_scheduler_from_args&quot;,
        return_value=(Mock(), &quot;flow_matching_value&quot;, &quot;noise_scheduler_value&quot;),
    )
    def test_init_noise_schedule(
        self, mock_load_scheduler_from_args, mock_parse_args, mock_misc_init
    ):
        trainer = Trainer()
        trainer.config = Mock()
        trainer.init_noise_schedule()
        self.assertEqual(trainer.config.flow_matching, &quot;flow_matching_value&quot;)
        self.assertEqual(trainer.noise_scheduler, &quot;noise_scheduler_value&quot;)
        self.assertEqual(trainer.lr, 0.0)
    @patch(&quot;helpers.training.trainer.logger&quot;)
    @patch(
        &quot;helpers.training.trainer.model_classes&quot;, {&quot;full&quot;: [&quot;sdxl&quot;, &quot;sd3&quot;, &quot;legacy&quot;]}
    )
    @patch(
        &quot;helpers.training.trainer.model_labels&quot;,
        {&quot;sdxl&quot;: &quot;SDXL&quot;, &quot;sd3&quot;: &quot;SD3&quot;, &quot;legacy&quot;: &quot;Legacy&quot;},
    )
    @patch(&quot;helpers.training.state_tracker.StateTracker&quot;)
    def test_set_model_family_default(self, mock_state_tracker, mock_logger):
        with patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;):
            with patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;):
                trainer = Trainer()
        trainer.config = Mock(model_family=None)
        trainer.config.pretrained_model_name_or_path = &quot;some/path&quot;
        trainer.config.pretrained_vae_model_name_or_path = None
        trainer.config.vae_path = None
        trainer.config.text_encoder_path = None
        trainer.config.text_encoder_subfolder = None
        trainer.config.model_family = &quot;sdxl&quot;
        with patch.object(trainer, &quot;_set_model_paths&quot;) as mock_set_model_paths:
            with patch(
                &quot;helpers.training.state_tracker.StateTracker.is_sdxl_refiner&quot;,
                return_value=False,
            ):
                trainer.set_model_family()
                self.assertEqual(trainer.config.model_type_label, &quot;SDXL&quot;)
                mock_logger.warning.assert_not_called()
                mock_set_model_paths.assert_called()
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    def test_set_model_family_invalid(self, mock_parse_args, mock_misc_init):
        trainer = Trainer()
        trainer.config = Mock(model_family=&quot;invalid_model_family&quot;)
        trainer.config.pretrained_model_name_or_path = &quot;some/path&quot;
        with self.assertRaises(ValueError) as context:
            trainer.set_model_family()
        self.assertIn(
            &quot;Invalid model family specified: invalid_model_family&quot;,
            str(context.exception),
        )
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.logger&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker&quot;)
    def test_epoch_rollover(
        self, mock_state_tracker, mock_logger, mock_parse_args, mock_misc_init
    ):
        trainer = Trainer()
        trainer.state = {&quot;first_epoch&quot;: 1, &quot;current_epoch&quot;: 1}
        trainer.config = Mock(
            num_train_epochs=5,
            aspect_bucket_disable_rebuild=False,
            lr_scheduler=&quot;cosine_with_restarts&quot;,
        )
        trainer.extra_lr_scheduler_kwargs = {}
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_data_backends&quot;,
            return_value={},
        ):
            trainer._epoch_rollover(2)
            self.assertEqual(trainer.state[&quot;current_epoch&quot;], 2)
            self.assertEqual(trainer.extra_lr_scheduler_kwargs[&quot;epoch&quot;], 2)
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    def test_epoch_rollover_same_epoch(self, mock_misc_init, mock_parse_args):
        trainer = Trainer(
            config={
                &quot;--num_train_epochs&quot;: 0,
                &quot;--model_family&quot;: &quot;pixart_sigma&quot;,
                &quot;--optimizer&quot;: &quot;adamw_bf16&quot;,
                &quot;--pretrained_model_name_or_path&quot;: &quot;some/path&quot;,
            }
        )
        trainer.state = {&quot;first_epoch&quot;: 1, &quot;current_epoch&quot;: 1}
        trainer._epoch_rollover(1)
        self.assertEqual(trainer.state[&quot;current_epoch&quot;], 1)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.os.makedirs&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.delete_cache_files&quot;)
    def test_init_clear_backend_cache_preserve(
        self, mock_delete_cache_files, mock_makedirs, mock_parse_args, mock_misc_init
    ):
        trainer = Trainer()
        trainer.config = Mock(
            output_dir=&quot;/path/to/output&quot;, preserve_data_backend_cache=True
        )
        trainer.init_clear_backend_cache()
        mock_makedirs.assert_called_with(&quot;/path/to/output&quot;, exist_ok=True)
        mock_delete_cache_files.assert_not_called()
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.os.makedirs&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker.delete_cache_files&quot;)
    def test_init_clear_backend_cache_delete(
        self, mock_delete_cache_files, mock_makedirs, mock_parse_args, mock_misc_init
    ):
        trainer = Trainer()
        trainer.config = Mock(
            output_dir=&quot;/path/to/output&quot;, preserve_data_backend_cache=False
        )
        trainer.init_clear_backend_cache()
        mock_makedirs.assert_called_with(&quot;/path/to/output&quot;, exist_ok=True)
        mock_delete_cache_files.assert_called_with(preserve_data_backend_cache=False)
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.huggingface_hub&quot;)
    @patch(&quot;helpers.training.trainer.HubManager&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker&quot;)
    @patch(&quot;accelerate.logging.MultiProcessAdapter.log&quot;)
    def test_init_huggingface_hub(
        self,
        mock_logger,
        mock_state_tracker,
        mock_hub_manager_class,
        mock_hf_hub,
        mock_parse_args,
        mock_misc_init,
    ):
        trainer = Trainer()
        trainer.config = Mock(push_to_hub=True, huggingface_token=&quot;fake_token&quot;)
        trainer.accelerator = Mock(is_main_process=True)
        mock_hf_hub.whoami = Mock(return_value={&quot;id&quot;: &quot;fake_id&quot;, &quot;name&quot;: &quot;foobar&quot;})
        trainer.init_huggingface_hub(access_token=&quot;fake_token&quot;)
        mock_hf_hub.login.assert_called_with(token=&quot;fake_token&quot;)
        mock_hub_manager_class.assert_called_with(config=trainer.config)
        mock_hf_hub.whoami.assert_called()
    @patch(&quot;helpers.training.trainer.Trainer._misc_init&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.Trainer.parse_arguments&quot;, return_value=Mock())
    @patch(&quot;helpers.training.trainer.logger&quot;)
    @patch(&quot;helpers.training.trainer.os.path.basename&quot;, return_value=&quot;checkpoint-100&quot;)
    @patch(
        &quot;helpers.training.trainer.os.listdir&quot;,
        return_value=[&quot;checkpoint-100&quot;, &quot;checkpoint-200&quot;],
    )
    @patch(
        &quot;helpers.training.trainer.os.path.join&quot;,
        side_effect=lambda *args: &quot;/&quot;.join(args),
    )
    @patch(&quot;helpers.training.trainer.os.path.exists&quot;, return_value=True)
    @patch(&quot;helpers.training.trainer.Accelerator&quot;)
    @patch(&quot;helpers.training.state_tracker.StateTracker&quot;)
    def test_init_resume_checkpoint(
        self,
        mock_state_tracker,
        mock_accelerator_class,
        mock_path_exists,
        mock_path_join,
        mock_os_listdir,
        mock_path_basename,
        mock_logger,
        mock_parse_args,
        mock_misc_init,
    ):
        trainer = Trainer()
        trainer.config = Mock(
            output_dir=&quot;/path/to/output&quot;,
            resume_from_checkpoint=&quot;latest&quot;,
            total_steps_remaining_at_start=100,
            global_resume_step=1,
            num_train_epochs=0,
            max_train_steps=100,
        )
        trainer.accelerator = Mock(num_processes=1)
        trainer.state = {&quot;global_step&quot;: 0, &quot;first_epoch&quot;: 1, &quot;current_epoch&quot;: 1}
        trainer.optimizer = Mock()
        trainer.config.lr_scheduler = &quot;constant&quot;
        trainer.config.learning_rate = 0.001
        trainer.config.is_schedulefree = False
        trainer.config.overrode_max_train_steps = False
        # Mock lr_scheduler
        lr_scheduler = Mock()
        lr_scheduler.state_dict.return_value = {&quot;base_lrs&quot;: [0.1], &quot;_last_lr&quot;: [0.1]}
        with patch(
            &quot;helpers.training.state_tracker.StateTracker.get_data_backends&quot;,
            return_value={},
        ):
            with patch(
                &quot;helpers.training.state_tracker.StateTracker.get_global_step&quot;,
                return_value=100,
            ):
                trainer.init_resume_checkpoint(lr_scheduler=lr_scheduler)
                mock_logger.info.assert_called()
                trainer.accelerator.load_state.assert_called_with(
                    &quot;/path/to/output/checkpoint-200&quot;
                )
    # Additional tests can be added for other methods as needed
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_training_sample.py">import unittest
from PIL import Image
import numpy as np
from helpers.image_manipulation.training_sample import TrainingSample
from helpers.training.state_tracker import StateTracker
from unittest.mock import MagicMock
class TestTrainingSample(unittest.TestCase):
    def setUp(self):
        # Create a simple image for testing
        self.image = Image.new(&quot;RGB&quot;, (1024, 768), &quot;white&quot;)
        self.data_backend_id = &quot;test_backend&quot;
        self.image_metadata = {&quot;original_size&quot;: (1024, 768)}
        # Assume StateTracker and other helpers are correctly set up to return meaningful values
        StateTracker.set_args(
            MagicMock(aspect_bucket_alignment=64, aspect_bucket_rounding=2)
        )
        StateTracker.get_data_backend_config = MagicMock(
            return_value={
                &quot;crop&quot;: True,
                &quot;crop_style&quot;: &quot;center&quot;,
                &quot;crop_aspect&quot;: &quot;square&quot;,
                &quot;resolution&quot;: 512,
                &quot;resolution_type&quot;: &quot;pixel&quot;,
                &quot;target_downsample_size&quot;: 768,
                &quot;maximum_image_size&quot;: 1024,
                &quot;aspect_bucket_alignment&quot;: 8,
            }
        )
        self.default_config = {
            &quot;crop&quot;: False,
            &quot;crop_style&quot;: &quot;center&quot;,
            &quot;crop_aspect&quot;: &quot;square&quot;,
            &quot;resolution&quot;: 512,
            &quot;resolution_type&quot;: &quot;pixel&quot;,
            &quot;target_downsample_size&quot;: None,
            &quot;maximum_image_size&quot;: None,
            &quot;aspect_bucket_alignment&quot;: 64,
        }
        # Basic 1024×768 test image
        self.test_image = Image.new(&quot;RGB&quot;, (1024, 768), &quot;white&quot;)
        self.test_metadata = {&quot;original_size&quot;: (1024, 768)}
        # Make sure to isolate your test config from others
        self.original_get_data_backend_config = StateTracker.get_data_backend_config
        StateTracker.get_data_backend_config = lambda x: self.default_config
    def test_image_initialization(self):
        &quot;&quot;&quot;Test that the image is correctly initialized and converted.&quot;&quot;&quot;
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        self.assertEqual(sample.original_size, (1024, 768))
    def test_image_downsample(self):
        &quot;&quot;&quot;Test that downsampling is correctly applied before cropping.&quot;&quot;&quot;
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        self.assertEqual(
            sample.current_size, (1024, 768), &quot;Size was not correct before prepare.&quot;
        )
        sample.prepare()
        self.assertEqual(
            sample.image.size,
            sample.current_size,
            f&quot;Sample current_size was not updated? {sample.__dict__}&quot;,
        )
        self.assertEqual(
            sample.image.size,
            sample.target_size,
            f&quot;Sample target size did not get reached by the image size. {sample.__dict__}&quot;,
        )
    def test_no_crop(self):
        &quot;&quot;&quot;Test handling when cropping is disabled.&quot;&quot;&quot;
        StateTracker.get_data_backend_config = lambda x: {
            &quot;crop&quot;: False,
            &quot;crop_style&quot;: &quot;random&quot;,
            &quot;resolution&quot;: 512,
            &quot;resolution_type&quot;: &quot;pixel&quot;,
        }
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        original_size = sample.image.size
        sample.prepare()
        self.assertNotEqual(sample.image.size, original_size)  # Ensure resizing occurs
    def test_crop_coordinates(self):
        &quot;&quot;&quot;Test that cropping returns correct coordinates.&quot;&quot;&quot;
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        sample.prepare()
        self.assertIsNotNone(sample.crop_coordinates)  # Crop coordinates should be set
    def test_aspect_ratio_square_up(self):
        &quot;&quot;&quot;Test that the aspect ratio is preserved after processing.&quot;&quot;&quot;
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        original_aspect = round(sample.original_size[0] / sample.original_size[1], 2)
        sample.prepare()
        processed_aspect = round(sample.image.size[0] / sample.image.size[1], 2)
        self.assertEqual(
            processed_aspect, 1.38
        )  # when 64px divisible, we&apos;re at 1.38 now.
    def test_return_tensor(self):
        &quot;&quot;&quot;Test tensor conversion if requested.&quot;&quot;&quot;
        sample = TrainingSample(self.image, self.data_backend_id, self.image_metadata)
        prepared_sample = sample.prepare(return_tensor=True)
        # Check if returned object is a tensor (mock or check type if actual tensor transformation is applied)
        self.assertTrue(
            isinstance(prepared_sample.aspect_ratio, float)
        )  # Placeholder check
    # -----------------------
    # New Tests for Video Data
    # -----------------------
    def test_video_initialization_4d(self):
        &quot;&quot;&quot;
        Test that a 4D NumPy array (frames, height, width, channels)
        is recognized and processed similarly to images.
        &quot;&quot;&quot;
        # Create a dummy &quot;video&quot; with shape [frames, H, W, C] = [10, 720, 1280, 3]
        video_data = np.zeros((10, 720, 1280, 3), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (1280, 720)}
        sample = TrainingSample(video_data, self.data_backend_id, video_metadata)
        self.assertEqual(sample.original_size, (1280, 720))
        # Confirm it doesn&apos;t crash
        sample.prepare()
        # After crop to square=512, we might see shape [frames, 512, 512, 3] or smaller
        self.assertTrue(isinstance(sample.image, np.ndarray))
        self.assertTrue(sample.image.shape[-1] == 3)  # last dim still color channels
    def test_video_initialization_5d_fails(self):
        &quot;&quot;&quot;
        Test that a 5D NumPy array (batch, frames, channels, height, width)
        fails since it is invalid.
        &quot;&quot;&quot;
        # Create a dummy &quot;video&quot; with shape [B, F, C, H, W] = [2, 10, 3, 720, 1280]
        video_data = np.zeros((2, 10, 3, 720, 1280), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (1280, 720)}
        with self.assertRaises(ValueError):
            TrainingSample(video_data, self.data_backend_id, video_metadata)
    def test_video_square_crop(self):
        &quot;&quot;&quot;
        Test that a &apos;square&apos; aspect ratio truly yields a square shape for 4D video data.
        &quot;&quot;&quot;
        # Create dummy video: [frames=5, H=600, W=800, C=3]
        video_data = np.zeros((5, 1024, 1024, 3), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (1024, 1024)}
        sample = TrainingSample(video_data, self.data_backend_id, video_metadata)
        sample.prepare()
        # The shape should reflect a final square dimension &lt;= 512
        final_shape = sample.image.shape
        # E.g. [5, newH, newW, 3]
        self.assertEqual(final_shape[-1], 3)
        self.assertEqual(
            final_shape[1], final_shape[2], &quot;Video should be square in H/W&quot;
        )
    def test_video_random_crop(self):
        &quot;&quot;&quot;
        Test that random cropping works for 4D video data.
        &quot;&quot;&quot;
        # Overwrite config to use random cropping
        StateTracker.get_data_backend_config = MagicMock(
            return_value={
                &quot;crop&quot;: True,
                &quot;crop_style&quot;: &quot;random&quot;,
                &quot;crop_aspect&quot;: &quot;square&quot;,
                &quot;resolution&quot;: 256,  # smaller for quick test
                &quot;resolution_type&quot;: &quot;pixel&quot;,
            }
        )
        # shape [frames=3, H=300, W=400, C=3]
        video_data = np.ones((3, 300, 400, 3), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (400, 300)}
        sample = TrainingSample(video_data, self.data_backend_id, video_metadata)
        sample.prepare()
        # The final shape should be [3, 256, 256, 3] or smaller
        self.assertEqual(sample.image.shape[0], 3)
        self.assertEqual(sample.image.shape[-1], 3)
        self.assertTrue(sample.image.shape[1] == sample.image.shape[2])
    def test_video_no_crop(self):
        &quot;&quot;&quot;
        Ensure that when crop=False, a video is simply resized or left alone,
        but does not do a random or center crop.
        &quot;&quot;&quot;
        # Overwrite config to disable cropping
        StateTracker.get_data_backend_config = MagicMock(
            return_value={
                &quot;crop&quot;: False,
                &quot;crop_style&quot;: &quot;center&quot;,
                &quot;resolution&quot;: 128,
                &quot;resolution_type&quot;: &quot;pixel&quot;,
            }
        )
        video_data = np.zeros((4, 240, 320, 3), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (320, 240)}
        sample = TrainingSample(video_data, self.data_backend_id, video_metadata)
        sample.prepare()
        # Without crop, the pipeline might just do a direct resize to e.g. 128 px on the shorter edge
        final_shape = sample.image.shape
        self.assertEqual(final_shape[0], 4)  # frames unchanged
        self.assertTrue(final_shape[1] &lt;= 128 or final_shape[2] &lt;= 128)
        # or whatever your code does if it sees crop=False
    def test_no_crop_preserves_aspect_ratio_if_not_forced(self):
        &quot;&quot;&quot;
        If `crop=False` and we do not forcibly set `crop_aspect=&apos;square&apos;` in the code,
        the result should keep the original aspect ratio or a uniform scale.
        &quot;&quot;&quot;
        self.default_config[&quot;crop&quot;] = False
        # To avoid forcing squares, let&apos;s not set &apos;crop_aspect&apos; to &apos;square&apos;
        self.default_config[&quot;crop_aspect&quot;] = (
            &quot;preserve&quot;  # or something that your code interprets as no forced square
        )
        self.default_config[&quot;resolution&quot;] = 256
        sample = TrainingSample(
            self.test_image,
            data_backend_id=self.data_backend_id,
            image_metadata=self.test_metadata,
        )
        sample.prepare()
        final_w, final_h = sample.image.size
        final_aspect = sample.aspect_ratio
        original_aspect = round(1024 / 768, 3)
        self.assertNotEqual(
            final_aspect,
            original_aspect,
        )
        self.assertNotEqual(
            final_aspect,
            1.0,
        )
        # Confirm we did scale down to 256 or smaller on at least one dimension
        self.assertTrue(
            final_w &lt;= 256 or final_h &lt;= 256,
            f&quot;Expected at least one dimension to be ≤ 256, but got ({final_w}, {final_h})&quot;,
        )
    def test_no_crop_forced_square_is_skipped(self):
        &quot;&quot;&quot;
        Even if &apos;crop_aspect&apos; is set to &apos;square&apos;, if crop=False, we expect the code
        to skip any forced square logic and preserve aspect ratio.
        &quot;&quot;&quot;
        self.default_config[&quot;crop&quot;] = False
        self.default_config[&quot;crop_aspect&quot;] = &quot;square&quot;
        self.default_config[&quot;resolution&quot;] = 512
        sample = TrainingSample(
            self.test_image,
            data_backend_id=self.data_backend_id,
            image_metadata=self.test_metadata,
        )
        sample.prepare()
        final_w, final_h = sample.image.size
        self.assertNotEqual(
            final_w,
            final_h,
            &quot;No-crop scenario should not force a square shape if we truly skip crop logic.&quot;,
        )
    def test_no_crop_64px_alignment(self):
        &quot;&quot;&quot;
        If we want to enforce that final dimensions are multiples of 64,
        test that the code does so without squishing.
        &quot;&quot;&quot;
        # Suppose we have logic that snaps to multiples of 64.
        self.default_config[&quot;crop&quot;] = False
        self.default_config[&quot;resolution&quot;] = (
            999  # something that won&apos;t be a multiple of 64
        )
        # We&apos;ll pretend there&apos;s some internal code that rounds final sizes to multiples of 64.
        sample = TrainingSample(
            self.test_image,
            data_backend_id=self.data_backend_id,
            image_metadata=self.test_metadata,
        )
        sample.prepare()
        final_w, final_h = sample.image.size
        # Check the code hasn&apos;t forced a square
        self.assertNotEqual(
            final_w,
            final_h,
            &quot;64px alignment should preserve aspect ratio unless the original was square.&quot;,
        )
        # Check multiples of 64
        self.assertEqual(
            final_w % 64, 0, f&quot;Expected width to be multiple of 64, got {final_w}&quot;
        )
        self.assertTrue(
            final_h % 64
            in [
                0,
                64,
            ],  # or 0, if your code also strictly enforces height to multiple of 64
            f&quot;Expected height to be multiple of 64, got {final_h}&quot;,
        )
    def test_no_crop_video_preserves_frames_and_aspect(self):
        &quot;&quot;&quot;
        For a video in 4D shape, ensure that no-crop scenario
        preserves original ratio (or a uniform scale) and the same frame count.
        &quot;&quot;&quot;
        self.default_config[&quot;crop&quot;] = False
        self.default_config[&quot;resolution&quot;] = 400
        # Dummy video: shape [frames=5, H=600, W=1200, C=3] =&gt; aspect ~2.0
        video_data = np.zeros((5, 600, 1200, 3), dtype=np.uint8)
        video_metadata = {&quot;original_size&quot;: (1200, 600)}
        sample = TrainingSample(video_data, self.data_backend_id, video_metadata)
        sample.prepare()
        final_frames, final_h, final_w, final_c = sample.image.shape
        self.assertEqual(
            final_frames, 5, &quot;Should preserve frame count in no-crop scenario.&quot;
        )
        self.assertEqual(final_c, 3, &quot;Color channels should remain 3.&quot;)
        final_aspect = round(final_w / final_h, 2)
        self.assertAlmostEqual(
            final_aspect,
            2.0,
            places=1,
            msg=f&quot;Should preserve ~2:1 ratio: {sample.__dict__}&quot;,
        )
        # Also confirm we scaled down
        self.assertTrue(
            final_w &lt;= 400 or final_h &lt;= 400,
            f&quot;Expected at least one dimension to be ≤ 400, got {final_w}x{final_h}&quot;,
        )
    def test_no_crop_too_big_image_downscale_only(self):
        &quot;&quot;&quot;
        If the original image is bigger than &apos;resolution&apos;, we only downscale,
        no cropping, preserving aspect.
        &quot;&quot;&quot;
        self.default_config[&quot;crop&quot;] = False
        self.default_config[&quot;resolution&quot;] = 256
        # 2000×1500 =&gt; aspect ~1.333
        huge_img = Image.new(&quot;RGB&quot;, (2000, 1500), &quot;white&quot;)
        sample = TrainingSample(
            huge_img, self.data_backend_id, {&quot;original_size&quot;: (2000, 1500)}
        )
        sample.prepare()
        final_w, final_h = sample.image.size
        self.assertTrue(
            final_w &lt;= 256 or final_h &lt;= 256,
            f&quot;Should be scaled down near 256 max, got {final_w}x{final_h}&quot;,
        )
        # Check aspect ratio
        expected_ratio = round(final_w / final_h, 2)
        actual_ratio = sample.aspect_ratio
        self.assertEqual(
            expected_ratio,
            actual_ratio,
            f&quot;Aspect ratio must be preserved for no-crop: {sample.__dict__}&quot;,
        )
        self.assertNotEqual(
            sample.aspect_ratio, 1.0, &quot;Should not force a square shape in this case.&quot;
        )
    def test_no_crop_small_image_upscale_only_if_configured(self):
        &quot;&quot;&quot;
        If the original image is smaller than resolution, decide if we actually upscale
        or leave it alone (depending on your logic). We can test either outcome.
        &quot;&quot;&quot;
        self.default_config[&quot;crop&quot;] = False
        self.default_config[&quot;resolution&quot;] = 512
        # 128×96 =&gt; aspect ~1.333
        small_img = Image.new(&quot;RGB&quot;, (128, 96), &quot;white&quot;)
        sample = TrainingSample(
            small_img, self.data_backend_id, {&quot;original_size&quot;: (128, 96)}
        )
        sample.prepare()
        final_w, final_h = sample.image.size
        # If your code does not upscale, we&apos;d expect still 128×96.
        # If your code upscales to 512 on one side, we&apos;d expect e.g. 512×384.
        # Let&apos;s assume we allow upscaling to 512.
        self.assertTrue(
            final_w &gt;= 128 and final_h &gt;= 96,
            &quot;Should have upscaled the small image in no-crop mode.&quot;,
        )
        # Check ratio is still ~1.333
        expected_ratio = round(final_w / final_h, 2)
        actual_ratio = sample.aspect_ratio
        self.assertEqual(
            expected_ratio,
            actual_ratio,
            f&quot;Aspect ratio must be preserved for no-crop upscaling.&quot;,
        )
        self.assertNotEqual(
            sample.aspect_ratio, 1.0, &quot;Should not force a square shape in this case.&quot;
        )
# Helper mock classes and functions
class MockCropper:
    def __init__(self, image, image_metadata):
        self.image = image
        self.image_metadata = image_metadata
    def crop(self, width, height):
        return self.image.crop((0, 0, width, height)), (0, 0, width, height)
def mock_resize_helper(aspect_ratio, resolution):
    # Simulates resizing logic
    width, height = resolution, int(resolution / aspect_ratio)
    return width, height, aspect_ratio
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_vae.py">from hashlib import sha256
from helpers.caching.vae import VAECache
import unittest
from PIL import Image
import numpy as np
from helpers.image_manipulation.training_sample import TrainingSample
from helpers.training.state_tracker import StateTracker
from unittest.mock import MagicMock
class TestVaeCache(unittest.TestCase):
    def test_filename_mapping(self):
        # Test cases
        test_cases = [
            # 0 Filepath ends with .pt (no change expected in the path)
            {&quot;image_path&quot;: &quot;/data/image1.pt&quot;, &quot;cache_path&quot;: &quot;/data/image1.pt&quot;},
            # 1 Normal filepath
            {&quot;image_path&quot;: &quot;/data/image1.png&quot;, &quot;cache_path&quot;: &quot;cache/image1.pt&quot;},
            # 2, 3 Nested subdirectories
            {
                &quot;image_path&quot;: &quot;/data/subdir1/subdir2/image2.jpg&quot;,
                &quot;cache_path&quot;: &quot;cache/subdir1/subdir2/image2.pt&quot;,
            },
            {
                &quot;image_path&quot;: &quot;data/subdir1/subdir2/image2.jpg&quot;,
                &quot;cache_path&quot;: &quot;cache/subdir1/subdir2/image2.pt&quot;,
                &quot;instance_dir&quot;: &quot;data&quot;,
            },
            # 4 No instance_data_dir, direct cache dir placement
            {
                &quot;image_path&quot;: &quot;/anotherdir/image3.png&quot;,
                &quot;cache_path&quot;: &quot;cache/image3.pt&quot;,
                &quot;instance_dir&quot;: None,
            },
            # 5 Instance data directory is None
            {
                &quot;image_path&quot;: &quot;/data/image4.png&quot;,
                &quot;cache_path&quot;: &quot;cache/image4.pt&quot;,
                &quot;instance_dir&quot;: None,
            },
            # 6 Filepath in root directory
            {&quot;image_path&quot;: &quot;/image5.png&quot;, &quot;cache_path&quot;: &quot;cache/image5.pt&quot;},
            # 7 Hash filenames enabled
            {
                &quot;image_path&quot;: &quot;/data/image6.png&quot;,
                &quot;cache_path&quot;: &quot;cache/&quot; + sha256(&quot;image6&quot;.encode()).hexdigest() + &quot;.pt&quot;,
                &quot;should_hash&quot;: True,
            },
            # 8 Invalid cache_dir
            {&quot;image_path&quot;: &quot;/data/image7.png&quot;, &quot;cache_path&quot;: &quot;cache/image7.pt&quot;},
        ]
        # Running test cases
        for i, test_case in enumerate(test_cases, 1):
            filepath = test_case[&quot;image_path&quot;]
            # expected = os.path.abspath(test_case[&apos;cache_path&apos;])
            expected = test_case[&quot;cache_path&quot;]
            cache_dir = test_case.get(&quot;cache_dir&quot;, &quot;cache&quot;)
            instance_dir = test_case.get(&quot;instance_dir&quot;, &quot;/data&quot;)
            should_hash = test_case.get(&quot;should_hash&quot;, False)
            vae_cache = VAECache(
                id=&quot;test-cache&quot;,
                vae=None,
                accelerator=None,
                metadata_backend=None,
                image_data_backend=None,
                hash_filenames=should_hash,
                instance_data_dir=instance_dir,
                cache_dir=cache_dir,
            )
            generated = vae_cache.generate_vae_cache_filename(filepath)[0]
            self.assertEqual(
                generated, expected, f&quot;Test {i} failed: {generated} != {expected}&quot;
            )
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="tests/test_webhooks.py">import unittest
from unittest.mock import patch, MagicMock
from helpers.webhooks.handler import WebhookHandler
from helpers.webhooks.config import WebhookConfig
from io import BytesIO
from PIL import Image
class TestWebhookHandler(unittest.TestCase):
    def setUp(self):
        # Create a mock for the WebhookConfig
        self.mock_config_instance = MagicMock(spec=WebhookConfig)
        self.mock_config_instance.webhook_url = &quot;http://example.com/webhook&quot;
        self.mock_config_instance.webhook_type = &quot;discord&quot;
        self.mock_config_instance.log_level = &quot;info&quot;
        self.mock_config_instance.message_prefix = &quot;TestPrefix&quot;
        self.mock_config_instance.values = {
            &quot;webhook_url&quot;: &quot;http://example.com/webhook&quot;,
            &quot;webhook_type&quot;: &quot;discord&quot;,
            &quot;log_level&quot;: &quot;info&quot;,
            &quot;message_prefix&quot;: &quot;TestPrefix&quot;,
        }
        # Mock the accelerator object
        self.mock_accelerator = MagicMock()
        self.mock_accelerator.is_main_process = True
        # Instantiate the handler with the mocked config
        self.handler = WebhookHandler(
            config_path=&quot;dummy_path&quot;,
            accelerator=self.mock_accelerator,
            project_name=&quot;TestProject&quot;,
            mock_webhook_config=self.mock_config_instance,
            args=MagicMock(framerate=99),
        )
    @patch(&quot;requests.post&quot;)
    def test_send_message_info_level(self, mock_post):
        # Test sending a simple info level message
        message = &quot;Test message&quot;
        self.handler.send(message, message_level=&quot;info&quot;)
        mock_post.assert_called_once()
        # Capture the call arguments
        args, kwargs = mock_post.call_args
        # Assuming the message is sent in &apos;data&apos; parameter
        self.assertIn(&quot;data&quot;, kwargs)
        self.assertIn(message, kwargs[&quot;data&quot;].get(&quot;content&quot;))
    @patch(&quot;requests.post&quot;)
    def test_debug_message_wont_send(self, mock_post):
        # Test that debug logs don&apos;t send when the log level is info
        self.handler.send(&quot;Test message&quot;, message_level=&quot;debug&quot;)
        mock_post.assert_not_called()
    @patch(&quot;requests.post&quot;)
    def test_do_not_send_lower_than_configured_level(self, mock_post):
        # Set a higher log level and test
        self.handler.log_level = 1  # Error level
        self.handler.send(&quot;Test message&quot;, message_level=&quot;info&quot;)
        mock_post.assert_not_called()
    @patch(&quot;requests.post&quot;)
    def test_send_with_images(self, mock_post):
        # Test sending messages with images
        image = Image.new(&quot;RGB&quot;, (60, 30), color=&quot;red&quot;)
        message = &quot;Test message with image&quot;
        self.handler.send(message, images=[image], message_level=&quot;info&quot;)
        args, kwargs = mock_post.call_args
        self.assertIn(&quot;files&quot;, kwargs)
        self.assertEqual(len(kwargs[&quot;files&quot;]), 1)
        # Check that the message is in the &apos;data&apos; parameter
        content = kwargs.get(&quot;data&quot;, {}).get(&quot;content&quot;, &quot;&quot;)
        self.assertIn(self.mock_config_instance.values.get(&quot;message_prefix&quot;), content)
        self.assertIn(&quot;data&quot;, kwargs, f&quot;Check data for contents: {kwargs}&quot;)
        self.assertIn(message, content)
    @patch(&quot;requests.post&quot;)
    def test_response_storage(self, mock_post):
        # Mock response object
        mock_response = MagicMock()
        mock_response.headers = {&quot;Content-Type&quot;: &quot;application/json&quot;}
        mock_post.return_value = mock_response
        self.handler.send(&quot;Test message&quot;, message_level=&quot;info&quot;, store_response=True)
        self.assertEqual(self.handler.stored_response, mock_response.headers)
        # Also check that the message is sent
        args, kwargs = mock_post.call_args
        content = kwargs.get(&quot;data&quot;, {}).get(&quot;content&quot;, &quot;&quot;)
        self.assertIn(self.mock_config_instance.values.get(&quot;message_prefix&quot;), content)
        self.assertIn(&quot;Test message&quot;, content)
if __name__ == &quot;__main__&quot;:
    unittest.main()</file><file path="toolkit/captioning/classes/Authorization.php">&lt;?php
/**
 * Authorization - Handles the authorization of the client
 */
class Authorization {
	/** @var string */
	private $client_id;
	/** @var string */
	private $secret;
	/** @var string */
	private $user_config_path;
	/** @var array */
	private $users;
	public function __construct(string $user_config_path, bool $test_authorization = true) {
		// Retrieve client_id and secret from POST params:
		if (!isset($_REQUEST[&apos;client_id&apos;]) || !isset($_REQUEST[&apos;secret&apos;])) {
			http_response_code(403);
			echo &apos;Unauthorized.&apos;;
			exit;
		}
		$this-&gt;client_id = $_REQUEST[&apos;client_id&apos;];
		$this-&gt;secret = $_REQUEST[&apos;secret&apos;];
		$this-&gt;user_config_path = $user_config_path;
		$this-&gt;load_user_database();
        if ($test_authorization) $this-&gt;authorize();
	}
    /**
     * Load the user database from disk.
     *
     * @return Authorization
     */
	private function load_user_database() {
		// Load the user database from the file:
		try {
			$this-&gt;users = json_decode(file_get_contents($this-&gt;user_config_path), true);
            return $this;
		} catch (Exception $e) {
            error_log($e-&gt;getMessage());
			http_response_code(500);
			echo &apos;Internal server error.&apos;;
			exit;
		}
	}
	public function authorize() {
		// Check if client_id and secret are valid:
		if (!in_array($this-&gt;client_id, array_keys($this-&gt;users)) || $this-&gt;secret !== $this-&gt;users[$this-&gt;client_id]) {
			http_response_code(403);
			echo &apos;Unauthorized.&apos;;
			exit;
		}
	}
}</file><file path="toolkit/captioning/classes/BackendController.php">&lt;?php
/**
 * BackendController
 *
 * Accept the incoming REQUEST and parse it.
 */
class BackendController {
	/** @var PDO */
	private $pdo;
	/** @var string */
	private $action;
	/** @var string */
	private $error;
	/** @var string */
	private $client_id;
	/** @var string */
	private $job_type;
	/** @var S3Uploader */
	private $s3_uploader;
	public function __construct(PDO $pdo, S3Uploader $s3_uploader) {
		$this-&gt;pdo = $pdo;
		$this-&gt;s3_uploader = $s3_uploader;
		$this-&gt;getParameters();
	}
	public function getParameters() {
		// Action handling
		$this-&gt;action = $_REQUEST[&apos;action&apos;] ?? &apos;&apos;;
		$this-&gt;job_type = $_REQUEST[&apos;job_type&apos;] ?? &apos;&apos;;
		$this-&gt;error = $_REQUEST[&apos;error&apos;] ?? &apos;&apos;;
	}
	public function handleRequest() {
		return $this-&gt;{$this-&gt;action}();
	}
	public function list_jobs() {
		try {
			$limit = 500; // Number of rows to fetch and randomize in PHP
			$count = $_GET[&apos;count&apos;] ?? 1; // Number of rows to actually return
			// Fetch the rows
			if ($this-&gt;job_type === &apos;vae&apos;) {
				$total_jobs = $this-&gt;pdo-&gt;query(&apos;SELECT COUNT(*) FROM dataset&apos;)-&gt;fetchColumn();
				$remaining_jobs = $this-&gt;pdo-&gt;query(&apos;SELECT COUNT(*) FROM dataset WHERE pending = 0&apos;)-&gt;fetchColumn();
				$stmt = $this-&gt;pdo-&gt;prepare(&apos;SELECT * FROM dataset WHERE pending = 0 LIMIT ?&apos;);
			} elseif ($this-&gt;job_type === &apos;dataset_upload&apos;) {
				$total_jobs = $this-&gt;pdo-&gt;query(&apos;SELECT COUNT(*) FROM dataset&apos;)-&gt;fetchColumn();
				$remaining_jobs = $this-&gt;pdo-&gt;query(&apos;SELECT COUNT(*) FROM dataset WHERE upload_pending = 0 AND result IS NULL&apos;)-&gt;fetchColumn();
				$stmt = $this-&gt;pdo-&gt;prepare(&apos;SELECT * FROM dataset WHERE result IS NULL LIMIT ?&apos;);
			}
			$stmt-&gt;bindValue(1, $limit, PDO::PARAM_INT);
			$stmt-&gt;execute();
			$jobs = $stmt-&gt;fetchAll();
			// Shuffle the array in PHP
			shuffle($jobs);
			// Slice the array to get only the number of rows specified by $count
			$jobs = array_slice($jobs, 0, $count);
			// Update the database for the selected jobs
			foreach ($jobs as $idx =&gt; $job) {
				if ($this-&gt;job_type === &apos;vae&apos;) {
					$updateStmt = $this-&gt;pdo-&gt;prepare(&apos;UPDATE dataset SET pending = 1, submitted_at = NOW(), attempts = attempts + 1 WHERE data_id = ?&apos;);
				} elseif ($this-&gt;job_type === &apos;dataset_upload&apos;) {
					$updateStmt = $this-&gt;pdo-&gt;prepare(&apos;UPDATE dataset SET upload_pending = 1 WHERE data_id = ?&apos;);
				}
				$updateStmt-&gt;execute([$job[&apos;data_id&apos;]]);
				$jobs[$idx][&apos;total_jobs&apos;] = $total_jobs;
				$jobs[$idx][&apos;remaining_jobs&apos;] = $remaining_jobs; // Update remaining jobs count
				$jobs[$idx][&apos;completed_jobs&apos;] = $total_jobs - $remaining_jobs;
				$jobs[$idx][&apos;job_type&apos;] = $this-&gt;job_type;
			}
			// Return the selected jobs
			return $jobs;
		} catch (\Throwable $ex) {
			echo &apos;An error occurred: &apos; . $ex-&gt;getMessage();
		}
	}
	public function submit_job() {
		try {
			$dataId = $_REQUEST[&apos;job_id&apos;] ?? &apos;&apos;;
			$result = $_REQUEST[&apos;result&apos;] ?? &apos;&apos;;
			$status = $_REQUEST[&apos;status&apos;] ?? &apos;success&apos;;
			if (!$result || !$dataId) {
				echo &apos;Job ID and result are required&apos;;
				exit;
			}
			if ($status == &apos;error&apos; &amp;&amp; !$this-&gt;error) {
				echo &quot;Error message required for status &apos;error&apos;&quot;;
				exit;
			}
			if ($status !== &apos;error&apos;) {
				$stmt = $this-&gt;pdo-&gt;prepare(&apos;SELECT data_id FROM dataset WHERE data_id = ?&apos;);
				$stmt-&gt;execute([$dataId]);
				$filename = $stmt-&gt;fetchColumn();
				if (!$filename) {
					echo &apos;Job ID not found&apos;;
					exit;
				}
				if ($this-&gt;job_type === &apos;vae&apos;) {
					if (!in_array(&apos;result_file&apos;, array_keys($_FILES))) {
						echo &apos;Result files are required for VAE tasks.&apos;;
						echo &apos;Provided files: &apos; . json_encode($_FILES);
						exit;
					}
					$result = $this-&gt;s3_uploader-&gt;uploadVAECache($_FILES[&apos;result_file&apos;][&apos;tmp_name&apos;], $filename . &apos;.pt&apos;);
					$updateStmt = $this-&gt;pdo-&gt;prepare(&apos;UPDATE dataset SET client_id = ?, error = ? WHERE data_id = ?&apos;);
					$updateStmt-&gt;execute([$this-&gt;client_id, $this-&gt;error, $dataId]);
				} elseif ($this-&gt;job_type === &apos;dataset_upload&apos;) {
					if (in_array(&apos;image_file&apos;, $_FILES)) $result = $this-&gt;s3_uploader-&gt;uploadImage($_FILES[&apos;image_file&apos;][&apos;tmp_name&apos;], $filename . &apos;.png&apos;);
					$updateStmt = $this-&gt;pdo-&gt;prepare(&apos;UPDATE dataset SET result = ?, upload_pending = 1 WHERE data_id = ?&apos;);
					$updateStmt-&gt;execute([$result, $dataId]);
				} elseif ($this-&gt;job_type === &apos;text&apos;) {
					$result = $this-&gt;s3_uploader-&gt;uploadTextCache($_FILES[&apos;result_file&apos;][&apos;tmp_name&apos;], $filename);
				} else {
					echo &apos;Invalid job type: &apos; . $this-&gt;job_type . &apos; - must be &quot;vae&quot; or &quot;text&quot;&apos;;
					exit;
				}
			}
			return [&apos;status&apos; =&gt; &apos;success&apos;, &apos;result&apos; =&gt; &apos;Job submitted successfully&apos;];
		} catch (\Throwable $ex) {
			echo &apos;An error occurred for FILES &apos; . json_encode($_FILES) . &apos;: &apos; . $ex-&gt;getMessage() . &apos;, traceback: &apos; . $ex-&gt;getTraceAsString();
		}
	}
}</file><file path="toolkit/captioning/classes/S3Uploader.php">&lt;?php
require &apos;vendor/autoload.php&apos;;
use Aws\S3\S3Client;
use Aws\Exception\AwsException;
class S3Uploader {
    /** @var S3Client */
    private $s3Client;
    /** @var string */
    private $bucket;
    /** @var string */
    private $vae_cache_prefix;
    /** @var string */
    private $image_data_prefix;
    /** @var string */
    private $text_cache_prefix;
    public function __construct($bucket, $region, $key, $secret, $endpoint, $vae_cache_prefix, $text_cache_prefix, $image_data_prefix) {
        $this-&gt;s3Client = new S3Client([
            &apos;version&apos; =&gt; &apos;latest&apos;,
            &apos;region&apos;  =&gt; $region,
            &apos;credentials&apos; =&gt; [
                &apos;key&apos;    =&gt; $key,
                &apos;secret&apos; =&gt; $secret,
            ],
            &apos;endpoint&apos; =&gt; $endpoint,
        ]);
        $this-&gt;bucket = $bucket;
        $this-&gt;vae_cache_prefix = $vae_cache_prefix;
        $this-&gt;text_cache_prefix = $text_cache_prefix;
        $this-&gt;image_data_prefix = $image_data_prefix;
    }
    /**
     * Upload a VAE cache file to S3, under the embed prefix as a .pt file.
     * 
     * A Client worker will POST this to us, we need to accept and forward to S3.
     */
    public function uploadVAECache($file, $key) {
        return $this-&gt;uploadFile($file, $this-&gt;vae_cache_prefix .&apos;/&apos;. $key);
    }
    /**
     * Upload an image file to S3, under the image data prefix as a .png file.
     * 
     * A Client worker will POST this to us, we need to accept and forward to S3.
     */
    public function uploadImage($file, $key) {
        return $this-&gt;uploadFile($file, $this-&gt;image_data_prefix .&apos;/&apos;. $key);
    }
    /**
     * Upload a text cache file to S3, under the embed prefix as a .pt file.
     * 
     * A Client worker will POST this to us, we need to accept and forward to S3.
     */
    public function uploadTextCache($file, $key) {
        return $this-&gt;uploadFile($file, $this-&gt;text_cache_prefix .&apos;/&apos;. $key);
    }
    public function uploadFile($file, $key) {
        try {
            $result = $this-&gt;s3Client-&gt;putObject([
                &apos;Bucket&apos; =&gt; $this-&gt;bucket,
                &apos;Key&apos;    =&gt; $key,
                &apos;SourceFile&apos;   =&gt; $file,
            ]);
            return $result[&apos;ObjectURL&apos;];
        } catch (AwsException $e) {
            // Output error message if fails
            error_log($e-&gt;getMessage());
            return null;
        }
    }
    public function uploadContent($content, $key, $contentType = &apos;text/plain&apos;) {
        try {
            $result = $this-&gt;s3Client-&gt;putObject([
                &apos;Bucket&apos; =&gt; $this-&gt;bucket,
                &apos;Key&apos;    =&gt; $key,
                &apos;Body&apos;   =&gt; $content,
                &apos;ContentType&apos; =&gt; $contentType,
            ]);
            return $result[&apos;ObjectURL&apos;];
        } catch (AwsException $e) {
            // Output error message if fails
            error_log($e-&gt;getMessage());
            return null;
        }
    }
}</file><file path="toolkit/captioning/caption_backend_server.php">&lt;?php
require_once __DIR__ . &apos;/classes/Authorization.php&apos;;
require_once __DIR__ . &apos;/classes/BackendController.php&apos;;
require_once __DIR__ . &apos;/classes/S3Uploader.php&apos;;
$authorization = new Authorization(&apos;/var/www/.users.json&apos;);
// Load MySQL credentials
$mysql_credentials = json_decode(file_get_contents(&apos;/var/www/.mysql.json&apos;), true);
$users = json_decode(file_get_contents(&apos;/var/www/.users.json&apos;), true);
// Create PDO connection
$dsn = &quot;mysql:host={$mysql_credentials[&apos;host&apos;]};dbname={$mysql_credentials[&apos;database&apos;]};charset=utf8mb4&quot;;
try {
	$pdo = new PDO($dsn, $mysql_credentials[&apos;user&apos;], $mysql_credentials[&apos;password&apos;], [
		PDO::ATTR_ERRMODE =&gt; PDO::ERRMODE_EXCEPTION,
		PDO::ATTR_DEFAULT_FETCH_MODE =&gt; PDO::FETCH_ASSOC,
	]);
} catch (PDOException $e) {
	die(&apos;Could not connect to the database (&apos; . $dsn . &apos;): &apos; . $e-&gt;getMessage());
}
// Create the `dataset` table if it does not exist
$pdo-&gt;exec(&quot;CREATE TABLE IF NOT EXISTS dataset (
  data_id int AUTO_INCREMENT PRIMARY KEY,
  URL varchar(255) NOT NULL,
  pending tinyint(1) NOT NULL DEFAULT &apos;0&apos;,
  result longtext,
  submitted_at datetime DEFAULT NULL,
  attempts int DEFAULT &apos;0&apos;,
  error text,
  client_id varchar(255) DEFAULT NULL,
  updated_at datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  job_group varchar(255) DEFAULT NULL
)&quot;);
$aws_config = json_decode(file_get_contents(&apos;/var/www/.aws.json&apos;), true);
$s3_uploader = new S3Uploader(
    $aws_config[&apos;aws_bucket_name&apos;],
    $aws_config[&apos;aws_region&apos;],
    $aws_config[&apos;aws_access_key_id&apos;],
    $aws_config[&apos;aws_secret_access_key&apos;],
    $aws_config[&apos;aws_endpoint_url&apos;],
    $aws_config[&apos;vae_cache_prefix&apos;],
    $aws_config[&apos;text_cache_prefix&apos;],
    $aws_config[&apos;image_data_prefix&apos;]
);
$backendController = new BackendController($pdo, $s3_uploader);
$result = $backendController-&gt;handleRequest();
echo json_encode($result);</file><file path="toolkit/captioning/caption_with_blip.py">import os, torch, logging, re, random
from PIL import Image
from clip_interrogator import Config, Interrogator, LabelTable, load_list
from clip_interrogator import clip_interrogator
clip_interrogator.CAPTION_MODELS.update({
    &apos;unography&apos;: &apos;unography/blip-large-long-cap&apos;,           # 1.9GB
})
print(f&quot;Models supported: {clip_interrogator.CAPTION_MODELS}&quot;)
# Directory where the images are located
input_directory_path = &quot;/Volumes/datasets/photo-concept-bucket/image_data&quot;
output_dir = &quot;/Volumes/datasets/photo-concept-bucket/image_data_captioned&quot;
caption_strategy = &quot;text&quot;
def content_to_filename(content):
    &quot;&quot;&quot;
    Function to convert content to filename by stripping everything after &apos;--&apos;,
    replacing non-alphanumeric characters and spaces, converting to lowercase,
    removing leading/trailing underscores, and limiting filename length to 128.
    &quot;&quot;&quot;
    # Split on &apos;--&apos; and take the first part
    content = content.split(&quot;--&quot;, 1)[0]
    # Remove URLs
    cleaned_content = re.sub(r&quot;https*://\S*&quot;, &quot;&quot;, content)
    # Replace non-alphanumeric characters and spaces, convert to lowercase, remove leading/trailing underscores
    cleaned_content = re.sub(r&quot;[^a-zA-Z0-9 ]&quot;, &quot;&quot;, cleaned_content)
    cleaned_content = cleaned_content.replace(&quot; &quot;, &quot;_&quot;).lower().strip(&quot;_&quot;)
    # If cleaned_content is empty after removing URLs, generate a random filename
    if cleaned_content == &quot;&quot;:
        cleaned_content = f&quot;midjourney_{random.randint(0, 1000000)}&quot;
    # Limit filename length to 128
    cleaned_content = (
        cleaned_content[:128] if len(cleaned_content) &gt; 128 else cleaned_content
    )
    return cleaned_content + &quot;.png&quot;
def interrogator(
    clip_model_name=&quot;ViT-H-14/laion2b_s32b_b79k&quot;, blip_model=&quot;unography&quot;
):
    # Create an Interrogator instance with the latest CLIP model for Stable Diffusion 2.1
    conf = Config(
        clip_model_name=clip_model_name, clip_offload=True, caption_offload=True, caption_max_length=170, device=&quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
    )
    conf.caption_model_name = blip_model
    ci = Interrogator(conf)
    return ci
def load_terms(filename, interrogator_instance):
    # Load your list of terms
    table = LabelTable(load_list(filename), &quot;terms&quot;, interrogator_instance)
    logging.debug(f&quot;Loaded {len(table)} terms from {filename}&quot;)
    return table
def process_directory(image_dir=&quot;images&quot;, terms_file=None, active_interrogator=None):
    if active_interrogator is None:
        active_interrogator = interrogator()
    if terms_file is not None:
        table = load_terms(terms_file, active_interrogator)
    for filename in os.listdir(image_dir):
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            process_directory(full_filepath, terms_file, active_interrogator)
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;)):
            try:
                image = Image.open(full_filepath).convert(&quot;RGB&quot;)
                if terms_file:
                    best_match = table.rank(
                        active_interrogator.image_to_features(image), top_count=1
                    )[0]
                else:
                    best_match = active_interrogator.generate_caption(image)
                logging.info(f&quot;Best match for {filename}: {best_match}&quot;)
                # Save based on caption strategy
                new_filename = (
                    content_to_filename(best_match)
                    if caption_strategy == &quot;filename&quot;
                    else filename
                )
                new_filepath = os.path.join(output_dir, new_filename)
                if caption_strategy == &quot;text&quot;:
                    with open(new_filepath + &quot;.txt&quot;, &quot;w&quot;) as f:
                        f.write(best_match)
                else:
                    # Ensure no overwriting
                    counter = 1
                    while os.path.exists(new_filepath):
                        new_filepath = os.path.join(
                            output_dir,
                            f&quot;{new_filename.rsplit(&apos;.&apos;, 1)[0]}_{counter}.{new_filename.rsplit(&apos;.&apos;, 1)[1]}&quot;,
                        )
                        counter += 1
                    image.save(new_filepath)
                image.close()
            except Exception as e:
                logging.error(f&quot;Error processing {filename}: {str(e)}&quot;)
if __name__ == &quot;__main__&quot;:
    logging.basicConfig(level=logging.INFO)
    # Ensure output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    process_directory(input_directory_path)</file><file path="toolkit/captioning/caption_with_blip3.py">import os
import logging
import re
import random
import argparse
import base64
import torch
from PIL import Image
from tqdm import tqdm
import requests
import io
from transformers import (
    AutoModelForVision2Seq,
    AutoTokenizer,
    AutoImageProcessor,
    StoppingCriteria,
)
import pandas as pd
import torch.nn as nn
logger = logging.getLogger(&quot;Captioner&quot;)
# Define the prompt template for BLIP3
def apply_prompt_template(prompt):
    s = (
        &quot;&lt;|system|&gt;\nYou are an image tagger. Provide image tags only separated by commas.&lt;|end|&gt;\n&quot;
        f&quot;&lt;|user|&gt;\n&lt;image&gt;\n{prompt}\n&lt;|end|&gt;\n&lt;|assistant|&gt;\n&quot;
    )
    return s
class EosListStoppingCriteria(StoppingCriteria):
    def __init__(self, eos_sequence=[32007]):
        self.eos_sequence = eos_sequence
    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
    ) -&gt; bool:
        last_ids = input_ids[:, -len(self.eos_sequence) :].tolist()
        return self.eos_sequence in last_ids
# load the existing parquet file if it exists
def load_input_parquet(parquet_path: str):
    df = pd.read_parquet(path=parquet_path)
    return df
# Load BLIP3 model
def load_blip3_model():
    model_name_or_path = &quot;Salesforce/xgen-mm-phi3-mini-instruct-r-v1&quot;
    model = AutoModelForVision2Seq.from_pretrained(
        model_name_or_path, trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, trust_remote_code=True, use_fast=False, legacy=False
    )
    image_processor = AutoImageProcessor.from_pretrained(
        model_name_or_path, trust_remote_code=True
    )
    tokenizer = model.update_special_tokens(tokenizer)
    return (
        model.to(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cuda&quot;),
        tokenizer,
        image_processor,
    )
# Function to evaluate BLIP3 model
def eval_blip3_model(query, raw_image, model, tokenizer, image_processor):
    prompt = apply_prompt_template(query)
    inputs = image_processor(
        [raw_image], return_tensors=&quot;pt&quot;, image_aspect_ratio=&quot;anyres&quot;
    )
    language_inputs = tokenizer([prompt], return_tensors=&quot;pt&quot;)
    inputs.update(language_inputs)
    inputs = {name: tensor.to(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cuda&quot;) for name, tensor in inputs.items()}
    generated_text = model.generate(
        **inputs,
        image_size=[raw_image.size],
        pad_token_id=tokenizer.pad_token_id,
        do_sample=True,
        max_new_tokens=512,
        top_p=0.95,
        top_k=50,
        num_beams=1,
        stopping_criteria=[EosListStoppingCriteria()],
    )
    prediction = tokenizer.decode(generated_text[0], skip_special_tokens=True).split(
        &quot;&lt;|end|&gt;&quot;
    )[0]
    # If it doesn&apos;t end on a complete sentence, remove everything after the final &apos;.&apos;
    # if not prediction.endswith(&quot;.&quot;):
    #     # Remove everything after the final &apos;.&apos;
    #     prediction = re.sub(r&quot;\.[^.]*$&quot;, &quot;.&quot;, prediction)
    return prediction
def process_and_evaluate_image(
    args, image_path: str, model, tokenizer, image_processor
):
    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):
        response = requests.get(image_path)
        image = Image.open(io.BytesIO(response.content))
    else:
        image = Image.open(image_path)
    def resize_for_condition_image(input_image: Image.Image, resolution: int):
        if resolution == 0:
            return input_image
        input_image = input_image.convert(&quot;RGB&quot;)
        W, H = input_image.size
        aspect_ratio = round(W / H, 2)
        if W &lt; H:
            W = resolution
            H = int(resolution / aspect_ratio)
        elif H &lt; W:
            H = resolution
            W = int(resolution * aspect_ratio)
        if W == H:
            W = resolution
            H = resolution
        img = input_image.resize((W, H), resample=Image.LANCZOS)
        return img
    result = eval_blip3_model(
        args.query_str,
        resize_for_condition_image(image, 384),
        model,
        tokenizer,
        image_processor,
    )
    print(f&quot;Result for captioning: {result}&quot;)
    return result
def process_directory(
    args,
    image_dir,
    output_parquet,
    model,
    tokenizer,
    image_processor,
    input_parquet=None,
    original_query_str=None,
):
    records = []
    parquet_path = f&quot;{output_parquet}.{os.path.basename(image_dir)}.parquet&quot;
    print(f&quot;Parquet: {parquet_path}&quot;)
    total_to_process = 10000
    total_processed = 0
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        if input_parquet is not None:
            # if the caption column at the filename position is non-empty, skip
            current_caption = input_parquet[input_parquet[&quot;filename&quot;] == filename]
            hint_column = args.input_parquet_hint_column
            hint_value = None
            if hint_column is not None and hint_column != &quot;&quot;:
                try:
                    hint_value = current_caption[hint_column].values[0]
                except:
                    hint_value = None
                if hint_value is not None and not hint_value == &quot;&quot;:
                    if original_query_str is not None:
                        args.query_str = original_query_str
                    args.query_str = args.query_str.replace(&quot;%s&quot;, hint_value)
                    logger.info(
                        f&quot;Using query string: {args.query_str} for hint value: {hint_value}&quot;
                    )
            try:
                if (
                    not current_caption.empty
                    and not current_caption[&quot;caption&quot;].isnull().values[0]
                ):
                    logger.debug(f&quot;Already has caption: {current_caption[&apos;caption&apos;]}&quot;)
                    continue
            except:
                logger.debug(f&quot;Error checking for existing caption: {current_caption}&quot;)
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logger.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(
                args,
                full_filepath,
                output_parquet,
                model,
                tokenizer,
                image_processor,
                input_parquet=input_parquet,
                original_query_str=original_query_str,
            )
            args.query_str = original_query_str
            original_query_str = None
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
            try:
                logger.debug(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logger.debug(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(
                        args, full_filepath, model, tokenizer, image_processor
                    )
                    total_processed += 1
                    logger.debug(f&quot;Best match for {filename}: {best_match}&quot;)
                    with Image.open(full_filepath) as img_file:
                        image_bytes = img_file.tobytes()
                    records.append({&quot;filename&quot;: filename, &quot;caption&quot;: best_match})
                    if total_processed &gt;= total_to_process:
                        break
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
                if &quot;CUDA error&quot; in str(e):
                    import sys
                    sys.exit(1)
    new_df = pd.DataFrame(records)
    if input_parquet is not None:
        # Merge new_df with input_parquet
        input_parquet.set_index(&quot;filename&quot;, inplace=True)
        new_df.set_index(&quot;filename&quot;, inplace=True)
        combined_df = input_parquet.combine_first(new_df).reset_index()
    else:
        combined_df = new_df
    # reduce duplicates by &quot;filename&quot; contents
    combined_df = combined_df.drop_duplicates(subset=[&quot;filename&quot;])
    combined_df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
    logger.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;Provide the most detailed caption.&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;],
        default=&quot;fp16&quot;,
        help=(&quot;Precision for loading the model. Default: fp16&quot;),
    )
    parser.add_argument(
        &quot;--input_parquet&quot;,
        type=str,
        default=None,
        help=&quot;Path to the input Parquet dataset which will be adjusted to have the new column.&quot;,
    )
    parser.add_argument(
        &quot;--input_parquet_hint_column&quot;,
        type=str,
        default=&quot;title&quot;,
        help=&quot;When set, the column to use as a hint for the input query str placement value. Default: title&quot;,
    )
    args = parser.parse_args()
    return args
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    input_database = None
    if args.input_parquet:
        if not os.path.exists(args.input_parquet):
            raise ValueError(&quot;The parquet file specified as input did not exist.&quot;)
        input_database = load_input_parquet(args.input_parquet)
    model, tokenizer, image_processor = load_blip3_model()
    process_directory(
        args,
        args.input_dir,
        args.output_parquet,
        model,
        tokenizer,
        image_processor,
        input_parquet=input_database,
        original_query_str=str(args.query_str),
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_cogvlm_remote.py">import threading
import queue
import requests
import torch, base64, logging, io, time
import argparse
from PIL import Image
from io import BytesIO
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, set_seed
from tqdm import tqdm as tq
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
# Initialize queues
job_queue = queue.Queue(maxsize=64)
submission_queue = queue.Queue()
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;, &quot;fp4&quot;, &quot;fp8&quot;],
        default=&quot;fp8&quot;,
        help=(
            &quot;When loading CogVLM, you can load it in fp16, bf16, fp8 or fp4 precision to reduce memory. Default: fp4&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_compile&quot;,
        type=bool,
        default=False,
        help=(
            &quot;Provide --disable_compile=true to disable Torch compile. Required on Mac and AMD, perhaps. Default: false.&quot;
        ),
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;Caption this image accurately, without speculation. Just describe what you see.&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--backend_url&quot;,
        type=str,
        required=True,
        help=(
            &quot;The URL of the backend to use for processing. This should be the URL of the backend that will be used to process the images.&quot;
        ),
    )
    parser.add_argument(
        &quot;--job_type&quot;,
        type=str,
        choices=[&quot;caption&quot;, &quot;vae&quot;, &quot;dataset_upload&quot;],
        required=True,
        help=(&quot;The type of encoding to produce.&quot;),
    )
    parser.add_argument(
        &quot;--client_id&quot;,
        type=str,
        required=True,
        help=(&quot;The client ID to use to login to the backend.&quot;),
    )
    parser.add_argument(
        &quot;--secret&quot;,
        type=str,
        required=True,
        help=(&quot;The secret to use to login to the backend.&quot;),
    )
    parser.add_argument(
        &quot;--eval_backend&quot;,
        type=str,
        default=&quot;transformers&quot;,
        choices=[&quot;oobabooga&quot;, &quot;transformers&quot;],
        help=(
            &quot;If transformers is provided, it will load the model using the transformers library. If oobabooga is provided, it will use the API for a running installation of Oobabooga&apos;s text-generation-webui.&quot;
        ),
    )
    parser.add_argument(
        &quot;--aws_config&quot;,
        type=str,
        default=None,
        help=(&quot;If provided, can post images directly to S3.&quot;),
    )
    parser.add_argument(
        &quot;--aws_endpoint_url&quot;,
        type=str,
        default=None,
    )
    parser.add_argument(
        &quot;--aws_region_name&quot;,
        type=str,
        default=None,
    )
    parser.add_argument(
        &quot;--aws_access_key_id&quot;,
        type=str,
        default=None,
    )
    parser.add_argument(
        &quot;--aws_secret_access_key&quot;,
        type=str,
        default=None,
    )
    parser.add_argument(
        &quot;--batch_size&quot;,
        type=int,
        default=16,
        help=(
            &quot;When querying the backend, how many jobs to request at once. Decreasing this will put more load on the remote server. Do not do that. Default: 16&quot;
        ),
    )
    return parser.parse_args()
def eval_image(
    image: Image.Image,
    model,
    tokenizer,
    torch_dtype,
    query: str,
):
    inputs = model.build_conversation_input_ids(
        tokenizer, query=query, history=[], images=[image]
    )  # chat mode
    inputs = {
        &quot;input_ids&quot;: inputs[&quot;input_ids&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;token_type_ids&quot;: inputs[&quot;token_type_ids&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;attention_mask&quot;: inputs[&quot;attention_mask&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;images&quot;: [[inputs[&quot;images&quot;][0].to(&quot;cuda&quot;).to(torch_dtype)]],
    }
    gen_kwargs = {&quot;max_new_tokens&quot;: 77, &quot;do_sample&quot;: False}
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
        outputs = outputs[:, inputs[&quot;input_ids&quot;].shape[1] :]
        return tokenizer.decode(outputs[0])
def eval_image_with_ooba(image: Image.Image, query: str) -&gt; str:
    CONTEXT = &quot;A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user&apos;s questions.\n&quot;
    img_str = base64.b64encode(image).decode(&quot;utf-8&quot;)
    prompt = (
        CONTEXT
        + f&apos;### Human: {query} \n&lt;img src=&quot;data:image/jpeg;base64,{img_str}&quot;&gt;### Assistant: &apos;
    )
    data = {
        &quot;mode&quot;: &quot;instruct&quot;,  # chat, instruct
        &quot;character&quot;: &quot;Example&quot;,
        #           &quot;instruction_template&quot;: &quot;LLaVA-v1&quot;,
        &quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: prompt}],
    }
    response = requests.post(&quot;http://127.0.0.1:5000/v1/chat/completions&quot;, json=data)
    if response.status_code != 200:
        print(
            f&quot;Request failed with status {response.status_code}. Response: {response.text}&quot;
        )
        import sys
        sys.exit(1)
    else:
        return response.json()[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
def encode_images(accelerator, vae, images, image_transform):
    &quot;&quot;&quot;
    Encode a batch of input images. Images must be the same dimension.
    &quot;&quot;&quot;
    pixel_values = [
        image_transform(image).to(accelerator.device, dtype=vae.dtype)
        for image in images
    ]
    with torch.no_grad():
        processed_images = torch.stack(pixel_values).to(
            accelerator.device, dtype=torch.bfloat16
        )
        latents = vae.encode(processed_images).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
    return latents
from math import sqrt
def round_to_nearest_multiple(value, multiple: int = 64):
    &quot;&quot;&quot;Round a value to the nearest multiple.&quot;&quot;&quot;
    rounded = round(value / multiple) * multiple
    return max(rounded, multiple)  # Ensure it&apos;s at least the value of &apos;multiple&apos;
def calculate_new_size_by_pixel_area(W: int, H: int, megapixels: float = 1.0):
    aspect_ratio = W / H
    total_pixels = megapixels * 1e6  # Convert megapixels to pixels
    W_new = int(round(sqrt(total_pixels * aspect_ratio)))
    H_new = int(round(sqrt(total_pixels / aspect_ratio)))
    # Ensure they are divisible by 64
    W_new = round_to_nearest_multiple(W_new)
    H_new = round_to_nearest_multiple(H_new)
    return W_new, H_new
def initialize_s3_client(args):
    &quot;&quot;&quot;Initialize the boto3 S3 client using the provided AWS credentials and settings.&quot;&quot;&quot;
    import boto3
    from botocore.config import Config
    s3_config = Config(max_pool_connections=100)
    s3_client = boto3.client(
        &quot;s3&quot;,
        endpoint_url=args.aws_endpoint_url,
        region_name=args.aws_region_name,
        aws_access_key_id=args.aws_access_key_id,
        aws_secret_access_key=args.aws_secret_access_key,
        config=s3_config,
    )
    return s3_client
def submit_response(args, files, object_etag, task):
    submission_response = requests.post(
        f&quot;{args.backend_url}/?action=submit_job&quot;,
        files=files,
        params={
            &quot;result&quot;: object_etag,
            &quot;job_id&quot;: task[&quot;data_id&quot;],
            &quot;client_id&quot;: args.client_id,
            &quot;secret&quot;: args.secret,
            &quot;status&quot;: &quot;success&quot;,
            &quot;job_type&quot;: &quot;dataset_upload&quot;,
        },
    )
def upload_sample(args, image, task, local_progress_bar, files=None):
    if args.aws_config:
        s3_client = initialize_s3_client(args)
    if image is None:
        return None
    image_buffer = BytesIO()
    image.save(image_buffer, format=&quot;PNG&quot;)
    image_buffer.seek(0)
    files = None
    if args.aws_config:
        # post the image to s3
        attempt = 0
        while attempt &lt; 3:
            try:
                tq.write(
                    f&quot;Attempting to upload to {args.aws_bucket_name}: image_data/{task[&apos;data_id&apos;]}.png&quot;
                )
                before_time = time.time()
                response_meta = s3_client.put_object(
                    Bucket=args.aws_bucket_name,
                    Key=f&quot;image_data/{task[&apos;data_id&apos;]}.png&quot;,
                    Body=image_buffer,
                )
                after_time = time.time()
                tq.write(f&quot;Received data in {after_time - before_time} seconds.&quot;)
                object_etag = response_meta[&quot;ETag&quot;]
                tq.write(f&quot;Object Etag: {object_etag}&quot;)
                break
            except Exception as e:
                tq.write(f&quot;Failed to upload image to s3: {e}. Attempting again.&quot;)
                attempt += 1
                time.sleep(5)
        if attempt == 3:
            tq.write(f&quot;Failed to upload image to s3. Skipping.&quot;)
            local_progress_bar.update(1)
            return None
    else:
        object_etag = None
        files = {
            &quot;image_file&quot;: (
                &quot;image.png&quot;,
                image_buffer,
                &quot;image/png&quot;,
            ),
        }
    before_time = time.time()
    submit_response(args, files, object_etag, task)
    after_time = time.time()
    tq.write(f&quot;Submitted result in {after_time - before_time} seconds.&quot;)
def load_image_from_url(url):
    tq.write(f&quot;Begin load URL: {url}&quot;)
    before_time = time.time()
    attempts = 0
    while attempts &lt; 3:
        try:
            result = Image.open(
                io.BytesIO(requests.get(url, timeout=10).content)
            ).convert(&quot;RGB&quot;)
            break
        except Exception as e:
            tq.write(f&quot;-&gt; [error] Could not load image from {url}. Retrying...&quot;)
            attempts += 1
            time.sleep(5)
    if attempts == 3:
        tq.write(f&quot;-&gt; [error] Failed to load image from {url}.&quot;)
        return None
    after_time = time.time()
    tq.write(f&quot;Received image in {after_time - before_time} seconds.&quot;)
    return result
def load_images_in_parallel(tasks):
    images = {}
    with ThreadPoolExecutor(max_workers=20) as executor:
        future_to_url = {
            executor.submit(load_image_from_url, task[&quot;URL&quot;]): task for task in tasks
        }
        for future in as_completed(future_to_url):
            task = future_to_url[future]
            try:
                image = future.result()
                if image is not None:
                    images[task[&quot;data_id&quot;]] = image
            except Exception as exc:
                print(f&apos;{task[&quot;data_id&quot;]} generated an exception: {exc}&apos;)
    return images
def main():
    args = parse_args()
    if args.aws_config:
        with open(args.aws_config, &quot;r&quot;) as f:
            import json
            aws_config = json.load(f)
            args.aws_endpoint_url = aws_config[&quot;aws_endpoint_url&quot;]
            args.aws_region_name = aws_config[&quot;aws_region_name&quot;]
            args.aws_access_key_id = aws_config[&quot;aws_access_key_id&quot;]
            args.aws_secret_access_key = aws_config[&quot;aws_secret_access_key&quot;]
            args.aws_bucket_name = aws_config[&quot;aws_bucket_name&quot;]
    logging.basicConfig(level=logging.INFO)
    import warnings
    warnings.filterwarnings(&quot;ignore&quot;)
    accelerator_project_config = ProjectConfiguration()
    accelerator = Accelerator(
        mixed_precision=&quot;fp16&quot;,
        log_with=None,
        project_config=accelerator_project_config,
    )
    if args.job_type == &quot;vae&quot;:
        from diffusers import AutoencoderKL
        vae = AutoencoderKL.from_pretrained(
            &quot;madebyollin/sdxl-vae-fp16-fix&quot;,
            force_upcast=False,
            torch_dtype=torch.bfloat16,
        ).to(accelerator.device)
        if not args.disable_compile:
            vae = torch.compile(vae, fullgraph=True)
        from torchvision import transforms
        image_transforms = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5]),
            ]
        )
    elif args.eval_backend == &quot;transformers&quot; and args.job_type == &quot;caption&quot;:
        send_to_cuda = load_in_4bit = load_in_8bit = False
        torch_dtype = torch.bfloat16
        if args.precision == &quot;bf16&quot; or args.precision == &quot;fp16&quot;:
            send_to_cuda = True
            if args.precision == &quot;bf16&quot;:
                torch_dtype = torch.bfloat16
            else:
                torch_dtype = torch.float16
        elif args.precision == &quot;fp4&quot;:
            load_in_4bit = True
        elif args.precision == &quot;fp8&quot;:
            load_in_8bit = True
        if accelerator.is_main_process:
            tq.write(&quot;Loading CogVLM model. This should only occur once.&quot;)
        from transformers import AutoModelForCausalLM, LlamaTokenizer
        tokenizer = LlamaTokenizer.from_pretrained(&quot;lmsys/vicuna-7b-v1.5&quot;)
        if accelerator.is_main_process:
            tq.write(f&quot;Loading CogVLM in {args.precision} precision.&quot;)
        model = AutoModelForCausalLM.from_pretrained(
            &quot;THUDM/cogvlm-chat-hf&quot;,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            load_in_4bit=load_in_4bit,
            load_in_8bit=load_in_8bit,
        ).eval()
        if send_to_cuda:
            if accelerator.is_main_process:
                tq.write(f&quot;Sending model to CUDA.&quot;)
            model.to(accelerator.device)
        if accelerator.is_main_process:
            tq.write(&quot;Completed loading model.&quot;)
    # Split device by : and use 2nd half as position
    local_bar_pos = 1
    cuda_device = f&quot;{accelerator.device}&quot;
    if &quot;:&quot; in cuda_device:
        local_bar_pos = int(cuda_device.split(&quot;:&quot;)[1]) + 1
    local_progress_bar = tq(
        desc=&quot;Local progress         &quot;,
        dynamic_ncols=False,
        ascii=False,
        # We want this to be positioned by the accelerator rank
        position=local_bar_pos,
        leave=True,
        ncols=125,
        disable=not accelerator.is_main_process,
    )
    initial_cluster_progress = 0
    global_progress_bar = tq(
        desc=&quot;Global cluster progress&quot;,
        dynamic_ncols=False,
        ascii=False,
        position=0,
        leave=True,
        ncols=125,
        # We want to disable if not on main proc.
        disable=not accelerator.is_main_process,
    )
    # Query backend for tasks, and loop.
    has_set_total = False
    while True:
        try:
            # Query backend for tasks
            before_time = time.time()
            response = requests.get(
                f&quot;{args.backend_url}/?action=list_jobs&quot;,
                timeout=30,
                params={
                    &quot;client_id&quot;: args.client_id,
                    &quot;secret&quot;: args.secret,
                    &quot;count&quot;: args.batch_size,
                    &quot;job_type&quot;: args.job_type,
                },
            )
            after_time = time.time()
            tq.write(f&quot;Received job in {after_time - before_time} seconds.&quot;)
            # 403? Exit.
            if response.status_code == 403:
                tq.write(&quot;Access denied. Exiting.&quot;)
                break
            # 500? Wait.
            if response.status_code == 500:
                tq.write(&quot;Server error. Waiting.&quot;)
                time.sleep(30)
                continue
            # Decode the JSON response?
            try:
                response_json = response.json()
            except:
                tq.write(f&quot;Could not decode JSON response: {response.text}&quot;)
                time.sleep(5)
                continue
            # Example:
            # [
            #     {
            #         &quot;data_id&quot;: 474,
            #         &quot;URL&quot;: &quot;https://images.pexels.com/photos/474/black-and-white-car-vehicle-vintage.jpg?cs=srgb&amp;dl=pexels-gratisography-474.jpg&amp;fm=jpg&quot;,
            #         &quot;pending&quot;: 0,
            #         &quot;result&quot;: None,
            #         &quot;submitted_at&quot;: None,
            #         &quot;attempts&quot;: 0,
            #         &quot;error&quot;: None,
            #         &quot;client_id&quot;: None,
            #         &quot;updated_at&quot;: &quot;2024-02-21 02:32:32&quot;,
            #     }
            # ]
            # Now, we evaluate the caption for each image.
            if response_json[0][&quot;job_type&quot;] == &quot;dataset_upload&quot;:
                # Prepare the file data for uploading
                tq.write(f&quot;Received data upload job: {response_json[0][&apos;data_id&apos;]}.&quot;)
                # Grab all images in the batch at once via threads:
                images = load_images_in_parallel(response_json)
                with ThreadPoolExecutor(max_workers=20) as executor:
                    future_to_task = {
                        executor.submit(
                            upload_sample,
                            args,
                            image,
                            {&quot;data_id&quot;: data_id},
                            local_progress_bar,
                        ): data_id
                        for data_id, image in images.items()
                    }
                    for future in as_completed(future_to_task):
                        data_id = future_to_task[future]
                        try:
                            future.result()
                        except Exception as exc:
                            print(f&quot;{data_id} upload generated an exception: {exc}&quot;)
                # close all images
                for image in images.values():
                    image.close()
                continue
            for task in response_json:
                if not has_set_total:
                    initial_cluster_progress = task.get(&quot;completed_jobs&quot;, 0)
                    global_progress_bar.total = task.get(&quot;remaining_jobs&quot;, 0)
                    local_progress_bar.total = task.get(&quot;remaining_jobs&quot;, 0)
                    has_set_total = True
                # Skip if the task is pending or has a result
                if &quot;pending&quot; not in task:
                    tq.write(f&quot;Received invalid task: {task}. Skipping.&quot;)
                    continue
                if (task[&quot;pending&quot;] == 1 and args.job_type == &quot;vae&quot;) or (
                    task[&quot;result&quot;] and args.job_type == &quot;dataset_upload&quot;
                ):
                    tq.write(
                        f&quot;-&gt; [warning] Task {task[&apos;data_id&apos;]} is pending? {task[&apos;pending&apos;]} or has a result {task[&apos;result&apos;]}. Skipping.&quot;
                    )
                    local_progress_bar.update(1)
                    continue
                # Load the image
                attempt_count = 0
                while attempt_count &lt; 3:
                    try:
                        # tq.write(f&quot;Loading image from {task[&apos;URL&apos;]}.&quot;)
                        response = requests.get(task[&quot;URL&quot;], timeout=10)
                        image = Image.open(io.BytesIO(response.content)).convert(&quot;RGB&quot;)
                        break
                    except Exception as e:
                        tq.write(
                            f&apos;-&gt; [error] Could not load image from {task[&quot;URL&quot;]}. {&quot;Retrying...&quot; if attempt_count &lt; 2 else &quot;Dropping sample.&quot;}&apos;
                        )
                        # Upload the error using endpoint?action=submit_job&amp;job_id=data_id&amp;error=message&amp;status=error
                        requests.post(
                            f&quot;{args.backend_url}/?action=submit_job&quot;,
                            params={
                                &quot;client_id&quot;: args.client_id,
                                &quot;secret&quot;: args.secret,
                                &quot;error&quot;: e,
                                &quot;job_id&quot;: {task[&quot;data_id&quot;]},
                            },
                        )
                        attempt_count += 1
                        time.sleep(5)
                if attempt_count == 3:
                    tq.write(f&quot;-&gt; [error] Failed to load image from {task[&apos;URL&apos;]}.&quot;)
                    local_progress_bar.update(1)
                    continue
                if &quot;job_type&quot; not in task:
                    tq.write(f&quot;Received invalid task: {task}. Skipping.&quot;)
                    continue
                if task[&quot;job_type&quot;] == &quot;caption&quot;:
                    # Generate the caption
                    caption_source = &quot;cogvlm&quot;
                    if args.eval_backend == &quot;transformers&quot;:
                        caption = eval_image(
                            image, model, tokenizer, torch_dtype, args.query_str
                        )
                    elif args.eval_backend == &quot;oobabooga&quot;:
                        # only really llava is supported by oobabooga, so we will assume here.
                        caption_source = &quot;llava&quot;
                        image_to_bytes = io.BytesIO()
                        image.save(image_to_bytes, format=&quot;JPEG&quot;)
                        image_to_bytes = image_to_bytes.getvalue()
                        caption = eval_image_with_ooba(image_to_bytes, args.query_str)
                    # Upload the caption using endpoint?action=submit_job&amp;job_id=data_id&amp;result=caption&amp;status=success
                    submission_response = requests.post(
                        f&quot;{args.backend_url}/?action=submit_job&quot;,
                        params={
                            &quot;result&quot;: caption,
                            &quot;job_id&quot;: task[&quot;data_id&quot;],
                            &quot;client_id&quot;: args.client_id,
                            &quot;secret&quot;: args.secret,
                            &quot;caption_source&quot;: caption_source,
                            &quot;status&quot;: &quot;success&quot;,
                        },
                    )
                elif task[&quot;job_type&quot;] == &quot;vae&quot;:
                    (
                        target_width,
                        target_height,
                    ) = calculate_new_size_by_pixel_area(image.width, image.height)
                    # Resize image
                    resized_image = image.resize(
                        (target_width, target_height), resample=Image.LANCZOS
                    )
                    # Generate the latent vector
                    latents = encode_images(
                        accelerator, vae, [resized_image], image_transforms
                    )
                    # Unsqueeze the latents into separate objects:
                    latents = latents[0]
                    # Save the tensor to a BytesIO object (in-memory file)
                    latents_buffer = BytesIO()
                    torch.save(latents, latents_buffer)
                    latents_buffer.seek(
                        0
                    )  # Important: move back to the start of the buffer
                    files = {
                        &quot;result_file&quot;: (
                            &quot;latents.pt&quot;,
                            latents_buffer,
                            &quot;application/octet-stream&quot;,
                        ),
                    }
                    submission_response = requests.post(
                        f&quot;{args.backend_url}/?action=submit_job&quot;,
                        files=files,
                        params={
                            &quot;result&quot;: &quot;encoded latents&quot;,
                            &quot;job_id&quot;: task[&quot;data_id&quot;],
                            &quot;client_id&quot;: args.client_id,
                            &quot;secret&quot;: args.secret,
                            &quot;status&quot;: &quot;success&quot;,
                            &quot;job_type&quot;: &quot;vae&quot;,
                        },
                    )
                    # print(f&quot;Submission response: {submission_response.text}&quot;)
                image.close()
                current_cluster_progress = (
                    task.get(&quot;completed_jobs&quot;, 0) - initial_cluster_progress
                )
                global_progress_bar.n = current_cluster_progress
                local_progress_bar.update(1)
            global_progress_bar.refresh()
        except Exception as e:
            import traceback
            tq.write(f&quot;An error occurred: {e}, traceback: {traceback.format_exc()}&quot;)
            time.sleep(15)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_cogvlm.py">import os, torch, logging, accelerate, re, random, argparse, io
from tqdm.auto import tqdm
from PIL import Image
import requests, boto3
from botocore.config import Config
logger = logging.getLogger(&quot;Captioner&quot;)
def upload_to_s3(s3_client, bucket_name, image_data, object_name):
    try:
        in_memory_file = io.BytesIO()
        # Save PIL image to the bytes buffer
        image_data.save(in_memory_file, format=&quot;PNG&quot;)
        in_memory_file.seek(0)  # Move to the beginning of the buffer
        s3_client.upload_fileobj(
            in_memory_file,
            bucket_name,
            object_name,
        )
    except Exception as e:
        logger.error(f&quot;Error uploading {object_name} to bucket {bucket_name}: {e}&quot;)
        raise
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--multidatabackend_config&quot;,
        type=str,
        required=True,
        help=&quot;Path to the multidatabackend config JSON file.&quot;,
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--output_dir&quot;,
        type=str,
        required=False,
        help=&quot;Directory to save processed images.&quot;,
    )
    parser.add_argument(
        &quot;--target_backend_id&quot;,
        type=str,
        default=None,
        help=(
            &quot;When this is provided, the script will upload the captioned file directly to an S3 backend in your multidatabackend.json file.&quot;
        ),
    )
    parser.add_argument(
        &quot;--batch_size&quot;, type=int, default=16, help=&quot;Batch size for processing.&quot;
    )
    parser.add_argument(
        &quot;--caption_strategy&quot;,
        type=str,
        default=&quot;filename&quot;,
        choices=[&quot;filename&quot;, &quot;text&quot;],
        help=&quot;Strategy for saving captions.&quot;,
    )
    parser.add_argument(
        &quot;--filter_list&quot;,
        type=str,
        default=None,
        help=(
            &quot;Path to a txt file containing terms or sentence fragments to filter out.&quot;
            &quot; These will be removed from the final caption.&quot;
        ),
    )
    parser.add_argument(
        &quot;--save_interval&quot;,
        type=int,
        default=100,
        help=&quot;Interval to save progress (number of files processed).&quot;,
    )
    parser.add_argument(
        &quot;--delete_after_caption&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;Delete *input* image files after captioning.&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;, &quot;fp4&quot;, &quot;fp8&quot;],
        default=&quot;fp4&quot;,
        help=(
            &quot;When loading CogVLM, you can load it in fp16, bf16, fp8 or fp4 precision to reduce memory. Default: fp4&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_filename_cleaning&quot;,
        type=bool,
        default=True,
        help=&quot;Disable filename cleaning. This may result in filenames that are too long for some operating systems, but better captions.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;Caption this image accurately, with as few words as possible.&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--model_path&quot;,
        type=str,
        default=&quot;THUDM/cogvlm2-llama3-chat-19B&quot;,
        help=&quot;Model path to load. Default: THUDM/cogvlm2-llama3-chat-19B, also useful is THUDM/cogvlm-chat-hf to utilise CogVLM v1.1&quot;,
    )
    return parser.parse_args()
def load_filter_list(filter_list_path):
    if filter_list_path and os.path.exists(filter_list_path):
        with open(filter_list_path, &quot;r&quot;) as file:
            return [line.strip() for line in file if line.strip()]
    return []
def eval_image(
    image: Image.Image,
    model,
    tokenizer,
    torch_dtype,
    query: str,
):
    inputs = model.build_conversation_input_ids(
        tokenizer, query=query, history=[], images=[image]
    )  # chat mode
    inputs = {
        &quot;input_ids&quot;: inputs[&quot;input_ids&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;token_type_ids&quot;: inputs[&quot;token_type_ids&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;attention_mask&quot;: inputs[&quot;attention_mask&quot;].unsqueeze(0).to(&quot;cuda&quot;),
        &quot;images&quot;: [[inputs[&quot;images&quot;][0].to(&quot;cuda&quot;).to(torch_dtype)]],
    }
    gen_kwargs = {&quot;max_new_tokens&quot;: 77, &quot;do_sample&quot;: False}
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
        outputs = outputs[:, inputs[&quot;input_ids&quot;].shape[1] :]
        return tokenizer.decode(outputs[0])
def content_to_filename(content, filter_terms, disable_filename_cleaning: bool = False):
    &quot;&quot;&quot;
    Function to convert content to filename by stripping specified terms,
    replacing non-alphanumeric characters and spaces, converting to lowercase,
    removing leading/trailing underscores, and limiting filename length to 230.
    &quot;&quot;&quot;
    cleaned_content = content.replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;)
    if disable_filename_cleaning:
        return f&quot;{cleaned_content}.png&quot;
    for term in filter_terms:
        cleaned_content = cleaned_content.replace(term, &quot;&quot;)
    # Split on &apos;--&apos; and take the first part
    cleaned_content = cleaned_content.split(&quot;--&quot;)[0]
    # Remove URLs
    cleaned_content = re.sub(r&quot;https*://\S*&quot;, &quot;&quot;, content)
    cleaned_content = cleaned_content.replace(&quot;The image showcases a &quot;, &quot;&quot;)
    cleaned_content = cleaned_content.replace(&quot; appears to be&quot;, &quot;&quot;)
    cleaned_content = cleaned_content.replace(&quot; its &quot;, &quot; &quot;)
    # Remove &lt;/s&gt;
    # Replace non-alphanumeric characters and spaces, convert to lowercase, remove leading/trailing underscores
    cleaned_content = re.sub(r&quot;[^a-zA-Z0-9 ]&quot;, &quot;&quot;, cleaned_content)
    cleaned_content = cleaned_content.replace(&quot; &quot;, &quot;_&quot;).lower().strip(&quot;_&quot;)
    # If cleaned_content is empty after removing URLs, generate a random filename
    if cleaned_content == &quot;&quot;:
        cleaned_content = f&quot;midjourney_{random.randint(0, 1000000)}&quot;
    # Limit filename length to 230
    cleaned_content = (
        cleaned_content[:230] if len(cleaned_content) &gt; 230 else cleaned_content
    )
    return cleaned_content + &quot;.png&quot;
def process_directory(
    args,
    image_dir,
    output_dir,
    model,
    tokenizer,
    processed_files,
    caption_strategy,
    save_interval,
    progress_file,
    filter_terms,
    torch_dtype,
    query_str: str,
):
    processed_file_counter = 0
    bucket_name = None
    if args.target_backend_id:
        if not args.multidatabackend_config:
            raise ValueError(
                &quot;A --multidatabackend_config must be provided when --target_backend_id is provided.&quot;
            )
        # Load the config
        import json
        with open(args.multidatabackend_config) as file:
            multidatabackend_config = json.load(file)
        for backend in multidatabackend_config:
            if backend[&quot;id&quot;] == args.target_backend_id and backend[&quot;type&quot;] != &quot;aws&quot;:
                raise ValueError(
                    &quot;Only S3 backends are supported for --target_backend_id.&quot;
                )
            elif backend[&quot;id&quot;] == args.target_backend_id:
                config = backend
                bucket_name = config[&quot;aws_bucket_name&quot;]
                break
        if not bucket_name:
            raise ValueError(
                f&quot;The backend id &apos;{args.target_backend_id}&apos; was not found in the multidatabackend config.&quot;
            )
        s3_config = Config(max_pool_connections=100)
        s3_client = boto3.client(
            &quot;s3&quot;,
            endpoint_url=backend[&quot;aws_endpoint_url&quot;],
            region_name=backend.get(&quot;aws_region_name&quot;, None),
            aws_access_key_id=backend[&quot;aws_access_key_id&quot;],
            aws_secret_access_key=backend[&quot;aws_secret_access_key&quot;],
            config=s3_config,
        )
    for filename in tqdm(
        os.listdir(image_dir),
        desc=f&quot;Processing directory {image_dir}&quot;,
        unit=&quot;images&quot;,
        leave=True,
        position=0,
        mininterval=0.5,
    ):
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            process_directory(
                args,
                full_filepath,
                output_dir,
                model,
                tokenizer,
                processed_files,
                caption_strategy,
                save_interval,
                progress_file,
                filter_terms,
                torch_dtype,
                query_str,
            )
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
            if filename in processed_files:
                # Remove the original file if args.delete_after_caption
                if args.delete_after_caption:
                    os.remove(full_filepath)
                continue
            try:
                with Image.open(full_filepath) as image:
                    # Convert to RGB
                    image = image.convert(&quot;RGB&quot;)
                    best_match = eval_image(
                        image, model, tokenizer, torch_dtype, query_str
                    )
                    logging.debug(f&quot;Best match for {filename}: {best_match}&quot;)
                    # Save based on caption strategy
                    new_filename = (
                        content_to_filename(
                            best_match, filter_terms, args.disable_filename_cleaning
                        )
                        if caption_strategy == &quot;filename&quot;
                        else filename
                    )
                    if args.output_dir:
                        new_filepath = os.path.join(output_dir, new_filename)
                        # Ensure no overwriting
                        counter = 1
                        while os.path.exists(new_filepath):
                            new_filepath = os.path.join(
                                output_dir,
                                f&quot;{new_filename.rsplit(&apos;.&apos;, 1)[0]}_{counter}.{new_filename.rsplit(&apos;.&apos;, 1)[1]}&quot;,
                            )
                            counter += 1
                        image.save(new_filepath)
                    else:
                        new_filepath = full_filepath
                    if args.target_backend_id:
                        upload_to_s3(s3_client, bucket_name, image, new_filename)
                    # Remove the original file if args.delete_after_caption
                    if args.delete_after_caption:
                        os.remove(full_filepath)
                if caption_strategy == &quot;text&quot;:
                    with open(new_filepath + &quot;.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                        f.write(best_match)
            except Exception as e:
                import traceback
                logging.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
        processed_files.add(filename)
        processed_file_counter += 1
        # Save progress at specified intervals
        if processed_file_counter % save_interval == 0:
            with open(progress_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                f.writelines(&quot;\n&quot;.join(processed_files))
    # Save remaining progress
    with open(progress_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.writelines(&quot;\n&quot;.join(processed_files))
def main():
    args = parse_args()
    send_to_cuda = load_in_4bit = load_in_8bit = False
    torch_dtype = torch.bfloat16
    if args.precision == &quot;bf16&quot; or args.precision == &quot;fp16&quot;:
        send_to_cuda = True
        if args.precision == &quot;bf16&quot;:
            torch_dtype = torch.bfloat16
        else:
            torch_dtype = torch.float16
    elif args.precision == &quot;fp4&quot;:
        load_in_4bit = True
    elif args.precision == &quot;fp8&quot;:
        load_in_8bit = True
    logging.basicConfig(level=logging.INFO)
    # Ensure output directory exists
    if args.output_dir and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    logger.info(&quot;Loading CogVLM model. This should only occur once.&quot;)
    from transformers import AutoModelForCausalLM, LlamaTokenizer, AutoTokenizer
    logger.info(f&quot;Loading CogVLM in {args.precision} precision.&quot;)
    if &quot;cogvlm2&quot; in args.model_path and torch.backends.mps.is_available():
        logger.warning(
            &quot;Can not run CogVLM 2 on MPS because Triton is unavailable. Falling back to CogVLM 1.1&quot;
        )
    elif &quot;cogvlm2&quot; in args.model_path:
        import sysconfig
        print(sysconfig.get_paths()[&quot;include&quot;])
        tokenizer = AutoTokenizer.from_pretrained(
            args.model_path, trust_remote_code=True
        )
    else:
        tokenizer = LlamaTokenizer.from_pretrained(&quot;lmsys/vicuna-7b-v1.5&quot;)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch_dtype,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        load_in_4bit=load_in_4bit,
        load_in_8bit=load_in_8bit,
    ).eval()
    if send_to_cuda:
        logger.info(f&quot;Sending model to CUDA.&quot;)
        model.to(&quot;cuda&quot;)
    logger.info(&quot;Completed loading model.&quot;)
    processed_files = set()
    progress_file = os.path.join(args.input_dir, &quot;processed_files.txt&quot;)
    # Load progress if exists
    if os.path.exists(progress_file):
        with open(progress_file, &quot;r&quot;) as f:
            processed_files = set(f.read().splitlines())
    # Load a list of &quot;filter terms&quot;. This is a text file that might look like:
    #
    # The image showcases a
    # closeup image of a
    # anotherterm
    #
    # Each filter term should be on its own line. The entire contents of the row will be removed.
    filter_terms = load_filter_list(args.filter_list)
    # Process directory
    process_directory(
        args,
        args.input_dir,
        args.output_dir,
        model,
        tokenizer,
        processed_files,
        args.caption_strategy,
        args.save_interval,
        progress_file,
        filter_terms,
        torch_dtype,
        args.query_str,
    )
    # Save progress
    with open(progress_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        f.writelines(&quot;\n&quot;.join(processed_files))
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_florence.py">import os
import logging
import re
import random
import argparse
import base64
import torch
from PIL import Image
from tqdm import tqdm
import requests
import io
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
)
import pandas as pd
import torch.nn as nn
logger = logging.getLogger(&quot;Captioner&quot;)
# load the existing parquet file if it exists
def load_input_parquet(parquet_path: str):
    df = pd.read_parquet(path=parquet_path)
    return df
# Load Florence model
def load_model(model_name_or_path=&quot;microsoft/Florence-2-large-ft&quot;):
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,  # cache_dir=&quot;/home/user/storage/hf_cache&quot;
    )
    processor = AutoProcessor.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,  # cache_dir=&quot;/home/user/storage/hf_cache&quot;
    )
    return (
        model.to(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cuda&quot;),
        processor,
    )
# Function to evaluate BLIP3 model
def eval_model(args, image, model, processor, task=&quot;&lt;MORE_DETAILED_CAPTION&gt;&quot;):
    inputs = processor(
        text=f&quot;{task}{args.query_str}&quot;, images=image, return_tensors=&quot;pt&quot;
    )
    generated_ids = model.generate(
        input_ids=inputs[&quot;input_ids&quot;].to(model.device),
        pixel_values=inputs[&quot;pixel_values&quot;].to(model.device, dtype=model.dtype),
        max_new_tokens=args.max_new_tokens,
        do_sample=args.do_sample,
        num_beams=3,
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    prediction = processor.post_process_generation(
        generated_text,
        task=&quot;&lt;MORE_DETAILED_CAPTION&gt;&quot;,
        image_size=(image.width, image.height),
    )
    # If it doesn&apos;t end on a complete sentence, remove everything after the final &apos;.&apos;
    # if not prediction.endswith(&quot;.&quot;):
    #     # Remove everything after the final &apos;.&apos;
    #     prediction = re.sub(r&quot;\.[^.]*$&quot;, &quot;.&quot;, prediction)
    return prediction[task]
def process_and_evaluate_image(args, image_path: str, model, image_processor):
    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):
        response = requests.get(image_path)
        image = Image.open(io.BytesIO(response.content))
    else:
        image = Image.open(image_path)
    result = eval_model(
        args,
        image,
        model,
        image_processor,
    )
    logger.info(f&quot;Result for captioning: {result}&quot;)
    return result
def process_directory(
    args,
    image_dir,
    output_parquet,
    model,
    image_processor,
    input_parquet=None,
    original_query_str=None,
    total_to_process: int = None,
):
    records = []
    parquet_path = f&quot;{output_parquet}.{os.path.basename(image_dir)}.parquet&quot;
    print(f&quot;Parquet: {parquet_path}&quot;)
    total_processed = 0
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        if input_parquet is not None:
            # if the caption column at the filename position is non-empty, skip
            current_caption = input_parquet[input_parquet[&quot;filename&quot;] == filename]
            hint_column = args.input_parquet_hint_column
            hint_value = None
            if hint_column is not None and hint_column != &quot;&quot;:
                try:
                    hint_value = current_caption[hint_column].values[0]
                except:
                    hint_value = None
                if hint_value is not None and not hint_value == &quot;&quot;:
                    if original_query_str is not None:
                        args.query_str = original_query_str
                    args.query_str = args.query_str.replace(&quot;%s&quot;, hint_value)
                    logger.info(
                        f&quot;Using query string: {args.query_str} for hint value: {hint_value}&quot;
                    )
            try:
                if (
                    not current_caption.empty
                    and not current_caption[&quot;caption&quot;].isnull().values[0]
                ):
                    logger.debug(f&quot;Already has caption: {current_caption[&apos;caption&apos;]}&quot;)
                    continue
            except:
                logger.debug(f&quot;Error checking for existing caption: {current_caption}&quot;)
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logger.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(
                args,
                full_filepath,
                output_parquet,
                model,
                image_processor,
                input_parquet=input_parquet,
                original_query_str=original_query_str,
            )
            args.query_str = original_query_str
            original_query_str = None
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
            try:
                logger.debug(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logger.debug(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(
                        args, full_filepath, model, image_processor
                    )
                    total_processed += 1
                    logger.debug(f&quot;Best match for {filename}: {best_match}&quot;)
                    # with Image.open(full_filepath) as img_file:
                    #     image_bytes = img_file.tobytes()
                    records.append({&quot;filename&quot;: filename, &quot;caption&quot;: best_match})
                    if (
                        total_to_process is not None
                        and total_processed &gt;= total_to_process
                    ):
                        break
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
                if &quot;CUDA error&quot; in str(e):
                    import sys
                    sys.exit(1)
    new_df = pd.DataFrame(records)
    if input_parquet is not None:
        # Merge new_df with input_parquet
        input_parquet.set_index(&quot;filename&quot;, inplace=True)
        new_df.set_index(&quot;filename&quot;, inplace=True)
        combined_df = input_parquet.combine_first(new_df).reset_index()
    else:
        combined_df = new_df
    # reduce duplicates by &quot;filename&quot; contents
    combined_df = combined_df.drop_duplicates(subset=[&quot;filename&quot;])
    combined_df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
    logger.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--model_name&quot;,
        type=str,
        default=&quot;microsoft/Florence-2-large-ft&quot;,
        help=&quot;Model name to use for captioning.&quot;,
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave. Not normally needed for Florence&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;],
        default=&quot;fp16&quot;,
        help=(&quot;Precision for loading the model. Default: fp16&quot;),
    )
    parser.add_argument(
        &quot;--input_parquet&quot;,
        type=str,
        default=None,
        help=&quot;Path to the input Parquet dataset which will be adjusted to have the new column.&quot;,
    )
    parser.add_argument(
        &quot;--input_parquet_hint_column&quot;,
        type=str,
        default=&quot;title&quot;,
        help=&quot;When set, the column to use as a hint for the input query str placement value. Default: title&quot;,
    )
    parser.add_argument(
        &quot;--max_new_tokens&quot;,
        type=int,
        default=1024,
        help=&quot;The maximum number of tokens to generate. Default: 1024&quot;,
    )
    parser.add_argument(
        &quot;--do_sample&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Whether to use sampling for generation. Makes model more responsive to input prompts.&quot;
            &quot; If not set, greedy decoding is used. Default: False&quot;
        ),
    )
    args = parser.parse_args()
    return args
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    input_database = None
    if args.input_parquet:
        if not os.path.exists(args.input_parquet):
            raise ValueError(&quot;The parquet file specified as input did not exist.&quot;)
        input_database = load_input_parquet(args.input_parquet)
    model, image_processor = load_model(args.model_name)
    process_directory(
        args,
        args.input_dir,
        args.output_parquet,
        model,
        image_processor,
        input_parquet=input_database,
        original_query_str=str(args.query_str),
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_gemini.py">import os
import logging
import argparse
import re
import random
import base64
import requests
import time
import io
import pandas as pd
import google.generativeai as genai
from PIL import Image
from tqdm import tqdm
import pyarrow.parquet as pq
logger = logging.getLogger(&quot;Captioner&quot;)
# Configure the API Key for Google Generative AI
genai.configure(api_key=os.environ.get(&quot;GEMINI_API_KEY&quot;))
# Function to upload image and generate caption using Google Generative AI
def generate_caption_with_genai(image_path, query_str: str):
    image_input = genai.upload_file(
        path=image_path,
        mime_type=&quot;image/jpeg&quot;,  # Adjust if your image is a different type
    )
    prompt = f&quot;{query_str}: {image_input.uri}&quot;
    model = genai.GenerativeModel(model_name=&quot;gemini-pro-vision&quot;)
    attempt = 0
    while attempt &lt; 3:
        try:
            caption = model.generate_content([prompt, image_input])
            return caption.text
        except Exception as e:
            logging.error(f&quot;Error generating caption: {e}&quot;)
            attempt += 1
            if attempt &gt;= 3:
                raise Exception(f&quot;Failed to generate caption after 3 attempts: {e}&quot;)
def get_quota_reset_time(headers):
    # Example header: &quot;X-Quota-Reset: 3600&quot; (in seconds)
    print(f&quot;Headers: {headers}&quot;)
    reset_time = headers.get(&quot;X-Quota-Reset&quot;, 3600)
    return int(reset_time)
def calculate_sleep_time(reset_time):
    # Current time in seconds since the epoch
    current_time = int(time.time())
    return max(0, reset_time - current_time)
def process_and_evaluate_image(args, image_path):
    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):
        response = requests.get(image_path)
        image = Image.open(io.BytesIO(response.content))
    else:
        image = Image.open(image_path)
    def resize_for_condition_image(input_image: Image.Image, resolution: int):
        if resolution == 0:
            return input_image
        input_image = input_image.convert(&quot;RGB&quot;)
        W, H = input_image.size
        aspect_ratio = round(W / H, 2)
        if W &lt; H:
            W = resolution
            H = int(resolution / aspect_ratio)
        elif H &lt; W:
            H = resolution
            W = int(resolution * aspect_ratio)
        if W == H:
            W = resolution
            H = resolution
        img = input_image.resize((W, H), resample=Image.LANCZOS)
        return img
    resized_image_path = &quot;/tmp/resized_image.jpg&quot;
    resize_for_condition_image(image, 384).save(resized_image_path)
    result = generate_caption_with_genai(resized_image_path, args.query_str)
    return result
def process_directory(args, image_dir, output_parquet):
    records = []
    parquet_path = f&quot;{output_parquet}.{os.path.basename(image_dir)}.parquet&quot;
    if os.path.exists(parquet_path):
        existing_df = pd.read_parquet(parquet_path)
        processed_files = set(existing_df[&quot;filename&quot;].tolist())
    else:
        existing_df = pd.DataFrame()
        processed_files = set()
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logging.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(args, full_filepath, output_parquet)
        elif (
            filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;))
            and filename not in processed_files
        ):
            try:
                logging.info(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logging.info(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(args, full_filepath)
                    logging.info(f&quot;Best match for {filename}: {best_match}&quot;)
                    records.append({&quot;filename&quot;: filename, &quot;caption&quot;: best_match})
                    if len(records) % 10 == 0:  # Save every 10 records
                        save_parquet(records, parquet_path, existing_df)
            except Exception as e:
                import traceback
                logging.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
                if &quot;check quota&quot; in str(e):
                    import sys
                    sys.exit(1)
    save_parquet(records, parquet_path, existing_df)
    logging.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def save_parquet(records, parquet_path, existing_df):
    df = pd.DataFrame(records)
    if not df.empty:
        combined_df = pd.concat([existing_df, df], ignore_index=True).drop_duplicates(
            subset=[&quot;filename&quot;]
        )
        combined_df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
        logging.info(f&quot;Saved {len(records)} records to {parquet_path}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;Provide the most detailed caption.&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;],
        default=&quot;fp16&quot;,
        help=(&quot;Precision for loading the model. Default: fp16&quot;),
    )
    args = parser.parse_args()
    return args
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    process_directory(args, args.input_dir, args.output_parquet)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_gemma.py">import os, json
import logging
import argparse
import requests
from PIL import Image
from tqdm import tqdm
import pandas as pd
import torch
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
logger = logging.getLogger(&quot;Captioner&quot;)
# Function to load PaliGemma model and processor
def load_pali_gemma_model(args):
    model_id = args.model_path
    model = (
        PaliGemmaForConditionalGeneration.from_pretrained(model_id)
        .to(torch.float32)
        .eval()
    )
    processor = AutoProcessor.from_pretrained(model_id)
    return model, processor
def generate_caption_with_pali_gemma(
    image_path, processor, model, query_strings, do_sample, temperature
):
    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):
        image = Image.open(requests.get(image_path, stream=True).raw)
    else:
        image = Image.open(image_path)
    model_inputs = processor(
        text=query_strings,
        images=[image] * len(query_strings),
        return_tensors=&quot;pt&quot;,
        padding=True,
    )
    input_len = model_inputs[&quot;input_ids&quot;].shape[-1]
    with torch.inference_mode():
        generation = model.generate(
            **model_inputs,
            max_new_tokens=512,
            do_sample=do_sample,
            # temperature=temperature,
            # top_p=0.5,
            # top_k=80
        )
        outputs = []
        for _generation in generation:
            decoded = processor.decode(
                _generation[input_len:], skip_special_tokens=True
            )
            outputs.append(decoded)
    return outputs
def process_and_evaluate_image(args, image_path, model, processor):
    query_strings = [&quot;caption en&quot;]
    result = generate_caption_with_pali_gemma(
        image_path, processor, model, query_strings, do_sample=True, temperature=1.2
    )
    return result[0]
    output = {query: result for query, result in zip(query_strings, result)}
    print(f&quot;Output: {json.dumps(output, indent=4)}&quot;)
    return output
def process_directory(args, image_dir, output_parquet, model, processor):
    records = []
    parquet_path = f&quot;{output_parquet}.{os.path.basename(image_dir)}.parquet&quot;
    print(f&quot;Parquet: {parquet_path}&quot;)
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logging.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(args, full_filepath, output_parquet, model, processor)
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;)):
            try:
                logging.info(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logging.debug(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(
                        args, full_filepath, model, processor
                    )
                    logging.info(f&quot;Best match for {filename}: {best_match}&quot;)
                    with Image.open(full_filepath) as img_file:
                        image_bytes = img_file.tobytes()
                    records.append(
                        {
                            &quot;filename&quot;: filename,
                            &quot;caption&quot;: best_match,
                            &quot;image&quot;: image_bytes,
                        }
                    )
            except Exception as e:
                import traceback
                logging.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
                if &quot;CUDA error&quot; in str(e):
                    import sys
                    sys.exit(1)
    df = pd.DataFrame(records)
    df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
    logging.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;],
        default=&quot;fp16&quot;,
        help=(&quot;Precision for loading the model. Default: fp16&quot;),
    )
    parser.add_argument(
        &quot;--model_path&quot;,
        type=str,
        default=&quot;google/paligemma-3b-mix-224&quot;,
        help=(&quot;Model path to load. Default: google/paligemma-3b-mix-224&quot;),
    )
    args = parser.parse_args()
    return args
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    model, processor = load_pali_gemma_model(args)
    process_directory(args, args.input_dir, args.output_parquet, model, processor)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_gpt4.py">import os
import logging
import requests
import random
import argparse
import base64
from PIL import Image
from tqdm import tqdm
import io
import pandas as pd
logger = logging.getLogger(&quot;Captioner&quot;)
# Function to encode the image
def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
        return base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;)
# Define the prompt template for GPT-4
def apply_prompt_template(prompt):
    return [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}]
# Load the existing parquet file if it exists
def load_input_parquet(parquet_path: str):
    df = pd.read_parquet(path=parquet_path)
    return df
# Function to get image captions from GPT-4 API
def get_image_captions(image_path, api_key, prompt):
    base64_image = encode_image(image_path)
    headers = {&quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;}
    payload = {
        &quot;model&quot;: &quot;gpt-4o&quot;,
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: apply_prompt_template(prompt)
                + [
                    {
                        &quot;type&quot;: &quot;image_url&quot;,
                        &quot;image_url&quot;: {&quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;},
                    }
                ],
            }
        ],
        &quot;max_tokens&quot;: 300,
    }
    response = requests.post(
        &quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload
    )
    if response.status_code == 200:
        result = response.json()
        caption = result[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
        return caption
    else:
        logger.error(f&quot;Error {response.status_code}: {response.text}&quot;)
        return &quot;Error in captioning&quot;
def process_and_evaluate_image(args, image_path: str, api_key):
    return get_image_captions(image_path, api_key, args.query_str)
def process_directory(
    args,
    image_dir,
    output_parquet,
    api_key,
    input_parquet=None,
    original_query_str=None,
):
    records = []
    parquet_path = f&quot;{output_parquet}.{os.path.basename(image_dir)}.parquet&quot;
    print(f&quot;Parquet: {parquet_path}&quot;)
    total_to_process = 10000
    total_processed = 0
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        if input_parquet is not None:
            current_caption = input_parquet[input_parquet[&quot;filename&quot;] == filename]
            hint_column = args.input_parquet_hint_column
            hint_value = None
            if hint_column is not None and hint_column != &quot;&quot;:
                try:
                    hint_value = current_caption[hint_column].values[0]
                except:
                    hint_value = None
                if hint_value is not None and not hint_value == &quot;&quot;:
                    if original_query_str is not None:
                        args.query_str = original_query_str
                    args.query_str = args.query_str.replace(&quot;%s&quot;, hint_value)
                    logger.info(
                        f&quot;Using query string: {args.query_str} for hint value: {hint_value}&quot;
                    )
            try:
                if (
                    not current_caption.empty
                    and not current_caption[&quot;caption&quot;].isnull().values[0]
                ):
                    logger.debug(f&quot;Already has caption: {current_caption[&apos;caption&apos;]}&quot;)
                    continue
            except:
                logger.debug(f&quot;Error checking for existing caption: {current_caption}&quot;)
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logger.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(
                args,
                full_filepath,
                output_parquet,
                api_key,
                input_parquet=input_parquet,
                original_query_str=original_query_str,
            )
            args.query_str = original_query_str
            original_query_str = None
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
            try:
                logger.debug(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logger.debug(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(
                        args, full_filepath, api_key
                    )
                    total_processed += 1
                    logger.debug(f&quot;Best match for {filename}: {best_match}&quot;)
                    with Image.open(full_filepath) as img_file:
                        image_bytes = img_file.tobytes()
                    records.append({&quot;filename&quot;: filename, &quot;caption&quot;: best_match})
                    if total_processed &gt;= total_to_process:
                        break
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
    new_df = pd.DataFrame(records)
    if input_parquet is not None:
        input_parquet.set_index(&quot;filename&quot;, inplace=True)
        new_df.set_index(&quot;filename&quot;, inplace=True)
        combined_df = input_parquet.combine_first(new_df).reset_index()
    else:
        combined_df = new_df
    combined_df = combined_df.drop_duplicates(subset=[&quot;filename&quot;])
    combined_df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
    logger.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;What’s in this image?&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--api_key&quot;,
        type=str,
        required=True,
        help=&quot;API key for accessing the GPT-4 API.&quot;,
    )
    parser.add_argument(
        &quot;--input_parquet&quot;,
        type=str,
        default=None,
        help=&quot;Path to the input Parquet dataset which will be adjusted to have the new column.&quot;,
    )
    parser.add_argument(
        &quot;--input_parquet_hint_column&quot;,
        type=str,
        default=&quot;title&quot;,
        help=&quot;When set, the column to use as a hint for the input query str placement value. Default: title&quot;,
    )
    args = parser.parse_args()
    return args
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    input_database = None
    if args.input_parquet:
        if not os.path.exists(args.input_parquet):
            raise ValueError(&quot;The parquet file specified as input did not exist.&quot;)
        input_database = load_input_parquet(args.input_parquet)
    process_directory(
        args,
        args.input_dir,
        args.output_parquet,
        args.api_key,
        input_parquet=input_database,
        original_query_str=str(args.query_str),
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_internvl.py">import os
from pathlib import Path
import logging
import re
import random
import argparse
import base64
import torch
from PIL import Image
from tqdm import tqdm
import requests
import io
import pandas as pd
import torch.nn as nn
import numpy as np
import glob
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    AutoModel,
    AutoTokenizer,
)
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)
device = (
    &quot;cuda&quot;
    if torch.cuda.is_available()
    else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
)
logger = logging.getLogger(&quot;Captioner&quot;)
# load the existing parquet file if it exists
def load_input_parquet(parquet_path: str):
    df = pd.read_parquet(path=parquet_path)
    return df
# Load InterVL2-8B model, only need 24G VRAM,if you wanna to load bigger models like 26B or 72B,you should need 1-3 80G A100
def load_model(model_name_or_path=&quot;OpenGVLab/InternVL2-8B&quot;):
    model = (
        AutoModel.from_pretrained(
            model_name_or_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        .eval()
        .to(device)
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, trust_remote_code=True, use_fast=False
    )
    return (model, tokenizer)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=True, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--model_name&quot;,
        type=str,
        default=&quot;OpenGVLab/InternVL2-8B&quot;,
        help=&quot;Model name to use for captioning.&quot;,
    )
    parser.add_argument(
        &quot;--output_parquet&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave. Not normally needed for InternVL&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;],
        default=&quot;fp16&quot;,
        help=(&quot;Precision for loading the model. Default: fp16&quot;),
    )
    parser.add_argument(
        &quot;--input_parquet&quot;,
        type=str,
        default=None,
        help=&quot;Path to the input Parquet dataset which will be adjusted to have the new column.&quot;,
    )
    parser.add_argument(
        &quot;--input_parquet_hint_column&quot;,
        type=str,
        default=&quot;title&quot;,
        help=&quot;When set, the column to use as a hint for the input query str placement value. Default: title&quot;,
    )
    parser.add_argument(
        &quot;--max_new_tokens&quot;,
        type=int,
        default=1024,
        help=&quot;The maximum number of tokens to generate. Default: 1024&quot;,
    )
    parser.add_argument(
        &quot;--do_sample&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=(
            &quot;Whether to use sampling for generation. Makes model more responsive to input prompts.&quot;
            &quot; If not set, greedy decoding is used. Default: False&quot;
        ),
    )
    args = parser.parse_args()
    return args
def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose(
        [
            T.Lambda(lambda img: img.convert(&quot;RGB&quot;) if img.mode != &quot;RGB&quot; else img),
            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
            T.ToTensor(),
            T.Normalize(mean=MEAN, std=STD),
        ]
    )
    return transform
def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float(&quot;inf&quot;)
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff &lt; best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area &gt; 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio
def dynamic_preprocess(
    image, min_num=1, max_num=12, image_size=448, use_thumbnail=False
):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height
    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j)
        for n in range(min_num, max_num + 1)
        for i in range(1, n + 1)
        for j in range(1, n + 1)
        if i * j &lt;= max_num and i * j &gt;= min_num
    )
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size
    )
    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size,
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images
def load_image(image_file, input_size=448, max_num=12):
    # print(f&quot;Original image filename: {image_file}&quot;)
    image = Image.open(image_file).convert(&quot;RGB&quot;)
    width, height = image.size
    # print(f&quot;Original image size: {image.size}&quot;)
    mode = image.mode
    # print(f&quot;Original image mode: {mode}&quot;)
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(
        image, image_size=input_size, use_thumbnail=True, max_num=max_num
    )
    # print(f&quot;Size after dynamic_preprocess: {images[0].size}&quot;)
    pixel_values = [transform(image) for image in images]
    # print(f&quot;Size after transform: {pixel_values[0].shape}&quot;)
    pixel_values = torch.stack(pixel_values)
    return pixel_values, width, height
def process_directory(
    args,
    image_dir,
    output_parquet,
    model,
    tokenizer,
    max_new_tokens,
    input_parquet=None,
    original_query_str=None,
    total_to_process: int = None,
):
    records = []
    directory_path = Path(image_dir)
    parquet_path = f&quot;{output_parquet}.{directory_path.name}.parquet&quot;
    print(f&quot;Parquet: {parquet_path}&quot;)
    total_processed = 0
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        if input_parquet is not None:
            # if the caption column at the filename position is non-empty, skip
            current_caption = input_parquet[input_parquet[&quot;filename&quot;] == filename]
            hint_column = args.input_parquet_hint_column
            hint_value = None
            if hint_column is not None and hint_column != &quot;&quot;:
                try:
                    hint_value = current_caption[hint_column].values[0]
                except:
                    hint_value = None
                if hint_value is not None and not hint_value == &quot;&quot;:
                    if original_query_str is not None:
                        args.query_str = original_query_str
                    args.query_str = args.query_str.replace(&quot;%s&quot;, hint_value)
                    logger.info(
                        f&quot;Using query string: {args.query_str} for hint value: {hint_value}&quot;
                    )
            try:
                if (
                    not current_caption.empty
                    and not current_caption[&quot;caption&quot;].isnull().values[0]
                ):
                    logger.debug(f&quot;Already has caption: {current_caption[&apos;caption&apos;]}&quot;)
                    continue
            except:
                logger.debug(f&quot;Error checking for existing caption: {current_caption}&quot;)
        full_filepath = os.path.join(image_dir, filename)
        if os.path.isdir(full_filepath):
            logger.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(
                args,
                full_filepath,
                output_parquet,
                model,
                tokenizer,
                input_parquet=input_parquet,
                original_query_str=original_query_str,
            )
            args.query_str = original_query_str
            original_query_str = None
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;, &quot;.jpeg&quot;)):
            try:
                logger.debug(f&quot;Attempting to load image: {filename}&quot;)
                logger.debug(f&quot;Processing image: {filename}&quot;)
                # set the max number of tiles in `max_num`
                pixel_values, width, height = load_image(full_filepath, max_num=12)
                pixel_values, width, height = (
                    pixel_values.to(torch.bfloat16).to(device),
                    width,
                    height,
                )
                generation_config = dict(max_new_tokens=max_new_tokens, do_sample=False)
                question = &quot;&lt;image&gt;\n&quot; + str(original_query_str)
                response = model.chat(
                    tokenizer,
                    pixel_values,
                    question,
                    generation_config,
                    history=None,
                    return_history=False,
                )
                total_processed += 1
                logger.debug(f&quot;Best match for {filename}: {response}&quot;)
                # with Image.open(full_filepath) as img_file:
                #     image_bytes = img_file.tobytes()
                records.append(
                    {
                        &quot;filename&quot;: filename,
                        &quot;caption&quot;: response,
                        &quot;width&quot;: width,
                        &quot;height&quot;: height,
                    }
                )
                if total_to_process is not None and total_processed &gt;= total_to_process:
                    break
            except Exception as e:
                import traceback
                logger.error(
                    f&quot;Error processing {filename}: {str(e)}, traceback: {traceback.format_exc()}&quot;
                )
                if &quot;CUDA error&quot; in str(e):
                    import sys
                    sys.exit(1)
    new_df = pd.DataFrame(records)
    if input_parquet is not None:
        # Merge new_df with input_parquet
        input_parquet.set_index(&quot;filename&quot;, inplace=True)
        new_df.set_index(&quot;filename&quot;, inplace=True)
        combined_df = input_parquet.combine_first(new_df).reset_index()
    else:
        combined_df = new_df
    # reduce duplicates by &quot;filename&quot; contents
    combined_df = combined_df.drop_duplicates(subset=[&quot;filename&quot;])
    combined_df.to_parquet(parquet_path, engine=&quot;pyarrow&quot;)
    logger.info(f&quot;Processed Parquet file saved to {output_parquet}&quot;)
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    input_database = None
    if args.input_parquet:
        if not os.path.exists(args.input_parquet):
            raise ValueError(&quot;The parquet file specified as input did not exist.&quot;)
        input_database = load_input_parquet(args.input_parquet)
    model, tokenizer = load_model(args.model_name)
    process_directory(
        args,
        args.input_dir,
        args.output_parquet,
        model,
        tokenizer,
        max_new_tokens=args.max_new_tokens,
        input_parquet=input_database,
        original_query_str=str(args.query_str),
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/caption_with_llava.py">import os, logging, re, random, argparse, json, torch
from PIL import Image
from tqdm import tqdm
import requests, io
try:
    from transformers import BitsAndBytesConfig
except:
    pass
try:
    from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
except:
    pass
from transformers import AutoProcessor, LlavaForConditionalGeneration
logger = logging.getLogger(&quot;Captioner&quot;)
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Process images and generate captions.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;, type=str, required=False, help=&quot;Directory containing the images.&quot;
    )
    parser.add_argument(
        &quot;--input_parquet&quot;,
        type=str,
        default=None,
        help=&quot;Path to the input Parquet dataset.&quot;,
    )
    parser.add_argument(
        &quot;--parquet_image_column&quot;,
        type=str,
        default=&quot;image_url&quot;,
        help=&quot;Column name in the Parquet file that contains the image URLs.&quot;,
    )
    parser.add_argument(
        &quot;--parquet_target_column&quot;,
        type=str,
        default=&quot;llava_caption&quot;,
        help=&quot;Column name in the Parquet file where the generated captions will be saved.&quot;,
    )
    parser.add_argument(
        &quot;--output_dir&quot;,
        type=str,
        required=False,
        help=&quot;Directory to save processed images.&quot;,
    )
    parser.add_argument(
        &quot;--batch_size&quot;, type=int, default=16, help=&quot;Batch size for processing.&quot;
    )
    parser.add_argument(
        &quot;--caption_strategy&quot;,
        type=str,
        default=&quot;filename&quot;,
        choices=[&quot;filename&quot;, &quot;text&quot;],
        help=&quot;Strategy for saving captions.&quot;,
    )
    parser.add_argument(
        &quot;--filter_list&quot;,
        type=str,
        default=None,
        help=(
            &quot;Path to a txt file containing terms or sentence fragments to filter out.&quot;
            &quot; These will be removed from the final caption.&quot;
        ),
    )
    parser.add_argument(
        &quot;--save_interval&quot;,
        type=int,
        default=100,
        help=&quot;Interval to save progress (number of files processed).&quot;,
    )
    parser.add_argument(
        &quot;--delete_after_caption&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;Delete *input* image files after captioning.&quot;,
    )
    parser.add_argument(
        &quot;--precision&quot;,
        type=str,
        choices=[&quot;bf16&quot;, &quot;fp16&quot;, &quot;fp4&quot;, &quot;fp8&quot;],
        default=&quot;fp4&quot;,
        help=(
            &quot;When loading CogVLM, you can load it in fp16, bf16, fp8 or fp4 precision to reduce memory. Default: fp4&quot;
        ),
    )
    parser.add_argument(
        &quot;--disable_filename_cleaning&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;Disable filename cleaning. This may result in filenames that are too long for some operating systems, but better captions.&quot;,
    )
    parser.add_argument(
        &quot;--query_str&quot;,
        type=str,
        default=&quot;Caption this image accurately, with as few words as possible.&quot;,
        help=&quot;The query string to use for captioning. This instructs the model how to behave.&quot;,
    )
    parser.add_argument(
        &quot;--model_path&quot;, type=str, default=&quot;llava-hf/llava-1.5-7b-hf&quot;, help=&quot;Model path&quot;
    )
    parser.add_argument(
        &quot;--progress_file&quot;,
        type=str,
        default=&quot;progress.json&quot;,
        help=&quot;File to save progress&quot;,
    )
    parser.add_argument(
        &quot;--max_new_tokens&quot;,
        type=int,
        default=90,
        help=(
            &quot;The maximum number of tokens a stable diffusion model can reasonably use is 77.&quot;
            &quot; This allows returning longer than 77 token long captions, though their utility may be reduced.&quot;
        ),
    )
    args = parser.parse_args()
    if not args.input_dir and not args.input_parquet:
        parser.error(&quot;Either --input_dir or --input_parquet must be provided.&quot;)
    if args.input_dir and args.input_parquet:
        parser.error(&quot;Only one of --input_dir or --input_parquet can be provided.&quot;)
    if args.output_dir and args.input_parquet:
        parser.error(
            &quot;--output_dir cannot be provided when --input_parquet is provided.&quot;
        )
    if not args.output_dir and args.input_dir:
        parser.error(&quot;--output_dir must be provided when --input_dir is provided.&quot;)
    return parser.parse_args()
import pandas as pd
def process_parquet_dataset(args, model, processor):
    df = pd.read_parquet(args.input_parquet)
    if args.parquet_image_column not in df.columns:
        logger.error(
            f&quot;Image column &apos;{args.parquet_image_column}&apos; not found in the Parquet file.&quot;
        )
        return
    if args.parquet_target_column not in df.columns:
        df[args.parquet_target_column] = &quot;&quot;
    for index, row in tqdm(
        df.iterrows(), total=df.shape[0], desc=&quot;Processing Parquet Dataset&quot;
    ):
        image_url = row[args.parquet_image_column]
        if not image_url:
            continue
        if row[args.parquet_target_column]:
            continue
        try:
            caption = process_and_evaluate_image(args, image_url, model, processor)
            df.at[index, args.parquet_target_column] = caption
            # Save the DataFrame back to a Parquet file
            output_parquet_path = (
                args.input_parquet.rsplit(&quot;.&quot;, 1)[0] + &quot;_captioned.parquet&quot;
            )
            df.to_parquet(output_parquet_path, engine=&quot;pyarrow&quot;)
        except Exception as e:
            logger.error(f&quot;Error processing image at {image_url}: {str(e)}&quot;)
    logger.info(f&quot;Processed Parquet file saved to {output_parquet_path}&quot;)
# Function to load LLaVA model
def load_llava_model(
    model_path: str = &quot;llava-hf/llava-1.5-7b-hf&quot;, precision: str = &quot;fp4&quot;
):
    try:
        bnb_config = BitsAndBytesConfig()
        if precision == &quot;fp4&quot;:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type=&quot;nf4&quot;,
                bnb_4bit_compute_dtype=torch.bfloat16,
            )
        elif precision == &quot;fp8&quot;:
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=True,
            )
        else:
            bnb_config = None
        torch_dtype = None
    except:
        bnb_config = None
        torch_dtype = torch.float16
    if &quot;1.6&quot; in model_path:
        logger.info(&quot;Using LLaVA 1.6+ model.&quot;)
        model = LlavaNextForConditionalGeneration.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            torch_dtype=torch_dtype,
            device_map=&quot;mps&quot; if torch.backends.mps.is_available() else &quot;auto&quot;,
        )
    else:
        logger.info(&quot;Using LLaVA 1.5 model.&quot;)
        model = LlavaForConditionalGeneration.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            torch_dtype=torch_dtype,
            device_map=&quot;auto&quot;,
        )
    if &quot;1.6&quot; in model_path:
        logger.info(&quot;Using LLaVA 1.6+ model processor.&quot;)
        autoprocessor_cls = LlavaNextProcessor
    else:
        logger.info(&quot;Using LLaVA 1.5 model processor.&quot;)
        autoprocessor_cls = AutoProcessor
    processor = autoprocessor_cls.from_pretrained(model_path)
    return model, processor
# Function to evaluate the model
def eval_model(args, image_file, model, processor):
    if type(processor) == LlavaNextProcessor:
        logging.info(&quot;Using LLaVA 1.6+ model.&quot;)
        prompt = f&quot;&lt;|im_start|&gt;system\nAnswer the question carefully, without speculation.&lt;|im_end|&gt;&lt;|im_start|&gt;user\n&lt;image&gt;\n{args.query_str}&lt;|im_end|&gt;&lt;|im_start|&gt;assistant\n&quot;
        inputs = processor(text=prompt, images=image_file, return_tensors=&quot;pt&quot;).to(
            &quot;cuda&quot;
            if torch.cuda.is_available()
            else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
        )
        logging.info(f&quot;Inputs: {inputs}&quot;)
    else:
        prompt = f&quot;&lt;image&gt;\nUSER: {args.query_str}\nASSISTANT:&quot;
        images = [image_file]
        inputs = processor(text=prompt, images=images, return_tensors=&quot;pt&quot;).to(
            &quot;cuda&quot;
            if torch.cuda.is_available()
            else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
        )
    with torch.inference_mode():
        generate_ids = model.generate(
            **inputs,
            max_length=args.max_new_tokens,
        )
    outputs = processor.batch_decode(
        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]
    # Pull everything after &quot;ASSISTANT&quot;:
    outputs = outputs.split(&quot;ASSISTANT:&quot;)[1].strip()
    return outputs
def process_and_evaluate_image(args, image_path: str, model, processor):
    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):
        response = requests.get(image_path)
        image = Image.open(io.BytesIO(response.content))
    else:
        image = Image.open(image_path)
    def resize_for_condition_image(input_image: Image.Image, resolution: int):
        if resolution == 0:
            return input_image
        input_image = input_image.convert(&quot;RGB&quot;)
        W, H = input_image.size
        aspect_ratio = round(W / H, 2)
        msg = f&quot;Inspecting image of aspect {aspect_ratio} and size {W}x{H} to &quot;
        if W &lt; H:
            W = resolution
            H = int(resolution / aspect_ratio)  # Calculate the new height
        elif H &lt; W:
            H = resolution
            W = int(resolution * aspect_ratio)  # Calculate the new width
        if W == H:
            W = resolution
            H = resolution
        msg = f&quot;{msg} {W}x{H}.&quot;
        logger.debug(msg)
        img = input_image.resize((W, H), resample=Image.LANCZOS)
        return img
    result = eval_model(args, resize_for_condition_image(image, 256), model, processor)
    print(f&quot;Result for captioning: {result}&quot;)
    return result
# Function to convert content to filename
def content_to_filename(args, content):
    &quot;&quot;&quot;
    Function to convert content to filename by stripping everything after &apos;--&apos;,
    replacing non-alphanumeric characters and spaces, converting to lowercase,
    removing leading/trailing underscores, and limiting filename length to 128.
    &quot;&quot;&quot;
    # If --disable_filename_cleaning is provided, we just append &quot;.png&quot;:
    if args.disable_filename_cleaning:
        return f&quot;{content}.png&quot;
    # Split on &apos;--&apos; and take the first part
    content = content.split(&quot;--&quot;)[0]
    # Remove URLs
    cleaned_content = re.sub(r&quot;https*://\S*&quot;, &quot;&quot;, content)
    # Replace non-alphanumeric characters and spaces, convert to lowercase, remove leading/trailing underscores
    # cleaned_content = re.sub(r&quot;[^a-zA-Z0-9 ]&quot;, &quot;&quot;, cleaned_content)
    # cleaned_content = cleaned_content.replace(&quot; &quot;, &quot;_&quot;).lower().strip(&quot;_&quot;)
    # If cleaned_content is empty after removing URLs, generate a random filename
    if cleaned_content == &quot;&quot;:
        cleaned_content = f&quot;midjourney_{random.randint(0, 1000000)}&quot;
    # Limit filename length to 128
    cleaned_content = (
        cleaned_content[:128] if len(cleaned_content) &gt; 128 else cleaned_content
    )
    return cleaned_content + &quot;.png&quot;
# Main processing function with progress saving
def process_directory(args, image_dir, output_dir, progress_file, model, processor):
    # Load progress if exists
    if os.path.exists(progress_file):
        with open(progress_file, &quot;r&quot;) as file:
            processed_files = json.load(file)
    else:
        processed_files = {}
    for filename in tqdm(os.listdir(image_dir), desc=&quot;Processing Images&quot;):
        full_filepath = os.path.join(image_dir, filename)
        if image_dir in processed_files and filename in processed_files[image_dir]:
            logging.info(f&quot;File has already been processed: {filename}&quot;)
            continue
        if os.path.isdir(full_filepath):
            logging.info(f&quot;Found directory to traverse: {full_filepath}&quot;)
            process_directory(
                args, full_filepath, output_dir, progress_file, model, processor
            )  # Recursive call for directories
        elif filename.lower().endswith((&quot;.jpg&quot;, &quot;.png&quot;)):
            try:
                logging.info(f&quot;Attempting to load image: {filename}&quot;)
                with Image.open(full_filepath) as image:
                    logging.info(f&quot;Processing image: {filename}, data: {image}&quot;)
                    best_match = process_and_evaluate_image(
                        args, full_filepath, model, processor
                    )
                    logging.info(f&quot;Best match for {filename}: {best_match}&quot;)
                    # Save based on caption strategy
                    new_filename = (
                        content_to_filename(args, best_match)
                        if args.caption_strategy == &quot;filename&quot;
                        else filename
                    )
                    new_filepath = os.path.join(output_dir, new_filename)
                    # Ensure no overwriting
                    counter = 1
                    while os.path.exists(new_filepath):
                        new_filepath = os.path.join(
                            output_dir,
                            f&quot;{new_filename.rsplit(&apos;.&apos;, 1)[0]}_{counter}.{new_filename.rsplit(&apos;.&apos;, 1)[1]}&quot;,
                        )
                        counter += 1
                    image.save(new_filepath)
                if args.caption_strategy == &quot;text&quot;:
                    with open(new_filepath + &quot;.txt&quot;, &quot;w&quot;) as f:
                        f.write(best_match)
                # Update progress
                if image_dir not in processed_files:
                    processed_files[image_dir] = {}
                processed_files[image_dir][filename] = best_match
                with open(progress_file, &quot;w&quot;) as file:
                    json.dump(processed_files, file)
            except Exception as e:
                logging.error(f&quot;Error processing {filename}: {str(e)}&quot;)
                if &quot;CUDA error&quot; in str(e):
                    import sys
                    sys.exit(1)
                if &quot;name too long&quot; in str(e):
                    # Loop and try to reduce the filename length until it works:
                    exception_error = str(e)
                    while &quot;name too long&quot; in exception_error:
                        # Cut the word down by one character:
                        new_filename = new_filename[:-1]
                        try:
                            new_filepath = os.path.join(output_dir, new_filename)
                            # Try to save again
                            image.save(new_filepath)
                            exception_error = &quot;&quot;
                        except Exception as e:
                            exception_error = str(e)
def main():
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    # Ensure output directory exists
    if args.output_dir is not None and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    # Load model
    model, processor = load_llava_model(args.model_path, args.precision)
    if args.input_parquet:
        # Process Parquet dataset
        process_parquet_dataset(args, model, processor)
    else:
        # Process directory
        process_directory(
            args,
            args.input_dir,
            args.output_dir,
            args.progress_file,
            model,
            processor,
        )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/captioning/composer.json">{
    &quot;require&quot;: {
        &quot;aws/aws-sdk-php&quot;: &quot;^3.300&quot;
    }
}</file><file path="toolkit/datasets/controlnet/create_canny_edge.py">import os
import cv2
from PIL import Image
def generate_canny_edge_dataset(input_dir, output_dir_original, output_dir_edges):
    # Create output directories if they do not exist
    if not os.path.exists(output_dir_original):
        os.makedirs(output_dir_original)
    if not os.path.exists(output_dir_edges):
        os.makedirs(output_dir_edges)
    # Process each image in the input directory
    for filename in os.listdir(input_dir):
        if filename.lower().endswith((&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;)):
            image_path = os.path.join(input_dir, filename)
            original_image = Image.open(image_path)
            original_image.save(os.path.join(output_dir_original, filename))
            # Read image in grayscale
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            # Apply Canny edge detection
            edges = cv2.Canny(image, 100, 200)
            # Save edge image
            edge_image_path = os.path.join(output_dir_edges, filename)
            cv2.imwrite(edge_image_path, edges)
            print(f&quot;Processed {filename}&quot;)
if __name__ == &quot;__main__&quot;:
    input_dir = (
        &quot;/Volumes/ml/datasets/animals/antelope&quot;  # Update this to your folder path
    )
    output_dir_original = &quot;/Volumes/ml/datasets/canny-edge/animals/antelope-data&quot;  # Update this to your desired output path for originals
    output_dir_edges = &quot;/Volumes/ml/datasets/canny-edge/animals/antelope-conditioning&quot;  # Update this to your desired output path for edges
    generate_canny_edge_dataset(input_dir, output_dir_original, output_dir_edges)</file><file path="toolkit/datasets/masked_loss/generate_dataset_masks_via_huggingface.py">import argparse
import os
import shutil
from gradio_client import Client, handle_file
def main():
    # Set up argument parser
    parser = argparse.ArgumentParser(
        description=&quot;Mask images in a directory using Florence SAM Masking.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;,
        type=str,
        required=True,
        help=&quot;Path to the input directory containing images.&quot;,
    )
    parser.add_argument(
        &quot;--output_dir&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output directory to save masked images.&quot;,
    )
    parser.add_argument(
        &quot;--text_input&quot;,
        type=str,
        default=&quot;person&quot;,
        help=&apos;Text prompt for masking (default: &quot;person&quot;).&apos;,
    )
    parser.add_argument(
        &quot;--model&quot;,
        type=str,
        default=&quot;SkalskiP/florence-sam-masking&quot;,
        help=&apos;Model name to use (default: &quot;SkalskiP/florence-sam-masking&quot;).&apos;,
    )
    args = parser.parse_args()
    input_path = args.input_dir
    output_path = args.output_dir
    text_input = args.text_input
    model_name = args.model
    # Create the output directory if it doesn&apos;t exist
    os.makedirs(output_path, exist_ok=True)
    # Initialize the Gradio client
    client = Client(model_name)
    # Get all files in the input directory
    files = os.listdir(input_path)
    # Iterate over all files
    for file in files:
        # Construct the full file path
        full_path = os.path.join(input_path, file)
        # Check if the file is an image
        if os.path.isfile(full_path) and full_path.lower().endswith(
            (&quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.png&quot;, &quot;.webp&quot;)
        ):
            # Define the path for the output mask
            mask_path = os.path.join(output_path, file)
            # Skip if the mask already exists
            if os.path.exists(mask_path):
                print(f&quot;Mask already exists for {file}, skipping.&quot;)
                continue
            # Predict the mask
            try:
                mask_filename = client.predict(
                    image_input=handle_file(full_path),
                    text_input=text_input,
                    api_name=&quot;/process_image&quot;,
                )
                # Move the generated mask to the output directory
                shutil.move(mask_filename, mask_path)
                print(f&quot;Saved mask to {mask_path}&quot;)
            except Exception as e:
                print(f&quot;Failed to process {file}: {e}&quot;)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/masked_loss/generate_dataset_masks.py">import argparse
import os
import PIL
import cv2  # Import OpenCV for image processing
import numpy as np
import supervision as sv
import torch
from PIL import Image, ImageOps
from typing import Union, Any, Tuple, Dict
from unittest.mock import patch
os.environ[&quot;PYTORCH_ENABLE_MPS_FALLBACK&quot;] = &quot;1&quot;
from gradio_client import handle_file
from transformers import AutoModelForCausalLM, AutoProcessor
from transformers.dynamic_module_utils import get_imports
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
# Constants
FLORENCE_CHECKPOINT = &quot;microsoft/Florence-2-large&quot;
FLORENCE_OPEN_VOCABULARY_DETECTION_TASK = &quot;&lt;OPEN_VOCABULARY_DETECTION&gt;&quot;
SAM_CONFIG = &quot;sam2_hiera_l.yaml&quot;
SAM_CHECKPOINT = &quot;checkpoints/sam2_hiera_large.pt&quot;
def load_sam_image_model(
    device: torch.device, config: str = SAM_CONFIG, checkpoint: str = SAM_CHECKPOINT
) -&gt; SAM2ImagePredictor:
    model = build_sam2(config, checkpoint, device=device)
    return SAM2ImagePredictor(sam_model=model)
def run_sam_inference(
    model: Any, image: Image.Image, detections: sv.Detections
) -&gt; sv.Detections:
    image_np = np.array(image.convert(&quot;RGB&quot;))
    model.set_image(image_np)
    masks, scores, _ = model.predict(box=detections.xyxy, multimask_output=False)
    # Ensure mask dimensions are correct
    if len(masks.shape) == 4:
        masks = np.squeeze(masks)
    detections.mask = masks.astype(bool)
    return detections
def fixed_get_imports(filename: Union[str, os.PathLike]) -&gt; list[str]:
    &quot;&quot;&quot;Workaround for specific import issues.&quot;&quot;&quot;
    if not str(filename).endswith(&quot;/modeling_florence2.py&quot;):
        return get_imports(filename)
    imports = get_imports(filename)
    if &quot;flash_attn&quot; in imports:
        imports.remove(&quot;flash_attn&quot;)
    return imports
def load_florence_model(
    device: torch.device, checkpoint: str = FLORENCE_CHECKPOINT
) -&gt; Tuple[Any, Any]:
    with patch(&quot;transformers.dynamic_module_utils.get_imports&quot;, fixed_get_imports):
        model = (
            AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True)
            .to(device)
            .eval()
        )
        processor = AutoProcessor.from_pretrained(checkpoint, trust_remote_code=True)
        return model, processor
def run_florence_inference(
    model: Any,
    processor: Any,
    device: torch.device,
    image: Image.Image,
    task: str,
    text: str = &quot;&quot;,
) -&gt; Tuple[str, Dict]:
    prompt = task + text
    inputs = processor(text=prompt, images=image, return_tensors=&quot;pt&quot;).to(device)
    generated_ids = model.generate(
        input_ids=inputs[&quot;input_ids&quot;],
        pixel_values=inputs[&quot;pixel_values&quot;],
        max_new_tokens=1024,
        num_beams=3,
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
    response = processor.post_process_generation(
        generated_text, task=task, image_size=image.size
    )
    return generated_text, response
def main():
    # Set up argument parser
    parser = argparse.ArgumentParser(
        description=&quot;Mask images in a directory using Florence SAM Masking.&quot;
    )
    parser.add_argument(
        &quot;--input_dir&quot;,
        type=str,
        required=True,
        help=&quot;Path to the input directory containing images.&quot;,
    )
    parser.add_argument(
        &quot;--output_dir&quot;,
        type=str,
        required=True,
        help=&quot;Path to the output directory to save masked images.&quot;,
    )
    parser.add_argument(
        &quot;--text_input&quot;,
        type=str,
        default=&quot;person&quot;,
        help=&apos;Text prompt for masking (default: &quot;person&quot;).&apos;,
    )
    parser.add_argument(
        &quot;--invert_mask&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Invert the mask to ignore the segmented portion instead of isolate it.&quot;,
    )
    parser.add_argument(
        &quot;--mask_padding&quot;,
        type=int,
        default=0,
        help=&quot;Number of pixels to pad the mask (default: 0).&quot;,
    )
    parser.add_argument(
        &quot;--mask_blur&quot;,
        type=int,
        default=0,
        help=&quot;Amount of Gaussian blur to apply to the mask edges (default: 0).&quot;,
    )
    args = parser.parse_args()
    if args.input_dir is None or args.output_dir is None:
        import sys
        sys.exit(1)
    input_path = args.input_dir
    output_path = args.output_dir
    text_input = args.text_input
    DEVICE = torch.device(
        &quot;cuda&quot;
        if torch.cuda.is_available()
        else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
    )
    # Retrieve model
    from huggingface_hub import hf_hub_download
    print(f&quot;Downloading SAM2 to {os.getcwd()}/checkpoints.&quot;)
    hf_hub_download(
        &quot;SkalskiP/florence-sam-masking&quot;,
        repo_type=&quot;space&quot;,
        subfolder=&quot;checkpoints&quot;,
        local_dir=&quot;./&quot;,
        filename=&quot;sam2_hiera_large.pt&quot;,
    )
    # Load models
    FLORENCE_MODEL, FLORENCE_PROCESSOR = load_florence_model(device=DEVICE)
    SAM_IMAGE_MODEL = load_sam_image_model(device=DEVICE)
    # Create the output directory if it doesn&apos;t exist
    os.makedirs(output_path, exist_ok=True)
    # Get all files in the input directory
    files = os.listdir(input_path)
    # Iterate over all files
    for file in files:
        # Construct the full file path
        full_path = os.path.join(input_path, file)
        # Check if the file is an image
        if os.path.isfile(full_path) and full_path.lower().endswith(
            (&quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.png&quot;, &quot;.webp&quot;)
        ):
            # Define the path for the output mask
            mask_path = os.path.join(output_path, file)
            # Skip if the mask already exists
            if os.path.exists(mask_path):
                print(f&quot;Mask already exists for {file}, skipping.&quot;)
                continue
            # Predict the mask
            try:
                image_input = Image.open(full_path)
                # Convert to RGB
                image_input = image_input.convert(&quot;RGB&quot;)
                _, result = run_florence_inference(
                    model=FLORENCE_MODEL,
                    processor=FLORENCE_PROCESSOR,
                    device=DEVICE,
                    image=image_input,
                    task=FLORENCE_OPEN_VOCABULARY_DETECTION_TASK,
                    text=text_input,
                )
                detections = sv.Detections.from_lmm(
                    lmm=sv.LMM.FLORENCE_2,
                    result=result,
                    resolution_wh=image_input.size,
                )
                if len(detections) == 0:
                    print(f&quot;No objects detected in {file}.&quot;)
                    continue
                detections = run_sam_inference(SAM_IMAGE_MODEL, image_input, detections)
                # Combine masks if multiple detections
                combined_mask = np.any(detections.mask, axis=0)
                # Apply mask padding if specified
                if args.mask_padding &gt; 0:
                    kernel = np.ones((3, 3), np.uint8)
                    combined_mask = cv2.dilate(
                        combined_mask.astype(np.uint8),
                        kernel,
                        iterations=args.mask_padding,
                    )
                # Apply mask blurring if specified
                if args.mask_blur &gt; 0:
                    combined_mask = combined_mask.astype(np.float32)
                    ksize = args.mask_blur * 2 + 1  # Kernel size must be odd
                    combined_mask = cv2.GaussianBlur(combined_mask, (ksize, ksize), 0)
                # Convert mask to image
                if args.mask_blur &gt; 0:
                    mask_image = Image.fromarray((combined_mask * 255).astype(np.uint8))
                else:
                    mask_image = Image.fromarray(combined_mask.astype(np.uint8) * 255)
                # Invert masks if necessary
                if args.invert_mask:
                    mask_image = ImageOps.invert(mask_image)
                mask_image.save(mask_path)
                print(f&quot;Saved mask to {mask_path}&quot;)
            except Exception as e:
                print(f&quot;Failed to process {file}: {e}&quot;)
    # Clean up
    os.remove(&quot;checkpoints/sam2_hiera_large.pt&quot;)
    os.rmdir(&quot;checkpoints&quot;)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/masked_loss/requirements.txt">tqdm
einops
timm
transformers
samv2
supervision
opencv-python
pytest</file><file path="toolkit/datasets/analyze_aspect_ratios_json.py">import json, os
import threading, logging
from concurrent.futures import ThreadPoolExecutor
from PIL import Image
# Allowed bucket values:
allowed = [1.0, 1.5, 0.67, 0.75, 1.78]
# Load from JSON.
with open(&quot;aspect_ratios.json&quot;, &quot;r&quot;) as f:
    aspect_ratios = json.load(f)
new_bucket = {}
for bucket, indices in aspect_ratios.items():
    logging.info(f&quot;{bucket}: {len(indices)}&quot;)
    if float(bucket) in allowed:
        logging.info(f&quot;Bucket {bucket} in {allowed}&quot;)
        new_bucket[bucket] = aspect_ratios[bucket]
least_amount = None
for bucket, indices in aspect_ratios.items():
    if least_amount is None or len(indices) &lt; least_amount:
        least_amount = len(indices)
# We don&apos;t want to limit square image training.
# buckets_to_skip = [ 1.0 ]
# for bucket, files in aspect_ratios.items():
#     if float(bucket) not in buckets_to_skip and len(files) &gt; least_amount:
#         logging.info(f&apos;We have to reduce the number of items in the bucket: {bucket}&apos;)
#         # &apos;files&apos; is a list of full file paths. we need to delete them randomly until the value of least_amount is reached.
#         # Get a random sample of files to delete.
#         import random
#         random.shuffle(files)
#         files_to_delete = files[least_amount:]
#         logging.info(f&apos;Files to delete: {len(files_to_delete)}&apos;)
#         for file in files_to_delete:
#             import os
#             os.remove(file)
def _resize_for_condition_image(self, input_image: Image.Image, resolution: int):
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    k = float(resolution) / min(H, W)
    H *= k
    W *= k
    H = int(round(H / 64.0)) * 64
    W = int(round(W / 64.0)) * 64
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img
def process_file(file):
    image = Image.open(file).convert(&quot;RGB&quot;)
    width, height = image.size
    if width &lt; 900 or height &lt; 900:
        logging.info(
            f&quot;Image does not meet minimum size requirements: {file}, size {image.size}&quot;
        )
        os.remove(file)
    else:
        logging.info(
            f&quot;Image meets minimum size requirements for conditioning: {file}, size {image.size}&quot;
        )
        image = _resize_for_condition_image(image, 1024)
        image.save(file)
def process_bucket(bucket, files):
    logging.info(f&quot;Processing bucket {bucket}: {len(files)} files&quot;)
    with ThreadPoolExecutor(max_workers=32) as executor:
        executor.map(process_file, files)
if __name__ == &quot;__main__&quot;:
    # Load aspect ratios from the JSON file
    with open(&quot;aspect_ratios.json&quot;, &quot;r&quot;) as f:
        aspect_ratios = json.load(f)
    threads = []
    for bucket, files in aspect_ratios.items():
        thread = threading.Thread(target=process_bucket, args=(bucket, files))
        threads.append(thread)
        thread.start()
    for thread in threads:
        thread.join()</file><file path="toolkit/datasets/analyze_laion_data.py">&quot;&quot;&quot;
Walk through a LAION dataset and analyze it.
&quot;&quot;&quot;
import os
import json
import concurrent.futures
from PIL import Image
def get_aspect_ratio(image_path):
    try:
        with Image.open(image_path) as img:
            width, height = img.size
            return image_path, width / height
    except Exception as e:
        os.remove(image_path)
        return None
def analyze_images(directory_path):
    aspect_ratios = {}
    good_count = 0
    bad_count = 0
    image_files = [
        os.path.join(directory_path, filename)
        for filename in os.listdir(directory_path)
        if filename.endswith(&quot;.jpg&quot;) or filename.endswith(&quot;.png&quot;)
    ]
    with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:
        futures = {
            executor.submit(get_aspect_ratio, image_path): image_path
            for image_path in image_files
        }
        for future in concurrent.futures.as_completed(futures):
            image_path = futures[future]
            try:
                aspect_ratio = future.result()
                if aspect_ratio is not None:
                    image_path, aspect_ratio = aspect_ratio
                    aspect_ratio = round(aspect_ratio, 2)  # round to 2 decimal places
                    if aspect_ratio not in aspect_ratios:
                        aspect_ratios[aspect_ratio] = []
                    aspect_ratios[aspect_ratio].append(image_path)
                    good_count += 1
                else:
                    bad_count += 1
            except Exception as e:
                pass
    print(f&quot;Good images: {good_count}, Bad images: {bad_count}&quot;)
    return aspect_ratios
def write_to_json(data, filename):
    with open(filename, &quot;w&quot;) as outfile:
        json.dump(data, outfile)
if __name__ == &quot;__main__&quot;:
    image_directory = &quot;/notebooks/datasets/laion-high-resolution/downloaded_images&quot;
    output_file = &quot;aspect_ratios.json&quot;
    aspect_ratios = analyze_images(image_directory)
    write_to_json(aspect_ratios, output_file)</file><file path="toolkit/datasets/check_latent_corruption.py">&quot;&quot;&quot;
2024-04-05 17:19:44,198 [DEBUG] (LocalDataBackend) Checking if /Volumes/models/training/vae_cache/sdxl/photo-concept-bucket/image_data/1027365.pt exists = True
2024-04-05 17:19:44,198 [DEBUG] (LocalDataBackend) Checking if /Volumes/models/training/vae_cache/sdxl/photo-concept-bucket/image_data/10064767.pt exists = True
2024-04-05 17:19:44,223 [DEBUG] (LocalDataBackend) Checking if /Volumes/models/training/vae_cache/sdxl/photo-concept-bucket/image_data/13997787.pt exists = True
2024-04-05 17:19:44,223 [DEBUG] (LocalDataBackend) Checking if /Volumes/models/training/vae_cache/sdxl/photo-concept-bucket/image_data/13565183.pt exists = True
&quot;&quot;&quot;
latent_file_paths = [&quot;1027365&quot;, &quot;10064767&quot;, &quot;13997787&quot;, &quot;13565183&quot;]
prefix = &quot;/Volumes/models/training/vae_cache/sdxl/photo-concept-bucket/image_data/&quot;
# load the latent_file_paths
import torch
for latent_file_path in latent_file_paths:
    print(f&quot;{prefix}{latent_file_path}.pt&quot;)
    latent = torch.load(
        f&quot;{prefix}{latent_file_path}.pt&quot;, map_location=torch.device(&quot;cpu&quot;)
    )
    print(f&quot;Shape: {latent.shape}&quot;)
    print(f&quot;Mean: {latent.mean()}&quot;)
    print(f&quot;Std: {latent.std()}&quot;)
    print(f&quot;Min: {latent.min()}&quot;)
    print(f&quot;Is corrupt: {torch.isnan(latent).any() or torch.isinf(latent).any()}&quot;)</file><file path="toolkit/datasets/clear_s3_bucket.py">import boto3, os, logging, argparse, datetime
from botocore.config import Config
# Set up logging
logging.basicConfig(level=os.getenv(&quot;LOGLEVEL&quot;, &quot;INFO&quot;))
logger = logging.getLogger(__name__)
def initialize_s3_client(args):
    &quot;&quot;&quot;Initialize the boto3 S3 client using the provided AWS credentials and settings.&quot;&quot;&quot;
    s3_config = Config(max_pool_connections=100)
    s3_client = boto3.client(
        &quot;s3&quot;,
        endpoint_url=args.aws_endpoint_url,
        region_name=args.aws_region_name,
        aws_access_key_id=args.aws_access_key_id,
        aws_secret_access_key=args.aws_secret_access_key,
        config=s3_config,
    )
    return s3_client
from concurrent.futures import ThreadPoolExecutor
def delete_object(s3_client, bucket_name, object_key):
    try:
        s3_client.delete_object(Bucket=bucket_name, Key=object_key)
        logger.info(f&quot;Deleted: {object_key}&quot;)
    except Exception as e:
        logger.error(f&quot;Error deleting {object_key} in bucket {bucket_name}: {e}&quot;)
def clear_s3_bucket(
    s3_client,
    bucket_name,
    num_workers=10,
    search_pattern: str = None,
    older_than_date: str = None,
):
    try:
        logger.info(f&quot;Clearing out bucket {bucket_name}&quot;)
        # Convert the date string to a datetime object
        if older_than_date:
            target_date = datetime.datetime.strptime(older_than_date, &quot;%Y-%m-%d&quot;)
        else:
            target_date = None
        # Initialize paginator
        paginator = s3_client.get_paginator(&quot;list_objects_v2&quot;)
        # Create a PageIterator from the Paginator
        page_iterator = paginator.paginate(Bucket=bucket_name)
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            for page in page_iterator:
                if &quot;Contents&quot; not in page:
                    logger.info(f&quot;No more items in bucket {bucket_name}&quot;)
                    break
                # Filter by the older_than_date if provided
                if target_date:
                    filtered_objects = [
                        s3_object
                        for s3_object in page[&quot;Contents&quot;]
                        if s3_object[&quot;LastModified&quot;].replace(tzinfo=None) &lt; target_date
                    ]
                else:
                    filtered_objects = page[&quot;Contents&quot;]
                if search_pattern is not None:
                    keys_to_delete = [
                        s3_object[&quot;Key&quot;]
                        for s3_object in filtered_objects
                        if search_pattern in s3_object[&quot;Key&quot;]
                    ]
                else:
                    keys_to_delete = [
                        s3_object[&quot;Key&quot;] for s3_object in filtered_objects
                    ]
                executor.map(
                    delete_object,
                    [s3_client] * len(keys_to_delete),
                    [bucket_name] * len(keys_to_delete),
                    keys_to_delete,
                )
        logger.info(f&quot;Cleared out bucket {bucket_name}&quot;)
    except Exception as e:
        logger.error(f&quot;Error clearing out bucket {bucket_name}: {e}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(description=&quot;Clear out an S3 bucket.&quot;)
    parser.add_argument(
        &quot;--aws_bucket_name&quot;,
        type=str,
        required=True,
        help=&quot;The AWS bucket name to clear.&quot;,
    )
    parser.add_argument(&quot;--aws_endpoint_url&quot;, type=str, help=&quot;The AWS server to use.&quot;)
    parser.add_argument(
        &quot;--num_workers&quot;,
        type=int,
        help=&quot;Number of workers to use for clearing.&quot;,
        default=10,
    )
    parser.add_argument(
        &quot;--search_pattern&quot;,
        type=str,
        help=&quot;If provided, files with this in their Content key will be removed only.&quot;,
        default=None,
    )
    parser.add_argument(&quot;--aws_region_name&quot;, type=str, help=&quot;The AWS region to use.&quot;)
    parser.add_argument(&quot;--aws_access_key_id&quot;, type=str, help=&quot;AWS access key ID.&quot;)
    parser.add_argument(
        &quot;--aws_secret_access_key&quot;, type=str, help=&quot;AWS secret access key.&quot;
    )
    parser.add_argument(
        &quot;--older_than_date&quot;,
        type=str,
        help=&quot;If provided, only files older than this date (format: YYYY-MM-DD) will be cleared.&quot;,
        default=None,
    )
    return parser.parse_args()
def main():
    args = parse_args()
    s3_client = initialize_s3_client(args)
    clear_s3_bucket(
        s3_client,
        args.aws_bucket_name,
        num_workers=args.num_workers,
        search_pattern=args.search_pattern,
        older_than_date=args.older_than_date,
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/crop.py">import cv2
import numpy as np
import os
from skimage.metrics import structural_similarity as compare_ssim
import imutils
def process_video(input_video_path, output_folder, detect_faces=False):
    cap = cv2.VideoCapture(input_video_path)
    frame_counter = 0
    previous_faces = {}
    last_detected_image = None
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
            resized_frame = resize_image(frame)
            cropped_frame = crop_image(resized_frame)
            if detect_faces:
                faces_detected = detect_faces_in_image(cropped_frame)
                for x, y, w, h in faces_detected:
                    face_crop = cropped_frame[y : y + h, x : x + w]
                    face_key = f&quot;{x}_{y}_{w}_{h}&quot;
                    score = 0.0
                    if last_detected_image is not None:
                        score = image_difference(last_detected_image, cropped_frame)
                    if last_detected_image is None or score &lt; 0.21:
                        cv2.imwrite(
                            os.path.join(
                                output_folder, f&quot;frame_{frame_counter:05d}.jpg&quot;
                            ),
                            cropped_frame,
                        )
                        previous_faces[face_key] = face_crop
                        last_detected_image = cropped_frame
            else:
                score = 0.0
                if last_detected_image is not None:
                    score = image_difference(last_detected_image, cropped_frame)
                if last_detected_image is None or score &lt; 0.21:
                    cv2.imwrite(
                        os.path.join(output_folder, f&quot;frame_{frame_counter:05d}.jpg&quot;),
                        cropped_frame,
                    )
                    last_detected_image = cropped_frame
            frame_counter += 1
        else:
            break
    # Release everything after the job is finished
    cap.release()
    cv2.destroyAllWindows()
def resize_image(image, height=768):
    # resizing image while maintaining aspect ratio
    ratio = height / image.shape[0]
    width = int(image.shape[1] * ratio)
    dim = (width, height)
    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)
    return resized
def crop_image(image):
    height, width = image.shape[:2]
    cropped_image = image[0:768, (width - 768) // 2 : (width + 768) // 2]
    return cropped_image
def detect_faces_in_image(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + &quot;haarcascade_frontalface_default.xml&quot;
    )
    faces = face_cascade.detectMultiScale(
        gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
    )
    faces_str = f&quot;{faces}&quot;
    if faces_str != &quot;()&quot;:
        logging.info(f&quot;Found faces {faces}&quot;)
    return faces
def image_difference(imageA, imageB):
    # convert the images to grayscale
    grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)
    grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)
    # compute the Structural Similarity Index (SSIM) between the two images, ensuring that the difference image is returned
    (score, diff) = compare_ssim(grayA, grayB, full=True)
    logging.info(f&quot;Returning score {score}&quot;)
    return score
from pathlib import Path
def process(input_path, output_folder, detect_faces=False):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    if os.path.isdir(input_path):
        # If input path is a directory, process each image file in the directory
        for image_file in Path(input_path).glob(&quot;*&quot;):
            image = cv2.imread(str(image_file))
            if image is None:
                logging.info(f&quot;Image had ERROR: {input_path}&quot;)
                continue
            process_image(image, output_folder, detect_faces)
    else:
        # If input path is not a directory, assume it&apos;s a video file and process it
        process_video(input_path, output_folder, detect_faces)
def process_image(image, output_folder, detect_faces=False):
    frame_counter = 0
    previous_faces = {}
    last_detected_image = None
    resized_image = resize_image(image)
    cropped_image = crop_image(resized_image)
    if detect_faces:
        faces_detected = detect_faces_in_image(cropped_image)
        for x, y, w, h in faces_detected:
            face_crop = cropped_image[y : y + h, x : x + w]
            face_key = f&quot;{x}_{y}_{w}_{h}&quot;
            score = 0.0
            if last_detected_image is not None:
                score = image_difference(last_detected_image, cropped_image)
            if last_detected_image is None or score &lt; 0.21:
                cv2.imwrite(
                    os.path.join(output_folder, f&quot;frame_{frame_counter:05d}.jpg&quot;),
                    cropped_image,
                )
                previous_faces[face_key] = face_crop
                last_detected_image = cropped_image
    else:
        score = 0.0
        if last_detected_image is not None:
            score = image_difference(last_detected_image, cropped_image)
        if last_detected_image is None or score &lt; 0.21:
            cv2.imwrite(
                os.path.join(output_folder, f&quot;frame_{frame_counter:05d}.jpg&quot;),
                cropped_image,
            )
            last_detected_image = cropped_image
    frame_counter += 1
if __name__ == &quot;__main__&quot;:
    input_path = (
        &quot;/notebooks/datasets/faces&quot;  # path to the input video or image directory
    )
    output_folder = &quot;/notebooks/datasets/processed_faces&quot;  # path to the output folder
    process(input_path, output_folder, detect_faces=False)</file><file path="toolkit/datasets/csv_to_s3.py">import os
import random
import argparse
import logging
import boto3
from io import BytesIO
import pandas as pd
from pathlib import Path
from PIL import Image, ExifTags
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from multiprocessing import Pool
import requests
import re
import shutil
from botocore.config import Config
from multiprocessing import Pool
from tqdm import tqdm
# Constants
conn_timeout = 6
read_timeout = 60
timeouts = (conn_timeout, read_timeout)
# Set up logging
logging.basicConfig(level=os.getenv(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
logger = logging.getLogger(__name__)
connection_logger = logging.getLogger(&quot;urllib3.connectionpool&quot;)
connection_logger.setLevel(logging.ERROR)
connection_logger = logging.getLogger(&quot;urllib3.connection&quot;)
connection_logger.setLevel(logging.ERROR)
pil_logger = logging.getLogger(&quot;PIL&quot;)
pil_logger.setLevel(logging.INFO)
pil_logger = logging.getLogger(&quot;PIL.Image&quot;)
pil_logger.setLevel(&quot;ERROR&quot;)
pil_logger = logging.getLogger(&quot;PIL.PngImagePlugin&quot;)
pil_logger.setLevel(&quot;ERROR&quot;)
loggers_to_silence = [
    &quot;botocore.hooks&quot;,
    &quot;botocore.auth&quot;,
    &quot;botocore.httpsession&quot;,
    &quot;botocore.parsers&quot;,
    &quot;botocore.retryhandler&quot;,
    &quot;botocore.loaders&quot;,
    &quot;botocore.regions&quot;,
    &quot;botocore.utils&quot;,
    &quot;botocore.client&quot;,
    &quot;botocore.handler&quot;,
    &quot;botocore.handlers&quot;,
    &quot;botocore.awsrequest&quot;,
]
for logger_name in loggers_to_silence:
    _logger = logging.getLogger(logger_name)
    _logger.setLevel(&quot;ERROR&quot;)
# Arguably, the most interesting one:
boto_logger = logging.getLogger(&quot;botocore.endpoint&quot;)
boto_logger.setLevel(os.environ.get(&quot;SIMPLETUNER_AWS_LOG_LEVEL&quot;, &quot;ERROR&quot;))
http = requests.Session()
adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100)
http.mount(&quot;http://&quot;, adapter)
http.mount(&quot;https://&quot;, adapter)
def shuffle_words_in_filename(filename):
    &quot;&quot;&quot;Shuffle the words in a filename while keeping the file extension unchanged.&quot;&quot;&quot;
    name, ext = os.path.splitext(filename)
    words = name.split(
        &quot;_&quot;
    )  # Assuming words in the filename are separated by underscores
    random.shuffle(words)
    return &quot;_&quot;.join(words) + ext
def resize_for_condition_image(input_image: Image.Image, resolution: int):
    if resolution == 0:
        return input_image
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    aspect_ratio = round(W / H, 2)
    msg = f&quot;Inspecting image of aspect {aspect_ratio} and size {W}x{H} to &quot;
    if W &lt; H:
        W = resolution
        H = int(resolution / aspect_ratio)  # Calculate the new height
    elif H &lt; W:
        H = resolution
        W = int(resolution * aspect_ratio)  # Calculate the new width
    if W == H:
        W = resolution
        H = resolution
    msg = f&quot;{msg} {W}x{H}.&quot;
    logger.debug(msg)
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img
def object_exists_in_s3(s3_client, bucket_name, object_name):
    &quot;&quot;&quot;Check if a specific object exists in the S3 bucket.&quot;&quot;&quot;
    try:
        s3_client.head_object(Bucket=bucket_name, Key=object_name)
        return True
    except:
        return False
def calculate_luminance(image: Image):
    &quot;&quot;&quot;Calculate the luminance of an image.&quot;&quot;&quot;
    grayscale = image.convert(&quot;L&quot;)
    histogram = grayscale.histogram()
    pixels = sum(histogram)
    brightness = scale = len(histogram)
    for index in range(0, scale):
        ratio = histogram[index] / pixels
        brightness += ratio * (-scale + index)
    luminance_value = 1 if brightness == 255 else brightness / scale
    logger.debug(f&quot;Calculated luminance: {luminance_value}&quot;)
    return luminance_value
def fetch_image(info, args):
    filename = info[&quot;filename&quot;]
    url = info[&quot;url&quot;]
    # Constants
    conn_timeout = args.connection_timeout
    read_timeout = args.read_timeout
    timeouts = (conn_timeout, read_timeout)
    current_file_path = os.path.join(args.temporary_folder, filename)
    if os.path.exists(current_file_path):
        return
    try:
        r = http.get(url, timeout=timeouts, stream=True)
        if r.status_code == 200:
            image = Image.open(BytesIO(r.content))
            width, height = image.size
            if width &lt; args.minimum_resolution or height &lt; args.minimum_resolution:
                os.remove(current_file_path)
                return
            if args.only_exif_images and not valid_exif_data(current_file_path):
                os.remove(current_file_path)
                return
            if args.min_luminance is not None or args.max_luminance is not None:
                image_luminance = calculate_luminance(image)
                if args.min_luminance and image_luminance &lt; args.min_luminance:
                    os.remove(current_file_path)
                    return
                if args.max_luminance and image_luminance &gt; args.max_luminance:
                    os.remove(current_file_path)
                    return
            image = resize_for_condition_image(image, args.condition_image_size)
            return image
        else:
            pass
    except Exception as e:
        import traceback
        logger.error(f&quot;Error: {e}, traceback: {traceback.format_exc()}&quot;)
        raise e
def parse_args():
    parser = argparse.ArgumentParser(
        description=&quot;Filter and upload images from Parquet files to S3.&quot;
    )
    # AWS-related arguments
    parser.add_argument(
        &quot;--data_backend&quot;,
        choices=[&quot;local&quot;, &quot;aws&quot;],
        default=&quot;aws&quot;,
        help=&quot;The data backend to use.&quot;,
    )
    parser.add_argument(
        &quot;--aws_bucket_name&quot;, type=str, help=&quot;The AWS bucket name to use.&quot;
    )
    parser.add_argument(&quot;--aws_endpoint_url&quot;, type=str, help=&quot;The AWS server to use.&quot;)
    parser.add_argument(&quot;--aws_region_name&quot;, type=str, help=&quot;The AWS region to use.&quot;)
    parser.add_argument(&quot;--aws_access_key_id&quot;, type=str, help=&quot;AWS access key ID.&quot;)
    parser.add_argument(
        &quot;--aws_secret_access_key&quot;, type=str, help=&quot;AWS secret access key.&quot;
    )
    parser.add_argument(
        &quot;--connection_timeout&quot;,
        type=int,
        default=3,
        help=&quot;Connection timeout in seconds.&quot;,
    )
    parser.add_argument(
        &quot;--midjourney_data_checks&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, only images with certain entries in the caption will be included. This is useful for midjourney data checks.&quot;,
    )
    parser.add_argument(
        &quot;--read_timeout&quot;,
        type=int,
        default=30,
        help=&quot;Read timeout in seconds.&quot;,
    )
    # Script-specific arguments
    parser.add_argument(
        &quot;--local_folder&quot;,
        type=str,
        help=&quot;Location of local folder containing images to upload.&quot;,
    )
    parser.add_argument(
        &quot;--parquet_folder&quot;, type=str, help=&quot;Location of the Parquet files.&quot;
    )
    parser.add_argument(&quot;--csv_folder&quot;, type=str, help=&quot;Location of the CSV files.&quot;)
    parser.add_argument(&quot;--git_lfs_repo&quot;, type=str, help=&quot;The Git LFS repository URL.&quot;)
    parser.add_argument(
        &quot;--delete_after_processing&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Delete original CSV/Parquet file after processing.&quot;,
    )
    parser.add_argument(
        &quot;--temporary_folder&quot;,
        type=str,
        required=True,
        help=&quot;Location of temporary data during upload.&quot;,
    )
    parser.add_argument(
        &quot;--pwatermark_threshold&quot;,
        type=float,
        default=0.7,
        help=&quot;Threshold for pwatermark value. A higher score indicates a more likely chance of a watermark. Default: 0.7&quot;,
    )
    parser.add_argument(
        &quot;--aesthetic_threshold&quot;,
        type=int,
        default=5,
        help=&quot;Threshold for aesthetic score, where a low score indicates a lower-quality image, often containing text. Default: 5&quot;,
    )
    parser.add_argument(
        &quot;--similarity_threshold&quot;,
        type=float,
        default=0.33,
        help=&quot;The similarity score of an image describes how closely its caption followed the embed. Higher = better. Default: 0.33&quot;,
    )
    parser.add_argument(
        &quot;--unsafe_threshold&quot;,
        type=float,
        default=0.5,
        help=&quot;The probability of an image containing harmful content. Values higher than this will be ignored, unless --inverse_unsafe_threshold is given. Default: 0.5&quot;,
    )
    parser.add_argument(
        &quot;--invert_unsafe_threshold&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, images with a probability of harmful content higher than --unsafe_threshold will be included. This may be useful for training eg. NSFW classifiers.&quot;,
    )
    parser.add_argument(
        &quot;--min_luminance&quot;,
        type=float,
        default=None,
        help=&quot;Minimum luminance threshold for images. If not provided, no lower cap is applied.&quot;,
    )
    parser.add_argument(
        &quot;--max_luminance&quot;,
        type=float,
        default=None,
        help=&quot;Maximum luminance threshold for images. If not provided, only capping is applied.&quot;,
    )
    parser.add_argument(
        &quot;--caption_field&quot;,
        type=str,
        default=None,
        help=&quot;Field to use for image filename. Leave unset to auto-detect.&quot;,
    )
    parser.add_argument(
        &quot;--num_workers&quot;,
        type=int,
        default=8,
        help=&quot;Number of worker threads for downloading images.&quot;,
    )
    parser.add_argument(
        &quot;--max_num_files&quot;,
        type=int,
        default=1000000,
        help=&quot;Maximum number of files to process.&quot;,
    )
    # Filtering images
    parser.add_argument(
        &quot;--minimum_resolution&quot;,
        type=int,
        default=0,
        help=&quot;Minimum resolution for images. Set to 0 to disable.&quot;,
    )
    parser.add_argument(
        &quot;--minimum_pixel_area&quot;,
        type=int,
        default=1,
        help=&quot;Minimum pixel area for images, measured in megapixels. Set to 0 to disable.&quot;,
    )
    parser.add_argument(
        &quot;--width_field&quot;,
        type=str,
        default=None,
        help=(&quot;Column name for image width. Auto-detected, if not supplied.&quot;),
    )
    parser.add_argument(
        &quot;--height_field&quot;,
        type=str,
        default=None,
        help=(
            &quot;The column name in the dataset for the image height. Auto-detected, if not supplied.&quot;
        ),
    )
    parser.add_argument(
        &quot;--condition_image_size&quot;,
        type=int,
        default=0,
        help=&quot;This option will by default, resize the smaller edge of an image to 1024px.&quot;,
    )
    parser.add_argument(
        &quot;--only_exif_images&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, only images with EXIF data will be included.&quot;,
    )
    parser.add_argument(
        &quot;--print_nonfatal_errors&quot;,
        action=&quot;store_true&quot;,
        help=&quot;If set, non-fatal errors will be printed. Remove this from the commandline to make output more streamlined/quieter.&quot;,
    )
    return parser.parse_args()
# Additional functions for handling diverse input datasets
def get_uri_column(df):
    if &quot;url&quot; in df.columns:
        return &quot;url&quot;
    if &quot;URL&quot; in df.columns:
        return &quot;URL&quot;
    elif &quot;Attachments&quot; in df.columns:
        return &quot;Attachments&quot;
    else:
        logger.error(&quot;No recognized URI column found in the dataset.&quot;)
        return None
def get_width_field(df):
    if &quot;WIDTH&quot; in df.columns:
        return &quot;WIDTH&quot;
    return &quot;width&quot;
def get_height_field(df):
    if &quot;HEIGHT&quot; in df.columns:
        return &quot;HEIGHT&quot;
    return &quot;height&quot;
def get_caption_column(df):
    if &quot;top_caption&quot; in df.columns:
        return &quot;top_caption&quot;
    if &quot;Content&quot; in df.columns:
        return &quot;Content&quot;
    if &quot;caption&quot; in df.columns:
        return &quot;Content&quot;
    if &quot;TEXT&quot; in df.columns:
        return &quot;TEXT&quot;
    if &quot;all_captions&quot; in df.columns:
        return &quot;all_captions&quot;
def initialize_s3_client(args):
    &quot;&quot;&quot;Initialize the boto3 S3 client using the provided AWS credentials and settings.&quot;&quot;&quot;
    s3_config = Config(max_pool_connections=100)
    s3_client = boto3.client(
        &quot;s3&quot;,
        endpoint_url=args.aws_endpoint_url,
        region_name=args.aws_region_name,
        aws_access_key_id=args.aws_access_key_id,
        aws_secret_access_key=args.aws_secret_access_key,
        config=s3_config,
    )
    return s3_client
def content_to_filename(content, args):
    &quot;&quot;&quot;
    Function to convert content to filename by stripping everything after &apos;--&apos;,
    replacing non-alphanumeric characters and spaces, converting to lowercase,
    removing leading/trailing underscores, and limiting filename length to 128.
    &quot;&quot;&quot;
    logger.debug(f&quot;Converting content to filename: {content}&quot;)
    filename = str(content)
    image_num_text = &quot;&quot;
    try:
        # Extract the &quot;Image #&quot; with its number using regex, careful not to grab anything past it.
        image_num_match = re.search(r&quot; - Image #\d+&quot;, filename)
        if image_num_match:
            image_num_text = image_num_match.group(0).strip()
            filename = filename.replace(image_num_text, &quot;&quot;)
            image_num_text = image_num_text.replace(
                &quot; - &quot;, &quot;_&quot;
            )  # Replace spaces and hyphens for consistency
        # Remove anything after &apos;--&apos;
        filename = filename.split(&quot;--&quot;, 1)[0]
        # Remove URLs
        filename = re.sub(r&quot;https?://\S*&quot;, &quot;&quot;, filename)
        # Replace non-alphanumeric characters with underscore
        # filename = re.sub(r&quot;[^a-zA-Z0-9\s]&quot;, &quot;_&quot;, filename)
        # Remove leading and trailing underscores
        filename = filename.strip(&quot;_&quot;)
        # Strip multiple whitespaces, replace with single whitespace
        filename = re.sub(r&quot;\s+&quot;, &quot; &quot;, filename)
        # Strip surrounding whitespace
        filename = filename.strip()
        # Convert to lowercase and limit the length to accommodate the image number and extension
        max_length = 2048 - len(image_num_text) - 4  # 4 for the &quot;.png&quot;
        filename = (filename.lower()[:max_length] + image_num_text).rstrip(&quot;_&quot;) + &quot;.png&quot;
        logger.debug(f&quot;-&gt; Resulting filename: {filename}&quot;)
        return filename
    except Exception as e:
        if hasattr(args, &quot;print_nonfatal_errors&quot;) and args.print_nonfatal_errors:
            logger.error(f&quot;Encountered error processing filename: {e}&quot;)
def valid_exif_data(image_path):
    &quot;&quot;&quot;Check if the image contains EXIF data typically associated with real cameras.&quot;&quot;&quot;
    try:
        image = Image.open(image_path)
        exif_data = image._getexif()
        # If no EXIF data, return False
        if not exif_data:
            return False
        # List of tags to check for real camera evidence
        tags_to_check = [&quot;Make&quot;, &quot;Model&quot;, &quot;DateTimeOriginal&quot;, &quot;LensModel&quot;, &quot;GPSInfo&quot;]
        # Check if any of the relevant tags exist in the EXIF data
        for tag, value in exif_data.items():
            tagname = ExifTags.TAGS.get(tag, tag)
            if tagname in tags_to_check:
                return True
        # If &quot;Software&quot; tag exists, it might be edited or generated, but this is not a surefire method
        if &quot;Software&quot; in exif_data:
            software_name = exif_data[&quot;Software&quot;].lower()
            if &quot;photoshop&quot; in software_name or &quot;gimp&quot; in software_name:
                return False
    except Exception as e:
        print(f&quot;Error processing {image_path}: {e}&quot;)
        pass
    return False
def list_all_s3_objects(s3_client, bucket_name):
    paginator = s3_client.get_paginator(&quot;list_objects_v2&quot;)
    existing_files = set()
    png_files = set()
    for page in paginator.paginate(Bucket=bucket_name, MaxKeys=1000):
        if &quot;Contents&quot; in page:
            for item in page[&quot;Contents&quot;]:
                existing_files.add(item[&quot;Key&quot;])
                if item[&quot;Key&quot;].endswith(&quot;.png&quot;):
                    png_files.add(item[&quot;Key&quot;])
                    logger.info(
                        f&quot;Found {len(png_files)} existing png files in the S3 bucket.&quot;
                    )
    return existing_files
def upload_pil_to_s3(image: Image.Image, filename, args, s3_client):
    &quot;&quot;&quot;Upload a PIL Image directly to S3 bucket&quot;&quot;&quot;
    if object_exists_in_s3(s3_client, args.aws_bucket_name, filename):
        return
    buffer = BytesIO()
    image.save(buffer, format=&quot;PNG&quot;)
    buffer.seek(0)
    s3_client.upload_fileobj(buffer, args.aws_bucket_name, filename)
    buffer.close()
def upload_to_s3(filename, args, s3_client):
    &quot;&quot;&quot;Upload the specified file to the S3 bucket with filename shuffling if needed.&quot;&quot;&quot;
    local_path = os.path.join(args.temporary_folder, filename)
    # Just use the base filename without any directory prefix for S3
    object_name = os.path.basename(filename)
    # Check if the file exists just before uploading
    if not os.path.exists(local_path):
        return
    if object_exists_in_s3(s3_client, args.aws_bucket_name, object_name):
        try:
            os.remove(local_path)
        except:
            pass
        return
    try:
        s3_client.upload_file(local_path, args.aws_bucket_name, object_name)
        # Delete the local file after successful upload
        os.remove(local_path)
    except Exception as e:
        logger.error(f&quot;Error uploading {object_name} to S3: {e}&quot;)
def upload_local_image_to_s3(image_path, args, s3_client):
    &quot;&quot;&quot;Upload local image directly to the S3 bucket.&quot;&quot;&quot;
    object_name = os.path.basename(image_path)
    # Check if the file exists just before uploading
    if not os.path.exists(image_path):
        return
    if object_exists_in_s3(s3_client, args.aws_bucket_name, object_name):
        try:
            os.remove(image_path)
        except:
            pass
        return
    try:
        s3_client.upload_file(image_path, args.aws_bucket_name, object_name)
        # Optionally, delete the local file after successful upload
        if args.delete_after_processing:
            os.remove(image_path)
    except Exception as e:
        logger.error(f&quot;Error uploading {object_name} to S3: {e}&quot;)
def process_git_lfs_images(args, s3_client):
    &quot;&quot;&quot;Scan the git-lfs-repo directory for image files and upload them.&quot;&quot;&quot;
    repo_path = os.path.join(args.temporary_folder, &quot;git-lfs-repo&quot;)
    image_exts = [&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.bmp&quot;, &quot;.tiff&quot;]
    for ext in image_exts:
        for image_path in Path(repo_path).rglob(f&quot;*{ext}&quot;):
            upload_local_image_to_s3(image_path, args, s3_client)
def process_local_folder_images(args, s3_client, existing_files: list):
    &quot;&quot;&quot;Scan the local folder directory for image files, apply checks, and upload them in parallel.&quot;&quot;&quot;
    if not os.path.exists(args.local_folder):
        logger.error(f&quot;Local folder &apos;{args.local_folder}&apos; does not exist.&quot;)
        return
    image_exts = [&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.bmp&quot;, &quot;.tiff&quot;]
    all_images = [
        image_path
        for ext in image_exts
        for image_path in Path(args.local_folder).rglob(f&quot;*{ext}&quot;)
    ]
    # Filter out already processed files
    images_to_process = [
        image_path for image_path in all_images if str(image_path) not in existing_files
    ]
    # Using Pool for parallel processing
    with Pool(processes=args.num_workers) as pool:
        list(
            tqdm(
                pool.imap(
                    process_and_upload,
                    [(image_path, args) for image_path in images_to_process],
                ),
                total=len(images_to_process),
            )
        )
def process_and_upload(image_path_args):
    image_path, args = image_path_args
    &quot;&quot;&quot;Process and upload a single image.&quot;&quot;&quot;
    # Place your existing logic here for processing a single image and uploading it to S3.
    # For example:
    try:
        image = Image.open(image_path)
    except Exception:
        return None
    width, height = image.size
    # Check minimum resolution
    if args.minimum_resolution &gt; 0 and (
        width &lt; args.minimum_resolution or height &lt; args.minimum_resolution
    ):
        return None
    # Check luminance if required
    if args.min_luminance is not None or args.max_luminance is not None:
        luminance = calculate_luminance(image)
        if args.min_luminance is not None and luminance &lt; args.min_luminance:
            return None
        if args.max_luminance is not None and luminance &gt; args.max_luminance:
            return None
    # Reinitialize S3 client for each process
    s3_client = initialize_s3_client(args)
    # Resize image for condition if required
    image = resize_for_condition_image(image, args.condition_image_size)
    temp_path = os.path.join(args.temporary_folder, os.path.basename(image_path))
    try:
        image.save(temp_path, format=&quot;PNG&quot;)
        image.close()
    except:
        logger.error(f&quot;Error saving image&quot;)
    # Upload to S3
    upload_local_image_to_s3(temp_path, args, s3_client)
def fetch_and_upload_image(info, args):
    &quot;&quot;&quot;Fetch the image, process it, and upload it to S3.&quot;&quot;&quot;
    try:
        s3_client = initialize_s3_client(args)
        image = fetch_image(info, args)
    except Exception as e:
        if args.print_nonfatal_errors:
            logger.error(f&quot;Encountered error fetching file: {e}&quot;)
    upload_pil_to_s3(image, info[&quot;filename&quot;], args, s3_client)
def fetch_data(data, args, uri_column):
    &quot;&quot;&quot;Function to fetch all images specified in data and upload them to S3.&quot;&quot;&quot;
    to_fetch = {}
    for row in data:
        new_filename = content_to_filename(row[args.caption_field], args)
        if (
            hasattr(args, &quot;midjourney_data_checks&quot;)
            and args.midjourney_data_checks
            and &quot;image #&quot; not in row[args.caption_field].lower()
        ):
            # Midjourney&apos;s upscaler sucks. We only want single images, non-tiled.
            logger.debug(f&quot;Skipping: {row[args.caption_field]}&quot;)
            continue
        if new_filename not in to_fetch:
            to_fetch[new_filename] = {
                &quot;url&quot;: row[uri_column],
                &quot;filename&quot;: new_filename,
                &quot;args&quot;: args,
            }
    logging.info(f&quot;Fetching {len(to_fetch)} images (truncated): {list(to_fetch)[:10]}&quot;)
    with Pool(processes=args.num_workers) as pool:
        results = pool.starmap(
            fetch_and_upload_image,
            [(item, args) for item in to_fetch.values()],
        )
def main():
    args = parse_args()
    # Initialize S3 client
    s3_client = initialize_s3_client(args)
    # List existing files in the S3 bucket
    existing_files = []
    existing_files = list_all_s3_objects(s3_client, args.aws_bucket_name)
    logger.info(f&quot;Found {len(existing_files)} existing files in the S3 bucket.&quot;)
    # Process and upload images from the local folder
    if args.local_folder:
        process_local_folder_images(args, s3_client, existing_files)
    if args.git_lfs_repo:
        repo_path = os.path.join(args.temporary_folder, &quot;git-lfs-repo&quot;)
        if not os.path.exists(repo_path):
            logger.info(f&quot;Thin-cloning Git LFS repo to {repo_path}&quot;)
            os.system(
                f&quot;env GIT_LFS_SKIP_SMUDGE=1 git clone {args.git_lfs_repo} {repo_path}&quot;
            )
        else:
            logger.info(
                f&quot;Git LFS repo already exists at {repo_path}. Using existing files.&quot;
            )
        # Do we have *.parquet files in the dir, or .csv files?
        parquet_file_list = [f for f in Path(repo_path).glob(&quot;*.parquet&quot;)]
        csv_file_list = [f for f in Path(repo_path).glob(&quot;*.csv&quot;)]
        if len(parquet_file_list) &gt; 0:
            args.parquet_folder = repo_path
            logger.info(f&quot;Using Parquet files from {args.parquet_folder}&quot;)
        if len(csv_file_list) &gt; 0:
            args.csv_folder = repo_path
            logger.info(f&quot;Using CSV files from {args.csv_folder}&quot;)
        # Process and upload images from the git-lfs-repo
        process_git_lfs_images(args, s3_client)
    # Check if input folder exists
    parquet_files = []
    if args.parquet_folder is not None:
        if not os.path.exists(args.parquet_folder):
            logger.error(f&quot;Input folder &apos;{args.parquet_folder}&apos; does not exist.&quot;)
            return
        # Read Parquet file as DataFrame
        parquet_files = [f for f in Path(args.parquet_folder).glob(&quot;*.parquet&quot;)]
    csv_files = []
    if args.csv_folder is not None:
        if not os.path.exists(args.csv_folder):
            logger.error(f&quot;Input folder &apos;{args.csv_folder}&apos; does not exist.&quot;)
            return
        # Read Parquet file as DataFrame
        csv_files = [f for f in Path(args.csv_folder).glob(&quot;*.csv&quot;)]
    all_files = parquet_files + csv_files
    random.shuffle(all_files)
    logger.info(f&quot;Discovered catalogues: {all_files}&quot;)
    total_files = len(all_files)
    for i, file in enumerate(
        tqdm(all_files, desc=f&quot;Processing {total_files} Parquet files&quot;)
    ):
        if content_to_filename(file.name, args) in existing_files:
            logger.info(f&quot;Skipping already processed file: {file}&quot;)
            continue
        # If it&apos;s a parquet file from the Git LFS repo, pull it Just-in-Time
        if file.suffix == &quot;.parquet&quot;:
            if args.git_lfs_repo:
                logger.info(f&quot;Fetching {file} from Git LFS&quot;)
                # chdir to repo_path
                cwd = os.getcwd()
                os.chdir(repo_path)
                os.system(f&quot;env GIT_LFS_SKIP_SMUDGE=0 git lfs pull -I {file.name}&quot;)
                # return to prev dir
                os.chdir(cwd)
            logger.info(f&quot;Loading file: {file}&quot;)
            df = pd.read_parquet(file)
        elif file.suffix == &quot;.csv&quot;:
            df = pd.read_csv(file)
        else:
            logger.warning(f&quot;Unsupported file format: {file.suffix}&quot;)
            continue
        # Determine the URI column
        uri_column = get_uri_column(df)
        if args.caption_field is None:
            args.caption_field = get_caption_column(df)
        logger.info(f&quot;Caption field: {args.caption_field}&quot;)
        if not uri_column:
            logger.warning(f&quot;Row has no uri_column: {uri_column}&quot;)
            continue
        logger.info(f&quot;URI field: {uri_column}&quot;)
        if args.height_field is None:
            args.height_field = get_height_field(df)
        if args.width_field is None:
            args.width_field = get_width_field(df)
        logger.info(
            f&quot;Resolution fields: &apos;{args.width_field}&apos; and &apos;{args.height_field}&apos;&quot;
        )
        logger.info(f&quot;Before filtering, we have {len(df)} rows.&quot;)
        # Apply filters
        if &quot;pwatermark&quot; in df.columns:
            logger.info(
                f&quot;Applying pwatermark filter with threshold {args.pwatermark_threshold}&quot;
            )
            df = df[df[&quot;pwatermark&quot;] &lt;= args.pwatermark_threshold]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if &quot;aesthetic&quot; in df.columns:
            logger.info(
                f&quot;Applying aesthetic filter with threshold {args.aesthetic_threshold}&quot;
            )
            df = df[df[&quot;aesthetic&quot;] &gt;= args.aesthetic_threshold]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if args.width_field in df.columns and args.minimum_resolution &gt; 0:
            logger.info(
                f&quot;Applying minimum resolution filter with threshold {args.minimum_resolution}&quot;
            )
            df = df[df[args.width_field] &gt;= args.minimum_resolution]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if args.height_field in df.columns and args.minimum_resolution &gt; 0:
            logger.info(
                f&quot;Applying minimum resolution filter with threshold {args.minimum_resolution}&quot;
            )
            df = df[df[args.height_field] &gt;= args.minimum_resolution]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if (
            args.width_field in df.columns
            and args.height_field in df.columns
            and args.minimum_pixel_area &gt; 0
        ):
            # megapixel to pixel:
            minimum_pixel_area = args.minimum_pixel_area * 1000000
            logger.info(
                f&quot;Applying minimum pixel area filter with threshold {minimum_pixel_area}&quot;
            )
            df = df[df[args.width_field] * df[args.height_field] &gt;= minimum_pixel_area]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if &quot;similarity&quot; in df.columns:
            logger.info(
                f&quot;Applying similarity filter with threshold {args.similarity_threshold}&quot;
            )
            df = df[df[&quot;similarity&quot;] &gt;= args.similarity_threshold]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        if &quot;punsafe&quot; in df.columns:
            logger.info(
                f&quot;Applying unsafe filter with threshold {args.unsafe_threshold}&quot;
            )
            if args.invert_unsafe_threshold:
                logger.info(
                    &quot;Inverting unsafe threshold, so that more harmful content is included, rather than excluded.&quot;
                )
                df = df[df[&quot;punsafe&quot;] &gt;= args.unsafe_threshold]
            else:
                df = df[df[&quot;punsafe&quot;] &lt;= args.unsafe_threshold]
            logger.info(f&quot;Filtered to {len(df)} rows.&quot;)
        # TODO: Add more filters as needed
        # Fetch and process images
        to_fetch = df.to_dict(orient=&quot;records&quot;)
        logger.info(f&quot;Fetching {len(to_fetch)} images (truncated): {to_fetch[:5]}&quot;)
        fetch_data(to_fetch, args, uri_column)
        # Remove source file if argument is provided
        if args.delete_after_processing:
            try:
                os.remove(file)
            except:
                pass
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/dataset_from_kellyc.py">import requests
import re
import os
import argparse
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from PIL import Image
def get_image_width(url):
    &quot;&quot;&quot;Extract width from the image URL.&quot;&quot;&quot;
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    return int(query_params.get(&quot;w&quot;, [0])[0])
def get_photo_id(url):
    &quot;&quot;&quot;Extract photo ID from the image URL.&quot;&quot;&quot;
    match = re.search(r&quot;/photos/(\d+)&quot;, url)
    return match.group(1) if match else None
conn_timeout = 6
read_timeout = 60
timeouts = (conn_timeout, read_timeout)
def download_image(url, output_path, minimum_image_size: int, minimum_pixel_area: int):
    &quot;&quot;&quot;Download an image.&quot;&quot;&quot;
    response = requests.get(url, timeout=timeouts, stream=True)
    if response.status_code == 200:
        filename = os.path.basename(url.split(&quot;?&quot;)[0])
        file_path = os.path.join(output_path, filename)
        # Convert path to PNG:
        file_path = file_path.replace(&quot;.jpg&quot;, &quot;.png&quot;)
        with open(file_path, &quot;wb&quot;) as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        # Check if the file meets the minimum size requirements
        image = Image.open(file_path)
        width, height = image.size
        if minimum_image_size &gt; 0 and (
            width &lt; minimum_image_size or height &lt; minimum_image_size
        ):
            os.remove(file_path)
            return f&quot;Nuked tiny image: {url}&quot;
        if minimum_pixel_area &gt; 0 and (width * height &lt; minimum_pixel_area):
            os.remove(file_path)
            return f&quot;Nuked tiny image: {url}&quot;
        return f&quot;Downloaded: {url}&quot;
    return f&quot;Failed to download: {url}&quot;
def process_urls(urls, output_path, minimum_image_size: int, minimum_pixel_area: int):
    &quot;&quot;&quot;Process a list of URLs.&quot;&quot;&quot;
    # Simple URL list
    results = []
    for url in urls:
        result = download_image(
            url, output_path, minimum_image_size, minimum_pixel_area
        )
        results.append(result)
    return &quot;\n&quot;.join(results)
def main(args):
    os.makedirs(args.output_path, exist_ok=True)
    url_groups = {}
    with open(args.file_path, &quot;r&quot;) as file:
        for line in file:
            urls = line.strip().split()
            # Treat as a simple URL list
            url_groups[line] = urls
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = [
            executor.submit(
                process_urls,
                urls,
                args.output_path,
                args.minimum_image_size,
                args.minimum_pixel_area,
            )
            for urls in url_groups.values()
        ]
        for future in tqdm(
            as_completed(futures), total=len(futures), desc=&quot;Downloading images&quot;
        ):
            if args.debug:
                print(future.result())
if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser(
        description=&quot;Download smallest images from Pexels.&quot;
    )
    parser.add_argument(
        &quot;--file_path&quot;, type=str, help=&quot;Path to the text file containing image URLs.&quot;
    )
    parser.add_argument(
        &quot;--output_path&quot;,
        type=str,
        help=&quot;Path to the directory where images will be saved.&quot;,
    )
    parser.add_argument(
        &quot;--minimum_image_size&quot;,
        type=int,
        default=0,
        help=&quot;Both sides of the image must be larger than this. ZERO disables this.&quot;,
    )
    parser.add_argument(
        &quot;--minimum_pixel_area&quot;,
        type=int,
        default=0,
        help=&quot;The total number of pixels in the image must be larger than this. ZERO disables this. Recommended value: 1024*1024&quot;,
    )
    parser.add_argument(
        &quot;--workers&quot;,
        type=int,
        default=64,
        help=&quot;Number of worker threads. Default is 64.&quot;,
    )
    parser.add_argument(
        &quot;--debug&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Print debug messages.&quot;,
    )
    args = parser.parse_args()
    main(args)</file><file path="toolkit/datasets/dataset_from_laion.py">from PIL import Image
import os, logging
import csv
import shutil
import requests
import re
import sys
import piexif
from concurrent.futures import ThreadPoolExecutor
def calculate_luminance(img: Image):
    pixels = list(img.getdata())
    luminance_values = []
    for pixel in pixels:
        r, g, b = pixel[:3]  # Assuming the image is RGB or RGBA
        luminance = 0.299 * r + 0.587 * g + 0.114 * b
        luminance_values.append(luminance)
    # Return average luminance for the entire image
    avg_luminance = sum(luminance_values) / len(luminance_values)
    return avg_luminance
def get_camera_model(img):
    try:
        if &quot;photoshop&quot; in img.info:
            return None
    except:
        pass
    try:
        exif_data = piexif.load(img.info[&quot;exif&quot;])
    except:
        return None
    if piexif.ImageIFD.Model in exif_data[&quot;0th&quot;]:
        camera_model = exif_data[&quot;0th&quot;][piexif.ImageIFD.Model]
        print(f&quot;Camera Model: {camera_model}&quot;)
        return str(camera_model)
    else:
        return None
# Constants
FILE = &quot;dataset.csv&quot;  # The CSV file to read data from
OUTPUT_DIR = &quot;/notebooks/datasets/camera&quot;  # Directory to save images
NUM_WORKERS = 8  # Number of worker threads for parallel downloading
logger = logging.getLogger(&quot;root&quot;)
logger.setLevel(logging.INFO)
# Check if output directory exists, create if it does not
if not os.path.exists(OUTPUT_DIR):
    try:
        os.makedirs(OUTPUT_DIR)
    except Exception as e:
        logging.info(f&quot;Could not create output directory: {e}&quot;)
        sys.exit(1)
# Check if CSV file exists
if not os.path.exists(FILE):
    logging.info(f&quot;Could not find CSV file: {FILE}&quot;)
    sys.exit(1)
import random
def content_to_filename(content):
    &quot;&quot;&quot;
    Function to convert content to filename by stripping everything after &apos;--&apos;,
    replacing non-alphanumeric characters and spaces, converting to lowercase,
    removing leading/trailing underscores, and limiting filename length to 128.
    &quot;&quot;&quot;
    # Split on &apos;--&apos; and take the first part
    content = content.split(&quot;--&quot;, 1)[0]
    # Split on &apos;Upscaled by&apos; and take the first part
    content = content.split(&quot; - Upscaled by&quot;, 1)[0]
    # Remove URLs
    cleaned_content = re.sub(r&quot;https*://\S*&quot;, &quot;&quot;, content)
    # Replace non-alphanumeric characters and spaces, convert to lowercase, remove leading/trailing underscores
    cleaned_content = re.sub(r&quot;[^a-zA-Z0-9 ]&quot;, &quot;&quot;, cleaned_content)
    cleaned_content = cleaned_content.replace(&quot; &quot;, &quot;_&quot;).lower().strip(&quot;_&quot;)
    # If cleaned_content is empty after removing URLs, generate a random filename
    if cleaned_content == &quot;&quot;:
        cleaned_content = f&quot;midjourney_{random.randint(0, 1000000)}&quot;
    # Limit filename length to 128
    cleaned_content = (
        cleaned_content[:128] if len(cleaned_content) &gt; 128 else cleaned_content
    )
    return cleaned_content + &quot;.png&quot;
def load_csv(file):
    &quot;&quot;&quot;
    Function to load CSV data into a list of dictionaries
    &quot;&quot;&quot;
    data = []
    with open(file, newline=&quot;&quot;) as csvfile:
        try:
            reader = csv.DictReader(csvfile)
            for row in reader:
                data.append(row)
        except Exception as e:
            logging.info(f&quot;Could not advance reader: {e}&quot;)
    return data
conn_timeout = 6
read_timeout = 60
timeouts = (conn_timeout, read_timeout)
def _resize_for_condition_image(input_image: Image.Image, resolution: int):
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    aspect_ratio = round(W / H, 2)
    msg = f&quot;Inspecting image of aspect {aspect_ratio} and size {W}x{H} to &quot;
    if W &lt; H:
        W = resolution
        H = int(resolution / aspect_ratio)  # Calculate the new height
    elif H &lt; W:
        H = resolution
        W = int(resolution * aspect_ratio)  # Calculate the new width
    if W == resolution and H == resolution:
        logging.debug(f&quot;Returning square image of size {resolution}x{resolution}&quot;)
        return input_image
    if W == H:
        W = resolution
        H = resolution
    msg = f&quot;{msg} {W}x{H}.&quot;
    logging.info(msg)
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img
def fetch_image(info):
    &quot;&quot;&quot;
    Function to fetch image from a URL and save it to disk if it is square.
    Enhanced to handle exceptions robustly.
    &quot;&quot;&quot;
    filename = info[&quot;filename&quot;]
    url = info[&quot;url&quot;]
    current_file_path = os.path.join(OUTPUT_DIR, filename)
    # Skip download if file already exists
    if os.path.exists(current_file_path):
        logging.info(f&quot;{filename} already exists, skipping download...&quot;)
        return
    try:
        r = requests.get(url, timeout=timeouts, stream=True)
        if r.status_code != 200:
            logging.warn(
                f&quot;Failed to fetch {filename} from {url}. Status code: {r.status_code}&quot;
            )
            return
        with open(current_file_path, &quot;wb&quot;) as f:
            r.raw.decode_content = True
            shutil.copyfileobj(r.raw, f)
        image = Image.open(current_file_path)
        if image.width &lt; 1024 or image.height &lt; 1024:
            logging.info(f&quot;Skipping small image ({image.width}x{image.height}).&quot;)
            image.close()
            os.remove(current_file_path)
            return
        camera_model = f&quot;{get_camera_model(image)}&quot;.lower()
        if camera_model is None or camera_model == &quot;none&quot;:
            image.close()
            os.remove(current_file_path)
            return
        if (
            &quot;nikon&quot; not in camera_model
            and &quot;canon&quot; not in camera_model
            and &quot;fujifilm&quot; not in camera_model
        ):
            logging.info(f&quot;Skipping non-Canon/Nikon/Fujifilm: {camera_model}&quot;)
            image.close()
            os.remove(current_file_path)
            return
        current_luminance = calculate_luminance(image)
        if current_luminance &gt; 0.12:
            logging.info(f&quot;Skipping bright image ({current_luminance}).&quot;)
            image.close()
            os.remove(current_file_path)
            return
        logging.info(f&quot;image luminance: {current_luminance}&quot;)
        image = _resize_for_condition_image(image, 1024)
        image.save(current_file_path, format=&quot;PNG&quot;)
        image.close()
        with open(os.path.join(OUTPUT_DIR, filename + &quot;.txt&quot;), &quot;w&quot;) as f:
            f.write(info[&quot;TEXT&quot;])
    except requests.RequestException as e:
        logging.error(f&quot;Error fetching {filename} from {url}: {e}&quot;)
        if os.path.exists(current_file_path):
            os.remove(current_file_path)
    except Exception as e:
        logging.error(f&quot;Unexpected error processing {filename}: {e}&quot;)
        if os.path.exists(current_file_path):
            os.remove(current_file_path)
def fetch_data(data):
    &quot;&quot;&quot;
    Function to fetch all images specified in data
    &quot;&quot;&quot;
    to_fetch = {}
    count = 0
    for row in data:
        try:
            if (
                float(int(row[&quot;WIDTH&quot;])) &lt; 1024
                or float(int(row[&quot;HEIGHT&quot;])) &lt; 1024
                or float(row[&quot;pwatermark&quot;]) &gt; 0.4
            ):
                continue
        except Exception as e:
            continue
        new_filename = content_to_filename(row[&quot;TEXT&quot;])
        if new_filename not in to_fetch and count &lt; 100000:
            to_fetch = {
                content_to_filename(row[&quot;TEXT&quot;]): {
                    &quot;url&quot;: row[&quot;URL&quot;],
                    &quot;filename&quot;: content_to_filename(row[&quot;TEXT&quot;]),
                    &quot;TEXT&quot;: row[&quot;TEXT&quot;],
                }
                for row in data
                if row.get(&quot;WIDTH&quot;)
                and float(row.get(&quot;WIDTH&quot;, 0)) &gt;= 1024
                and row.get(&quot;HEIGHT&quot;)
                and float(row.get(&quot;HEIGHT&quot;, 0)) &gt;= 1024
                and row.get(&quot;pwatermark&quot;)
                and float(row.get(&quot;pwatermark&quot;, 1)) &lt;= 0.4
            }
            count += 1
    logging.info(f&quot;Fetching {len(to_fetch)} images...&quot;)
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
        executor.map(fetch_image, to_fetch.values())
def main():
    &quot;&quot;&quot;
    Main function to load CSV and fetch images
    &quot;&quot;&quot;
    data = load_csv(FILE)
    fetch_data(data)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/dataset_from_pixilart.py">import os
import json
import time
import random
import logging
import requests
import pandas as pd
from tqdm import tqdm
from io import BytesIO
from PIL import Image
from concurrent.futures import ThreadPoolExecutor
# Configuration and setup
http = requests.Session()
timeouts = (6, 60)
output_path = &quot;/Volumes/ml/datasets/pixilart&quot;
output_dataframe_path = &quot;pixilart.parquet&quot;
subsets = [&quot;highlighted&quot;, &quot;rising&quot;, &quot;popular&quot;, &quot;featured&quot;]
max_workers = 5
number_of_random_values = 1000
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not os.path.exists(output_path):
    os.makedirs(output_path, exist_ok=True)
# Function to fetch data from Pixilart API
def pixilart(sequence: int, subset: str, x_csrf_token: str, x_xsrf_token: str):
    url = f&quot;https://www.pixilart.com/api/w/gallery/{sequence}/0/{subset}?user=true&amp;liked=true&amp;comments=true&quot;
    headers = {
        &quot;accept&quot;: &quot;application/json, text/plain, */*&quot;,
        &quot;cookie&quot;: &apos;pa_st={&quot;challenge&quot;:0}; XSRF-TOKEN=&apos;
        + x_csrf_token
        + &quot;; pixil_session=&quot;
        + x_xsrf_token,
        &quot;dnt&quot;: &quot;1&quot;,
        &quot;priority&quot;: &quot;u=1, i&quot;,
        &quot;referer&quot;: &quot;https://www.pixilart.com/gallery/staff-picks&quot;,
        &quot;sec-ch-ua&quot;: &apos;&quot;Chromium&quot;;v=&quot;124&quot;, &quot;Google Chrome&quot;;v=&quot;124&quot;, &quot;Not-A.Brand&quot;;v=&quot;99&quot;&apos;,
        &quot;sec-ch-ua-mobile&quot;: &quot;?0&quot;,
        &quot;sec-ch-ua-platform&quot;: &apos;&quot;macOS&quot;&apos;,
        &quot;sec-fetch-dest&quot;: &quot;empty&quot;,
        &quot;sec-fetch-mode&quot;: &quot;cors&quot;,
        &quot;sec-fetch-site&quot;: &quot;same-origin&quot;,
        &quot;sec-gpc&quot;: &quot;1&quot;,
        &quot;user-agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36&quot;,
        &quot;x-csrf-token&quot;: x_csrf_token,
        &quot;x-requested-with&quot;: &quot;XMLHttpRequest&quot;,
        &quot;x-xsrf-token&quot;: x_xsrf_token,
    }
    for attempt in range(3):
        try:
            response = http.get(url, headers=headers, timeout=timeouts)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            logger.warning(f&quot;Attempt {attempt + 1} failed: {e}&quot;)
            time.sleep(5)
    return None
# Function to download images
def download_image(url, filename):
    response = requests.get(url)
    with BytesIO(response.content) as img_data:
        image = Image.open(img_data)
        image.save(filename)
# Load or initialize dataframe
if os.path.exists(output_dataframe_path):
    df = pd.read_parquet(output_dataframe_path)
else:
    df = pd.DataFrame(
        columns=[
            &quot;subset&quot;,
            &quot;sequence&quot;,
            &quot;subset-sequence-element&quot;,
            &quot;title&quot;,
            &quot;description&quot;,
            &quot;views&quot;,
            &quot;filename&quot;,
            &quot;pixel_size&quot;,
            &quot;has_watermark&quot;,
            &quot;image_hash&quot;,
            &quot;image_url&quot;,
            &quot;full_image_url&quot;,
            &quot;likes_count&quot;,
            &quot;comments_count&quot;,
            &quot;width&quot;,
            &quot;height&quot;,
            &quot;date_created&quot;,
            &quot;content_warning&quot;,
            &quot;warning&quot;,
            &quot;liked&quot;,
        ]
    )
# Generate random sequence values
random_values = range(1, number_of_random_values + 1)
# Process sequences and subsets
records_to_add = []
with ThreadPoolExecutor(max_workers=max_workers) as executor:
    for sequence in tqdm(random_values, desc=&quot;Sequences&quot;, position=1):
        for subset in tqdm(subsets, desc=&quot;Subsets&quot;, position=2, leave=False):
            if df[(df[&quot;subset&quot;] == subset) &amp; (df[&quot;sequence&quot;] == sequence)].empty:
                result = pixilart(
                    sequence, subset, &quot;your_x_csrf_token&quot;, &quot;your_x_xsrf_token&quot;
                )
                if result and &quot;art&quot; in result:
                    for idx, element in enumerate(result[&quot;art&quot;], start=1):
                        record = {
                            &quot;subset&quot;: subset,
                            &quot;sequence&quot;: sequence,
                            &quot;subset-sequence-element&quot;: f&quot;{subset}.{sequence}.{idx}&quot;,
                            &quot;title&quot;: element[&quot;title&quot;],
                            &quot;description&quot;: element[&quot;description&quot;],
                            &quot;filename&quot;: f&quot;{element[&apos;image_id&apos;]}-{idx} {element[&apos;title&apos;]}.png&quot;,
                            &quot;views&quot;: element[&quot;views&quot;],
                            &quot;image_hash&quot;: element[&quot;image_id&quot;],
                            &quot;image_url&quot;: element[&quot;image_url&quot;],
                            &quot;full_image_url&quot;: element[&quot;full_image_url&quot;],
                            &quot;likes_count&quot;: element[&quot;likes_count&quot;],
                            &quot;pixel_size&quot;: element.get(&quot;pixel_size&quot;, 0),
                            &quot;has_watermark&quot;: element.get(&quot;has_watermark&quot;, False),
                            &quot;comments_count&quot;: element[&quot;comments_count&quot;],
                            &quot;width&quot;: element[&quot;width&quot;],
                            &quot;height&quot;: element[&quot;height&quot;],
                            &quot;date_created&quot;: element[&quot;date_created&quot;],
                            &quot;content_warning&quot;: element.get(&quot;content_warning&quot;),
                            &quot;warning&quot;: str(element[&quot;warning&quot;]),
                            &quot;liked&quot;: element[&quot;liked&quot;],
                        }
                        records_to_add.append(record)
                        image_filename = os.path.join(output_path, record[&quot;filename&quot;])
                        executor.submit(
                            download_image, record[&quot;full_image_url&quot;], image_filename
                        )
# Update dataframe and save to parquet
if records_to_add:
    new_records_df = pd.DataFrame(records_to_add)
    df = pd.concat([df, new_records_df], ignore_index=True)
    df.drop_duplicates(subset=[&quot;full_image_url&quot;], inplace=True)
    df.to_parquet(output_dataframe_path)</file><file path="toolkit/datasets/discord_scrape.py">import requests
import re
import pandas as pd
from tqdm import tqdm
import signal
import sys
import time
import json
from concurrent.futures import ThreadPoolExecutor
import argparse
# Define a signal handler function to handle Ctrl+C
def signal_handler(sig, frame):
    print(f&quot;Ctrl+C detected. Saving data to mj-{current_channel}.parquet...&quot;)
    df = pd.DataFrame(all_data)
    df.to_parquet(f&quot;mj-{current_channel}.parquet&quot;, engine=&quot;pyarrow&quot;)
    print(f&quot;Data saved to mj-{current_channel}.parquet. Exiting...&quot;)
    sys.exit(0)
def process_channel(channel_id, position, headers):
    &quot;&quot;&quot;Processes a Discord channel to collect and save message data.
    Args:
        channel_id (int): The ID of the Discord channel to process.
    &quot;&quot;&quot;
    global retry_after
    url = f&quot;https://discord.com/api/v9/channels/{channel_id}/messages?limit=50&quot;
    all_data = []  # List to collect all the processed data for this channel
    last_message_id = None  # To keep track of the last message ID for pagination
    for page in tqdm(
        range(2000), position=position.index(channel_id), desc=f&quot;Channel {channel_id}&quot;
    ):
        page_url = url
        if last_message_id:
            page_url += f&quot;&amp;before={last_message_id}&quot;
        time.sleep(retry_after)
        response = requests.get(page_url, headers=headers)
        if response.status_code == 429:
            print(
                f&quot;Rate limited. Waiting for {response.headers[&apos;Retry-After&apos;]} seconds...&quot;
            )
            time.sleep(retry_after)
            continue
        elif not response.status_code == 200:
            print(f&quot;Failed to retrieve data: {response.status_code}&quot;)
            break
        messages = response.json()
        if not messages:
            break  # No more messages to fetch
        last_message_id = messages[-1][
            &quot;id&quot;
        ]  # Update the last_message_id for the next page
        target_source = &quot;Midjourney Bot&quot;
        for entry in messages:
            if entry[&quot;author&quot;][&quot;username&quot;] != target_source:
                continue
            if &quot;Variations&quot; in entry[&quot;content&quot;] or &quot;Image #&quot; not in entry[&quot;content&quot;]:
                continue
            # print(f&quot;Entry id: {entry[&apos;id&apos;]}, author: {entry[&apos;author&apos;][&apos;username&apos;]}, attachments: {entry[&apos;attachments&apos;]}&quot;)
            # Capture first text group between &quot;**&quot;s using regex
            search = re.search(r&quot;\*\*(.*?)\*\*&quot;, entry[&quot;content&quot;])
            stripped_content = &quot;&quot;
            if hasattr(search, &quot;group&quot;):
                stripped_content = search.group(1)
            # Remove any &lt;http(s)://..&gt; url with surrounding brackets:
            stripped_content = re.sub(r&quot;&lt;(http[s]?://.*?)&gt;&quot;, &quot;&quot;, stripped_content)
            # Split the prompt into two pieces, and use only the first piece before --
            pieces = stripped_content.split(&quot;--&quot;)
            stripped_content = pieces[0].strip()
            version = 5.2
            arguments = pieces[1].strip() if len(pieces) &gt; 1 else &quot;&quot;
            # If we have --v &lt;float&gt; inside the arguments, that is our version:
            version_search = re.search(r&quot;v\s(\d+.\d+)&quot;, arguments)
            if hasattr(version_search, &quot;group&quot;):
                version = version_search.group(1)
            # Sometimes, people put &quot;ar x:y&quot; as in, &quot;ar 16 9&quot; or &quot;ar 16:9&quot; without the --, but we want to remove any mention of aspect ratio:
            stripped_content = re.sub(r&quot;ar\s\d+:\d+&quot;, &quot;&quot;, stripped_content)
            # We likely need to shorten the output so that it can work as a Linux filename:
            stripped_content = stripped_content[:225]
            # print(f&quot;unfilteredcontent: {entry[&apos;content&apos;]}&quot;)
            # print(f&quot;-&gt; content: {stripped_content}\n&quot;)
            # print(f&quot;-&gt; attachments: {entry[&apos;attachments&apos;]}\n&quot;)
            # Collecting data
            if len(entry[&quot;attachments&quot;]) == 0:
                continue
            processed_data = {
                &quot;id&quot;: entry[&quot;id&quot;],
                &quot;version&quot;: str(version),
                &quot;arguments&quot;: arguments,
                &quot;original_text&quot;: entry[&quot;content&quot;],
                &quot;caption&quot;: stripped_content,
                &quot;url&quot;: entry[&quot;attachments&quot;][0][&quot;url&quot;].split(&quot;?&quot;)[0],
                &quot;width&quot;: entry[&quot;attachments&quot;][0][&quot;width&quot;],
                &quot;height&quot;: entry[&quot;attachments&quot;][0][&quot;height&quot;],
            }
            all_data.append(processed_data)
    # Save the data to a Parquet file at the end of processing this channel
    df = pd.DataFrame(all_data)
    df.to_parquet(f&quot;mj-general-{channel_id}.parquet&quot;, engine=&quot;pyarrow&quot;)
def load_config(config_file):
    &quot;&quot;&quot;Loads configuration from a JSON file.
    Args:
        config_file (str): Path to the JSON configuration file.
    Returns:
        dict: The configuration data.
    &quot;&quot;&quot;
    with open(config_file, &quot;r&quot;) as file:
        config = json.load(file)
    return config
def main():
    parser = argparse.ArgumentParser(
        description=&quot;Collect and save message data from Discord channels.&quot;
    )
    parser.add_argument(&quot;config_file&quot;, help=&quot;Path to the JSON configuration file.&quot;)
    args = parser.parse_args()
    config = load_config(args.config_file)
    # Set the signal handler for SIGINT (Ctrl+C)
    signal.signal(signal.SIGINT, signal_handler)
    # Discord credentials and settings from configuration
    headers = config[&quot;headers&quot;]
    channel_list = config[&quot;channel_list&quot;]
    # Use ThreadPoolExecutor to process each channel id in parallel
    with ThreadPoolExecutor() as executor:
        executor.map(process_channel, channel_list)
    print(&quot;All data saved.&quot;)
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/enhance_with_controlnet.py">import torch
from PIL import Image
from diffusers import ControlNetModel, DiffusionPipeline
from diffusers.utils import load_image
def resize_for_condition_image(input_image: Image.Image, resolution: int):
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    k = float(resolution) / min(H, W)
    H *= k
    W *= k
    H = int(round(H / 64.0)) * 64
    W = int(round(W / 64.0)) * 64
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img
controlnet = ControlNetModel.from_pretrained(
    &quot;lllyasviel/control_v11f1e_sd15_tile&quot;, torch_dtype=torch.bfloat16
)
pipe = DiffusionPipeline.from_pretrained(
    &quot;SG161222/Realistic_Vision_V5.0_noVAE&quot;,
    custom_pipeline=&quot;stable_diffusion_controlnet_img2img&quot;,
    controlnet=controlnet,
    torch_dtype=torch.bfloat16,
).to(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
from diffusers import DDIMScheduler
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
# pipe.unet.set_attention_slice(1)
source_image = load_image(
    &quot;/Volumes/models/training/datasets/animals/antelope/0e17715606.jpg&quot;
)
condition_image = resize_for_condition_image(source_image, 1024)
image = pipe(
    prompt=&quot;best quality&quot;,
    negative_prompt=&quot;deformed eyes, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated&quot;,
    image=condition_image,
    controlnet_conditioning_image=condition_image,
    width=condition_image.size[0],
    height=condition_image.size[1],
    strength=1.0,
    generator=torch.manual_seed(20),
    num_inference_steps=32,
).images[0]
image.save(&quot;output.png&quot;)</file><file path="toolkit/datasets/folder_to_parquet.py">&quot;&quot;&quot;
This script exists to scan a folder and import all of the image data into equally sized parquet files.
Fields collected:
filename
image hash
width
height
luminance
image_data
&quot;&quot;&quot;
import os, argparse
from PIL import Image
import numpy as np
import pandas as pd
from tqdm import tqdm
def get_image_hash(image):
    &quot;&quot;&quot;Calculate the hash of an image.&quot;&quot;&quot;
    image = image.convert(&quot;L&quot;).resize((8, 8))
    pixels = list(image.getdata())
    avg = sum(pixels) / len(pixels)
    bits = &quot;&quot;.join(&quot;1&quot; if pixel &gt; avg else &quot;0&quot; for pixel in pixels)
    return int(bits, 2)
def get_image_luminance(image):
    &quot;&quot;&quot;Calculate the luminance of an image.&quot;&quot;&quot;
    image = image.convert(&quot;L&quot;)
    pixels = list(image.getdata())
    return sum(pixels) / len(pixels)
def get_size(image):
    &quot;&quot;&quot;Get the size of an image.&quot;&quot;&quot;
    return image.size
argparser = argparse.ArgumentParser()
argparser.add_argument(&quot;input_folder&quot;, help=&quot;Folder to scan for images&quot;)
argparser.add_argument(&quot;output_folder&quot;, help=&quot;Folder to save parquet files&quot;)
args = argparser.parse_args()
os.makedirs(args.output_folder, exist_ok=True)
data = []
for root, _, files in os.walk(args.input_folder):
    for file in tqdm(files, desc=&quot;Processing images&quot;):
        try:
            image = Image.open(os.path.join(root, file))
        except:
            continue
        width, height = get_size(image)
        luminance = get_image_luminance(image)
        image_hash = get_image_hash(image)
        # Get the smallest original compressed representation of the image
        file_data = open(os.path.join(root, file), &quot;rb&quot;).read()
        image_data = np.frombuffer(file_data, dtype=np.uint8)
        data.append((file, image_hash, width, height, luminance, image_data))
df = pd.DataFrame(
    data, columns=[&quot;filename&quot;, &quot;image_hash&quot;, &quot;width&quot;, &quot;height&quot;, &quot;luminance&quot;, &quot;image&quot;]
)
df.to_parquet(os.path.join(args.output_folder, &quot;images.parquet&quot;), index=False)
print(&quot;Done!&quot;)</file><file path="toolkit/datasets/random_recrop_for_json_image_metadata.py">&apos;&apos;&apos;
This script is used to recrop your JSON image metadata dataset so that you can
safely delete the VAE cache and then recreate it with new crops. Just point it
at your multidatabackend.json file and it will take care of the rest, then
delete your VAE cache folder and let ST recache.
&apos;&apos;&apos;
import sys
import json
import random
import shutil
import os
def update_crop_coordinates_from_multidatabackend(multidatabackend_file):
    # Read the multidatabackend.json file
    with open(multidatabackend_file, &apos;r&apos;) as f:
        datasets = json.load(f)
    # Ensure datasets is a list
    if not isinstance(datasets, list):
        datasets = [datasets]
    for dataset in datasets:
        # Skip datasets that are disabled
        if dataset.get(&apos;disabled&apos;, False):
            continue
        # Get required fields
        instance_data_dir = dataset.get(&apos;instance_data_dir&apos;)
        cache_file_suffix = dataset.get(&apos;cache_file_suffix&apos;)
        if not instance_data_dir or not cache_file_suffix:
            print(f&quot;Skipping dataset {dataset.get(&apos;id&apos;, &apos;unknown&apos;)} due to missing &apos;instance_data_dir&apos; or &apos;cache_file_suffix&apos;&quot;)
            continue
        # Build the metadata file path
        metadata_file = os.path.join(instance_data_dir, f&apos;aspect_ratio_bucket_metadata_{cache_file_suffix}.json&apos;)
        # Check if metadata file exists
        if not os.path.exists(metadata_file):
            print(f&quot;Metadata file {metadata_file} does not exist, skipping&quot;)
            continue
        # Now process the metadata file
        with open(metadata_file, &apos;r&apos;) as f:
            data = json.load(f)
        for key in data:
            metadata = data[key]
            inter_size = metadata.get(&apos;intermediary_size&apos;)
            target_size = metadata.get(&apos;target_size&apos;)
            if inter_size is None or target_size is None:
                continue
            # Assuming sizes are in (height, width) format
            inter_height, inter_width = inter_size
            target_height, target_width = target_size
            max_crop_top = max(inter_height - target_height, 0)
            max_crop_left = max(inter_width - target_width, 0)
            crop_top = random.randint(0, max_crop_top)
            crop_left = random.randint(0, max_crop_left)
            # Update the crop_coordinates
            metadata[&apos;crop_coordinates&apos;] = [crop_top, crop_left]
        # Backup the original metadata file
        backup_file = metadata_file + &apos;.bak&apos;
        shutil.copyfile(metadata_file, backup_file)
        # Write the updated data back to the metadata file
        with open(metadata_file, &apos;w&apos;) as f:
            json.dump(data, f, indent=2)
        print(f&quot;Updated crop_coordinates in {metadata_file}, backup saved as {backup_file}&quot;)
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 2:
        print(&quot;Usage: python update_crop_coordinates.py multidatabackend.json&quot;)
        sys.exit(1)
    multidatabackend_file = sys.argv[1]
    update_crop_coordinates_from_multidatabackend(multidatabackend_file)</file><file path="toolkit/datasets/retrieve_s3_bucket.py">import boto3, os, logging, argparse, datetime
from botocore.config import Config
from PIL import Image
# Set up logging
logging.basicConfig(level=os.getenv(&quot;LOGLEVEL&quot;, &quot;INFO&quot;))
logger = logging.getLogger(__name__)
def initialize_s3_client(args):
    &quot;&quot;&quot;Initialize the boto3 S3 client using the provided AWS credentials and settings.&quot;&quot;&quot;
    s3_config = Config(max_pool_connections=100)
    s3_client = boto3.client(
        &quot;s3&quot;,
        endpoint_url=args.aws_endpoint_url,
        region_name=args.aws_region_name,
        aws_access_key_id=args.aws_access_key_id,
        aws_secret_access_key=args.aws_secret_access_key,
        config=s3_config,
    )
    return s3_client
from concurrent.futures import ThreadPoolExecutor
def resize_for_condition_image(input_image, resolution):
    if resolution == 0:
        return input_image
    input_image = input_image.convert(&quot;RGB&quot;)
    W, H = input_image.size
    aspect_ratio = round(W / H, 2)
    if W &lt; H:
        W = resolution
        H = int(resolution / aspect_ratio)
    elif H &lt; W:
        H = resolution
        W = int(resolution * aspect_ratio)
    if W == H:
        W = resolution
        H = resolution
    img = input_image.resize((W, H), resample=Image.LANCZOS)
    return img
def retrieve_object(s3_client, bucket_name, object_key):
    try:
        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
        if &quot;.parquet&quot; in object_key or &quot;.json&quot; in object_key:
            # save file as is
            with open(object_key, &quot;wb&quot;) as f:
                f.write(response[&quot;Body&quot;].read())
            return
        logger.info(f&quot;Retrieved: {object_key}&quot;)
        # create PIL Image
        image = Image.open(response[&quot;Body&quot;])
        # resize image
        resized_image = resize_for_condition_image(image, 1024)
        resized_image.save(object_key, format=&quot;PNG&quot;)
    except Exception as e:
        logger.error(f&quot;Error retrieving {object_key} in bucket {bucket_name}: {e}&quot;)
def retrieve_s3_bucket(
    s3_client,
    bucket_name,
    num_workers=10,
    search_pattern: str = None,
    older_than_date: str = None,
):
    try:
        logger.info(f&quot;Retrieving bucket {bucket_name}&quot;)
        # Convert the date string to a datetime object
        if older_than_date:
            target_date = datetime.datetime.strptime(older_than_date, &quot;%Y-%m-%d&quot;)
        else:
            target_date = None
        # Initialize paginator
        paginator = s3_client.get_paginator(&quot;list_objects_v2&quot;)
        # Create a PageIterator from the Paginator
        page_iterator = paginator.paginate(Bucket=bucket_name)
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            for page in page_iterator:
                if &quot;Contents&quot; not in page:
                    logger.info(f&quot;No more items in bucket {bucket_name}&quot;)
                    break
                # Filter by the older_than_date if provided
                if target_date:
                    filtered_objects = [
                        s3_object
                        for s3_object in page[&quot;Contents&quot;]
                        if s3_object[&quot;LastModified&quot;].replace(tzinfo=None) &lt; target_date
                    ]
                else:
                    filtered_objects = page[&quot;Contents&quot;]
                if search_pattern is not None:
                    keys_to_retrieve = [
                        s3_object[&quot;Key&quot;]
                        for s3_object in filtered_objects
                        if search_pattern in s3_object[&quot;Key&quot;]
                    ]
                else:
                    keys_to_retrieve = [
                        s3_object[&quot;Key&quot;] for s3_object in filtered_objects
                    ]
                executor.map(
                    retrieve_object,
                    [s3_client] * len(keys_to_retrieve),
                    [bucket_name] * len(keys_to_retrieve),
                    keys_to_retrieve,
                )
        logger.info(f&quot;Retrieved bucket {bucket_name}&quot;)
    except Exception as e:
        logger.error(f&quot;Error retrieving bucket {bucket_name}: {e}&quot;)
def parse_args():
    parser = argparse.ArgumentParser(description=&quot;Download an S3 bucket.&quot;)
    parser.add_argument(
        &quot;--aws_bucket_name&quot;,
        type=str,
        required=True,
        help=&quot;The AWS bucket name to clear.&quot;,
    )
    parser.add_argument(&quot;--aws_endpoint_url&quot;, type=str, help=&quot;The AWS server to use.&quot;)
    parser.add_argument(
        &quot;--num_workers&quot;,
        type=int,
        help=&quot;Number of workers to use for retrieving.&quot;,
        default=64,
    )
    parser.add_argument(
        &quot;--search_pattern&quot;,
        type=str,
        help=&quot;If provided, files with this in their Content key will be retrieved only.&quot;,
        default=None,
    )
    parser.add_argument(&quot;--aws_region_name&quot;, type=str, help=&quot;The AWS region to use.&quot;)
    parser.add_argument(&quot;--aws_access_key_id&quot;, type=str, help=&quot;AWS access key ID.&quot;)
    parser.add_argument(
        &quot;--aws_secret_access_key&quot;, type=str, help=&quot;AWS secret access key.&quot;
    )
    parser.add_argument(
        &quot;--older_than_date&quot;,
        type=str,
        help=&quot;If provided, only files older than this date (format: YYYY-MM-DD) will be retrieved.&quot;,
        default=None,
    )
    return parser.parse_args()
def main():
    args = parse_args()
    s3_client = initialize_s3_client(args)
    retrieve_s3_bucket(
        s3_client,
        args.aws_bucket_name,
        num_workers=args.num_workers,
        search_pattern=args.search_pattern,
        older_than_date=args.older_than_date,
    )
if __name__ == &quot;__main__&quot;:
    main()</file><file path="toolkit/datasets/update_parquet.py">import os
import pandas as pd
from PIL import Image
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm
# set &apos;fork&apos; spawn mode
import multiprocessing as mp
mp.set_start_method(&quot;fork&quot;)
PARQUET_FILE = &quot;photo-concept-bucket.parquet&quot;
IMAGE_DATA = &quot;output_dir&quot;
# Load the parquet file
df = pd.read_parquet(PARQUET_FILE, engine=&quot;pyarrow&quot;)
# Function to process a chunk of IDs
import json
def process_images(ids_chunk):
    summary = {
        &quot;id&quot;: [],
        &quot;old_width&quot;: [],
        &quot;new_width&quot;: [],
        &quot;old_height&quot;: [],
        &quot;new_height&quot;: [],
        &quot;old_aspect_ratio&quot;: [],
        &quot;new_aspect_ratio&quot;: [],
    }
    for id in ids_chunk:
        metadata_path = os.path.join(IMAGE_DATA, f&quot;{id}.json&quot;)
        if not os.path.exists(metadata_path):
            continue
        # Use the simpletuner data if the image is not found
        try:
            with open(metadata_path) as f:
                row = json.load(f)
            width, height = row[&quot;image_size&quot;]
            aspect_ratio = row[&quot;aspect_ratio&quot;]
        except KeyError:
            print(f&quot;Image {metadata_path} not found in simpletuner data&quot;)
            continue
        # Locate the row in the DataFrame
        row = df.loc[df[&quot;id&quot;] == id]
        # Check for differences
        if not row.empty and (
            row.iloc[0][&quot;width&quot;] != width or row.iloc[0][&quot;height&quot;] != height
        ):
            print(
                f&quot;Updated image {id}: {row.iloc[0][&apos;width&apos;]}x{row.iloc[0][&apos;height&apos;]} -&gt; {width}x{height}&quot;
            )
            summary[&quot;id&quot;].append(id)
            summary[&quot;old_width&quot;].append(row.iloc[0][&quot;width&quot;])
            summary[&quot;new_width&quot;].append(width)
            summary[&quot;old_height&quot;].append(row.iloc[0][&quot;height&quot;])
            summary[&quot;new_height&quot;].append(height)
            summary[&quot;old_aspect_ratio&quot;].append(row.iloc[0][&quot;aspect_ratio&quot;])
            summary[&quot;new_aspect_ratio&quot;].append(aspect_ratio)
    return summary
# Split IDs into chunks for parallel processing
ids = df[&quot;id&quot;].values
num_processes = os.cpu_count()
chunk_size = len(ids) // num_processes + (len(ids) % num_processes &gt; 0)
id_chunks = [ids[i : i + chunk_size] for i in range(0, len(ids), chunk_size)]
# Process the images in parallel
with ProcessPoolExecutor(max_workers=num_processes) as executor:
    results = list(tqdm(executor.map(process_images, id_chunks), total=len(id_chunks)))
# Combine results from all processes
combined_summary = pd.DataFrame()
for result in results:
    combined_summary = pd.concat([combined_summary, pd.DataFrame(result)])
# Update the DataFrame based on the combined summary
for index, row in combined_summary.iterrows():
    idx = df.index[df[&quot;id&quot;] == row[&quot;id&quot;]].tolist()[0]
    df.at[idx, &quot;width&quot;] = row[&quot;new_width&quot;]
    df.at[idx, &quot;height&quot;] = row[&quot;new_height&quot;]
    df.at[idx, &quot;aspect_ratio&quot;] = row[&quot;new_aspect_ratio&quot;]
# Save the updated DataFrame to the parquet file
df.to_parquet(PARQUET_FILE, engine=&quot;pyarrow&quot;)</file><file path="toolkit/README.md">SimpleTuner contains some ad-hoc tools for generating and managing the training data and checkpoints.

#### Captioning

When captioning a dataset, relying on a single caption model is a bad practice as it pins the model to whatever the chosen caption model knows.

A variety of caption options are provided:

* `caption_with_blip.py` - This is the original BLIP / BLIP2 captioning script which leverages the `interrogate` python library to run the (by default Flan-T5) BLIP model.
* `caption_with_blip3.py` - Built on top of the Phi LLM, BLIP3 aka XGEN-MM is an excellent option for captioning, relatively lightweight and yet very powerful.
* `caption_with_cogvlm_remote.py` - A script used by the volunteer cluster run via the Terminus Research Group
* `caption_with_cogvlm.py` - If you want CogVLM captioning, use this - though there&apos;s some potentially erratic results from Cog where it might repeat words.
* `caption_with_gemini.py` - Set `GEMINI_API_KEY` in your environment from one obtained via [Google AI](https://ai.google.dev) and you can caption images for free using Gemini Pro Vision.
* `caption_with_llava.py` - Use Llava 1.5 or 1.6 and run pretty much the same way the CogVLM script does, albeit in a different style.
* `caption_with_internvl.py` - Uses InternVL2 by default to caption images direclty into parquet tables for use by SimpleTuner.


#### Datasets

* `csv_to_s3.py` - given a folder of CSV webdataset as inputs, download/caption/transform images before stuffing them into an S3 bucket.
* `clear_s3_bucket.py` - Just a convenient way to clear an S3 bucket that&apos;s been used with this tool.
* `dataset_from_kellyc.py` - If you use the KellyC browser extension for image scraping, this will build a dataset from the URL list it saves.
* `dataset_from_csv.py` - Download a chunk of data to local storage from a single csv dataset document.
* `dataset_from_laion.py` - A variant of the above script.
* `analyze_laion_data.py` - After downloading a lot of LAION&apos;s data, you can use this to throw a lot of it away.
* `analyze_aspect_ratios_json.py` - Use the output from `analyze_laion_data.py` to nuke images that do not fit our aspect goals.
* `check_latent_corruption.py` - Scan and remove any images that will not load properly.
* `update_parquet.py` - A scaffold for updating the contents of a parquet file.
* `folder_to_parquet.py` - Import a folder of images into a parquet file.
* `discord_scrape.py` - Scrape the Midjourney server into a local folder and/or parquet files.
* `enhance_with_controlnet.py` - An incomplete script which aims to demonstrate improving a dataset using ControlNet Tile before training.

#### Inference

* `inference.py` - Generate validation results from the prompts catalogue (`prompts.py`) using DDIMScheduler.
* `inference_ddpm.py` - Use DDPMScheduler to assemble a checkpoint from a base model configuration and run through validation prompts.
* `inference_karras.py` - Use the Karras sigmas with DPM 2M Karras. Useful for testing what might happen in Automatic1111.
* `tile_shortnames.py` - Tile the outputs from the above scripts into strips.

* `inference_snr_test.py` - Generate a large number of CFG range images, and catalogue the results for tiling.
* `tile_images.py` - Generate large image tiles to compare CFG results for zero SNR training / inference tuning.</file><file path=".gitignore">*.code-workspace
multidatabackend*json
# Python and virtual environment files
output/
temp/
env.sh
*.pem
*/config/config.json
user_prompt_library.json
__pycache__/
.venv/
cache/
*.pyc
*.pyo
*.pyd
*.pyc
*.pyo
*.pyd
*.so
*.dylib
*.egg-info/
dist/
build/
*.egg
*.pyc
*.pyo
*.pyd
*.so
*.dylib
*.egg-info/
dist/
build/
*.egg
venv/
*.log

# IDE files
.vscode/
.idea/
*.swp
*.swo
*.swn

# OS generated files
.DS_Store
Thumbs.db

files.tbz
*/config/auth.json
work
multidatabackend.json
multidatabackend_sd2x.json

config/*.json
config/*.env
wandb/
cache/
vae_cache/
vendor/
inference/
webhooks.json
untracked/
config/*/
config/*.toml
static
templates
api_state.json</file><file path="configure.py">import os
import huggingface_hub
import torch
from helpers.training import quantised_precision_levels, lycoris_defaults
from helpers.training.optimizer_param import optimizer_choices
bf16_only_optims = [
    key
    for key, value in optimizer_choices.items()
    if value.get(&quot;precision&quot;, &quot;any&quot;) == &quot;bf16&quot;
]
any_precision_optims = [
    key
    for key, value in optimizer_choices.items()
    if value.get(&quot;precision&quot;, &quot;any&quot;) == &quot;any&quot;
]
model_classes = {
    &quot;full&quot;: [
        &quot;flux&quot;,
        &quot;sdxl&quot;,
        &quot;pixart_sigma&quot;,
        &quot;kolors&quot;,
        &quot;sd3&quot;,
        &quot;legacy&quot;,
        &quot;ltxvideo&quot;,
        &quot;sana&quot;,
    ],
    &quot;lora&quot;: [&quot;flux&quot;, &quot;sdxl&quot;, &quot;kolors&quot;, &quot;sd3&quot;, &quot;legacy&quot;, &quot;ltxvideo&quot;],
    &quot;controlnet&quot;: [&quot;sdxl&quot;, &quot;legacy&quot;],
}
default_models = {
    &quot;flux&quot;: &quot;black-forest-labs/FLUX.1-dev&quot;,
    &quot;sdxl&quot;: &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;,
    &quot;pixart_sigma&quot;: &quot;PixArt-alpha/PixArt-Sigma-XL-2-1024-MS&quot;,
    &quot;kolors&quot;: &quot;kwai-kolors/kolors-diffusers&quot;,
    &quot;terminus&quot;: &quot;ptx0/terminus-xl-velocity-v2&quot;,
    &quot;sd3&quot;: &quot;stabilityai/stable-diffusion-3.5-large&quot;,
    &quot;legacy&quot;: &quot;stabilityai/stable-diffusion-2-1-base&quot;,
    &quot;sana&quot;: &quot;terminusresearch/sana-1.6b-1024px&quot;,
    &quot;ltxvideo&quot;: &quot;Lightricks/LTX-Video&quot;,
}
default_cfg = {
    &quot;flux&quot;: 3.0,
    &quot;sdxl&quot;: 4.2,
    &quot;pixart_sigma&quot;: 3.4,
    &quot;kolors&quot;: 5.0,
    &quot;terminus&quot;: 8.0,
    &quot;sd3&quot;: 5.0,
    &quot;ltxvideo&quot;: 4.0,
    &quot;sana&quot;: 3.8,
}
model_labels = {
    &quot;sd3&quot;: &quot;Stable Diffusion 3&quot;,
    &quot;flux&quot;: &quot;FLUX&quot;,
    &quot;pixart_sigma&quot;: &quot;PixArt Sigma&quot;,
    &quot;kolors&quot;: &quot;Kwai Kolors&quot;,
    &quot;terminus&quot;: &quot;Terminus&quot;,
    &quot;sdxl&quot;: &quot;Stable Diffusion XL&quot;,
    &quot;legacy&quot;: &quot;Stable Diffusion&quot;,
    &quot;ltxvideo&quot;: &quot;LTX Video&quot;,
    &quot;sana&quot;: &quot;Sana&quot;,
}
lora_ranks = [1, 16, 64, 128, 256]
learning_rates_by_rank = {
    1: &quot;3e-4&quot;,
    16: &quot;1e-4&quot;,
    64: &quot;8e-5&quot;,
    128: &quot;6e-5&quot;,
    256: &quot;5.09e-5&quot;,
}
def print_config(env_contents: dict, extra_args: list):
    # env_contents[&quot;TRAINER_EXTRA_ARGS&quot;] = &quot; &quot;.join(extra_args)
    # output = json.dumps(env_contents, indent=4)
    # print(output)
    pass
def prompt_user(prompt, default=None):
    if default:
        prompt = f&quot;{prompt} (default: {default})&quot;
    user_input = input(f&quot;{prompt}: &quot;)
    return user_input.strip() or default
def configure_lycoris():
    print(&quot;Let&apos;s configure your LyCORIS model!\n&quot;)
    print(&quot;Select a LyCORIS algorithm:\n&quot;)
    print(
        &quot;1. LoRA - Efficient, balanced fine-tuning. Good for general tasks. (algo=lora)&quot;
    )
    print(
        &quot;2. LoHa - Advanced, strong dampening. Ideal for multi-concept fine-tuning. (algo=loha)&quot;
    )
    print(
        &quot;3. LoKr - Kronecker product-based. Use for complex transformations. (algo=lokr)&quot;
    )
    print(&quot;4. Full Fine-Tuning - Traditional full model tuning. (algo=full)&quot;)
    print(&quot;5. IA^3 - Efficient, tiny files, best for styles. (algo=ia3)&quot;)
    print(&quot;6. DyLoRA - Dynamic updates, efficient with large dims. (algo=dylora)&quot;)
    print(&quot;7. Diag-OFT - Fast convergence with orthogonal fine-tuning. (algo=diag-oft)&quot;)
    print(&quot;8. BOFT - Advanced version of Diag-OFT with more flexibility. (algo=boft)&quot;)
    print(&quot;9. GLoRA - Generalized LoRA. (algo=glora)\n&quot;)
    # Prompt user to select an algorithm
    algo = prompt_user(
        f&quot;Which LyCORIS algorithm would you like to use? (Enter the number corresponding to the algorithm)&quot;,
        &quot;3&quot;,  # Default to LoKr
    )
    # Map the selected number to the actual algorithm name
    algo_map = {
        &quot;1&quot;: &quot;lora&quot;,
        &quot;2&quot;: &quot;loha&quot;,
        &quot;3&quot;: &quot;lokr&quot;,
        &quot;4&quot;: &quot;full&quot;,
        &quot;5&quot;: &quot;ia3&quot;,
        &quot;6&quot;: &quot;dylora&quot;,
        &quot;7&quot;: &quot;diag-oft&quot;,
        &quot;8&quot;: &quot;boft&quot;,
        &quot;9&quot;: &quot;glora&quot;,
    }
    algo = algo_map.get(algo, &quot;lokr&quot;).lower()
    # Get the default configuration for the selected algorithm
    default_config = lycoris_defaults.get(algo, {}).copy()
    # Continue with further configuration
    print(f&quot;\nConfiguring {algo.upper()} algorithm...\n&quot;)
    multiplier = float(
        prompt_user(
            f&quot;Set the effect multiplier. Adjust for stronger or subtler effects. &quot;
            f&quot;(default: {default_config.get(&apos;multiplier&apos;, 1.0)})&quot;,
            default_config.get(&quot;multiplier&quot;, 1.0),
        )
    )
    linear_dim = int(
        prompt_user(
            f&quot;Set the linear dimension. Higher values mean more capacity but use more resources. &quot;
            f&quot;(default: {default_config.get(&apos;linear_dim&apos;, 1000000)})&quot;,
            default_config.get(&quot;linear_dim&quot;, 1000000),
        )
    )
    linear_alpha = int(
        prompt_user(
            f&quot;Set the alpha scaling factor. Controls the impact on the model. &quot;
            f&quot;(default: {default_config.get(&apos;linear_alpha&apos;, 1)})&quot;,
            default_config.get(&quot;linear_alpha&quot;, 1),
        )
    )
    # Update basic parameters in config
    default_config.update(
        {
            &quot;multiplier&quot;: multiplier,
            &quot;linear_dim&quot;: linear_dim,
            &quot;linear_alpha&quot;: linear_alpha,
        }
    )
    # Conditional prompts based on the selected algorithm
    if algo == &quot;lokr&quot;:
        factor = int(
            prompt_user(
                f&quot;Set the factor for compression/expansion. &quot;
                f&quot;(default: {default_config.get(&apos;factor&apos;, 16)})&quot;,
                default_config.get(&quot;factor&quot;, 16),
            )
        )
        default_config.update({&quot;factor&quot;: factor})
        if linear_dim &gt;= 10000:  # Handle full-dimension case
            print(&quot;Full-dimension mode activated. Alpha will be set to 1.&quot;)
            default_config[&quot;linear_alpha&quot;] = 1
    elif algo == &quot;loha&quot;:
        if linear_dim &gt; 32:
            print(&quot;Warning: High dim values with LoHa may cause instability.&quot;)
        # Additional LoHa-specific configurations can be added here if needed
    elif algo == &quot;dylora&quot;:
        block_size = int(
            prompt_user(
                f&quot;Set block size for DyLoRA (rows/columns updated per step). &quot;
                f&quot;(default: {default_config.get(&apos;block_size&apos;, 0)})&quot;,
                default_config.get(&quot;block_size&quot;, 0),
            )
        )
        default_config.update({&quot;block_size&quot;: block_size})
    elif algo in [&quot;diag-oft&quot;, &quot;boft&quot;]:
        constraint = (
            prompt_user(
                f&quot;Enforce constraints (e.g., orthogonality)? &quot;
                f&quot;(True/False, default: {default_config.get(&apos;constraint&apos;, False)})&quot;,
                str(default_config.get(&quot;constraint&quot;, False)),
            ).lower()
            == &quot;true&quot;
        )
        rescaled = (
            prompt_user(
                f&quot;Rescale transformations? Adjusts model impact. &quot;
                f&quot;(True/False, default: {default_config.get(&apos;rescaled&apos;, False)})&quot;,
                str(default_config.get(&quot;rescaled&quot;, False)),
            ).lower()
            == &quot;true&quot;
        )
        default_config.update(
            {
                &quot;constraint&quot;: constraint,
                &quot;rescaled&quot;: rescaled,
            }
        )
    # Handle presets for specific modules
    if &quot;apply_preset&quot; in default_config:
        print(&quot;\nNext, configure the modules to target with this algorithm.&quot;)
        target_module = prompt_user(
            f&quot;Which modules should the {algo.upper()} algorithm be applied to? &quot;
            f&quot;(default: {&apos;, &apos;.join(default_config[&apos;apply_preset&apos;][&apos;target_module&apos;])})&quot;,
            &quot;, &quot;.join(default_config[&quot;apply_preset&quot;][&quot;target_module&quot;]),
        ).split(&quot;,&quot;)
        default_config[&quot;apply_preset&quot;][&quot;target_module&quot;] = [
            m.strip() for m in target_module
        ]
        for module_name, module_config in default_config[&quot;apply_preset&quot;][
            &quot;module_algo_map&quot;
        ].items():
            for param, value in module_config.items():
                user_value = prompt_user(
                    f&quot;Set {param} for {module_name}. &quot; f&quot;(default: {value})&quot;, value
                )
                module_config[param] = (
                    int(user_value) if isinstance(value, int) else float(user_value)
                )
    print(&quot;\nLyCORIS configuration complete: &quot;, default_config)
    return default_config
def configure_env():
    print(&quot;Welcome to SimpleTuner!&quot;)
    print(&quot;This script will guide you through setting up your config.json file.\n&quot;)
    env_contents = {
        &quot;--resume_from_checkpoint&quot;: &quot;latest&quot;,
        &quot;--data_backend_config&quot;: &quot;config/multidatabackend.json&quot;,
        &quot;--aspect_bucket_rounding&quot;: 2,
        &quot;--seed&quot;: 42,
        &quot;--minimum_image_size&quot;: 0,
        &quot;--disable_benchmark&quot;: False,
    }
    extra_args = []
    output_dir = prompt_user(
        &quot;Enter the directory where you want to store your outputs&quot;, &quot;output/models&quot;
    )
    while not os.path.exists(output_dir):
        should_create = (
            prompt_user(
                &quot;That directory did not exist. Should I create it? Answer &apos;n&apos; to select a new location. ([y]/n)&quot;,
                &quot;y&quot;,
            )
            == &quot;y&quot;
        )
        if should_create:
            os.makedirs(output_dir, exist_ok=True)
        else:
            print(
                f&quot;Directory {output_dir} does not exist. Please create it and try again.&quot;
            )
            output_dir = prompt_user(
                &quot;Enter the directory where you want to store your outputs&quot;,
                &quot;output/models&quot;,
            )
    env_contents[&quot;--output_dir&quot;] = output_dir
    # Start with the basic options
    model_type = prompt_user(
        &quot;What type of model are you training? (Options: [lora], full)&quot;, &quot;lora&quot;
    ).lower()
    use_lycoris = False
    use_lora = False
    if model_type == &quot;lora&quot;:
        use_lora = True
        use_lycoris = (
            prompt_user(&quot;Would you like to train a LyCORIS model? ([y]/n)&quot;, &quot;y&quot;).lower()
            == &quot;y&quot;
        )
        if use_lycoris:
            env_contents[&quot;--lora_type&quot;] = &quot;lycoris&quot;
            lycoris_config = configure_lycoris()
            env_contents[&quot;--lycoris_config&quot;] = &quot;config/lycoris_config.json&quot;
            # write json to file
            import json
            # approximate the rank of the lycoris
            lora_rank = 16
            with open(&quot;config/lycoris_config.json&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                f.write(json.dumps(lycoris_config, indent=4))
        else:
            env_contents[&quot;--lora_type&quot;] = &quot;standard&quot;
            use_dora = prompt_user(
                &quot;Would you like to train a DoRA model? (y/[n])&quot;, &quot;n&quot;
            ).lower()
            if use_dora == &quot;y&quot;:
                env_contents[&quot;--use_dora&quot;] = &quot;true&quot;
            lora_rank = None
            while lora_rank not in lora_ranks:
                if lora_rank is not None:
                    print(f&quot;Invalid LoRA rank: {lora_rank}&quot;)
                lora_rank = int(
                    prompt_user(
                        f&quot;Set the LoRA rank (Options: {&apos;, &apos;.join([str(x) for x in lora_ranks])})&quot;,
                        &quot;64&quot;,
                    )
                )
            env_contents[&quot;--lora_rank&quot;] = lora_rank
    elif model_type == &quot;full&quot;:
        use_ema = prompt_user(
            &quot;Would you like to use EMA for training? (y/[n])&quot;, &quot;n&quot;
        ).lower()
        if use_ema == &quot;y&quot;:
            env_contents[&quot;--use_ema&quot;] = &quot;true&quot;
    print(&quot;We&apos;ll try and login to Hugging Face Hub..&quot;)
    whoami = None
    try:
        whoami = huggingface_hub.whoami()
    except:
        pass
    should_retry = True
    while not whoami and should_retry:
        should_retry = (
            prompt_user(
                &quot;You are not currently logged into Hugging Face Hub. Would you like to login? (y/n)&quot;,
                &quot;y&quot;,
            ).lower()
            == &quot;y&quot;
        )
        if not should_retry:
            whoami = None
            print(&quot;Will not be logged into Hugging Face Hub.&quot;)
            break
        huggingface_hub.login()
        whoami = huggingface_hub.whoami()
    finishing_count_type = prompt_user(
        &quot;Should we schedule the end of training by epochs, or steps?&quot;, &quot;steps&quot;
    ).lower()
    while finishing_count_type not in [&quot;steps&quot;, &quot;epochs&quot;]:
        print(f&quot;Invalid finishing count type: {finishing_count_type}&quot;)
        finishing_count_type = prompt_user(
            &quot;Should we schedule the end of training by epochs, or steps?&quot;, &quot;steps&quot;
        ).lower()
    default_checkpointing_interval = 500
    if finishing_count_type == &quot;steps&quot;:
        env_contents[&quot;--max_train_steps&quot;] = int(
            prompt_user(&quot;Set the maximum number of steps&quot;, 10000)
        )
        if env_contents[&quot;--max_train_steps&quot;] &lt; default_checkpointing_interval:
            # reduce the default checkpointing interval offered to the user so that they get a reasonable value.
            default_checkpointing_interval = env_contents[&quot;--max_train_steps&quot;] // 10
        env_contents[&quot;--num_train_epochs&quot;] = 0
    else:
        env_contents[&quot;--num_train_epochs&quot;] = prompt_user(
            &quot;Set the maximum number of epochs&quot;, 100
        )
        env_contents[&quot;--max_train_steps&quot;] = 0
    checkpointing_interval = prompt_user(
        &quot;Set the checkpointing interval (in steps)&quot;, default_checkpointing_interval
    )
    env_contents[&quot;--checkpointing_steps&quot;] = int(checkpointing_interval)
    checkpointing_limit = prompt_user(
        &quot;How many checkpoints do you want to keep? LoRA are small, and you can keep more than a full finetune.&quot;,
        5,
    )
    env_contents[&quot;--checkpoints_total_limit&quot;] = int(checkpointing_limit)
    if whoami is not None:
        print(&quot;Connected to Hugging Face Hub as:&quot;, whoami[&quot;name&quot;])
        should_push_to_hub = (
            prompt_user(
                &quot;Do you want to push your model to Hugging Face Hub when it is completed uploading? (y/n)&quot;,
                &quot;y&quot;,
            ).lower()
            == &quot;y&quot;
        )
        if should_push_to_hub:
            env_contents[&quot;--hub_model_id&quot;] = prompt_user(
                f&quot;What do you want the name of your Hugging Face Hub model to be? This will be accessible as https://huggingface.co/{whoami[&apos;name&apos;]}/your-model-name-here&quot;,
                f&quot;simpletuner-{model_type}&quot;,
            )
            should_push_checkpoints = False
            env_contents[&quot;--push_to_hub&quot;] = &quot;true&quot;
            should_push_checkpoints = (
                prompt_user(
                    &quot;Do you want to push intermediary checkpoints to Hugging Face Hub? ([y]/n)&quot;,
                    &quot;y&quot;,
                ).lower()
                == &quot;y&quot;
            )
            if should_push_checkpoints:
                env_contents[&quot;--push_checkpoints_to_hub&quot;] = &quot;true&quot;
            model_card_safe_for_work = (
                prompt_user(
                    &quot;Is your target model considered safe-for-work? Answering yes here will remove the NSFW warning from the Hugging Face Hub model card. If you are unsure, please leave this as &apos;no&apos;. (y/[n])&quot;,
                    &quot;n&quot;,
                ).lower()
                == &quot;y&quot;
            )
            if model_card_safe_for_work:
                env_contents[&quot;--model_card_safe_for_work&quot;] = &quot;true&quot;
    report_to_wandb = (
        prompt_user(
            &quot;Would you like to report training statistics to Weights &amp; Biases? ([y]/n)&quot;,
            &quot;y&quot;,
        ).lower()
        == &quot;y&quot;
    )
    report_to_tensorboard = (
        prompt_user(
            &quot;Would you like to report training statistics to TensorBoard? (y/[n])&quot;, &quot;n&quot;
        ).lower()
        == &quot;y&quot;
    )
    env_contents[&quot;--attention_mechanism&quot;] = &quot;diffusers&quot;
    use_sageattention = (
        prompt_user(
            &quot;Would you like to use SageAttention for image validation generation? (y/[n])&quot;,
            &quot;n&quot;,
        ).lower()
        == &quot;y&quot;
    )
    if use_sageattention:
        env_contents[&quot;--attention_mechanism&quot;] = &quot;sageattention&quot;
        env_contents[&quot;--sageattention_usage&quot;] = &quot;inference&quot;
        use_sageattention_training = (
            prompt_user(
                (
                    &quot;Would you like to use SageAttention to cover the forward and backward pass during training?&quot;
                    &quot; This has the undesirable consequence of leaving the attention layers untrained,&quot;
                    &quot; as SageAttention lacks the capability to fully track gradients through quantisation.&quot;
                    &quot; If you are not training the attention layers for some reason, this may not matter and&quot;
                    &quot; you can safely enable this. For all other use-cases, reconsideration and caution are warranted.&quot;
                ),
                &quot;n&quot;,
            ).lower()
            == &quot;y&quot;
        )
        if use_sageattention_training:
            env_contents[&quot;--sageattention_usage&quot;] = &quot;both&quot;
    # properly disable wandb/tensorboard/comet_ml etc by default
    report_to_str = &quot;none&quot;
    if report_to_wandb or report_to_tensorboard:
        tracker_project_name = prompt_user(
            &quot;Enter the name of your Weights &amp; Biases project&quot;, f&quot;{model_type}-training&quot;
        )
        env_contents[&quot;--tracker_project_name&quot;] = tracker_project_name
        tracker_run_name = prompt_user(
            &quot;Enter the name of your Weights &amp; Biases runs. This can use shell commands, which can be used to dynamically set the run name.&quot;,
            f&quot;simpletuner-{model_type}&quot;,
        )
        env_contents[&quot;--tracker_run_name&quot;] = tracker_run_name
        if report_to_wandb:
            report_to_str = &quot;wandb&quot;
        if report_to_tensorboard:
            if report_to_str != &quot;none&quot;:
                # report to both WandB and Tensorboard if the user wanted.
                report_to_str += &quot;,&quot;
            else:
                # remove &apos;none&apos; from the option
                report_to_str = &quot;&quot;
            report_to_str += &quot;tensorboard&quot;
    env_contents[&quot;--report_to&quot;] = report_to_str
    print_config(env_contents, extra_args)
    model_class = None
    while model_class not in model_classes[model_type]:
        if model_class is not None:
            print(f&quot;Invalid model class: {model_class}&quot;)
        model_class = prompt_user(
            f&quot;Which model family are you training? ({&apos;/&apos;.join(model_classes[model_type])})&quot;,
            &quot;flux&quot;,
        ).lower()
    can_load_model = False
    model_name = None
    while not can_load_model:
        if model_name is not None:
            print(
                &quot;For some reason, we can not load that model. Can you check your Hugging Face login and try again?&quot;
            )
        model_name = prompt_user(
            &quot;Enter the model name from Hugging Face Hub&quot;, default_models[model_class]
        )
        try:
            model_info = huggingface_hub.model_info(model_name)
            if hasattr(model_info, &quot;id&quot;):
                can_load_model = True
        except:
            continue
    env_contents[&quot;--model_type&quot;] = model_type
    env_contents[&quot;--pretrained_model_name_or_path&quot;] = model_name
    env_contents[&quot;--model_family&quot;] = model_class.lower()
    # Flux-specific options
    if &quot;FLUX&quot; in env_contents and env_contents[&quot;--model_family&quot;] == &quot;flux&quot;:
        if env_contents[&quot;--model_type&quot;].lower() == &quot;lora&quot; and not use_lycoris:
            flux_targets = [
                &quot;mmdit&quot;,
                &quot;context&quot;,
                &quot;all&quot;,
                &quot;all+ffs&quot;,
                &quot;ai-toolkit&quot;,
                &quot;tiny&quot;,
                &quot;nano&quot;,
            ]
            flux_target_layers = None
            while flux_target_layers not in flux_targets:
                if flux_target_layers:
                    print(f&quot;Invalid Flux target layers: {flux_target_layers}&quot;)
                flux_target_layers = prompt_user(
                    f&quot;Set Flux target layers (Options: {&apos;/&apos;.join(flux_targets)})&quot;,
                    &quot;all&quot;,
                )
            env_contents[&quot;--flux_lora_target&quot;] = flux_target_layers
    print_config(env_contents, extra_args)
    # Additional settings
    env_contents[&quot;--train_batch_size&quot;] = int(
        prompt_user(
            &quot;Set the training batch size. Larger values will require larger datasets, more VRAM, and slow things down.&quot;,
            1,
        )
    )
    env_contents[&quot;--gradient_checkpointing&quot;] = &quot;true&quot;
    if env_contents[&quot;--model_family&quot;] in [&quot;sdxl&quot;, &quot;flux&quot;, &quot;sd3&quot;, &quot;sana&quot;]:
        gradient_checkpointing_interval = prompt_user(
            &quot;Would you like to configure a gradient checkpointing interval? A value larger than 1 will increase VRAM usage but speed up training by skipping checkpoint creation every Nth layer, and a zero will disable this feature.&quot;,
            0,
        )
        try:
            if int(gradient_checkpointing_interval) &gt; 1:
                env_contents[&quot;--gradient_checkpointing_interval&quot;] = int(
                    gradient_checkpointing_interval
                )
        except:
            print(&quot;Could not parse gradient checkpointing interval. Not enabling.&quot;)
            pass
    env_contents[&quot;--caption_dropout_probability&quot;] = float(
        prompt_user(
            &quot;Set the caption dropout rate, or use 0.0 to disable it. Dropout might be a good idea to disable for Flux training, but experimentation is warranted.&quot;,
            &quot;0.05&quot; if any([use_lora, use_lycoris]) else &quot;0.1&quot;,
        )
    )
    resolution_types = [&quot;pixel&quot;, &quot;area&quot;, &quot;pixel_area&quot;]
    env_contents[&quot;--resolution_type&quot;] = None
    while env_contents[&quot;--resolution_type&quot;] not in resolution_types:
        if env_contents[&quot;--resolution_type&quot;]:
            print(f&quot;Invalid resolution type: {env_contents[&apos;--resolution_type&apos;]}&quot;)
        env_contents[&quot;--resolution_type&quot;] = prompt_user(
            &quot;How do you want to measure dataset resolutions? &apos;pixel&apos; will size images with the shorter edge, &apos;area&apos; will measure in megapixels, and is great for aspect-bucketing. &apos;pixel_area&apos; is a combination of these two ideas, which lets you set your area using pixels instead of megapixels.&quot;,
            &quot;pixel_area&quot;,
        ).lower()
    if (
        env_contents[&quot;--resolution_type&quot;] == &quot;pixel&quot;
        or env_contents[&quot;--resolution_type&quot;] == &quot;pixel_area&quot;
    ):
        default_resolution = 1024
        resolution_unit = &quot;pixel&quot;
    else:
        default_resolution = 1.0
        resolution_unit = &quot;megapixel&quot;
    env_contents[&quot;--resolution&quot;] = prompt_user(
        f&quot;What would you like the default resolution of your datasets to be? The default for is {env_contents[&apos;--resolution_type&apos;]} is {default_resolution} {resolution_unit}s.&quot;,
        default_resolution,
    )
    # remove spaces from validation resolution, ensure it&apos;s a single WxH or a comma-separated list of WxH
    env_contents[&quot;--validation_seed&quot;] = prompt_user(&quot;Set the seed for validation&quot;, 42)
    env_contents[&quot;--validation_steps&quot;] = prompt_user(
        &quot;How many steps in between validation outputs?&quot;,
        env_contents[&quot;--checkpointing_steps&quot;],
    )
    env_contents[&quot;--validation_resolution&quot;] = None
    while (
        env_contents[&quot;--validation_resolution&quot;] is None
        or &quot;x&quot; not in env_contents[&quot;--validation_resolution&quot;]
    ):
        if env_contents[&quot;--validation_resolution&quot;] is not None:
            print(
                &quot;Invalid resolution format. Please enter a single resolution, or a comma-separated list. Example: 1024x1024,1280x768&quot;
            )
        env_contents[&quot;--validation_resolution&quot;] = prompt_user(
            &quot;Set the validation resolution. Format could be a single resolution, or comma-separated.&quot;,
            &quot;1024x1024&quot;,
        )
        env_contents[&quot;--validation_resolution&quot;] = &quot;,&quot;.join(
            [x.strip() for x in env_contents[&quot;--validation_resolution&quot;].split(&quot;,&quot;)]
        )
    env_contents[&quot;--validation_guidance&quot;] = prompt_user(
        &quot;Set the guidance scale for validation&quot;, default_cfg.get(model_class, 3.0)
    )
    env_contents[&quot;--validation_guidance_rescale&quot;] = prompt_user(
        &quot;Set the guidance re-scale for validation - this is called dynamic thresholding and is used mostly for zero-terminal SNR models.&quot;,
        &quot;0.0&quot;,
    )
    env_contents[&quot;--validation_num_inference_steps&quot;] = prompt_user(
        &quot;Set the number of inference steps for validation&quot;, &quot;20&quot;
    )
    env_contents[&quot;--validation_prompt&quot;] = prompt_user(
        &quot;Set the validation prompt&quot;, &quot;A photo-realistic image of a cat&quot;
    )
    print_config(env_contents, extra_args)
    # Advanced options
    if torch.cuda.is_available():
        use_tf32 = (
            prompt_user(&quot;Would you like to enable TF32 mode? ([y]/n)&quot;, &quot;y&quot;).lower()
            == &quot;y&quot;
        )
        if not use_tf32:
            env_contents[&quot;--disable_tf32&quot;] = &quot;true&quot;
    mixed_precision_options = [&quot;bf16&quot;, &quot;no&quot;]
    env_contents[&quot;--mixed_precision&quot;] = None
    while (
        not env_contents[&quot;--mixed_precision&quot;]
        or env_contents[&quot;--mixed_precision&quot;] not in mixed_precision_options
    ):
        if env_contents[&quot;--mixed_precision&quot;]:
            print(
                f&quot;Invalid mixed precision option: {env_contents[&apos;--mixed_precision&apos;]}&quot;
            )
        env_contents[&quot;--mixed_precision&quot;] = prompt_user(
            &quot;Set mixed precision mode (Options: bf16, no (fp32))&quot;, &quot;bf16&quot;
        )
    if env_contents[&quot;--mixed_precision&quot;] == &quot;bf16&quot;:
        compatible_optims = bf16_only_optims + any_precision_optims
    else:
        compatible_optims = any_precision_optims
    env_contents[&quot;--optimizer&quot;] = None
    while (
        not env_contents[&quot;--optimizer&quot;]
        or env_contents[&quot;--optimizer&quot;] not in compatible_optims
    ):
        if env_contents[&quot;--optimizer&quot;]:
            print(f&quot;Invalid optimizer: {env_contents[&apos;--optimizer&apos;]}&quot;)
        env_contents[&quot;--optimizer&quot;] = prompt_user(
            f&quot;Choose an optimizer (Options: {&apos;/&apos;.join(compatible_optims)})&quot;,
            compatible_optims[0],
        )
    lr_schedulers = [&quot;polynomial&quot;, &quot;constant&quot;]
    lr_scheduler = None
    while lr_scheduler not in lr_schedulers:
        if lr_scheduler:
            print(f&quot;Invalid learning rate scheduler: {lr_scheduler}&quot;)
        lr_scheduler = prompt_user(
            f&quot;Set the learning rate scheduler. Options: {&apos;/&apos;.join(lr_schedulers)}&quot;,
            lr_schedulers[0],
        )
    learning_rate = prompt_user(
        &quot;Set the learning rate&quot;,
        (
            learning_rates_by_rank[lora_rank]
            if model_type == &quot;lora&quot;
            else 1.0 if env_contents[&quot;--optimizer&quot;] == &quot;prodigy&quot; else &quot;1e-6&quot;
        ),
    )
    lr_warmup_steps = prompt_user(
        &quot;Set the number of warmup steps before the learning rate reaches its peak. This is set to 10 percent of the total runtime by default, or 100 steps, whichever is higher.&quot;,
        min(100, int(env_contents[&quot;--max_train_steps&quot;]) // 10),
    )
    env_contents[&quot;--learning_rate&quot;] = learning_rate
    env_contents[&quot;--lr_scheduler&quot;] = lr_scheduler
    if lr_scheduler == &quot;polynomial&quot;:
        extra_args.append(&quot;--lr_end=1e-8&quot;)
    env_contents[&quot;--lr_warmup_steps&quot;] = lr_warmup_steps
    quantization = (
        prompt_user(
            f&quot;Would you like to enable model quantization? {&apos;NOTE: Currently, a bug prevents multi-GPU training with LoRA&apos; if use_lora else &apos;&apos;}. ([y]/n)&quot;,
            &quot;y&quot;,
        ).lower()
        == &quot;y&quot;
    )
    if quantization:
        if env_contents.get(&quot;--use_dora&quot;) == &quot;true&quot;:
            print(&quot;DoRA will be disabled for quantisation.&quot;)
            del env_contents[&quot;--use_dora&quot;]
        quantization_type = None
        while (
            not quantization_type or quantization_type not in quantised_precision_levels
        ):
            if quantization_type:
                print(f&quot;Invalid quantization type: {quantization_type}&quot;)
            quantization_type = prompt_user(
                f&quot;Choose quantization type. (Options: {&apos;/&apos;.join(quantised_precision_levels)})&quot;,
                &quot;int8-quanto&quot;,
            )
        env_contents[&quot;--base_model_precision&quot;] = quantization_type
    print_config(env_contents, extra_args)
    compress_disk_cache = (
        prompt_user(&quot;Would you like to compress the disk cache? (y/n)&quot;, &quot;y&quot;).lower()
        == &quot;y&quot;
    )
    if compress_disk_cache:
        extra_args.append(&quot;--compress_disk_cache&quot;)
    # torch compile
    torch_compile = (
        prompt_user(
            &quot;Would you like to use torch compile during validations? (y/n)&quot;, &quot;n&quot;
        ).lower()
        == &quot;y&quot;
    )
    env_contents[&quot;--validation_torch_compile&quot;] = &quot;false&quot;
    if torch_compile:
        env_contents[&quot;--validation_torch_compile&quot;] = &quot;true&quot;
    # Summary and confirmation
    print_config(env_contents, extra_args)
    confirm = prompt_user(&quot;Does this look correct? (y/n)&quot;, &quot;y&quot;).lower() == &quot;y&quot;
    if confirm:
        # Write to .env file
        with open(&quot;config/config.json&quot;, &quot;w&quot;) as env_file:
            import json
            env_file.write(json.dumps(env_contents, indent=4))
        print(&quot;\nConfiguration file created successfully!&quot;)
    else:
        print(&quot;\nConfiguration aborted. No changes were made.&quot;)
        import sys
        sys.exit(1)
    # dataloader configuration
    resolution_configs = {
        256: {&quot;resolution&quot;: 256, &quot;minimum_image_size&quot;: 128},
        512: {&quot;resolution&quot;: 512, &quot;minimum_image_size&quot;: 256},
        768: {&quot;resolution&quot;: 768, &quot;minimum_image_size&quot;: 512},
        1024: {&quot;resolution&quot;: 1024, &quot;minimum_image_size&quot;: 768},
        1440: {&quot;resolution&quot;: 1440, &quot;minimum_image_size&quot;: 1024},
        2048: {&quot;resolution&quot;: 2048, &quot;minimum_image_size&quot;: 1440},
    }
    default_dataset_configuration = {
        &quot;id&quot;: &quot;PLACEHOLDER&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: None,
        &quot;crop&quot;: False,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;metadata_backend&quot;: &quot;discovery&quot;,
        &quot;caption_strategy&quot;: &quot;filename&quot;,
        &quot;cache_dir_vae&quot;: &quot;vae&quot;,
    }
    default_cropped_dataset_configuration = {
        &quot;id&quot;: &quot;PLACEHOLDER-crop&quot;,
        &quot;type&quot;: &quot;local&quot;,
        &quot;instance_data_dir&quot;: None,
        &quot;crop&quot;: True,
        &quot;crop_aspect&quot;: &quot;square&quot;,
        &quot;crop_style&quot;: &quot;center&quot;,
        &quot;vae_cache_clear_each_epoch&quot;: False,
        &quot;resolution_type&quot;: &quot;pixel_area&quot;,
        &quot;metadata_backend&quot;: &quot;discovery&quot;,
        &quot;caption_strategy&quot;: &quot;filename&quot;,
        &quot;cache_dir_vae&quot;: &quot;vae-crop&quot;,
    }
    default_local_configuration = [
        {
            &quot;id&quot;: &quot;text-embed-cache&quot;,
            &quot;dataset_type&quot;: &quot;text_embeds&quot;,
            &quot;default&quot;: True,
            &quot;type&quot;: &quot;local&quot;,
            &quot;cache_dir&quot;: &quot;text&quot;,
            &quot;write_batch_size&quot;: 128,
        },
    ]
    # Let&apos;s offer to generate a prompt library for the user. Preserve their existing one if it already exists.
    should_generate_by_default = &quot;n&quot;
    if not os.path.exists(&quot;config/user_prompt_library.json&quot;):
        should_generate_by_default = &quot;y&quot;
    should_generate_prompt_library = (
        prompt_user(
            (
                &quot;Would you like to generate a very rudimentary subject-centric prompt library for your dataset?&quot;
                &quot; This will download a small 1B Llama 3.2 model.&quot;
                &quot; If a user prompt library exists, it will be overwritten. (y/n)&quot;
            ),
            should_generate_by_default,
        ).lower()
        == &quot;y&quot;
    )
    if should_generate_prompt_library:
        try:
            user_caption_trigger = prompt_user(
                &quot;Enter a trigger word (or a few words) that you would like Llama 3.2 1B to expand.&quot;,
                &quot;Character Name&quot;,
            )
            number_of_prompts = int(
                prompt_user(&quot;How many prompts would you like to generate?&quot;, 8)
            )
            from helpers.prompt_expander import PromptExpander
            PromptExpander.initialize_model()
            user_prompt_library = PromptExpander.generate_prompts(
                trigger_phrase=user_caption_trigger, num_prompts=number_of_prompts
            )
            with open(&quot;config/user_prompt_library.json&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
                f.write(json.dumps(user_prompt_library, indent=4))
            print(&quot;Prompt library generated successfully!&quot;)
            env_contents[&quot;--user_prompt_library&quot;] = &quot;config/user_prompt_library.json&quot;
        except Exception as e:
            print(f&quot;(warning) Failed to generate prompt library: {e}&quot;)
    # now we ask user the path to their data, the path to the cache (cache/), number of repeats, update the id placeholder based on users dataset name
    # then we&apos;ll write the file to multidatabackend.json
    should_configure_dataloader = (
        prompt_user(&quot;Would you like to configure your dataloader? (y/n)&quot;, &quot;y&quot;).lower()
        == &quot;y&quot;
    )
    if not should_configure_dataloader:
        print(&quot;Skipping dataloader configuration.&quot;)
        return
    dataset_id = prompt_user(
        &quot;Enter the name of your dataset. This will be used to generate the cache directory. It should be simple, and not contain spaces or special characters.&quot;,
        &quot;my-dataset&quot;,
    )
    dataset_path = prompt_user(
        &quot;Enter the path to your dataset. This should be a directory containing images and text files for their caption. For reliability, use an absolute (full) path, beginning with a &apos;/&apos;&quot;,
        &quot;/datasets/my-dataset&quot;,
    )
    dataset_caption_strategy = prompt_user(
        (
            &quot;How should the dataloader handle captions?&quot;
            &quot;\n-&gt; &apos;filename&apos; will use the names of your image files as the caption&quot;
            &quot;\n-&gt; &apos;textfile&apos; requires a image.txt file to go next to your image.png file&quot;
            &quot;\n-&gt; &apos;instanceprompt&apos; will just use one trigger phrase for all images&quot;
            &quot;\n&quot;
            &quot;\n(Options: filename, textfile, instanceprompt)&quot;
        ),
        &quot;textfile&quot;,
    )
    if dataset_caption_strategy not in [&quot;filename&quot;, &quot;textfile&quot;, &quot;instanceprompt&quot;]:
        print(f&quot;Invalid caption strategy: {dataset_caption_strategy}&quot;)
        dataset_caption_strategy = &quot;textfile&quot;
    dataset_instance_prompt = None
    if &quot;instanceprompt&quot; in dataset_caption_strategy:
        dataset_instance_prompt = prompt_user(
            &quot;Enter the instance_prompt you want to use for all images in this dataset&quot;,
            &quot;Character Name&quot;,
        )
    dataset_repeats = int(
        prompt_user(
            &quot;How many times do you want to repeat each image in the dataset? A value of zero means the dataset will only be seen once; a value of one will cause the dataset to be sampled twice.&quot;,
            10,
        )
    )
    default_base_resolutions = &quot;1024&quot;
    multi_resolution_recommendation_text = (
        &quot;Multiple resolutions may be provided, but this is only recommended for Flux.&quot;
    )
    multi_resolution_capable_models = [&quot;flux&quot;]
    if env_contents[&quot;--model_family&quot;] in multi_resolution_capable_models:
        default_base_resolutions = &quot;256,512,768,1024,1440&quot;
    multi_resolution_recommendation_text = &quot;A comma-separated list of values or a single item can be given to train on multiple base resolutions.&quot;
    dataset_resolutions = prompt_user(
        f&quot;Which resolutions do you want to train? {multi_resolution_recommendation_text}&quot;,
        default_base_resolutions,
    )
    if &quot;,&quot; in dataset_resolutions:
        # most models don&apos;t work with multi base resolution training.
        if env_contents[&quot;--model_family&quot;] not in multi_resolution_capable_models:
            print(
                &quot;WARNING: Most models do not play well with multi-resolution training, resulting in degraded outputs and broken hearts. Proceed with caution.&quot;
            )
        dataset_resolutions = [int(res) for res in dataset_resolutions.split(&quot;,&quot;)]
    else:
        try:
            dataset_resolutions = [int(dataset_resolutions)]
        except:
            print(&quot;Invalid resolution value. Using 1024 instead.&quot;)
            dataset_resolutions = [1024]
    dataset_cache_prefix = prompt_user(
        &quot;Where will your VAE and text encoder caches be written to? Subdirectories will be created inside for you automatically.&quot;,
        &quot;cache/&quot;,
    )
    has_very_large_images = (
        prompt_user(
            &quot;Do you have very-large images in the dataset (eg. much larger than 1024x1024)? (y/n)&quot;,
            &quot;n&quot;,
        ).lower()
        == &quot;y&quot;
    )
    # Now we&apos;ll modify the default json and if has_very_large_images is true, we will add two keys to each image dataset, &apos;maximum_image_size&apos; and &apos;target_downsample_size&apos; equal to the dataset&apos;s resolution value
    def create_dataset_config(resolution, default_config):
        dataset = default_config.copy()
        dataset.update(resolution_configs[resolution])
        dataset[&quot;id&quot;] = f&quot;{dataset[&apos;id&apos;]}-{resolution}&quot;
        dataset[&quot;instance_data_dir&quot;] = os.path.abspath(dataset_path)
        dataset[&quot;repeats&quot;] = dataset_repeats
        # we want the absolute path, as this works best with datasets containing nested subdirectories.
        dataset[&quot;cache_dir_vae&quot;] = os.path.abspath(
            os.path.join(
                dataset_cache_prefix,
                env_contents[&quot;--model_family&quot;],
                dataset[&quot;cache_dir_vae&quot;],
                str(resolution),
            )
        )
        if has_very_large_images:
            dataset[&quot;maximum_image_size&quot;] = dataset[&quot;resolution&quot;]
            dataset[&quot;target_downsample_size&quot;] = dataset[&quot;resolution&quot;]
        dataset[&quot;id&quot;] = dataset[&quot;id&quot;].replace(&quot;PLACEHOLDER&quot;, dataset_id)
        if dataset_instance_prompt:
            dataset[&quot;instance_prompt&quot;] = dataset_instance_prompt
        dataset[&quot;caption_strategy&quot;] = dataset_caption_strategy
        if has_very_large_images:
            dataset[&quot;maximum_image_size&quot;] = dataset[&quot;resolution&quot;]
            dataset[&quot;target_downsample_size&quot;] = dataset[&quot;resolution&quot;]
        return dataset
    # this is because the text embed dataset is in the default config list at the top.
    # it&apos;s confusingly written because i&apos;m lazy, but you could do this any number of ways.
    default_local_configuration[0][&quot;cache_dir&quot;] = os.path.abspath(
        os.path.join(dataset_cache_prefix, env_contents[&quot;--model_family&quot;], &quot;text&quot;)
    )
    for resolution in dataset_resolutions:
        uncropped_dataset = create_dataset_config(
            resolution, default_dataset_configuration
        )
        default_local_configuration.append(uncropped_dataset)
        cropped_dataset = create_dataset_config(
            resolution, default_cropped_dataset_configuration
        )
        default_local_configuration.append(cropped_dataset)
    print(&quot;Dataloader configuration:&quot;)
    print(default_local_configuration)
    confirm = prompt_user(&quot;Does this look correct? (y/n)&quot;, &quot;y&quot;).lower() == &quot;y&quot;
    if confirm:
        import json
        with open(&quot;config/multidatabackend.json&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            f.write(json.dumps(default_local_configuration, indent=4))
        print(&quot;Dataloader configuration written successfully!&quot;)
if __name__ == &quot;__main__&quot;:
    configure_env()</file><file path="convert_sd_checkpoint.py"># Script for converting a HF Diffusers saved pipeline to a Stable Diffusion checkpoint.
# *Only* converts the UNet, VAE, and Text Encoder.
# Does not convert optimizer state or any other thing.
import argparse
import os.path as osp
import re
import torch
from safetensors.torch import load_file, save_file
# =================#
# UNet Conversion #
# =================#
unet_conversion_map = [
    # (stable-diffusion, HF Diffusers)
    (&quot;time_embed.0.weight&quot;, &quot;time_embedding.linear_1.weight&quot;),
    (&quot;time_embed.0.bias&quot;, &quot;time_embedding.linear_1.bias&quot;),
    (&quot;time_embed.2.weight&quot;, &quot;time_embedding.linear_2.weight&quot;),
    (&quot;time_embed.2.bias&quot;, &quot;time_embedding.linear_2.bias&quot;),
    (&quot;input_blocks.0.0.weight&quot;, &quot;conv_in.weight&quot;),
    (&quot;input_blocks.0.0.bias&quot;, &quot;conv_in.bias&quot;),
    (&quot;out.0.weight&quot;, &quot;conv_norm_out.weight&quot;),
    (&quot;out.0.bias&quot;, &quot;conv_norm_out.bias&quot;),
    (&quot;out.2.weight&quot;, &quot;conv_out.weight&quot;),
    (&quot;out.2.bias&quot;, &quot;conv_out.bias&quot;),
]
unet_conversion_map_resnet = [
    # (stable-diffusion, HF Diffusers)
    (&quot;in_layers.0&quot;, &quot;norm1&quot;),
    (&quot;in_layers.2&quot;, &quot;conv1&quot;),
    (&quot;out_layers.0&quot;, &quot;norm2&quot;),
    (&quot;out_layers.3&quot;, &quot;conv2&quot;),
    (&quot;emb_layers.1&quot;, &quot;time_emb_proj&quot;),
    (&quot;skip_connection&quot;, &quot;conv_shortcut&quot;),
]
unet_conversion_map_layer = []
# hardcoded number of downblocks and resnets/attentions...
# would need smarter logic for other networks.
for i in range(4):
    # loop over downblocks/upblocks
    for j in range(2):
        # loop over resnets/attentions for downblocks
        hf_down_res_prefix = f&quot;down_blocks.{i}.resnets.{j}.&quot;
        sd_down_res_prefix = f&quot;input_blocks.{3*i + j + 1}.0.&quot;
        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))
        if i &lt; 3:
            # no attention layers in down_blocks.3
            hf_down_atn_prefix = f&quot;down_blocks.{i}.attentions.{j}.&quot;
            sd_down_atn_prefix = f&quot;input_blocks.{3*i + j + 1}.1.&quot;
            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))
    for j in range(3):
        # loop over resnets/attentions for upblocks
        hf_up_res_prefix = f&quot;up_blocks.{i}.resnets.{j}.&quot;
        sd_up_res_prefix = f&quot;output_blocks.{3*i + j}.0.&quot;
        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))
        if i &gt; 0:
            # no attention layers in up_blocks.0
            hf_up_atn_prefix = f&quot;up_blocks.{i}.attentions.{j}.&quot;
            sd_up_atn_prefix = f&quot;output_blocks.{3*i + j}.1.&quot;
            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))
    if i &lt; 3:
        # no downsample in down_blocks.3
        hf_downsample_prefix = f&quot;down_blocks.{i}.downsamplers.0.conv.&quot;
        sd_downsample_prefix = f&quot;input_blocks.{3*(i+1)}.0.op.&quot;
        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))
        # no upsample in up_blocks.3
        hf_upsample_prefix = f&quot;up_blocks.{i}.upsamplers.0.&quot;
        sd_upsample_prefix = f&quot;output_blocks.{3*i + 2}.{1 if i == 0 else 2}.&quot;
        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))
hf_mid_atn_prefix = &quot;mid_block.attentions.0.&quot;
sd_mid_atn_prefix = &quot;middle_block.1.&quot;
unet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))
for j in range(2):
    hf_mid_res_prefix = f&quot;mid_block.resnets.{j}.&quot;
    sd_mid_res_prefix = f&quot;middle_block.{2*j}.&quot;
    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))
def convert_unet_state_dict(unet_state_dict):
    # buyer beware: this is a *brittle* function,
    # and correct output requires that all of these pieces interact in
    # the exact order in which I have arranged them.
    mapping = {k: k for k in unet_state_dict.keys()}
    for sd_name, hf_name in unet_conversion_map:
        mapping[hf_name] = sd_name
    for k, v in mapping.items():
        if &quot;resnets&quot; in k:
            for sd_part, hf_part in unet_conversion_map_resnet:
                v = v.replace(hf_part, sd_part)
            mapping[k] = v
    for k, v in mapping.items():
        for sd_part, hf_part in unet_conversion_map_layer:
            v = v.replace(hf_part, sd_part)
        mapping[k] = v
    new_state_dict = {v: unet_state_dict[k] for k, v in mapping.items()}
    return new_state_dict
# ================#
# VAE Conversion #
# ================#
vae_conversion_map = [
    # (stable-diffusion, HF Diffusers)
    (&quot;nin_shortcut&quot;, &quot;conv_shortcut&quot;),
    (&quot;norm_out&quot;, &quot;conv_norm_out&quot;),
    (&quot;mid.attn_1.&quot;, &quot;mid_block.attentions.0.&quot;),
]
for i in range(4):
    # down_blocks have two resnets
    for j in range(2):
        hf_down_prefix = f&quot;encoder.down_blocks.{i}.resnets.{j}.&quot;
        sd_down_prefix = f&quot;encoder.down.{i}.block.{j}.&quot;
        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))
    if i &lt; 3:
        hf_downsample_prefix = f&quot;down_blocks.{i}.downsamplers.0.&quot;
        sd_downsample_prefix = f&quot;down.{i}.downsample.&quot;
        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))
        hf_upsample_prefix = f&quot;up_blocks.{i}.upsamplers.0.&quot;
        sd_upsample_prefix = f&quot;up.{3-i}.upsample.&quot;
        vae_conversion_map.append((sd_upsample_prefix, hf_upsample_prefix))
    # up_blocks have three resnets
    # also, up blocks in hf are numbered in reverse from sd
    for j in range(3):
        hf_up_prefix = f&quot;decoder.up_blocks.{i}.resnets.{j}.&quot;
        sd_up_prefix = f&quot;decoder.up.{3-i}.block.{j}.&quot;
        vae_conversion_map.append((sd_up_prefix, hf_up_prefix))
# this part accounts for mid blocks in both the encoder and the decoder
for i in range(2):
    hf_mid_res_prefix = f&quot;mid_block.resnets.{i}.&quot;
    sd_mid_res_prefix = f&quot;mid.block_{i+1}.&quot;
    vae_conversion_map.append((sd_mid_res_prefix, hf_mid_res_prefix))
vae_conversion_map_attn = [
    # (stable-diffusion, HF Diffusers)
    (&quot;norm.&quot;, &quot;group_norm.&quot;),
    (&quot;q.&quot;, &quot;query.&quot;),
    (&quot;k.&quot;, &quot;key.&quot;),
    (&quot;v.&quot;, &quot;value.&quot;),
    (&quot;proj_out.&quot;, &quot;proj_attn.&quot;),
]
def reshape_weight_for_sd(w):
    # convert HF linear weights to SD conv2d weights
    return w.reshape(*w.shape, 1, 1)
def convert_vae_state_dict(vae_state_dict):
    mapping = {k: k for k in vae_state_dict.keys()}
    for k, v in mapping.items():
        for sd_part, hf_part in vae_conversion_map:
            v = v.replace(hf_part, sd_part)
        mapping[k] = v
    for k, v in mapping.items():
        if &quot;attentions&quot; in k:
            for sd_part, hf_part in vae_conversion_map_attn:
                v = v.replace(hf_part, sd_part)
            mapping[k] = v
    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}
    weights_to_convert = [&quot;q&quot;, &quot;k&quot;, &quot;v&quot;, &quot;proj_out&quot;]
    for k, v in new_state_dict.items():
        for weight_name in weights_to_convert:
            if f&quot;mid.attn_1.{weight_name}.weight&quot; in k:
                logging.info(f&quot;Reshaping {k} for SD format&quot;)
                new_state_dict[k] = reshape_weight_for_sd(v)
    return new_state_dict
# =========================#
# Text Encoder Conversion #
# =========================#
textenc_conversion_lst = [
    # (stable-diffusion, HF Diffusers)
    (&quot;resblocks.&quot;, &quot;text_model.encoder.layers.&quot;),
    (&quot;ln_1&quot;, &quot;layer_norm1&quot;),
    (&quot;ln_2&quot;, &quot;layer_norm2&quot;),
    (&quot;.c_fc.&quot;, &quot;.fc1.&quot;),
    (&quot;.c_proj.&quot;, &quot;.fc2.&quot;),
    (&quot;.attn&quot;, &quot;.self_attn&quot;),
    (&quot;ln_final.&quot;, &quot;transformer.text_model.final_layer_norm.&quot;),
    (
        &quot;token_embedding.weight&quot;,
        &quot;transformer.text_model.embeddings.token_embedding.weight&quot;,
    ),
    (
        &quot;positional_embedding&quot;,
        &quot;transformer.text_model.embeddings.position_embedding.weight&quot;,
    ),
]
protected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}
textenc_pattern = re.compile(&quot;|&quot;.join(protected.keys()))
# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp
code2idx = {&quot;q&quot;: 0, &quot;k&quot;: 1, &quot;v&quot;: 2}
def convert_text_enc_state_dict_v20(text_enc_dict):
    new_state_dict = {}
    capture_qkv_weight = {}
    capture_qkv_bias = {}
    for k, v in text_enc_dict.items():
        if (
            k.endswith(&quot;.self_attn.q_proj.weight&quot;)
            or k.endswith(&quot;.self_attn.k_proj.weight&quot;)
            or k.endswith(&quot;.self_attn.v_proj.weight&quot;)
        ):
            k_pre = k[: -len(&quot;.q_proj.weight&quot;)]
            k_code = k[-len(&quot;q_proj.weight&quot;)]
            if k_pre not in capture_qkv_weight:
                capture_qkv_weight[k_pre] = [None, None, None]
            capture_qkv_weight[k_pre][code2idx[k_code]] = v
            continue
        if (
            k.endswith(&quot;.self_attn.q_proj.bias&quot;)
            or k.endswith(&quot;.self_attn.k_proj.bias&quot;)
            or k.endswith(&quot;.self_attn.v_proj.bias&quot;)
        ):
            k_pre = k[: -len(&quot;.q_proj.bias&quot;)]
            k_code = k[-len(&quot;q_proj.bias&quot;)]
            if k_pre not in capture_qkv_bias:
                capture_qkv_bias[k_pre] = [None, None, None]
            capture_qkv_bias[k_pre][code2idx[k_code]] = v
            continue
        relabelled_key = textenc_pattern.sub(
            lambda m: protected[re.escape(m.group(0))], k
        )
        new_state_dict[relabelled_key] = v
    for k_pre, tensors in capture_qkv_weight.items():
        if None in tensors:
            raise Exception(
                &quot;CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing&quot;
            )
        relabelled_key = textenc_pattern.sub(
            lambda m: protected[re.escape(m.group(0))], k_pre
        )
        new_state_dict[relabelled_key + &quot;.in_proj_weight&quot;] = torch.cat(tensors)
    for k_pre, tensors in capture_qkv_bias.items():
        if None in tensors:
            raise Exception(
                &quot;CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing&quot;
            )
        relabelled_key = textenc_pattern.sub(
            lambda m: protected[re.escape(m.group(0))], k_pre
        )
        new_state_dict[relabelled_key + &quot;.in_proj_bias&quot;] = torch.cat(tensors)
    return new_state_dict
def convert_text_enc_state_dict(text_enc_dict):
    return text_enc_dict
if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &quot;--model_path&quot;,
        default=None,
        type=str,
        required=True,
        help=&quot;Path to the model to convert.&quot;,
    )
    parser.add_argument(
        &quot;--checkpoint_path&quot;,
        default=None,
        type=str,
        required=True,
        help=&quot;Path to the output model.&quot;,
    )
    parser.add_argument(
        &quot;--half&quot;, action=&quot;store_true&quot;, help=&quot;Save weights in half precision.&quot;
    )
    parser.add_argument(
        &quot;--use_safetensors&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Save weights use safetensors, default is ckpt.&quot;,
    )
    args = parser.parse_args()
    assert args.model_path is not None, &quot;Must provide a model path!&quot;
    assert args.checkpoint_path is not None, &quot;Must provide a checkpoint path!&quot;
    # Path for safetensors
    unet_path = osp.join(args.model_path, &quot;unet&quot;, &quot;diffusion_pytorch_model.safetensors&quot;)
    vae_path = osp.join(args.model_path, &quot;vae&quot;, &quot;diffusion_pytorch_model.safetensors&quot;)
    text_enc_path = osp.join(args.model_path, &quot;text_encoder&quot;, &quot;model.safetensors&quot;)
    # Load models from safetensors if it exists, if it doesn&apos;t pytorch
    if osp.exists(unet_path):
        unet_state_dict = load_file(unet_path, device=&quot;cpu&quot;)
    else:
        unet_path = osp.join(args.model_path, &quot;unet&quot;, &quot;diffusion_pytorch_model.bin&quot;)
        unet_state_dict = torch.load(unet_path, map_location=&quot;cpu&quot;)
    if osp.exists(vae_path):
        vae_state_dict = load_file(vae_path, device=&quot;cpu&quot;)
    else:
        vae_path = osp.join(args.model_path, &quot;vae&quot;, &quot;diffusion_pytorch_model.bin&quot;)
        vae_state_dict = torch.load(vae_path, map_location=&quot;cpu&quot;)
    if osp.exists(text_enc_path):
        text_enc_dict = load_file(text_enc_path, device=&quot;cpu&quot;)
    else:
        text_enc_path = osp.join(args.model_path, &quot;text_encoder&quot;, &quot;pytorch_model.bin&quot;)
        text_enc_dict = torch.load(text_enc_path, map_location=&quot;cpu&quot;)
    # Convert the UNet model
    unet_state_dict = convert_unet_state_dict(unet_state_dict)
    unet_state_dict = {
        &quot;model.diffusion_model.&quot; + k: v for k, v in unet_state_dict.items()
    }
    # Convert the VAE model
    vae_state_dict = convert_vae_state_dict(vae_state_dict)
    vae_state_dict = {&quot;first_stage_model.&quot; + k: v for k, v in vae_state_dict.items()}
    # Easiest way to identify v2.0 model seems to be that the text encoder (OpenCLIP) is deeper
    is_v20_model = &quot;text_model.encoder.layers.22.layer_norm2.bias&quot; in text_enc_dict
    if is_v20_model:
        # Need to add the tag &apos;transformer&apos; in advance so we can knock it out from the final layer-norm
        text_enc_dict = {&quot;transformer.&quot; + k: v for k, v in text_enc_dict.items()}
        text_enc_dict = convert_text_enc_state_dict_v20(text_enc_dict)
        text_enc_dict = {
            &quot;cond_stage_model.model.&quot; + k: v for k, v in text_enc_dict.items()
        }
    else:
        text_enc_dict = convert_text_enc_state_dict(text_enc_dict)
        text_enc_dict = {
            &quot;cond_stage_model.transformer.&quot; + k: v for k, v in text_enc_dict.items()
        }
    # Put together new checkpoint
    state_dict = {**unet_state_dict, **vae_state_dict, **text_enc_dict}
    if args.half:
        state_dict = {k: v.half() for k, v in state_dict.items()}
    if args.use_safetensors:
        save_file(state_dict, args.checkpoint_path)
    else:
        state_dict = {&quot;state_dict&quot;: state_dict}
        torch.save(state_dict, args.checkpoint_path)</file><file path="convert_sdxl_checkpoint.py"># Copied from: https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_diffusers_to_original_sdxl.py
# Script for converting a HF Diffusers saved pipeline to a Stable Diffusion checkpoint.
# *Only* converts the UNet, VAE, and Text Encoder.
# Does not convert optimizer state or any other thing.
import argparse
import os.path as osp
import re
import torch
from safetensors.torch import load_file, save_file
# =================#
# UNet Conversion #
# =================#
unet_conversion_map = [
    # (stable-diffusion, HF Diffusers)
    (&quot;time_embed.0.weight&quot;, &quot;time_embedding.linear_1.weight&quot;),
    (&quot;time_embed.0.bias&quot;, &quot;time_embedding.linear_1.bias&quot;),
    (&quot;time_embed.2.weight&quot;, &quot;time_embedding.linear_2.weight&quot;),
    (&quot;time_embed.2.bias&quot;, &quot;time_embedding.linear_2.bias&quot;),
    (&quot;input_blocks.0.0.weight&quot;, &quot;conv_in.weight&quot;),
    (&quot;input_blocks.0.0.bias&quot;, &quot;conv_in.bias&quot;),
    (&quot;out.0.weight&quot;, &quot;conv_norm_out.weight&quot;),
    (&quot;out.0.bias&quot;, &quot;conv_norm_out.bias&quot;),
    (&quot;out.2.weight&quot;, &quot;conv_out.weight&quot;),
    (&quot;out.2.bias&quot;, &quot;conv_out.bias&quot;),
    # the following are for sdxl
    (&quot;label_emb.0.0.weight&quot;, &quot;add_embedding.linear_1.weight&quot;),
    (&quot;label_emb.0.0.bias&quot;, &quot;add_embedding.linear_1.bias&quot;),
    (&quot;label_emb.0.2.weight&quot;, &quot;add_embedding.linear_2.weight&quot;),
    (&quot;label_emb.0.2.bias&quot;, &quot;add_embedding.linear_2.bias&quot;),
]
unet_conversion_map_resnet = [
    # (stable-diffusion, HF Diffusers)
    (&quot;in_layers.0&quot;, &quot;norm1&quot;),
    (&quot;in_layers.2&quot;, &quot;conv1&quot;),
    (&quot;out_layers.0&quot;, &quot;norm2&quot;),
    (&quot;out_layers.3&quot;, &quot;conv2&quot;),
    (&quot;emb_layers.1&quot;, &quot;time_emb_proj&quot;),
    (&quot;skip_connection&quot;, &quot;conv_shortcut&quot;),
]
unet_conversion_map_layer = []
# hardcoded number of downblocks and resnets/attentions...
# would need smarter logic for other networks.
for i in range(3):
    # loop over downblocks/upblocks
    for j in range(2):
        # loop over resnets/attentions for downblocks
        hf_down_res_prefix = f&quot;down_blocks.{i}.resnets.{j}.&quot;
        sd_down_res_prefix = f&quot;input_blocks.{3*i + j + 1}.0.&quot;
        unet_conversion_map_layer.append((sd_down_res_prefix, hf_down_res_prefix))
        if i &gt; 0:
            hf_down_atn_prefix = f&quot;down_blocks.{i}.attentions.{j}.&quot;
            sd_down_atn_prefix = f&quot;input_blocks.{3*i + j + 1}.1.&quot;
            unet_conversion_map_layer.append((sd_down_atn_prefix, hf_down_atn_prefix))
    for j in range(4):
        # loop over resnets/attentions for upblocks
        hf_up_res_prefix = f&quot;up_blocks.{i}.resnets.{j}.&quot;
        sd_up_res_prefix = f&quot;output_blocks.{3*i + j}.0.&quot;
        unet_conversion_map_layer.append((sd_up_res_prefix, hf_up_res_prefix))
        if i &lt; 2:
            # no attention layers in up_blocks.0
            hf_up_atn_prefix = f&quot;up_blocks.{i}.attentions.{j}.&quot;
            sd_up_atn_prefix = f&quot;output_blocks.{3 * i + j}.1.&quot;
            unet_conversion_map_layer.append((sd_up_atn_prefix, hf_up_atn_prefix))
    if i &lt; 3:
        # no downsample in down_blocks.3
        hf_downsample_prefix = f&quot;down_blocks.{i}.downsamplers.0.conv.&quot;
        sd_downsample_prefix = f&quot;input_blocks.{3*(i+1)}.0.op.&quot;
        unet_conversion_map_layer.append((sd_downsample_prefix, hf_downsample_prefix))
        # no upsample in up_blocks.3
        hf_upsample_prefix = f&quot;up_blocks.{i}.upsamplers.0.&quot;
        sd_upsample_prefix = f&quot;output_blocks.{3*i + 2}.{1 if i == 0 else 2}.&quot;
        unet_conversion_map_layer.append((sd_upsample_prefix, hf_upsample_prefix))
unet_conversion_map_layer.append((&quot;output_blocks.2.2.conv.&quot;, &quot;output_blocks.2.1.conv.&quot;))
hf_mid_atn_prefix = &quot;mid_block.attentions.0.&quot;
sd_mid_atn_prefix = &quot;middle_block.1.&quot;
unet_conversion_map_layer.append((sd_mid_atn_prefix, hf_mid_atn_prefix))
for j in range(2):
    hf_mid_res_prefix = f&quot;mid_block.resnets.{j}.&quot;
    sd_mid_res_prefix = f&quot;middle_block.{2*j}.&quot;
    unet_conversion_map_layer.append((sd_mid_res_prefix, hf_mid_res_prefix))
def convert_unet_state_dict(unet_state_dict):
    # buyer beware: this is a *brittle* function,
    # and correct output requires that all of these pieces interact in
    # the exact order in which I have arranged them.
    mapping = {k: k for k in unet_state_dict.keys()}
    for sd_name, hf_name in unet_conversion_map:
        mapping[hf_name] = sd_name
    for k, v in mapping.items():
        if &quot;resnets&quot; in k:
            for sd_part, hf_part in unet_conversion_map_resnet:
                v = v.replace(hf_part, sd_part)
            mapping[k] = v
    for k, v in mapping.items():
        for sd_part, hf_part in unet_conversion_map_layer:
            v = v.replace(hf_part, sd_part)
        mapping[k] = v
    new_state_dict = {sd_name: unet_state_dict[hf_name] for hf_name, sd_name in mapping.items()}
    return new_state_dict
# ================#
# VAE Conversion #
# ================#
vae_conversion_map = [
    # (stable-diffusion, HF Diffusers)
    (&quot;nin_shortcut&quot;, &quot;conv_shortcut&quot;),
    (&quot;norm_out&quot;, &quot;conv_norm_out&quot;),
    (&quot;mid.attn_1.&quot;, &quot;mid_block.attentions.0.&quot;),
]
for i in range(4):
    # down_blocks have two resnets
    for j in range(2):
        hf_down_prefix = f&quot;encoder.down_blocks.{i}.resnets.{j}.&quot;
        sd_down_prefix = f&quot;encoder.down.{i}.block.{j}.&quot;
        vae_conversion_map.append((sd_down_prefix, hf_down_prefix))
    if i &lt; 3:
        hf_downsample_prefix = f&quot;down_blocks.{i}.downsamplers.0.&quot;
        sd_downsample_prefix = f&quot;down.{i}.downsample.&quot;
        vae_conversion_map.append((sd_downsample_prefix, hf_downsample_prefix))
        hf_upsample_prefix = f&quot;up_blocks.{i}.upsamplers.0.&quot;
        sd_upsample_prefix = f&quot;up.{3-i}.upsample.&quot;
        vae_conversion_map.append((sd_upsample_prefix, hf_upsample_prefix))
    # up_blocks have three resnets
    # also, up blocks in hf are numbered in reverse from sd
    for j in range(3):
        hf_up_prefix = f&quot;decoder.up_blocks.{i}.resnets.{j}.&quot;
        sd_up_prefix = f&quot;decoder.up.{3-i}.block.{j}.&quot;
        vae_conversion_map.append((sd_up_prefix, hf_up_prefix))
# this part accounts for mid blocks in both the encoder and the decoder
for i in range(2):
    hf_mid_res_prefix = f&quot;mid_block.resnets.{i}.&quot;
    sd_mid_res_prefix = f&quot;mid.block_{i+1}.&quot;
    vae_conversion_map.append((sd_mid_res_prefix, hf_mid_res_prefix))
vae_conversion_map_attn = [
    # (stable-diffusion, HF Diffusers)
    (&quot;norm.&quot;, &quot;group_norm.&quot;),
    # the following are for SDXL
    (&quot;q.&quot;, &quot;to_q.&quot;),
    (&quot;k.&quot;, &quot;to_k.&quot;),
    (&quot;v.&quot;, &quot;to_v.&quot;),
    (&quot;proj_out.&quot;, &quot;to_out.0.&quot;),
]
def reshape_weight_for_sd(w):
    # convert HF linear weights to SD conv2d weights
    return w.reshape(*w.shape, 1, 1)
def convert_vae_state_dict(vae_state_dict):
    mapping = {k: k for k in vae_state_dict.keys()}
    for k, v in mapping.items():
        for sd_part, hf_part in vae_conversion_map:
            v = v.replace(hf_part, sd_part)
        mapping[k] = v
    for k, v in mapping.items():
        if &quot;attentions&quot; in k:
            for sd_part, hf_part in vae_conversion_map_attn:
                v = v.replace(hf_part, sd_part)
            mapping[k] = v
    new_state_dict = {v: vae_state_dict[k] for k, v in mapping.items()}
    weights_to_convert = [&quot;q&quot;, &quot;k&quot;, &quot;v&quot;, &quot;proj_out&quot;]
    for k, v in new_state_dict.items():
        for weight_name in weights_to_convert:
            if f&quot;mid.attn_1.{weight_name}.weight&quot; in k:
                print(f&quot;Reshaping {k} for SD format&quot;)
                new_state_dict[k] = reshape_weight_for_sd(v)
    return new_state_dict
# =========================#
# Text Encoder Conversion #
# =========================#
textenc_conversion_lst = [
    # (stable-diffusion, HF Diffusers)
    (&quot;transformer.resblocks.&quot;, &quot;text_model.encoder.layers.&quot;),
    (&quot;ln_1&quot;, &quot;layer_norm1&quot;),
    (&quot;ln_2&quot;, &quot;layer_norm2&quot;),
    (&quot;.c_fc.&quot;, &quot;.fc1.&quot;),
    (&quot;.c_proj.&quot;, &quot;.fc2.&quot;),
    (&quot;.attn&quot;, &quot;.self_attn&quot;),
    (&quot;ln_final.&quot;, &quot;text_model.final_layer_norm.&quot;),
    (&quot;token_embedding.weight&quot;, &quot;text_model.embeddings.token_embedding.weight&quot;),
    (&quot;positional_embedding&quot;, &quot;text_model.embeddings.position_embedding.weight&quot;),
]
protected = {re.escape(x[1]): x[0] for x in textenc_conversion_lst}
textenc_pattern = re.compile(&quot;|&quot;.join(protected.keys()))
# Ordering is from https://github.com/pytorch/pytorch/blob/master/test/cpp/api/modules.cpp
code2idx = {&quot;q&quot;: 0, &quot;k&quot;: 1, &quot;v&quot;: 2}
def convert_openclip_text_enc_state_dict(text_enc_dict):
    new_state_dict = {}
    capture_qkv_weight = {}
    capture_qkv_bias = {}
    for k, v in text_enc_dict.items():
        if (
            k.endswith(&quot;.self_attn.q_proj.weight&quot;)
            or k.endswith(&quot;.self_attn.k_proj.weight&quot;)
            or k.endswith(&quot;.self_attn.v_proj.weight&quot;)
        ):
            k_pre = k[: -len(&quot;.q_proj.weight&quot;)]
            k_code = k[-len(&quot;q_proj.weight&quot;)]
            if k_pre not in capture_qkv_weight:
                capture_qkv_weight[k_pre] = [None, None, None]
            capture_qkv_weight[k_pre][code2idx[k_code]] = v
            continue
        if (
            k.endswith(&quot;.self_attn.q_proj.bias&quot;)
            or k.endswith(&quot;.self_attn.k_proj.bias&quot;)
            or k.endswith(&quot;.self_attn.v_proj.bias&quot;)
        ):
            k_pre = k[: -len(&quot;.q_proj.bias&quot;)]
            k_code = k[-len(&quot;q_proj.bias&quot;)]
            if k_pre not in capture_qkv_bias:
                capture_qkv_bias[k_pre] = [None, None, None]
            capture_qkv_bias[k_pre][code2idx[k_code]] = v
            continue
        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k)
        new_state_dict[relabelled_key] = v
    for k_pre, tensors in capture_qkv_weight.items():
        if None in tensors:
            raise Exception(&quot;CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing&quot;)
        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)
        new_state_dict[relabelled_key + &quot;.in_proj_weight&quot;] = torch.cat(tensors)
    for k_pre, tensors in capture_qkv_bias.items():
        if None in tensors:
            raise Exception(&quot;CORRUPTED MODEL: one of the q-k-v values for the text encoder was missing&quot;)
        relabelled_key = textenc_pattern.sub(lambda m: protected[re.escape(m.group(0))], k_pre)
        new_state_dict[relabelled_key + &quot;.in_proj_bias&quot;] = torch.cat(tensors)
    return new_state_dict
def convert_openai_text_enc_state_dict(text_enc_dict):
    return text_enc_dict
if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--model_path&quot;, default=None, type=str, required=True, help=&quot;Path to the model to convert.&quot;)
    parser.add_argument(&quot;--checkpoint_path&quot;, default=None, type=str, required=True, help=&quot;Path to the output model.&quot;)
    parser.add_argument(&quot;--half&quot;, action=&quot;store_true&quot;, help=&quot;Save weights in half precision.&quot;)
    parser.add_argument(
        &quot;--use_safetensors&quot;, action=&quot;store_true&quot;, help=&quot;Save weights use safetensors, default is ckpt.&quot;
    )
    args = parser.parse_args()
    assert args.model_path is not None, &quot;Must provide a model path!&quot;
    assert args.checkpoint_path is not None, &quot;Must provide a checkpoint path!&quot;
    # Path for safetensors
    unet_path = osp.join(args.model_path, &quot;unet&quot;, &quot;diffusion_pytorch_model.safetensors&quot;)
    vae_path = osp.join(args.model_path, &quot;vae&quot;, &quot;diffusion_pytorch_model.safetensors&quot;)
    text_enc_path = osp.join(args.model_path, &quot;text_encoder&quot;, &quot;model.safetensors&quot;)
    text_enc_2_path = osp.join(args.model_path, &quot;text_encoder_2&quot;, &quot;model.safetensors&quot;)
    # Load models from safetensors if it exists, if it doesn&apos;t pytorch
    if osp.exists(unet_path):
        unet_state_dict = load_file(unet_path, device=&quot;cpu&quot;)
    else:
        unet_path = osp.join(args.model_path, &quot;unet&quot;, &quot;diffusion_pytorch_model.bin&quot;)
        unet_state_dict = torch.load(unet_path, map_location=&quot;cpu&quot;)
    if osp.exists(vae_path):
        vae_state_dict = load_file(vae_path, device=&quot;cpu&quot;)
    else:
        vae_path = osp.join(args.model_path, &quot;vae&quot;, &quot;diffusion_pytorch_model.bin&quot;)
        vae_state_dict = torch.load(vae_path, map_location=&quot;cpu&quot;)
    if osp.exists(text_enc_path):
        text_enc_dict = load_file(text_enc_path, device=&quot;cpu&quot;)
    else:
        text_enc_path = osp.join(args.model_path, &quot;text_encoder&quot;, &quot;pytorch_model.bin&quot;)
        text_enc_dict = torch.load(text_enc_path, map_location=&quot;cpu&quot;)
    if osp.exists(text_enc_2_path):
        text_enc_2_dict = load_file(text_enc_2_path, device=&quot;cpu&quot;)
    else:
        text_enc_2_path = osp.join(args.model_path, &quot;text_encoder_2&quot;, &quot;pytorch_model.bin&quot;)
        text_enc_2_dict = torch.load(text_enc_2_path, map_location=&quot;cpu&quot;)
    # Convert the UNet model
    unet_state_dict = convert_unet_state_dict(unet_state_dict)
    unet_state_dict = {&quot;model.diffusion_model.&quot; + k: v for k, v in unet_state_dict.items()}
    # Convert the VAE model
    vae_state_dict = convert_vae_state_dict(vae_state_dict)
    vae_state_dict = {&quot;first_stage_model.&quot; + k: v for k, v in vae_state_dict.items()}
    text_enc_dict = convert_openai_text_enc_state_dict(text_enc_dict)
    text_enc_dict = {&quot;conditioner.embedders.0.transformer.&quot; + k: v for k, v in text_enc_dict.items()}
    text_enc_2_dict = convert_openclip_text_enc_state_dict(text_enc_2_dict)
    text_enc_2_dict = {&quot;conditioner.embedders.1.model.&quot; + k: v for k, v in text_enc_2_dict.items()}
    # Put together new checkpoint
    state_dict = {**unet_state_dict, **vae_state_dict, **text_enc_dict, **text_enc_2_dict}
    if args.half:
        state_dict = {k: v.half() for k, v in state_dict.items()}
    if args.use_safetensors:
        save_file(state_dict, args.checkpoint_path)
    else:
        state_dict = {&quot;state_dict&quot;: state_dict}
        torch.save(state_dict, args.checkpoint_path)</file><file path="docker-start.sh">#!/bin/bash
# Export useful ENV variables, including all Runpod specific vars, to /etc/rp_environment
# This file can then later be sourced in a login shell
echo &quot;Exporting environment variables...&quot;
printenv |
	grep -E &apos;^RUNPOD_|^PATH=|^HF_HOME=|^HUGGING_FACE_HUB_TOKEN=|^_=&apos; |
	sed &apos;s/^\(.*\)=\(.*\)$/export \1=&quot;\2&quot;/&apos; &gt;&gt;/etc/rp_environment
# Add it to Bash login script
echo &apos;source /etc/rp_environment&apos; &gt;&gt;~/.bashrc
# Vast.ai uses $SSH_PUBLIC_KEY
if [[ $SSH_PUBLIC_KEY ]]; then
	PUBLIC_KEY=&quot;${SSH_PUBLIC_KEY}&quot;
fi
# Runpod uses $PUBLIC_KEY
if [[ $PUBLIC_KEY ]]; then
	mkdir -p ~/.ssh
	chmod 700 ~/.ssh
	echo &quot;${PUBLIC_KEY}&quot; &gt;&gt;~/.ssh/authorized_keys
	chmod 700 -R ~/.ssh
fi
# Start SSH server
service ssh start
# Load HF, WanDB tokens
if [ -n &quot;$HUGGING_FACE_HUB_TOKEN&quot; ]; then huggingface-cli login --token &quot;$HUGGING_FACE_HUB_TOKEN&quot; --add-to-git-credential; else echo &quot;HUGGING_FACE_HUB_TOKEN not set; skipping login&quot;; fi
if [ -n &quot;$WANDB_TOKEN&quot; ]; then wandb login &quot;$WANDB_TOKEN&quot;; else echo &quot;WANDB_TOKEN not set; skipping login&quot;; fi
# 🫡
sleep infinity</file><file path="Dockerfile"># SimpleTuner needs CU141
FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04

# /workspace is the default volume for Runpod &amp; other hosts
WORKDIR /workspace

# Update apt-get
RUN apt-get update -y

# Prevents different commands from being stuck by waiting
# on user input during build
ENV DEBIAN_FRONTEND noninteractive

# Install libg dependencies
RUN apt install libgl1-mesa-glx -y
RUN apt-get install &apos;ffmpeg&apos;\
    &apos;libsm6&apos;\
    &apos;libxext6&apos;  -y

# Install misc unix libraries
RUN apt-get install -y --no-install-recommends openssh-server \
                                               openssh-client \
                                               git \
                                               git-lfs \
                                               wget \
                                               curl \
                                               tmux \
                                               tldr \
                                               nvtop \
                                               vim \
                                               rsync \
                                               net-tools \
                                               less \
                                               iputils-ping \
                                               7zip \
                                               zip \
                                               unzip \
                                               htop \
                                               inotify-tools

# Set up git to support LFS, and to store credentials; useful for Huggingface Hub
RUN git config --global credential.helper store &amp;&amp; \
    git lfs install

# Install Python VENV
RUN apt-get install -y python3.10-venv

# Ensure SSH access. Not needed for Runpod but is required on Vast and other Docker hosts
EXPOSE 22/tcp

# Python
RUN apt-get update -y &amp;&amp; apt-get install -y python3 python3-pip
RUN python3 -m pip install pip --upgrade

# HF
ENV HF_HOME=/workspace/huggingface

RUN pip3 install &quot;huggingface_hub[cli]&quot;

# WanDB
RUN pip3 install wandb

# Clone SimpleTuner
RUN git clone https://github.com/bghira/SimpleTuner --branch release
# RUN git clone https://github.com/bghira/SimpleTuner --branch main # Uncomment to use latest (possibly unstable) version

# Install SimpleTuner
RUN pip3 install poetry
RUN cd SimpleTuner &amp;&amp; python3 -m venv .venv &amp;&amp; poetry install --no-root
RUN chmod +x SimpleTuner/train.sh

# Copy start script with exec permissions
COPY --chmod=755 docker-start.sh /start.sh

# Dummy entrypoint
ENTRYPOINT [ &quot;/start.sh&quot; ]</file><file path="filter_list.txt">The image does not provide a clear view of the entire scene, making it challenging to accurately caption. However, based on the visible elements, a possible caption could be: 
The image does not .* depict .* caption like 
The image does not depict .* South America.* shows a 
The image does not depict a South American 
The photo does not prominently feature any anatomical features. It primarily showcases 
The image showcases 
The image features 
The image captures
The image depicts
The photo showcases
The photo features
The photo captures
The image does not have a caption provided in the image itself. However, based on the content, a suitable caption might be: 
The image does not have a clear caption as it is an experimental photo. However, one could describe it as 
The image does not have a clear subject or context .* for a precise caption. 
Caption: 
The image does not have a clear caption .* describe it as 
The image does not have a clear caption .* could be:
The image does not have .* appears to be 
The image does not have a clear subject or context .* appears to be 
The image does not have a clear subject or context .* could be:
The image does not have a direct caption .*:
The image does not require a caption .* &apos;
smoking vaping 
^there is 
araffe 
arafed 
^someone is 
^The image is 
^setting. 
The caption for .* could be: 
The image does not .* showcases 
The image does not .* shows</file><file path="inference_comparison.py">###
# This script will generate images for the same seed/prompt across many models and stitch the outputs together.
###
from diffusers import AutoPipelineForText2Image
from torch import manual_seed, float16
import os
from PIL import Image, ImageDraw, ImageFont
from helpers.prompts import prompts
# Define your pipelines and settings in a list of dictionaries
pipelines_info = [
    {
        &quot;label&quot;: &quot;velocity-v1&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-velocity-v1&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    {
        &quot;label&quot;: &quot;gamma-v1&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-gamma-v1&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    {
        &quot;label&quot;: &quot;gamma-v2&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-gamma-v2&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    {
        &quot;label&quot;: &quot;otaku-v1&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-otaku-v1&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    {
        &quot;label&quot;: &quot;gamma-training&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-gamma-training&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    {
        &quot;label&quot;: &quot;gamma-v2-1&quot;,
        &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-gamma-v2-1&quot;,
        &quot;settings&quot;: {
            &quot;guidance_scale&quot;: 8.0,
            &quot;guidance_rescale&quot;: 0.7,
            &quot;num_inference_steps&quot;: 30,
            &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;,
        },
    },
    # {&quot;label&quot;: &quot;v2.1+LoRA&quot;, &quot;pretrained_model&quot;: &quot;ptx0/terminus-xl-gamma-v2-1&quot;, &quot;lora&quot;: {&quot;weights&quot;: &quot;ptx0/simpletuner-lora-test&quot;, &quot;weight_name&quot;: &quot;pytorch_lora_weights.safetensors&quot;}, &quot;settings&quot;: {&quot;guidance_scale&quot;: 8.0, &quot;guidance_rescale&quot;: 0.7, &quot;num_inference_steps&quot;: 30, &quot;negative_prompt&quot;: &quot;blurry, cropped, ugly, upscaled&quot;}},
]
def combine_and_label_images(images_info, output_path):
    # Assume images_info is a list of tuples: (Image object, label)
    # Initial setup based on the first image dimensions and number of images
    label_height = 45
    total_width = sum(image.width for image, _ in images_info)
    max_height = max(image.height for image, _ in images_info) + label_height
    combined_image = Image.new(&quot;RGB&quot;, (total_width, max_height), &quot;white&quot;)
    # Combine images and labels
    current_x = 0
    for image, label in images_info:
        combined_image.paste(image, (current_x, label_height))
        current_x += image.width
    # Adding labels using a uniform method for text placement
    draw = ImageDraw.Draw(combined_image)
    try:
        # Attempt to use a specific font
        font = ImageFont.truetype(
            &quot;.venv/lib/python3.11/site-packages/cv2/qt/fonts/DejaVuSans.ttf&quot;, 40
        )  # Adjust font path and size
    except IOError:
        # Fallback to default font
        font = ImageFont.load_default()
    current_x = 0
    for _, label in images_info:
        draw.text((current_x + 10, 2), label, fill=&quot;black&quot;, font=font)
        current_x += image.width
    combined_image.save(output_path)
# Processing pipelines
base_pipeline = AutoPipelineForText2Image.from_pretrained(
    &quot;ptx0/terminus-xl-gamma-v2&quot;, torch_dtype=float16
).to(&quot;cuda&quot;)
text_encoder_1 = base_pipeline.components[&quot;text_encoder&quot;]
text_encoder_2 = base_pipeline.components[&quot;text_encoder_2&quot;]
vae = base_pipeline.components[&quot;vae&quot;]
for shortname, prompt in prompts.items():
    print(f&quot;Processing: {shortname}&quot;)
    target_dir = f&quot;inference/images/{shortname}&quot;
    # Does the combined image exist? Skip it then.
    if os.path.exists(f&quot;{target_dir}/combined_image.png&quot;):
        continue
    os.makedirs(target_dir, exist_ok=True)
    images_info = []
    for pipeline_info in pipelines_info:
        image_path = f&apos;{target_dir}/image-{pipeline_info[&quot;label&quot;].replace(&quot;+&quot;, &quot;plus&quot;).lower()}.png&apos;
        if os.path.exists(image_path):
            continue
        # Initialize pipeline
        pipeline = AutoPipelineForText2Image.from_pretrained(
            pipeline_info[&quot;pretrained_model&quot;],
            text_encoder=text_encoder_1,
            text_encoder_2=text_encoder_2,
            vae=vae,
            torch_dtype=float16,
        ).to(&quot;cuda&quot;)
        # Load LoRA weights if specified
        if &quot;lora&quot; in pipeline_info:
            pipeline.load_lora_weights(
                pipeline_info[&quot;lora&quot;][&quot;weights&quot;],
                weight_name=pipeline_info[&quot;lora&quot;][&quot;weight_name&quot;],
            )
        # Generate image with specified settings
        settings = pipeline_info.get(&quot;settings&quot;, {})
        image = pipeline(prompt, generator=manual_seed(420420420), **settings).images[0]
        # Unload LoRA weights if they were loaded
        if &quot;lora&quot; in pipeline_info:
            pipeline.unload_lora_weights()
        del pipeline
        image.save(image_path, format=&quot;PNG&quot;)
        images_info.append((image, pipeline_info[&quot;label&quot;]))
    # Combine and label images
    combine_and_label_images(images_info, f&quot;{target_dir}/combined_image.png&quot;)</file><file path="inference.py">from accelerate import Accelerator
from diffusers import (
    DiffusionPipeline,
    UNet2DConditionModel,
    DDPMScheduler,
    DDIMScheduler,
)
from transformers import CLIPTextModel
from helpers.prompts import prompts
from compel import Compel
import torch, os, logging
logger = logging.getLogger(&quot;SimpleTuner-inference&quot;)
logger.setLevel(logging.INFO)
# Load the pipeline with the same arguments (model, revision) that were used for training
model_id = &quot;stabilityai/stable-diffusion-2&quot;
model_id = &quot;ptx0/terminus-xl-gamma-v2-1&quot;
base_dir = &quot;/Volumes/models/training&quot;
model_path = os.path.join(base_dir, &quot;models&quot;)
# output_test_dir = os.path.join(base_dir, &apos;test_results&apos;)
output_test_dir = os.path.join(base_dir, &quot;encoder_test&quot;)
save_pretrained = False
torch_seed = 4202420420
# Find the latest checkpoint
import os
checkpoints = [
    int(x.split(&quot;-&quot;)[1]) for x in os.listdir(model_path) if x.startswith(&quot;checkpoint-&quot;)
]
checkpoints.sort()
range_begin = 0
range_step = 100
base_checkpoint_for_unet = (
    0  # Use the unet from this model for comparison against text encoder progress.
)
try:
    range_end = checkpoints[-1]
except Exception as e:
    range_end = range_begin
logging.info(f&quot;Highest checkpoint found so far: {range_end}&quot;)
# Convert numeric range to an array of string numerics:
# checkpoints = [ str(x) for x in range(range_begin, range_end + range_step, range_step) ]
checkpoints.reverse()
torch.set_float32_matmul_precision(&quot;high&quot;)
negative = &quot;deep fried watermark cropped out-of-frame low quality low res oorly drawn bad anatomy wrong anatomy extra limb missing limb floating limbs (mutated hands and fingers)1.4 disconnected limbs mutation mutated ugly disgusting blurry amputation synthetic rendering&quot;
for checkpoint in checkpoints:
    for enable_textencoder in [False]:
        suffix = (
            &quot;t&quot; if enable_textencoder else &quot;b&quot; if enable_textencoder is None else &quot;u&quot;
        )
        if len(checkpoints) &gt; 1 and os.path.isfile(
            f&quot;{output_test_dir}/target-{checkpoint}_{base_checkpoint_for_unet}{suffix}.png&quot;
        ):
            continue
        try:
            logging.info(f&quot;Loading checkpoint: {model_path}/checkpoint-{checkpoint}&quot;)
            # Does the checkpoint path exist?
            if checkpoint != &quot;0&quot; and not os.path.exists(
                f&quot;{model_path}/checkpoint-{checkpoint}&quot;
            ):
                logging.info(f&quot;Checkpoint {checkpoint} does not exist.&quot;)
                continue
            if checkpoint != &quot;0&quot;:
                logging.info(f&quot;Loading non-base ckpt.&quot;)
                if enable_textencoder is None:
                    logging.info(f&quot;Loading full unet and te&quot;)
                    # Enable fully-trained text_encoder and unet
                    text_encoder = CLIPTextModel.from_pretrained(
                        f&quot;{model_path}/checkpoint-{checkpoint}/text_encoder&quot;
                    )
                    unet = UNet2DConditionModel.from_pretrained(
                        f&quot;{model_path}/checkpoint-{checkpoint}/unet&quot;
                    )
                    pipeline = DiffusionPipeline.from_pretrained(
                        model_id, unet=unet, text_encoder=text_encoder
                    )
                elif enable_textencoder:
                    # Enable the fully-trained text encoder with the 4200 ckpt unet
                    logging.info(f&quot;Loading full te and base unet&quot;)
                    text_encoder = CLIPTextModel.from_pretrained(
                        f&quot;{model_path}/checkpoint-{checkpoint}/text_encoder&quot;
                    )
                    pipeline = DiffusionPipeline.from_pretrained(
                        model_id, text_encoder=text_encoder
                    )
                else:
                    # Enable the fully-trained unet with the 4200 ckpt text encoder
                    logging.info(f&quot;Loading full unet and base te&quot;)
                    unet = UNet2DConditionModel.from_pretrained(
                        f&quot;{model_path}/checkpoint-{checkpoint}/unet&quot;
                    )
                    pipeline = DiffusionPipeline.from_pretrained(model_id, unet=unet)
            else:
                # Do the base model.
                logging.info(f&quot;Loading base ckpt.&quot;)
                pipeline = DiffusionPipeline.from_pretrained(model_id)
            pipeline.unet = torch.compile(pipeline.unet)
            compel = Compel(
                tokenizer=pipeline.tokenizer, text_encoder=pipeline.text_encoder
            )
            negative_embed = compel.build_conditioning_tensor(negative)
            pipeline.scheduler = DDIMScheduler.from_pretrained(
                model_id,
                subfolder=&quot;scheduler&quot;,
                rescale_betas_zero_snr=True,
                timestep_spacing=&quot;trailing&quot;,
            )
            pipeline.to(
                &quot;cuda&quot;
                if torch.cuda.is_available()
                else &quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;
            )
        except Exception as e:
            logging.info(
                f&quot;Could not generate pipeline for checkpoint {checkpoint}: {e}&quot;
            )
            continue
        # Does the file exist already?
        import os
        for shortname, prompt in prompts.items():
            if not os.path.isfile(
                f&quot;{output_test_dir}/{shortname}-{checkpoint}_{base_checkpoint_for_unet}{suffix}.png&quot;
            ):
                logging.info(
                    f&quot;Generating {shortname} at {checkpoint}_{base_checkpoint_for_unet}{suffix}&quot;
                )
                logging.info(f&quot;Shortname: {shortname}, Prompt: {prompt}&quot;)
                logging.info(f&quot;Negative: {negative}&quot;)
                conditioning = compel.build_conditioning_tensor(prompt)
                generator = torch.Generator(device=&quot;cuda&quot;).manual_seed(torch_seed)
                pipeline.do_guidance_rescale_before = 20
                output = pipeline(
                    generator=generator,
                    negative_prompt_embeds=negative_embed,
                    prompt_embeds=conditioning,
                    guidance_scale=7.5,
                    guidance_rescale=0.0,
                    width=1152,
                    height=768,
                    num_inference_steps=25,
                ).images[0]
                output.save(
                    f&quot;{output_test_dir}/{shortname}-{checkpoint}_{base_checkpoint_for_unet}{suffix}.png&quot;
                )
                del output
        if save_pretrained and not os.path.exists(f&quot;{model_path}/pipeline&quot;):
            logging.info(f&quot;Saving pretrained pipeline.&quot;)
            pipeline.save_pretrained(
                f&quot;{model_path}/pseudo-real&quot;, safe_serialization=True
            )
        elif save_pretrained:
            raise Exception(&quot;Can not save pretrained model, path already exists.&quot;)
logging.info(f&quot;Exit.&quot;)</file><file path="INSTALL.md">## Setup

For users that wish to make use of Docker or another container orchestration platform, see [this document](/documentation/DOCKER.md) first.

### Installation

For  users operating on Windows 10 or newer, an installation guide based on Docker and WSL is available here [this document](/documentation/DOCKER.md).

Clone the SimpleTuner repository and set up the python venv:

```bash
git clone --branch=release https://github.com/bghira/SimpleTuner.git

cd SimpleTuner

# if python --version shows 3.11 you can just also use the &apos;python&apos; command here.
python3.11 -m venv .venv

source .venv/bin/activate

pip install -U poetry pip

# Necessary on some systems to prevent it from deciding it knows better than us.
poetry config virtualenvs.create false
```

&gt; ℹ️ You can use your own custom venv path by setting `export VENV_PATH=/path/to/.venv` in your `config/config.env` file.

**Note:** We&apos;re currently installing the `release` branch here; the `main` branch may contain experimental features that might have better results or lower memory use.

Depending on your system, you will run one of 3 commands:

```bash
# MacOS
poetry install -C install/apple

# Linux
poetry install

# Linux with ROCM
poetry install -C install/rocm
```

#### NVIDIA Hopper / Blackwell follow-up steps

Optionally, Hopper (or newer) equipment can make use of FlashAttention3 for improved inference and training performance when making use of `torch.compile`

You&apos;ll need to run the following sequence of commands from your SimpleTuner directory, with your venv active:

```bash
git clone https://github.com/Dao-AILab/flash-attention
pushd flash-attention
  pushd hopper
    python setup.py install
  popd
popd
```

&gt; ⚠️ Managing the flash_attn build is poorly-supported in SimpleTuner, currently. This can break on updates, requiring you to re-run this build procedure manually from time-to-time.

#### AMD ROCm follow-up steps

The following must be executed for an AMD MI300X to be useable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
  python3 -m pip install --upgrade pip
  python3 -m pip install .
popd
```

### All platforms

- 2a. **Option One (Recommended)**: Run `configure.py`
- 2b. **Option Two**: Copy `config/config.json.example` to `config/config.json` and then fill in the details.

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.

#### Multiple GPU training

**Note**: For MultiGPU setup, you will have to set all of these variables in `config/config.env`

```bash
TRAINING_NUM_PROCESSES=1
TRAINING_NUM_MACHINES=1
TRAINING_DYNAMO_BACKEND=&apos;no&apos;
# this is auto-detected, and not necessary. but can be set explicitly.
CONFIG_BACKEND=&apos;json&apos;
```

Any missing values from your user config will fallback to the defaults.

3. If you are using `--report_to=&apos;wandb&apos;` (the default), the following will help you report your statistics:

```bash
wandb login
```

Follow the instructions that are printed, to locate your API key and configure it.

Once that is done, any of your training sessions and validation data will be available on Weights &amp; Biases.

&gt; ℹ️ If you would like to disable Weights &amp; Biases or Tensorboard reporting entirely, use `--report-to=none`


4. Launch the `train.sh` script; logs will be written to `debug.log`

```bash
./train.sh
```

&gt; ⚠️ At this point, if you used `configure.py`, you are done! If not - these commands will work, but further configuration is required. See [the tutorial](/TUTORIAL.md) for more information.

### Run unit tests

To run unit tests to ensure that installation has completed successfully:

```bash
poetry run python -m unittest discover tests/
```

## Advanced: Multiple configuration environments

For users who train multiple models or need to quickly switch between different datasets or settings, two environment variables are inspected at startup.

To use them:

```bash
env ENV=default CONFIG_BACKEND=env bash train.sh
```

- `ENV` will default to `default`, which points to the typical `SimpleTuner/config/` directory that this guide helped you configure
  - Using `ENV=pixart ./train.sh` would use `SimpleTuner/config/pixart` directory to find `config.env`
- `CONFIG_BACKEND` will default to `env`, which uses the typical `config.env` file this guide helped you configure
  - Supported options: `env`, `json`, `toml`, or `cmd` if you rely on running `train.py` manually
  - Using `CONFIG_BACKEND=json ./train.sh` would search for `SimpleTuner/config/config.json` instead of `config.env`
  - Similarly, `CONFIG_BACKEND=toml` will use `config.env`

You can create `config/config.env` that contains one or both of these values:

```bash
ENV=default
CONFIG_BACKEND=json
```

They will be remembered upon subsequent runs. Note that these can be added in addition to the multiGPU options described [above](#multiple-gpu-training).</file><file path="LICENSE">GNU AFFERO GENERAL PUBLIC LICENSE
Version 3, 19 November 2007

Copyright © 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;
Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.

Preamble
The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.

The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.

When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.

Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.

A secondary benefit of defending all users&apos; freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.

The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.

An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.

The precise terms and conditions for copying, distribution and modification follow.

TERMS AND CONDITIONS
0. Definitions.
&quot;This License&quot; refers to version 3 of the GNU Affero General Public License.

&quot;Copyright&quot; also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.

&quot;The Program&quot; refers to any copyrightable work licensed under this License. Each licensee is addressed as &quot;you&quot;. &quot;Licensees&quot; and &quot;recipients&quot; may be individuals or organizations.

To &quot;modify&quot; a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a &quot;modified version&quot; of the earlier work or a work &quot;based on&quot; the earlier work.

A &quot;covered work&quot; means either the unmodified Program or a work based on the Program.

To &quot;propagate&quot; a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.

To &quot;convey&quot; a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.

An interactive user interface displays &quot;Appropriate Legal Notices&quot; to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.

1. Source Code.
The &quot;source code&quot; for a work means the preferred form of the work for making modifications to it. &quot;Object code&quot; means any non-source form of a work.

A &quot;Standard Interface&quot; means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.

The &quot;System Libraries&quot; of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A &quot;Major Component&quot;, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.

The &quot;Corresponding Source&quot; for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work&apos;s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.

The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.

The Corresponding Source for a work in source code form is that same work.

2. Basic Permissions.
All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.

You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.

Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.

3. Protecting Users&apos; Legal Rights From Anti-Circumvention Law.
No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.

When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work&apos;s users, your or third parties&apos; legal rights to forbid circumvention of technological measures.

4. Conveying Verbatim Copies.
You may convey verbatim copies of the Program&apos;s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.

You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.

5. Conveying Modified Source Versions.
You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:

a) The work must carry prominent notices stating that you modified it, and giving a relevant date.
b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to &quot;keep intact all notices&quot;.
c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.
d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.
A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an &quot;aggregate&quot; if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation&apos;s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.

6. Conveying Non-Source Forms.
You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:

a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.
b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.
c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.
d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.
e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.
A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.

A &quot;User Product&quot; is either (1) a &quot;consumer product&quot;, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, &quot;normally used&quot; refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.

&quot;Installation Information&quot; for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.

If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).

The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.

Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.

7. Additional Terms.
&quot;Additional permissions&quot; are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.

When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.

Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:

a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or
b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or
c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or
d) Limiting the use for publicity purposes of names of licensors or authors of the material; or
e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or
f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.
All other non-permissive additional terms are considered &quot;further restrictions&quot; within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.

If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.

Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.

8. Termination.
You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).

However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.

Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.

Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.

9. Acceptance Not Required for Having Copies.
You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.

10. Automatic Licensing of Downstream Recipients.
Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.

An &quot;entity transaction&quot; is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party&apos;s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.

You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.

11. Patents.
A &quot;contributor&quot; is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor&apos;s &quot;contributor version&quot;.

A contributor&apos;s &quot;essential patent claims&quot; are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, &quot;control&quot; includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.

Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor&apos;s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.

In the following three paragraphs, a &quot;patent license&quot; is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To &quot;grant&quot; such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.

If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. &quot;Knowingly relying&quot; means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient&apos;s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.

If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.

A patent license is &quot;discriminatory&quot; if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.

Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.

12. No Surrender of Others&apos; Freedom.
If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.

13. Remote Network Interaction; Use with the GNU General Public License.
Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.

Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.

14. Revised Versions of this License.
The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.

Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License &quot;or any later version&quot; applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.

If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy&apos;s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.

Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.

15. Disclaimer of Warranty.
THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM &quot;AS IS&quot; WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

16. Limitation of Liability.
IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

17. Interpretation of Sections 15 and 16.
If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.

END OF TERMS AND CONDITIONS

How to Apply These Terms to Your New Programs
If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.

To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the &quot;copyright&quot; line and a pointer to where the full notice is found.

    &lt;one line to give the program&apos;s name and a brief idea of what it does.&gt;
    Copyright (C) &lt;year&gt;  &lt;name of author&gt;

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as
    published by the Free Software Foundation, either version 3 of the
    License, or (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.
Also add information on how to contact you by electronic and paper mail.

If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a &quot;Source&quot; link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.

You should also get your employer (if you work as a programmer) or school, if any, to sign a &quot;copyright disclaimer&quot; for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see &lt;https://www.gnu.org/licenses/&gt;.</file><file path="notebook.ipynb">{
 &quot;cells&quot;: [
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 1,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;#!poetry install&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 2,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;# Update this for your data path.\n&quot;,
    &quot;instance_data_dir = \&quot;/Volumes/ml/datasets/test_datasets/single_image_dataset\&quot;\n&quot;,
    &quot;pretrained_model_name_or_path = \&quot;black-forest-labs/FLUX.1-dev\&quot;\n&quot;,
    &quot;# Your public model name after it&apos;s pushed to the hub.\n&quot;,
    &quot;hub_model_id = \&quot;simpletuner-lora\&quot;\n&quot;,
    &quot;tracker_project_name = \&quot;flux-training\&quot;\n&quot;,
    &quot;\n&quot;,
    &quot;# Validation prompt\n&quot;,
    &quot;validation_prompt = \&quot;A photo-realistic image of a cat\&quot;\n&quot;,
    &quot;\n&quot;,
    &quot;train_batch_size = 1\n&quot;,
    &quot;learning_rate = 1e-4\n&quot;,
    &quot;\n&quot;,
    &quot;# choices: int8-quanto, fp8-quanto, no_change (mac and a100/h100 users get int4 and int2 as well)\n&quot;,
    &quot;base_model_precision = \&quot;no_change\&quot;&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 3,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;lycoris_config = {\n&quot;,
    &quot;    \&quot;algo\&quot;: \&quot;lokr\&quot;,\n&quot;,
    &quot;    \&quot;multiplier\&quot;: 1.0,\n&quot;,
    &quot;    \&quot;linear_dim\&quot;: 10000,\n&quot;,
    &quot;    \&quot;linear_alpha\&quot;: 1,\n&quot;,
    &quot;    \&quot;factor\&quot;: 12,\n&quot;,
    &quot;    \&quot;apply_preset\&quot;: {\n&quot;,
    &quot;        \&quot;target_module\&quot;: [\n&quot;,
    &quot;            \&quot;Attention\&quot;,\n&quot;,
    &quot;            \&quot;FeedForward\&quot;\n&quot;,
    &quot;        ],\n&quot;,
    &quot;        \&quot;module_algo_map\&quot;: {\n&quot;,
    &quot;            \&quot;Attention\&quot;: {\n&quot;,
    &quot;                \&quot;factor\&quot;: 12\n&quot;,
    &quot;            },\n&quot;,
    &quot;            \&quot;FeedForward\&quot;: {\n&quot;,
    &quot;                \&quot;factor\&quot;: 6\n&quot;,
    &quot;            }\n&quot;,
    &quot;        }\n&quot;,
    &quot;    }\n&quot;,
    &quot;}\n&quot;,
    &quot;# write to config/lycoris_config.json\n&quot;,
    &quot;import json\n&quot;,
    &quot;with open(\&quot;config/lycoris_config.json\&quot;, \&quot;w\&quot;) as f:\n&quot;,
    &quot;    json.dump(lycoris_config, f)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 4,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;training_config = {\n&quot;,
    &quot;    \&quot;mixed_precision\&quot;:\&quot;bf16\&quot;,\n&quot;,
    &quot;    \&quot;model_type\&quot;:\&quot;lora\&quot;,\n&quot;,
    &quot;    \&quot;pretrained_model_name_or_path\&quot;:pretrained_model_name_or_path,\n&quot;,
    &quot;    \&quot;gradient_checkpointing\&quot;:True,\n&quot;,
    &quot;    \&quot;cache_dir\&quot;: \&quot;cache\&quot;,\n&quot;,
    &quot;    \&quot;set_grads_to_none\&quot;:True,\n&quot;,
    &quot;    \&quot;gradient_accumulation_steps\&quot;:1,\n&quot;,
    &quot;    \&quot;resume_from_checkpoint\&quot;:\&quot;latest\&quot;,\n&quot;,
    &quot;    \&quot;snr_gamma\&quot;:5,\n&quot;,
    &quot;    \&quot;num_train_epochs\&quot;:0,\n&quot;,
    &quot;    \&quot;max_train_steps\&quot;:10000,\n&quot;,
    &quot;    \&quot;metadata_update_interval\&quot;:65,\n&quot;,
    &quot;    \&quot;optimizer\&quot;:\&quot;adamw_bf16\&quot;,\n&quot;,
    &quot;    \&quot;learning_rate\&quot;:learning_rate,\n&quot;,
    &quot;    \&quot;lr_scheduler\&quot;:\&quot;polynomial\&quot;,\n&quot;,
    &quot;    \&quot;seed\&quot;:42,\n&quot;,
    &quot;    \&quot;lr_warmup_steps\&quot;:100,\n&quot;,
    &quot;    \&quot;output_dir\&quot;:\&quot;output/models\&quot;,\n&quot;,
    &quot;    \&quot;non_ema_revision\&quot;: False,\n&quot;,
    &quot;    \&quot;aspect_bucket_rounding\&quot;:2,\n&quot;,
    &quot;    \&quot;inference_scheduler_timestep_spacing\&quot;:\&quot;trailing\&quot;,\n&quot;,
    &quot;    \&quot;training_scheduler_timestep_spacing\&quot;:\&quot;trailing\&quot;,\n&quot;,
    &quot;    \&quot;report_to\&quot;:\&quot;wandb\&quot;,\n&quot;,
    &quot;    \&quot;lr_end\&quot;:1e-8,\n&quot;,
    &quot;    \&quot;compress_disk_cache\&quot;:True,\n&quot;,
    &quot;    \&quot;push_to_hub\&quot;:True,\n&quot;,
    &quot;    \&quot;hub_model_id\&quot;:hub_model_id,\n&quot;,
    &quot;    \&quot;push_checkpoints_to_hub\&quot;:True,\n&quot;,
    &quot;    \&quot;model_family\&quot;:\&quot;flux\&quot;,\n&quot;,
    &quot;    \&quot;disable_benchmark\&quot;:False,\n&quot;,
    &quot;    \&quot;train_batch\&quot;:train_batch_size,\n&quot;,
    &quot;    \&quot;max_workers\&quot;:32,\n&quot;,
    &quot;    \&quot;read_batch_size\&quot;:25,\n&quot;,
    &quot;    \&quot;write_batch_size\&quot;:64,\n&quot;,
    &quot;    \&quot;caption_dropout_probability\&quot;:0.1,\n&quot;,
    &quot;    \&quot;torch_num_threads\&quot;:8,\n&quot;,
    &quot;    \&quot;image_processing_batch_size\&quot;:32,\n&quot;,
    &quot;    \&quot;vae_batch_size\&quot;:4,\n&quot;,
    &quot;    \&quot;validation_prompt\&quot;:validation_prompt,\n&quot;,
    &quot;    \&quot;num_validation_images\&quot;:1,\n&quot;,
    &quot;    \&quot;validation_num_inference_steps\&quot;:20,\n&quot;,
    &quot;    \&quot;validation_seed\&quot;:42,\n&quot;,
    &quot;    \&quot;minimum_image_size\&quot;:0,\n&quot;,
    &quot;    \&quot;resolution\&quot;:1024,\n&quot;,
    &quot;    \&quot;validation_resolution\&quot;:\&quot;1024x1024\&quot;,\n&quot;,
    &quot;    \&quot;resolution_type\&quot;:\&quot;pixel_area\&quot;,\n&quot;,
    &quot;    \&quot;lycoris_config\&quot;:\&quot;config/lycoris_config.json\&quot;,\n&quot;,
    &quot;    \&quot;lora_type\&quot;:\&quot;lycoris\&quot;,\n&quot;,
    &quot;    \&quot;base_model_precision\&quot;:base_model_precision,\n&quot;,
    &quot;    \&quot;checkpointing_steps\&quot;:500,\n&quot;,
    &quot;    \&quot;checkpoints_total_limit\&quot;:5,\n&quot;,
    &quot;    \&quot;validation_steps\&quot;:500,\n&quot;,
    &quot;    \&quot;tracker_run_name\&quot;:hub_model_id,\n&quot;,
    &quot;    \&quot;tracker_project_name\&quot;:tracker_project_name,\n&quot;,
    &quot;    \&quot;validation_guidance\&quot;:3.0,\n&quot;,
    &quot;    \&quot;validation_guidance_real\&quot;:1.0,\n&quot;,
    &quot;    \&quot;validation_guidance_rescale\&quot;:0.0,\n&quot;,
    &quot;    \&quot;validation_negative_prompt\&quot;:\&quot;blurry, cropped, ugly\&quot;,\n&quot;,
    &quot;}\n&quot;,
    &quot;# write to config/config.json\n&quot;,
    &quot;with open(\&quot;config/config.json\&quot;, \&quot;w\&quot;) as f:\n&quot;,
    &quot;    json.dump(training_config, f, indent=4)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 5,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;dataloader_config = [\n&quot;,
    &quot;    {\n&quot;,
    &quot;        \&quot;id\&quot;: \&quot;my-dataset-512\&quot;,\n&quot;,
    &quot;        \&quot;type\&quot;: \&quot;local\&quot;,\n&quot;,
    &quot;        \&quot;instance_data_dir\&quot;: instance_data_dir,\n&quot;,
    &quot;        \&quot;crop\&quot;: False,\n&quot;,
    &quot;        \&quot;crop_style\&quot;: \&quot;random\&quot;,\n&quot;,
    &quot;        \&quot;minimum_image_size\&quot;: 128,\n&quot;,
    &quot;        \&quot;resolution\&quot;: 512,\n&quot;,
    &quot;        \&quot;resolution_type\&quot;: \&quot;pixel_area\&quot;,\n&quot;,
    &quot;        \&quot;repeats\&quot;: \&quot;4\&quot;,\n&quot;,
    &quot;        \&quot;metadata_backend\&quot;: \&quot;discovery\&quot;,\n&quot;,
    &quot;        \&quot;caption_strategy\&quot;: \&quot;filename\&quot;,\n&quot;,
    &quot;        \&quot;cache_dir_vae\&quot;: \&quot;cache/vae-512\&quot;\n&quot;,
    &quot;    },\n&quot;,
    &quot;    {\n&quot;,
    &quot;        \&quot;id\&quot;: \&quot;my-dataset-1024\&quot;,\n&quot;,
    &quot;        \&quot;type\&quot;: \&quot;local\&quot;,\n&quot;,
    &quot;        \&quot;instance_data_dir\&quot;: instance_data_dir,\n&quot;,
    &quot;        \&quot;crop\&quot;: False,\n&quot;,
    &quot;        \&quot;crop_style\&quot;: \&quot;random\&quot;,\n&quot;,
    &quot;        \&quot;minimum_image_size\&quot;: 128,\n&quot;,
    &quot;        \&quot;resolution\&quot;: 1024,\n&quot;,
    &quot;        \&quot;resolution_type\&quot;: \&quot;pixel_area\&quot;,\n&quot;,
    &quot;        \&quot;repeats\&quot;: \&quot;4\&quot;,\n&quot;,
    &quot;        \&quot;metadata_backend\&quot;: \&quot;discovery\&quot;,\n&quot;,
    &quot;        \&quot;caption_strategy\&quot;: \&quot;filename\&quot;,\n&quot;,
    &quot;        \&quot;cache_dir_vae\&quot;: \&quot;cache/vae-1024\&quot;\n&quot;,
    &quot;    },\n&quot;,
    &quot;    {\n&quot;,
    &quot;        \&quot;id\&quot;: \&quot;my-dataset-512-crop\&quot;,\n&quot;,
    &quot;        \&quot;type\&quot;: \&quot;local\&quot;,\n&quot;,
    &quot;        \&quot;instance_data_dir\&quot;: instance_data_dir,\n&quot;,
    &quot;        \&quot;crop\&quot;: False,\n&quot;,
    &quot;        \&quot;crop_style\&quot;: \&quot;random\&quot;,\n&quot;,
    &quot;        \&quot;minimum_image_size\&quot;: 128,\n&quot;,
    &quot;        \&quot;resolution\&quot;: 512,\n&quot;,
    &quot;        \&quot;resolution_type\&quot;: \&quot;pixel_area\&quot;,\n&quot;,
    &quot;        \&quot;repeats\&quot;: \&quot;4\&quot;,\n&quot;,
    &quot;        \&quot;metadata_backend\&quot;: \&quot;discovery\&quot;,\n&quot;,
    &quot;        \&quot;caption_strategy\&quot;: \&quot;filename\&quot;,\n&quot;,
    &quot;        \&quot;cache_dir_vae\&quot;: \&quot;cache/vae-512-crop\&quot;\n&quot;,
    &quot;    },\n&quot;,
    &quot;    {\n&quot;,
    &quot;        \&quot;id\&quot;: \&quot;my-dataset-1024-crop\&quot;,\n&quot;,
    &quot;        \&quot;type\&quot;: \&quot;local\&quot;,\n&quot;,
    &quot;        \&quot;instance_data_dir\&quot;: instance_data_dir,\n&quot;,
    &quot;        \&quot;crop\&quot;: False,\n&quot;,
    &quot;        \&quot;crop_style\&quot;: \&quot;random\&quot;,\n&quot;,
    &quot;        \&quot;minimum_image_size\&quot;: 128,\n&quot;,
    &quot;        \&quot;resolution\&quot;: 1024,\n&quot;,
    &quot;        \&quot;resolution_type\&quot;: \&quot;pixel_area\&quot;,\n&quot;,
    &quot;        \&quot;repeats\&quot;: \&quot;4\&quot;,\n&quot;,
    &quot;        \&quot;metadata_backend\&quot;: \&quot;discovery\&quot;,\n&quot;,
    &quot;        \&quot;caption_strategy\&quot;: \&quot;filename\&quot;,\n&quot;,
    &quot;        \&quot;cache_dir_vae\&quot;: \&quot;cache/vae-1024-crop\&quot;\n&quot;,
    &quot;    },\n&quot;,
    &quot;    {\n&quot;,
    &quot;        \&quot;id\&quot;: \&quot;text-embed-cache\&quot;,\n&quot;,
    &quot;        \&quot;dataset_type\&quot;: \&quot;text_embeds\&quot;,\n&quot;,
    &quot;        \&quot;default\&quot;: True,\n&quot;,
    &quot;        \&quot;type\&quot;: \&quot;local\&quot;,\n&quot;,
    &quot;        \&quot;cache_dir\&quot;: \&quot;cache/text\&quot;\n&quot;,
    &quot;    }\n&quot;,
    &quot;]\n&quot;,
    &quot;# write to config/multidatabackend.json\n&quot;,
    &quot;import json\n&quot;,
    &quot;with open(\&quot;config/multidatabackend.json\&quot;, \&quot;w\&quot;) as f:\n&quot;,
    &quot;    json.dump(dataloader_config, f)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 6,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;/Users/bghira/src/SimpleTuner/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n&quot;,
      &quot;  from .autonotebook import tqdm as notebook_tqdm\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;from helpers.training.trainer import Trainer\n&quot;,
    &quot;from helpers.training.state_tracker import StateTracker\n&quot;,
    &quot;from helpers import log_format\n&quot;,
    &quot;import logging\n&quot;,
    &quot;from os import environ\n&quot;,
    &quot;\n&quot;,
    &quot;logger = logging.getLogger(\&quot;SimpleTuner\&quot;)\n&quot;,
    &quot;logger.setLevel(environ.get(\&quot;SIMPLETUNER_LOG_LEVEL\&quot;, \&quot;INFO\&quot;))&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 7,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:05,034 [WARNING] (SimpleTuner) Skipping false argument: non_ema_revision\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;from helpers.configuration.json_file import normalize_args\n&quot;,
    &quot;import os\n&quot;,
    &quot;os.environ[&apos;CONFIG_BACKEND&apos;] = &apos;cmd&apos;\n&quot;,
    &quot;os.environ[&apos;ENV&apos;] = &apos;default&apos;\n&quot;,
    &quot;StateTracker.set_config_path(&apos;config/&apos;)\n&quot;,
    &quot;loaded_config = normalize_args(training_config)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 8,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;optimizer: {&apos;precision&apos;: &apos;bf16&apos;, &apos;default_settings&apos;: {&apos;betas&apos;: (0.9, 0.999), &apos;weight_decay&apos;: 0.01, &apos;eps&apos;: 1e-06}, &apos;class&apos;: &lt;class &apos;helpers.training.optimizers.adamw_bfloat16.AdamWBF16&apos;&gt;}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:05,043 [WARNING] (ArgsParser) The VAE model madebyollin/sdxl-vae-fp16-fix is not compatible. Please use a compatible VAE to eliminate this warning. The baked-in VAE will be used, instead.\n&quot;,
      &quot;2024-08-31 20:56:05,043 [INFO] (ArgsParser) VAE Model: black-forest-labs/FLUX.1-dev\n&quot;,
      &quot;2024-08-31 20:56:05,044 [INFO] (ArgsParser) Default VAE Cache location: \n&quot;,
      &quot;2024-08-31 20:56:05,044 [INFO] (ArgsParser) Text Cache location: cache\n&quot;,
      &quot;2024-08-31 20:56:05,045 [WARNING] (ArgsParser) Updating T5 XXL tokeniser max length to 512 for Flux.\n&quot;,
      &quot;2024-08-31 20:56:05,046 [WARNING] (ArgsParser) No data backend config provided. Using default config at config/multidatabackend.json.\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;Model family: flux\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;try:\n&quot;,
    &quot;    trainer = Trainer(loaded_config)\n&quot;,
    &quot;except Exception as e:\n&quot;,
    &quot;    import traceback\n&quot;,
    &quot;    logger.error(f\&quot;Failed to create Trainer: {e}, {traceback.format_exc()}\&quot;)\n&quot;,
    &quot;    raise e&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 9,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:05,554 [INFO] (helpers.training.trainer) Logged into Hugging Face Hub as &apos;bghira&apos;\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;try:\n&quot;,
    &quot;    trainer.configure_webhook()\n&quot;,
    &quot;    trainer.init_noise_schedule()\n&quot;,
    &quot;    trainer.init_seed()\n&quot;,
    &quot;\n&quot;,
    &quot;    trainer.init_huggingface_hub()\n&quot;,
    &quot;except Exception as e:\n&quot;,
    &quot;    logger.error(f\&quot;Failed to configure Trainer: {e}\&quot;)\n&quot;,
    &quot;    raise e&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 10,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:05,559 [INFO] (helpers.training.trainer) Load VAE: black-forest-labs/FLUX.1-dev\n&quot;,
      &quot;2024-08-31 20:56:05,843 [INFO] (helpers.training.trainer) Loading VAE onto accelerator, converting from torch.float32 to torch.bfloat16\n&quot;,
      &quot;2024-08-31 20:56:05,952 [INFO] (helpers.training.trainer) Load tokenizers\n&quot;,
      &quot;You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n&quot;,
      &quot;2024-08-31 20:56:06,440 [INFO] (helpers.training.text_encoding) Loading OpenAI CLIP-L text encoder from black-forest-labs/FLUX.1-dev/text_encoder..\n&quot;,
      &quot;2024-08-31 20:56:06,683 [INFO] (helpers.training.text_encoding) Loading T5 XXL v1.1 text encoder from black-forest-labs/FLUX.1-dev/text_encoder_2..\n&quot;,
      &quot;Downloading shards: 100%|██████████| 2/2 [00:00&lt;00:00, 2430.07it/s]\n&quot;,
      &quot;Loading checkpoint shards: 100%|██████████| 2/2 [00:00&lt;00:00,  9.30it/s]\n&quot;,
      &quot;2024-08-31 20:56:08,494 [INFO] (helpers.training.trainer) Moving text encoder to GPU.\n&quot;,
      &quot;2024-08-31 20:56:08,736 [INFO] (helpers.training.trainer) Moving text encoder 2 to GPU.\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;try:\n&quot;,
    &quot;    trainer.init_preprocessing_models()\n&quot;,
    &quot;except Exception as e:\n&quot;,
    &quot;    logger.error(f\&quot;Failed to initialize preprocessing models: {e}\&quot;)\n&quot;,
    &quot;    raise e&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 11,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:14,161 [INFO] (DataBackendFactory) Loading data backend config from config/multidatabackend.json\n&quot;,
      &quot;2024-08-31 20:56:14,162 [INFO] (DataBackendFactory) Configuring text embed backend: text-embed-cache\n&quot;,
      &quot;Loading pipeline components...: 100%|██████████| 5/5 [00:00&lt;00:00, 1132.49it/s]\n&quot;,
      &quot;2024-08-31 20:56:14,337 [INFO] (TextEmbeddingCache) (Rank: 0) (id=text-embed-cache) Listing all text embed cache entries\n&quot;,
      &quot;2024-08-31 20:56:14,339 [INFO] (DataBackendFactory) Pre-computing null embedding\n&quot;,
      &quot;2024-08-31 20:56:19,345 [INFO] (DataBackendFactory) Completed loading text embed services.\n&quot;,
      &quot;2024-08-31 20:56:19,347 [INFO] (DataBackendFactory) Configuring data backend: my-dataset-512\n&quot;,
      &quot;2024-08-31 20:56:19,351 [INFO] (DataBackendFactory) (id=my-dataset-512) Loading bucket manager.\n&quot;,
      &quot;2024-08-31 20:56:19,352 [INFO] (DiscoveryMetadataBackend) Checking for cache file: /Volumes/ml/datasets/test_datasets/single_image_dataset/aspect_ratio_bucket_indices_my-dataset-512.json\n&quot;,
      &quot;2024-08-31 20:56:19,353 [INFO] (DiscoveryMetadataBackend) Pulling cache file from storage\n&quot;,
      &quot;2024-08-31 20:56:19,354 [INFO] (DataBackendFactory) (id=my-dataset-512) Refreshing aspect buckets on main process.\n&quot;,
      &quot;2024-08-31 20:56:19,355 [INFO] (BaseMetadataBackend) Discovering new files...\n&quot;,
      &quot;2024-08-31 20:56:19,358 [INFO] (BaseMetadataBackend) Compressed 11 existing files from 2.\n&quot;,
      &quot;2024-08-31 20:56:19,359 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.\n&quot;,
      &quot;2024-08-31 20:56:19,360 [INFO] (BaseMetadataBackend) Statistics: {&apos;total_processed&apos;: 0, &apos;skipped&apos;: {&apos;already_exists&apos;: 11, &apos;metadata_missing&apos;: 0, &apos;not_found&apos;: 0, &apos;too_small&apos;: 0, &apos;other&apos;: 0}}\n&quot;,
      &quot;2024-08-31 20:56:19,360 [WARNING] (DataBackendFactory) Key crop_aspect not found in the current backend config, using the existing value &apos;square&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,361 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value &apos;False&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,362 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value &apos;1&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,362 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value &apos;True&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,363 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-512&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 0.262144, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 1, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x368b5b1f0&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x368b5a470&gt;}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;(Rank: 0)  | Bucket     | Image Count (per-GPU)\n&quot;,
      &quot;------------------------------\n&quot;,
      &quot;(Rank: 0)  | 1.0        | 10          \n&quot;,
      &quot;(Rank: 0)  | 0.7        | 1           \n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:19,365 [INFO] (DataBackendFactory) (id=my-dataset-512) Collecting captions.\n&quot;,
      &quot;2024-08-31 20:56:19,367 [INFO] (DataBackendFactory) (id=my-dataset-512) Initialise text embed pre-computation using the filename caption strategy. We have 11 captions to process.\n&quot;,
      &quot;2024-08-31 20:56:19,368 [INFO] (DataBackendFactory) (id=my-dataset-512) Completed processing 11 captions.\n&quot;,
      &quot;2024-08-31 20:56:19,369 [INFO] (DataBackendFactory) (id=my-dataset-512) Creating VAE latent cache.\n&quot;,
      &quot;2024-08-31 20:56:19,370 [INFO] (DataBackendFactory) (id=my-dataset-512) Discovering cache objects..\n&quot;,
      &quot;2024-08-31 20:56:19,371 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-512&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 0.262144, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 1, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x368b5b1f0&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x368b5a470&gt;, &apos;train_dataset&apos;: &lt;helpers.multiaspect.dataset.MultiAspectDataset object at 0x39a2abb20&gt;, &apos;sampler&apos;: &lt;helpers.multiaspect.sampler.MultiAspectSampler object at 0x39a2ab910&gt;, &apos;train_dataloader&apos;: &lt;torch.utils.data.dataloader.DataLoader object at 0x39a2abac0&gt;, &apos;text_embed_cache&apos;: &lt;helpers.caching.text_embeds.TextEmbeddingCache object at 0x368b5a110&gt;, &apos;vaecache&apos;: &lt;helpers.caching.vae.VAECache object at 0x39a1eec20&gt;}\n&quot;,
      &quot;2024-08-31 20:56:19,372 [INFO] (DataBackendFactory) Configuring data backend: my-dataset-1024\n&quot;,
      &quot;2024-08-31 20:56:19,374 [INFO] (DataBackendFactory) (id=my-dataset-1024) Loading bucket manager.\n&quot;,
      &quot;2024-08-31 20:56:19,374 [INFO] (DiscoveryMetadataBackend) Checking for cache file: /Volumes/ml/datasets/test_datasets/single_image_dataset/aspect_ratio_bucket_indices_my-dataset-1024.json\n&quot;,
      &quot;2024-08-31 20:56:19,374 [INFO] (DiscoveryMetadataBackend) Pulling cache file from storage\n&quot;,
      &quot;2024-08-31 20:56:19,375 [INFO] (DataBackendFactory) (id=my-dataset-1024) Refreshing aspect buckets on main process.\n&quot;,
      &quot;2024-08-31 20:56:19,376 [INFO] (BaseMetadataBackend) Discovering new files...\n&quot;,
      &quot;2024-08-31 20:56:19,378 [INFO] (BaseMetadataBackend) Compressed 11 existing files from 2.\n&quot;,
      &quot;2024-08-31 20:56:19,379 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.\n&quot;,
      &quot;2024-08-31 20:56:19,379 [INFO] (BaseMetadataBackend) Statistics: {&apos;total_processed&apos;: 0, &apos;skipped&apos;: {&apos;already_exists&apos;: 11, &apos;metadata_missing&apos;: 0, &apos;not_found&apos;: 0, &apos;too_small&apos;: 0, &apos;other&apos;: 0}}\n&quot;,
      &quot;2024-08-31 20:56:19,380 [WARNING] (DataBackendFactory) Key crop_aspect not found in the current backend config, using the existing value &apos;square&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,380 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value &apos;False&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,381 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value &apos;2&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,381 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value &apos;True&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,382 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-1024&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 1.048576, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2b8c70&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b8eb0&gt;}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;(Rank: 0)  | Bucket     | Image Count (per-GPU)\n&quot;,
      &quot;------------------------------\n&quot;,
      &quot;(Rank: 0)  | 1.0        | 10          \n&quot;,
      &quot;(Rank: 0)  | 0.65       | 1           \n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:19,384 [INFO] (DataBackendFactory) (id=my-dataset-1024) Collecting captions.\n&quot;,
      &quot;2024-08-31 20:56:19,385 [INFO] (DataBackendFactory) (id=my-dataset-1024) Initialise text embed pre-computation using the filename caption strategy. We have 11 captions to process.\n&quot;,
      &quot;2024-08-31 20:56:19,386 [INFO] (DataBackendFactory) (id=my-dataset-1024) Completed processing 11 captions.\n&quot;,
      &quot;2024-08-31 20:56:19,386 [INFO] (DataBackendFactory) (id=my-dataset-1024) Creating VAE latent cache.\n&quot;,
      &quot;2024-08-31 20:56:19,387 [INFO] (DataBackendFactory) (id=my-dataset-1024) Discovering cache objects..\n&quot;,
      &quot;2024-08-31 20:56:19,389 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-1024&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 1.048576, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2b8c70&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b8eb0&gt;, &apos;train_dataset&apos;: &lt;helpers.multiaspect.dataset.MultiAspectDataset object at 0x39a2aba00&gt;, &apos;sampler&apos;: &lt;helpers.multiaspect.sampler.MultiAspectSampler object at 0x39a2b8850&gt;, &apos;train_dataloader&apos;: &lt;torch.utils.data.dataloader.DataLoader object at 0x39a2b89d0&gt;, &apos;text_embed_cache&apos;: &lt;helpers.caching.text_embeds.TextEmbeddingCache object at 0x368b5a110&gt;, &apos;vaecache&apos;: &lt;helpers.caching.vae.VAECache object at 0x39a2b8d30&gt;}\n&quot;,
      &quot;2024-08-31 20:56:19,389 [INFO] (DataBackendFactory) Configuring data backend: my-dataset-512-crop\n&quot;,
      &quot;2024-08-31 20:56:19,390 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Loading bucket manager.\n&quot;,
      &quot;2024-08-31 20:56:19,391 [INFO] (DiscoveryMetadataBackend) Checking for cache file: /Volumes/ml/datasets/test_datasets/single_image_dataset/aspect_ratio_bucket_indices_my-dataset-512-crop.json\n&quot;,
      &quot;2024-08-31 20:56:19,391 [INFO] (DiscoveryMetadataBackend) Pulling cache file from storage\n&quot;,
      &quot;2024-08-31 20:56:19,392 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Refreshing aspect buckets on main process.\n&quot;,
      &quot;2024-08-31 20:56:19,392 [INFO] (BaseMetadataBackend) Discovering new files...\n&quot;,
      &quot;2024-08-31 20:56:19,395 [INFO] (BaseMetadataBackend) Compressed 11 existing files from 2.\n&quot;,
      &quot;2024-08-31 20:56:19,396 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.\n&quot;,
      &quot;2024-08-31 20:56:19,396 [INFO] (BaseMetadataBackend) Statistics: {&apos;total_processed&apos;: 0, &apos;skipped&apos;: {&apos;already_exists&apos;: 11, &apos;metadata_missing&apos;: 0, &apos;not_found&apos;: 0, &apos;too_small&apos;: 0, &apos;other&apos;: 0}}\n&quot;,
      &quot;2024-08-31 20:56:19,397 [WARNING] (DataBackendFactory) Key crop_aspect not found in the current backend config, using the existing value &apos;square&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,397 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value &apos;False&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,398 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value &apos;2&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,398 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value &apos;True&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,399 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-512-crop&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 0.262144, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2b9900&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b9390&gt;}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;(Rank: 0)  | Bucket     | Image Count (per-GPU)\n&quot;,
      &quot;------------------------------\n&quot;,
      &quot;(Rank: 0)  | 1.0        | 10          \n&quot;,
      &quot;(Rank: 0)  | 0.7        | 1           \n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:19,401 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Collecting captions.\n&quot;,
      &quot;2024-08-31 20:56:19,402 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Initialise text embed pre-computation using the filename caption strategy. We have 11 captions to process.\n&quot;,
      &quot;2024-08-31 20:56:19,403 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Completed processing 11 captions.\n&quot;,
      &quot;2024-08-31 20:56:19,404 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Creating VAE latent cache.\n&quot;,
      &quot;2024-08-31 20:56:19,404 [INFO] (DataBackendFactory) (id=my-dataset-512-crop) Discovering cache objects..\n&quot;,
      &quot;2024-08-31 20:56:19,406 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-512-crop&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 0.262144, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2b9900&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b9390&gt;, &apos;train_dataset&apos;: &lt;helpers.multiaspect.dataset.MultiAspectDataset object at 0x39a2b8100&gt;, &apos;sampler&apos;: &lt;helpers.multiaspect.sampler.MultiAspectSampler object at 0x39a2b9a50&gt;, &apos;train_dataloader&apos;: &lt;torch.utils.data.dataloader.DataLoader object at 0x39a2b9ea0&gt;, &apos;text_embed_cache&apos;: &lt;helpers.caching.text_embeds.TextEmbeddingCache object at 0x368b5a110&gt;, &apos;vaecache&apos;: &lt;helpers.caching.vae.VAECache object at 0x39a2b8b50&gt;}\n&quot;,
      &quot;2024-08-31 20:56:19,407 [INFO] (DataBackendFactory) Configuring data backend: my-dataset-1024-crop\n&quot;,
      &quot;2024-08-31 20:56:19,408 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Loading bucket manager.\n&quot;,
      &quot;2024-08-31 20:56:19,408 [INFO] (DiscoveryMetadataBackend) Checking for cache file: /Volumes/ml/datasets/test_datasets/single_image_dataset/aspect_ratio_bucket_indices_my-dataset-1024-crop.json\n&quot;,
      &quot;2024-08-31 20:56:19,409 [INFO] (DiscoveryMetadataBackend) Pulling cache file from storage\n&quot;,
      &quot;2024-08-31 20:56:19,409 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Refreshing aspect buckets on main process.\n&quot;,
      &quot;2024-08-31 20:56:19,409 [INFO] (BaseMetadataBackend) Discovering new files...\n&quot;,
      &quot;2024-08-31 20:56:19,412 [INFO] (BaseMetadataBackend) Compressed 11 existing files from 2.\n&quot;,
      &quot;2024-08-31 20:56:19,413 [INFO] (BaseMetadataBackend) No new files discovered. Doing nothing.\n&quot;,
      &quot;2024-08-31 20:56:19,414 [INFO] (BaseMetadataBackend) Statistics: {&apos;total_processed&apos;: 0, &apos;skipped&apos;: {&apos;already_exists&apos;: 11, &apos;metadata_missing&apos;: 0, &apos;not_found&apos;: 0, &apos;too_small&apos;: 0, &apos;other&apos;: 0}}\n&quot;,
      &quot;2024-08-31 20:56:19,414 [WARNING] (DataBackendFactory) Key crop_aspect not found in the current backend config, using the existing value &apos;square&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,415 [WARNING] (DataBackendFactory) Key disable_validation not found in the current backend config, using the existing value &apos;False&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,415 [WARNING] (DataBackendFactory) Key config_version not found in the current backend config, using the existing value &apos;2&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,415 [WARNING] (DataBackendFactory) Key hash_filenames not found in the current backend config, using the existing value &apos;True&apos;.\n&quot;,
      &quot;2024-08-31 20:56:19,416 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-1024-crop&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 1.048576, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2ba140&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b9750&gt;}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;(Rank: 0)  | Bucket     | Image Count (per-GPU)\n&quot;,
      &quot;------------------------------\n&quot;,
      &quot;(Rank: 0)  | 1.0        | 10          \n&quot;,
      &quot;(Rank: 0)  | 0.65       | 1           \n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:19,418 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Collecting captions.\n&quot;,
      &quot;2024-08-31 20:56:19,419 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Initialise text embed pre-computation using the filename caption strategy. We have 11 captions to process.\n&quot;,
      &quot;2024-08-31 20:56:19,420 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Completed processing 11 captions.\n&quot;,
      &quot;2024-08-31 20:56:19,421 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Creating VAE latent cache.\n&quot;,
      &quot;2024-08-31 20:56:19,421 [INFO] (DataBackendFactory) (id=my-dataset-1024-crop) Discovering cache objects..\n&quot;,
      &quot;2024-08-31 20:56:19,423 [INFO] (DataBackendFactory) Configured backend: {&apos;id&apos;: &apos;my-dataset-1024-crop&apos;, &apos;config&apos;: {&apos;repeats&apos;: &apos;4&apos;, &apos;crop&apos;: False, &apos;crop_aspect&apos;: &apos;square&apos;, &apos;crop_style&apos;: &apos;random&apos;, &apos;disable_validation&apos;: False, &apos;resolution&apos;: 1.048576, &apos;resolution_type&apos;: &apos;area&apos;, &apos;caption_strategy&apos;: &apos;filename&apos;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;maximum_image_size&apos;: None, &apos;target_downsample_size&apos;: None, &apos;config_version&apos;: 2, &apos;hash_filenames&apos;: True}, &apos;dataset_type&apos;: &apos;image&apos;, &apos;data_backend&apos;: &lt;helpers.data_backend.local.LocalDataBackend object at 0x39a2ba140&gt;, &apos;instance_data_dir&apos;: &apos;/Volumes/ml/datasets/test_datasets/single_image_dataset&apos;, &apos;metadata_backend&apos;: &lt;helpers.metadata.backends.discovery.DiscoveryMetadataBackend object at 0x39a2b9750&gt;, &apos;train_dataset&apos;: &lt;helpers.multiaspect.dataset.MultiAspectDataset object at 0x39a2b9b40&gt;, &apos;sampler&apos;: &lt;helpers.multiaspect.sampler.MultiAspectSampler object at 0x39a2b9cc0&gt;, &apos;train_dataloader&apos;: &lt;torch.utils.data.dataloader.DataLoader object at 0x39a2b9c30&gt;, &apos;text_embed_cache&apos;: &lt;helpers.caching.text_embeds.TextEmbeddingCache object at 0x368b5a110&gt;, &apos;vaecache&apos;: &lt;helpers.caching.vae.VAECache object at 0x39a2ba590&gt;}\n&quot;,
      &quot;2024-08-31 20:56:20,265 [INFO] (validation) Precomputing the negative prompt embed for validations.\n&quot;,
      &quot;2024-08-31 20:56:20,810 [INFO] (helpers.training.trainer) Calculated our maximum training steps at 10000 because we have 46 epochs and 220 steps per epoch.\n&quot;,
      &quot;2024-08-31 20:56:20,811 [INFO] (helpers.training.trainer) Collected the following data backends: [&apos;text-embed-cache&apos;, &apos;my-dataset-512&apos;, &apos;my-dataset-1024&apos;, &apos;my-dataset-512-crop&apos;, &apos;my-dataset-1024-crop&apos;]\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;try:\n&quot;,
    &quot;    trainer.init_data_backend()\n&quot;,
    &quot;except Exception as e:\n&quot;,
    &quot;    logger.error(f\&quot;Failed to initialize data backend: {e}\&quot;)\n&quot;,
    &quot;    raise e&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 12,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:21,375 [INFO] (validation) Precomputing the negative prompt embed for validations.\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;try:\n&quot;,
    &quot;    trainer.init_validation_prompts()\n&quot;,
    &quot;except Exception as e:\n&quot;,
    &quot;    logger.error(f\&quot;Failed to initialize validation prompts: {e}\&quot;)\n&quot;,
    &quot;    raise e&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 13,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:21,936 [INFO] (helpers.training.trainer) Unloading text encoders, as they are not being trained.\n&quot;,
      &quot;2024-08-31 20:56:22,832 [INFO] (helpers.training.trainer) After nuking text encoders from orbit, we freed 9.1 GB of VRAM. The real memories were the friends we trained a model on along the way.\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.init_unload_text_encoder()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 14,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:22,994 [INFO] (helpers.training.trainer) After nuking the VAE from orbit, we freed 163.84 MB of VRAM.\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.init_unload_vae()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 15,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;Fetching 3 files: 100%|██████████| 3/3 [00:00&lt;00:00, 64198.53it/s]\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.init_load_base_model()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 16,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_precision()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 17,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_controlnet_model()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 18,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_freeze_models()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 19,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:24,559 [INFO] (helpers.training.trainer) Using lycoris training mode\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:24|[LyCORIS]-INFO: Using rank adaptation algo: lokr\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-INFO: Use Dropout value: 0.0\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-INFO: Create LyCORIS Module\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-WARNING: lora_dim 10000 is too large for dim=3072 and factor=12, using full matrix mode.\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-WARNING: lora_dim 10000 is too large for dim=12288 and factor=6, using full matrix mode.\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-INFO: create LyCORIS: 342 modules.\n&quot;,
      &quot;2024-08-31 20:56:24|[LyCORIS]-INFO: module type table: {&apos;LokrModule&apos;: 342}\n&quot;
     ]
    },
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:24,657 [INFO] (helpers.training.trainer) LyCORIS network has been initialized with 97,165,392 parameters\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.init_trainable_peft_adapter()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 20,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_ema_model()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.move_models(destination=\&quot;accelerator\&quot;)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 21,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_validations()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 22,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.init_benchmark_base_model()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 23,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:24,674 [INFO] (helpers.training.trainer) Learning rate: 0.0001\n&quot;,
      &quot;2024-08-31 20:56:24,676 [INFO] (helpers.training.optimizer_param) cls: &lt;class &apos;helpers.training.optimizers.adamw_bfloat16.AdamWBF16&apos;&gt;, settings: {&apos;betas&apos;: (0.9, 0.999), &apos;weight_decay&apos;: 0.01, &apos;eps&apos;: 1e-06}\n&quot;,
      &quot;2024-08-31 20:56:24,679 [INFO] (helpers.training.trainer) Optimizer arguments, weight_decay=0.01 eps=1e-08, extra_arguments={&apos;lr&apos;: 0.0001, &apos;betas&apos;: (0.9, 0.999), &apos;weight_decay&apos;: 0.01, &apos;eps&apos;: 1e-06}\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.resume_and_prepare()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 29,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stderr&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;2024-08-31 20:56:32,220 [ERROR] (wandb.jupyter) Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n&quot;,
      &quot;huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n&quot;,
      &quot;To disable this warning, you can either:\n&quot;,
      &quot;\t- Avoid using `tokenizers` before the fork if possible\n&quot;,
      &quot;\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n&quot;,
      &quot;huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n&quot;,
      &quot;To disable this warning, you can either:\n&quot;,
      &quot;\t- Avoid using `tokenizers` before the fork if possible\n&quot;,
      &quot;\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;trainer.init_trackers()&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: null,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;trainer.train()&quot;
   ]
  }
 ],
 &quot;metadata&quot;: {
  &quot;kernelspec&quot;: {
   &quot;display_name&quot;: &quot;.venv&quot;,
   &quot;language&quot;: &quot;python&quot;,
   &quot;name&quot;: &quot;python3&quot;
  },
  &quot;language_info&quot;: {
   &quot;codemirror_mode&quot;: {
    &quot;name&quot;: &quot;ipython&quot;,
    &quot;version&quot;: 3
   },
   &quot;file_extension&quot;: &quot;.py&quot;,
   &quot;mimetype&quot;: &quot;text/x-python&quot;,
   &quot;name&quot;: &quot;python&quot;,
   &quot;nbconvert_exporter&quot;: &quot;python&quot;,
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
   &quot;version&quot;: &quot;3.10.14&quot;
  }
 },
 &quot;nbformat&quot;: 4,
 &quot;nbformat_minor&quot;: 2
}</file><file path="OPTIONS.md"># SimpleTuner Training Script Options

## Overview

This guide provides a user-friendly breakdown of the command-line options available in SimpleTuner&apos;s `train.py` script. These options offer a high degree of customization, allowing you to train your model to suit your specific requirements.

### JSON Configuration file format

The JSON filename expected is `config.json` and the key names are the same as the below `--arguments`. The leading `--` is not required for the JSON file, but it can be left in as well.

### Easy configure script (***RECOMMENDED***)

The script `configure.py` in the project root can be used via `python configure.py` to set up a `config.json` file with mostly-ideal default settings.

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.

---

## 🌟 Core Model Configuration

### `--model_type`

- **What**: Select whether a LoRA or full fine-tune are created.
- **Choices**: lora, full.
- **Default**: lora
  - If lora is used, `--lora_type` dictates whether PEFT or LyCORIS are in use. Some models (PixArt) work only with LyCORIS adapters.

### `--model_family`

- **What**: Determines which model architecture is being trained.
- **Choices**: pixart_sigma, flux, sd3, sdxl, kolors, legacy

### `--pretrained_model_name_or_path`

- **What**: Path to the pretrained model or its identifier from https://huggingface.co/models.
- **Why**: To specify the base model you&apos;ll start training from. Use `--revision` and `--variant` to specify specific versions from a repository. This also supports single-file `.safetensors` paths for SDXL, Flux, and SD3.x.

### `--pretrained_t5_model_name_or_path`

- **What**: Path to the pretrained T5 model or its identifier from https://huggingface.co/models.
- **Why**: When training PixArt, you might want to use a specific source for your T5 weights so that you can avoid downloading them multiple times when switching the base model you train from.

### `--gradient_checkpointing`

- **What**: During training, gradients will be calculated layerwise and accumulated to save on peak VRAM requirements at the cost of slower training.

### `--gradient_checkpointing_interval`

- **What**: Checkpoint only every _n_ blocks, where _n_ is a value greater than zero. A value of 1 is effectively the same as just leaving `--gradient_checkpointing` enabled, and a value of 2 will checkpoint every other block.
- **Note**: SDXL and Flux are currently the only models supporting this option. SDXL uses a hackish implementation.

### `--refiner_training`

- **What**: Enables training a custom mixture-of-experts model series. See [Mixture-of-Experts](/documentation/MIXTURE_OF_EXPERTS.md) for more information on these options.

## Precision

### `--quantize_via`

- **Choices**: `cpu`, `accelerator`
  - On `accelerator`, it may work moderately faster at the risk of possibly OOM&apos;ing on 24G cards for a model as large as Flux.
  - On `cpu`, quantisation takes about 30 seconds. (**Default**)


### `--base_model_precision`

- **What**: Reduce model precision and train using less memory. There are currently two supported quantisation backends: quanto and torchao.

#### Optimum Quanto

Provided by Hugging Face, the optimum-quanto library has robust support across all supported platforms.

- `int8-quanto` is the most broadly compatible and probably produces the best results
  - fastest training for RTX4090 and probably other GPUs
  - uses hardware-accelerated matmul on CUDA devices for int8, int4
    - int4 is still abysmally slow
  - works with `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`)
- `fp8uz-quanto` is an experimental fp8 variant for CUDA and ROCm devices.
  - better-supported on AMD silicon such as Instinct or newer architecture
  - can be slightly faster than `int8-quanto` on a 4090 for training, but not inference (1 second slower)
  - works with `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`)
- `fp8-quanto` will not (currently) use fp8 matmul, does not work on Apple systems.
  - does not have hardware fp8 matmul yet on CUDA or ROCm devices, so it will possibly be noticeably slower than int8
    - uses MARLIN kernel for fp8 GEMM
  - incompatible with dynamo, will automatically disable dynamo if the combination is attempted.
  
#### TorchAO

A newer library from Pytorch, AO allows us to replace the linears and 2D convolutions (eg. unet style models) with quantised counterparts.
&lt;!-- Additionally, it provides an experimental CPU offload optimiser that essentially provides a simpler reimplementation of DeepSpeed. --&gt;

- `int8-torchao` will reduce memory consumption to the same level as any of Quanto&apos;s precision levels
  - at the time of writing, runs slightly slower (11s/iter) than Quanto does (9s/iter) on Apple MPS
  - When not using `torch.compile`, same speed and memory use as `int8-quanto` on CUDA devices, unknown speed profile on ROCm
  - When using `torch.compile`, slower than `int8-quanto`
- `fp8-torchao` is only available for Hopper (H100, H200) or newer (Blackwell B200) accelerators

##### Optimisers

TorchAO includes generally-available 4bit and 8bit optimisers: `ao-adamw8bit`, `ao-adamw4bit`

It also provides two optimisers that are directed toward Hopper (H100 or better) users: `ao-adamfp8`, and `ao-adamwfp8`

#### Torch Dynamo

To enable `torch.compile()`, add the following line to `config/config.env`:
```bash
TRAINING_DYNAMO_BACKEND=inductor
```

If you wish to use added features like max-autotune, run the following:

```bash
accelerate config
```

Carefully answer the questions and use bf16 mixed precision training when prompted. Say **yes** to using Dynamo, **no** to fullgraph, and **yes** to max-autotune.

Note that the first several steps of training will be slower than usual because of compilation occuring in the background.

### `--attention_mechanism`

Alternative attention mechanisms are supported, with varying levels of compatibility or other trade-offs;

- `diffusers` uses the native Pytorch SDPA functions and is the default attention mechanism
- `xformers` allows the use of Meta&apos;s [xformers](https://github.com/facebook/xformers) attention implementation which supports both training and inference fully
- `sageattention` is an inference-focused attention mechanism which does not fully support being used for training ([SageAttention](https://github.com/thu-ml/SageAttention) project page)
  - In simplest terms, SageAttention reduces compute requirement for inference

Using `--sageattention_usage` to enable training with SageAttention should be enabled with care, as it does not track or propagate gradients from its custom CUDA implementations for the QKV linears.
  - This results in these layers being completely untrained, which might cause model collapse or, slight improvements in short training runs.

---

## 📰 Publishing

### `--push_to_hub`

- **What**: If provided, your model will be uploaded to [Huggingface Hub](https://huggingface.co) once training completes. Using `--push_checkpoints_to_hub` will additionally push every intermediary checkpoint.

### `--hub_model_id`

- **What**: The name of the Huggingface Hub model and local results directory.
- **Why**: This value is used as the directory name under the location specified as `--output_dir`. If `--push_to_hub` is provided, this will become the name of the model on Huggingface Hub.


### `--disable_benchmark`

- **What**: Disable the startup validation/benchmark that occurs at step 0 on the base model. These outputs are stitchd to the left side of your trained model validation images.

## 📂 Data Storage and Management

### `--data_backend_config`

- **What**: Path to your SimpleTuner dataset configuration.
- **Why**: Multiple datasets on different storage medium may be combined into a single training session.
- **Example**: See (multidatabackend.json.example)[/multidatabackend.json.example] for an example configuration, and [this document](/documentation/DATALOADER.md) for more information on configuring the data loader.

### `--override_dataset_config`

- **What**: When provided, will allow SimpleTuner to ignore differences between the cached config inside the dataset and the current values.
- **Why**: When SimplerTuner is run for the first time on a dataset, it will create a cache document containing information about everything in that dataset. This includes the dataset config, including its &quot;crop&quot; and &quot;resolution&quot; related configuration values. Changing these arbitrarily or by accident could result in your training jobs crashing randomly, so it&apos;s highly recommended to not use this parameter, and instead resolve the differences you&apos;d like to apply in your dataset some other way.

### `--data_backend_sampling`

- **What**: When using multiple data backends, sampling can be done using different strategies.
- **Options**:
  - `uniform` - the previous behaviour from v0.9.8.1 and earlier where dataset length was not considered, only manual probability weightings.
  - `auto-weighting` - the default behaviour where dataset length is used to equally sample all datasets, maintaining a uniform sampling of the entire data distribution.
    - This is required if you have differently-sized datasets that you want the model to learn equally.
    - But adjusting `repeats` manually is **required** to properly sample Dreambooth images against your regularisation set

### `--vae_cache_scan_behaviour`

- **What**: Configure the behaviour of the integrity scan check.
- **Why**: A dataset could have incorrect settings applied at multiple points of training, eg. if you accidentally delete the `.json` cache files from your dataset and switch the data backend config to use square images rather than aspect-crops. This will result in an inconsistent data cache, which can be corrected by setting `scan_for_errors` to `true` in your `multidatabackend.json` configuration file. When this scan runs, it relies on the setting of `--vae_cache_scan_behaviour` to determine how to resolve the inconsistency: `recreate` (the default) will remove the offending cache entry so that it can be recreated, and `sync` will update the bucket metadata to reflect the reality of the real training sample. Recommended value: `recreate`.

### `--dataloader_prefetch`

- **What**: Retrieve batches ahead-of-time.
- **Why**: Especially when using large batch sizes, training will &quot;pause&quot; while samples are retrieved from disk (even NVMe), impacting GPU utilisation metrics. Enabling dataloader prefetch will keep a buffer full of entire batches, so that they can be loaded instantly.

&gt; ⚠️ This is really only relevant for H100 or better at a low resolution where I/O becomes the bottleneck. For most other use cases, it is an unnecessary complexity.

### `--dataloader_prefetch_qlen`

- **What**: Increase or reduce the number of batches held in memory.
- **Why**: When using dataloader prefetch, a default of 10 entries are kept in memory per GPU/process. This may be too much or too little. This value can be adjusted to increase the number of batches prepared in advance.

### `--compress_disk_cache`

- **What**: Compress the VAE and text embed caches on-disk.
- **Why**: The T5 encoder used by DeepFloyd, SD3, and PixArt, produces very-large text embeds that end up being mostly empty space for shorter or redundant captions. Enabling `--compress_disk_cache` can reduce space consumed by up to 75%, with average savings of 40%.

&gt; ⚠️ You will need to manually remove the existing cache directories so they can be recreated with compression by the trainer.

---

## 🌈 Image and Text Processing

A lot of settings are instead set through the [dataloader config](/documentation/DATALOADER.md), but these will apply globally.

### `--resolution_type`

- **What**: This tells SimpleTuner whether to use `area` size calculations or `pixel` edge calculations. A hybrid approach of `pixel_area` is also supported, which allows using pixel instead of megapixel for `area` measurements.
- **Options**: 
  - `resolution_type=pixel_area`
    - A `resolution` value of 1024 will be internally mapped to an accurate area measurement for efficient aspect bucketing.
    - Example resulting sizes for `1024`: 1024x1024, 1216x832, 832x1216
  - `resolution_type=pixel`
    - All images in the dataset will have their smaller edge resized to this resolution for training, which could result in a lot of VRAM use due to the size of the resulting images.
    - Example resulting sizes for `1024`: 1024x1024, 1766x1024, 1024x1766
  - `resolution_type=area`
    - **Deprecated**. Use `pixel_area` instead.

### `--resolution`

- **What**: Input image resolution expressed in pixel edge length
- **Default**: 1024
- **Note**: This is the global default, if a dataset does not have a resolution set.

### `--validation_resolution`

- **What**: Output image resolution, measured in pixels, or, formatted as: `widthxheight`, as in `1024x1024`. Multiple resolutions can be defined, separated by commas.
- **Why**: All images generated during validation will be this resolution. Useful if the model is being trained with a different resolution.

### `--evaluation_type`

- **What**: Enable CLIP evaluation of generated images during validations.
- **Why**: CLIP scores calculate the distance of the generated image features to the provided validation prompt. This can give an idea of whether prompt adherence is improving, though it requires a large number of validation prompts to have any meaningful value.
- **Options**: &quot;none&quot; or &quot;clip&quot;

### `--crop`

- **What**: When `--crop=true` is supplied, SimpleTuner will crop all (new) images in the training dataset. It will not re-process old images.
- **Why**: Training on cropped images seems to result in better fine detail learning, especially on SDXL models.

### `--crop_style`

- **What**: When `--crop=true`, the trainer may be instructed to crop in different ways.
- **Why**: The `crop_style` option can be set to `center` (or `centre`) for a classic centre-crop, `corner` to elect for the lowest-right corner, `face` to detect and centre upon the largest subject face, and `random` for a random image slice. Default: random.

### `--crop_aspect`

- **What**: When using `--crop=true`, the `--crop_aspect` option may be supplied with a value of `square` or `preserve`.
- **Options**: If cropping is enabled, default behaviour is to crop all images to a square aspect ratio.
  - `crop_aspect=preserve` will crop images to a size matching their original aspect ratio.
  - `crop_aspect=closest` will use the closest value from `crop_aspect_buckets`
  - `crop_aspect=random` will use a random aspect value from `crop_aspect_buckets` without going too far - it will use square crops if your aspects are incompatible
  - `crop_aspect=square` will use the standard square crop style

### `--caption_strategy`

- **What**: Strategy for deriving image captions. **Choices**: `textfile`, `filename`, `parquet`, `instanceprompt`
- **Why**: Determines how captions are generated for training images.
  - `textfile` will use the contents of a `.txt` file with the same filename as the image
  - `filename` will apply some cleanup to the filename before using it as the caption.
  - `parquet` requires a parquet file to be present in the dataset, and will use the `caption` column as the caption unless `parquet_caption_column` is provided. All captions must be present unless a `parquet_fallback_caption_column` is provided.
  - `instanceprompt` will use the value for `instance_prompt` in the dataset config as the prompt for every image in the dataset.

---

## 🎛 Training Parameters

### `--num_train_epochs`

- **What**: Number of training epochs (the number of times that all images are seen). Setting this to 0 will allow `--max_train_steps` to take precedence.
- **Why**: Determines the number of image repeats, which impacts the duration of the training process. More epochs tends to result in overfitting, but might be required to pick up the concepts you wish to train in. A reasonable value might be from 5 to 50.

### `--max_train_steps`

- **What**: Number of training steps to exit training after. If set to 0, will allow `--num_train_epochs` to take priority.
- **Why**: Useful for shortening the length of training.

### `--ignore_final_epochs`

- **What**: Ignore the final counted epochs in favour of `--max_train_steps`.
- **Why**: When changing the dataloader length, training may end earlier than you want because the epoch calculation changes. This option will ignore the final epochs and instead continue to train until `--max_train_steps` is reached.

### `--learning_rate`

- **What**: Initial learning rate after potential warmup.
- **Why**: The learning rate behaves as a sort of &quot;step size&quot; for gradient updates - too high, and we overstep the solution. Too low, and we never reach the ideal solution. A minimal value for a `full` tune might be as low as `1e-7` to a max of `1e-6` while for `lora` tuning a minimal value might be `1e-5` with a maximal value as high as `1e-3`. When a higher learning rate is used, it&apos;s advantageous to use an EMA network with a learning rate warmup - see `--use_ema`, `--lr_warmup_steps`, and `--lr_scheduler`.

### `--lr_scheduler`

- **What**: How to scale the learning rate over time.
- **Choices**: constant, constant_with_warmup, cosine, cosine_with_restarts, **polynomial** (recommended), linear
- **Why**: Models benefit from continual learning rate adjustments to further explore the loss landscape. A cosine schedule is used as the default; this allows the training to smoothly transition between two extremes. If using a constant learning rate, it is common to select a too-high or too-low value, causing divergence (too high) or getting stuck in a local minima (too low). A polynomial schedule is best paired with a warmup, where it will gradually approach the `learning_rate` value before then slowing down and approaching `--lr_end` by the end.

### `--optimizer`

- **What**: The optimizer to use for training.
- **Choices**: adamw_bf16, ao-adamw8bit, ao-adamw4bit, ao-adamfp8, ao-adamwfp8, adamw_schedulefree, adamw_schedulefree+aggressive, adamw_schedulefree+no_kahan, optimi-stableadamw, optimi-adamw, optimi-lion, optimi-radam, optimi-ranger, optimi-adan, optimi-adam, optimi-sgd, soap, bnb-adagrad, bnb-adagrad8bit, bnb-adam, bnb-adam8bit, bnb-adamw, bnb-adamw8bit, bnb-adamw-paged, bnb-adamw8bit-paged, bnb-lion, bnb-lion8bit, bnb-lion-paged, bnb-lion8bit-paged, bnb-ademamix, bnb-ademamix8bit, bnb-ademamix-paged, bnb-ademamix8bit-paged, prodigy

&gt; Note: Some optimisers may not be available on non-NVIDIA hardware.

### `--optimizer_config`

- **What**: Tweak optimizer settings.
- **Why**: Because optimizers have so many different settings, it&apos;s not feasible to provide a command-line argument for each one. Instead, you can provide a comma-separated list of values to override any of the default settings.
- **Example**: You may wish to set the `d_coef` for the **prodigy** optimizer: `--optimizer_config=d_coef=0.1`

&gt; Note: Optimizer betas are overridden using dedicated parameters, `--optimizer_beta1`, `--optimizer_beta2`.

### `--train_batch_size`

- **What**: Batch size for the training data loader.
- **Why**: Affects the model&apos;s memory consumption, convergence quality, and training speed. The higher the batch size, the better the results will be, but a very high batch size might result in overfitting or destabilized training, as well as increasing the duration of the training session unnecessarily. Experimentation is warranted, but in general, you want to try to max out your video memory while not decreasing the training speed.

### `--gradient_accumulation_steps`

- **What**: Number of update steps to accumulate before performing a backward/update pass, essentially splitting the work over multiple batches to save memory at the cost of a higher training runtime.
- **Why**: Useful for handling larger models or datasets.

&gt; Note: Do not enable fused backward pass for any optimizers when using gradient accumulation steps.

---

## 🛠 Advanced Optimizations

### `--use_ema`

- **What**: Keeping an exponential moving average of your weights over the models&apos; training lifetime is like periodically back-merging the model into itself.
- **Why**: It can improve training stability at the cost of more system resources, and a slight increase in training runtime.

## `--ema_device`

- **Choices**: `cpu`, `accelerator`, default: `cpu`
- **What**: Place the EMA weights on the accelerator instead of CPU.
- **Why**: The default location of CPU for EMA weights might result in a substantial slowdown on some systems. However, `--ema_cpu_only` will override this value if provided.

### `--ema_cpu_only`

- **What**: Keeps EMA weights on the CPU. The default behaviour is to move the EMA weights to the GPU before updating them.
- **Why**: Moving the EMA weights to the GPU is unnecessary, as the update on CPU can be nearly just as quick. However, some systems may experience a substantial slowdown, so EMA weights will remain on GPU by default.

### `--ema_update_interval`

- **What**: Reduce the update interval of your EMA shadow parameters.
- **Why**: Updating the EMA weights on every step could be an unnecessary waste of resources. Providing `--ema_update_interval=100` will update the EMA weights only once every 100 optimizer steps.


### `--snr_gamma`

- **What**: Utilising min-SNR weighted loss factor.
- **Why**: Minimum SNR gamma weights the loss factor of a timestep by its position in the schedule. Overly noisy timesteps have their contributions reduced, and less-noisy timesteps have it increased. Value recommended by the original paper is **5** but you can use values as low as **1** or as high as **20**, typically seen as the maximum value - beyond a value of 20, the math does not change things much. A value of **1** is the strongest.

### `--use_soft_min_snr`

- **What**: Train a model using a more gradual weighting on the loss landscape.
- **Why**: When training pixel diffusion models, they will simply degrade without using a specific loss weighting schedule. This is the case with DeepFloyd, where soft-min-snr-gamma was found to essentially be mandatory for good results. You may find success with latent diffusion model training, but in small experiments, it was found to potentially produce blurry results.

---

## 🔄 Checkpointing and Resumption

### `--checkpointing_steps`

- **What**: Interval at which training state checkpoints are saved.
- **Why**: Useful for resuming training and for inference. Every _n_ iterations, a partial checkpoint will be saved in the `.safetensors` format, via the Diffusers filesystem layout.

### `--resume_from_checkpoint`

- **What**: Specifies if and from where to resume training.
- **Why**: Allows you to continue training from a saved state, either manually specified or the latest available. A checkpoint is composed of a `unet` and optionally, a `unet_ema` subfolder. The `unet` may be dropped into any Diffusers layout SDXL model, allowing it to be used as a normal model would.

&gt; ℹ️ Transformer models such as PixArt, SD3, or Hunyuan, use the `transformer` and `transformer_ema` subfolder names.

---

## 📊 Logging and Monitoring

### `--logging_dir`

- **What**: Directory for TensorBoard logs.
- **Why**: Allows you to monitor training progress and performance metrics.

### `--report_to`

- **What**: Specifies the platform for reporting results and logs.
- **Why**: Enables integration with platforms like TensorBoard, wandb, or comet_ml for monitoring. Use multiple values separated by a comma to report to multiple trackers;
- **Choices**: wandb, tensorboard, comet_ml

# Environment configuration variables

The above options apply for the most part, to `config.json` - but some entries must be set inside `config.env` instead.

- `TRAINING_NUM_PROCESSES` should be set to the number of GPUs in the system. For most use-cases, this is enough to enable DistributedDataParallel (DDP) training
- `TRAINING_DYNAMO_BACKEND` defaults to `no` but can be set to `inductor` for substantial speed improvements on NVIDIA hardware
- `SIMPLETUNER_LOG_LEVEL` defaults to `INFO` but can be set to `DEBUG` to add more information for issue reports into `debug.log`
- `VENV_PATH` can be set to the location of your python virtual env, if it is not in the typical `.venv` location
- `ACCELERATE_EXTRA_ARGS` can be left unset, or, contain extra arguments to add like `--multi_gpu` or FSDP-specific flags

---

This is a basic overview meant to help you get started. For a complete list of options and more detailed explanations, please refer to the full specification:

```
usage: train.py [-h] [--snr_gamma SNR_GAMMA] [--use_soft_min_snr]
                [--soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA]
                --model_family
                {pixart_sigma,sana,kolors,sd3,flux,smoldit,sdxl,ltxvideo,legacy}
                [--model_type {full,lora,deepfloyd-full,deepfloyd-lora,deepfloyd-stage2,deepfloyd-stage2-lora}]
                [--flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano}]
                [--flow_matching_sigmoid_scale FLOW_MATCHING_SIGMOID_SCALE]
                [--flow_sigmoid_scale FLOW_SIGMOID_SCALE]
                [--flux_fast_schedule] [--flux_use_uniform_schedule]
                [--flow_use_uniform_schedule] [--flux_use_beta_schedule]
                [--flow_use_beta_schedule]
                [--flux_beta_schedule_alpha FLUX_BETA_SCHEDULE_ALPHA]
                [--flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA]
                [--flux_beta_schedule_beta FLUX_BETA_SCHEDULE_BETA]
                [--flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA]
                [--flux_schedule_shift FLUX_SCHEDULE_SHIFT]
                [--flow_schedule_shift FLOW_SCHEDULE_SHIFT]
                [--flux_schedule_auto_shift] [--flow_schedule_auto_shift]
                [--flux_guidance_mode {constant,random-range}]
                [--flux_guidance_value FLUX_GUIDANCE_VALUE]
                [--flux_guidance_min FLUX_GUIDANCE_MIN]
                [--flux_guidance_max FLUX_GUIDANCE_MAX]
                [--flux_attention_masked_training]
                [--ltx_train_mode {t2v,i2v}] [--ltx_i2v_prob LTX_I2V_PROB]
                [--ltx_protect_first_frame]
                [--ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION]
                [--t5_padding {zero,unmodified}] [--smoldit]
                [--smoldit_config {smoldit-small,smoldit-swiglu,smoldit-base,smoldit-large,smoldit-huge}]
                [--flow_matching_loss {diffusers,compatible,diffusion,sd35}]
                [--sd3_clip_uncond_behaviour {empty_string,zero}]
                [--sd3_t5_uncond_behaviour {empty_string,zero}]
                [--lora_type {standard,lycoris}]
                [--lora_init_type {default,gaussian,loftq,olora,pissa}]
                [--init_lora INIT_LORA] [--lora_rank LORA_RANK]
                [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]
                [--lycoris_config LYCORIS_CONFIG]
                [--init_lokr_norm INIT_LOKR_NORM] [--controlnet]
                [--controlnet_model_name_or_path]
                --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH
                [--pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH]
                [--pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER]
                [--pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH]
                [--pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER]
                [--pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH]
                [--pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH]
                [--prediction_type {epsilon,v_prediction,sample}]
                [--snr_weight SNR_WEIGHT]
                [--training_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--inference_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--refiner_training] [--refiner_training_invert_schedule]
                [--refiner_training_strength REFINER_TRAINING_STRENGTH]
                [--timestep_bias_strategy {earlier,later,range,none}]
                [--timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER]
                [--timestep_bias_begin TIMESTEP_BIAS_BEGIN]
                [--timestep_bias_end TIMESTEP_BIAS_END]
                [--timestep_bias_portion TIMESTEP_BIAS_PORTION]
                [--disable_segmented_timestep_sampling]
                [--rescale_betas_zero_snr]
                [--vae_dtype {default,fp16,fp32,bf16}]
                [--vae_batch_size VAE_BATCH_SIZE] [--vae_enable_tiling]
                [--vae_cache_scan_behaviour {recreate,sync}]
                [--vae_cache_ondemand] [--compress_disk_cache]
                [--aspect_bucket_disable_rebuild] [--keep_vae_loaded]
                [--skip_file_discovery SKIP_FILE_DISCOVERY]
                [--revision REVISION] [--variant VARIANT]
                [--preserve_data_backend_cache] [--use_dora]
                [--override_dataset_config] [--cache_dir_text CACHE_DIR_TEXT]
                [--cache_dir_vae CACHE_DIR_VAE]
                [--data_backend_config DATA_BACKEND_CONFIG]
                [--data_backend_sampling {uniform,auto-weighting}]
                [--ignore_missing_files] [--write_batch_size WRITE_BATCH_SIZE]
                [--read_batch_size READ_BATCH_SIZE]
                [--image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE]
                [--enable_multiprocessing] [--max_workers MAX_WORKERS]
                [--aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS]
                [--torch_num_threads TORCH_NUM_THREADS]
                [--dataloader_prefetch]
                [--dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN]
                [--aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT]
                [--cache_dir CACHE_DIR] [--cache_clear_validation_prompts]
                [--caption_strategy {filename,textfile,instance_prompt,parquet}]
                [--parquet_caption_column PARQUET_CAPTION_COLUMN]
                [--parquet_filename_column PARQUET_FILENAME_COLUMN]
                [--instance_prompt INSTANCE_PROMPT] [--output_dir OUTPUT_DIR]
                [--seed SEED] [--seed_for_each_device SEED_FOR_EACH_DEVICE]
                [--framerate FRAMERATE] [--resolution RESOLUTION]
                [--resolution_type {pixel,area,pixel_area}]
                [--aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}]
                [--aspect_bucket_alignment {8,64}]
                [--minimum_image_size MINIMUM_IMAGE_SIZE]
                [--maximum_image_size MAXIMUM_IMAGE_SIZE]
                [--target_downsample_size TARGET_DOWNSAMPLE_SIZE]
                [--train_text_encoder]
                [--tokenizer_max_length TOKENIZER_MAX_LENGTH]
                [--train_batch_size TRAIN_BATCH_SIZE]
                [--num_train_epochs NUM_TRAIN_EPOCHS]
                [--max_train_steps MAX_TRAIN_STEPS] [--ignore_final_epochs]
                [--checkpointing_steps CHECKPOINTING_STEPS]
                [--checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT]
                [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                [--gradient_checkpointing]
                [--gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL]
                [--learning_rate LEARNING_RATE]
                [--text_encoder_lr TEXT_ENCODER_LR] [--lr_scale]
                [--lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                [--lr_warmup_steps LR_WARMUP_STEPS]
                [--lr_num_cycles LR_NUM_CYCLES] [--lr_power LR_POWER]
                [--use_ema] [--ema_device {cpu,accelerator}]
                [--ema_validation {none,ema_only,comparison}] [--ema_cpu_only]
                [--ema_foreach_disable]
                [--ema_update_interval EMA_UPDATE_INTERVAL]
                [--ema_decay EMA_DECAY] [--non_ema_revision NON_EMA_REVISION]
                [--offload_param_path OFFLOAD_PARAM_PATH] --optimizer
                {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,bnb-adagrad,bnb-adagrad8bit,bnb-adam,bnb-adam8bit,bnb-adamw,bnb-adamw8bit,bnb-adamw-paged,bnb-adamw8bit-paged,bnb-lion,bnb-lion8bit,bnb-lion-paged,bnb-lion8bit-paged,bnb-ademamix,bnb-ademamix8bit,bnb-ademamix-paged,bnb-ademamix8bit-paged,prodigy}
                [--optimizer_config OPTIMIZER_CONFIG]
                [--optimizer_cpu_offload_method {none}]
                [--optimizer_offload_gradients] [--fuse_optimizer]
                [--optimizer_beta1 OPTIMIZER_BETA1]
                [--optimizer_beta2 OPTIMIZER_BETA2]
                [--optimizer_release_gradients] [--adam_beta1 ADAM_BETA1]
                [--adam_beta2 ADAM_BETA2]
                [--adam_weight_decay ADAM_WEIGHT_DECAY]
                [--adam_epsilon ADAM_EPSILON] [--prodigy_steps PRODIGY_STEPS]
                [--max_grad_norm MAX_GRAD_NORM]
                [--grad_clip_method {value,norm}] [--push_to_hub]
                [--push_checkpoints_to_hub] [--hub_model_id HUB_MODEL_ID]
                [--model_card_note MODEL_CARD_NOTE]
                [--model_card_safe_for_work] [--logging_dir LOGGING_DIR]
                [--benchmark_base_model] [--disable_benchmark]
                [--evaluation_type {clip,none}] [--eval_dataset_pooling]
                [--pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH]
                [--validation_on_startup] [--validation_seed_source {gpu,cpu}]
                [--validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH]
                [--validation_torch_compile]
                [--validation_torch_compile_mode {max-autotune,reduce-overhead,default}]
                [--validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS]
                [--validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START]
                [--validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP]
                [--validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE]
                [--sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION]
                [--allow_tf32] [--disable_tf32] [--validation_using_datasets]
                [--webhook_config WEBHOOK_CONFIG]
                [--webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL]
                [--report_to REPORT_TO] [--tracker_run_name TRACKER_RUN_NAME]
                [--tracker_project_name TRACKER_PROJECT_NAME]
                [--tracker_image_layout {gallery,table}]
                [--validation_prompt VALIDATION_PROMPT]
                [--validation_prompt_library]
                [--user_prompt_library USER_PROMPT_LIBRARY]
                [--validation_negative_prompt VALIDATION_NEGATIVE_PROMPT]
                [--num_validation_images NUM_VALIDATION_IMAGES]
                [--validation_steps VALIDATION_STEPS]
                [--eval_steps_interval EVAL_STEPS_INTERVAL]
                [--eval_timesteps EVAL_TIMESTEPS]
                [--num_eval_images NUM_EVAL_IMAGES]
                [--eval_dataset_id EVAL_DATASET_ID]
                [--validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS]
                [--validation_resolution VALIDATION_RESOLUTION]
                [--validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc}]
                [--validation_disable_unconditional] [--enable_watermark]
                [--mixed_precision {bf16,fp16,no}]
                [--gradient_precision {unmodified,fp32}]
                [--quantize_via {cpu,accelerator}]
                [--base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}]
                [--quantize_activations]
                [--base_model_default_dtype {bf16,fp32}]
                [--text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}]
                [--text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}]
                [--text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}]
                [--local_rank LOCAL_RANK]
                [--attention_mechanism {diffusers,xformers,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}]
                [--sageattention_usage {training,inference,training+inference}]
                [--enable_xformers_memory_efficient_attention]
                [--set_grads_to_none] [--noise_offset NOISE_OFFSET]
                [--noise_offset_probability NOISE_OFFSET_PROBABILITY]
                [--validation_guidance VALIDATION_GUIDANCE]
                [--validation_guidance_real VALIDATION_GUIDANCE_REAL]
                [--validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP]
                [--validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE]
                [--validation_randomize] [--validation_seed VALIDATION_SEED]
                [--fully_unload_text_encoder]
                [--freeze_encoder_before FREEZE_ENCODER_BEFORE]
                [--freeze_encoder_after FREEZE_ENCODER_AFTER]
                [--freeze_encoder_strategy FREEZE_ENCODER_STRATEGY]
                [--layer_freeze_strategy {none,bitfit}]
                [--unet_attention_slice] [--print_filenames]
                [--print_sampler_statistics]
                [--metadata_update_interval METADATA_UPDATE_INTERVAL]
                [--debug_aspect_buckets] [--debug_dataset_loader]
                [--freeze_encoder FREEZE_ENCODER] [--save_text_encoder]
                [--text_encoder_limit TEXT_ENCODER_LIMIT]
                [--prepend_instance_prompt] [--only_instance_prompt]
                [--data_aesthetic_score DATA_AESTHETIC_SCORE]
                [--sdxl_refiner_uses_full_range]
                [--caption_dropout_probability CAPTION_DROPOUT_PROBABILITY]
                [--delete_unwanted_images] [--delete_problematic_images]
                [--disable_bucket_pruning] [--offset_noise]
                [--input_perturbation INPUT_PERTURBATION]
                [--input_perturbation_steps INPUT_PERTURBATION_STEPS]
                [--lr_end LR_END] [--i_know_what_i_am_doing]
                [--accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL]

The following SimpleTuner command-line options are available:

options:
  -h, --help            show this help message and exit
  --snr_gamma SNR_GAMMA
                        SNR weighting gamma to be used if rebalancing the
                        loss. Recommended value is 5.0. More details here:
                        https://arxiv.org/abs/2303.09556.
  --use_soft_min_snr    If set, will use the soft min SNR calculation method.
                        This method uses the sigma_data parameter. If not
                        provided, the method will raise an error.
  --soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA
                        The standard deviation of the data used in the soft
                        min weighting method. This is required when using the
                        soft min SNR calculation method.
  --model_family {pixart_sigma,sana,kolors,sd3,flux,smoldit,sdxl,ltxvideo,legacy}
                        The model family to train. This option is required.
  --model_type {full,lora,deepfloyd-full,deepfloyd-lora,deepfloyd-stage2,deepfloyd-stage2-lora}
                        The training type to use. &apos;full&apos; will train the full
                        model, while &apos;lora&apos; will train the LoRA model. LoRA is
                        a smaller model that can be used for faster training.
  --flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano}
                        This option only applies to Standard LoRA, not
                        Lycoris. Flux has single and joint attention blocks.
                        By default, all attention layers are trained, but not
                        the feed-forward layers If &apos;mmdit&apos; is provided, the
                        text input layers will not be trained. If &apos;context&apos; is
                        provided, then ONLY the text attention layers are
                        trained If &apos;context+ffs&apos; is provided, then text
                        attention and text feed-forward layers are trained.
                        This is somewhat similar to text-encoder-only training
                        in earlier SD versions. If &apos;all&apos; is provided, all
                        layers will be trained, minus feed-forward. If
                        &apos;all+ffs&apos; is provided, all layers will be trained
                        including feed-forward. If &apos;ai-toolkit&apos; is provided,
                        all layers will be trained including feed-forward and
                        norms (based on ostris/ai-toolkit). If &apos;tiny&apos; is
                        provided, only two layers will be trained. If &apos;nano&apos;
                        is provided, only one layers will be trained.
  --flow_matching_sigmoid_scale FLOW_MATCHING_SIGMOID_SCALE
                        Deprecated option. Replaced with --flow_sigmoid_scale.
  --flow_sigmoid_scale FLOW_SIGMOID_SCALE
                        Scale factor for sigmoid timestep sampling for flow-
                        matching models.
  --flux_fast_schedule  An experimental feature to train Flux.1S using a noise
                        schedule closer to what it was trained with, which has
                        improved results in short experiments. Thanks to
                        @mhirki for the contribution.
  --flux_use_uniform_schedule
                        Deprecated option. Replaced with
                        --flow_use_uniform_schedule.
  --flow_use_uniform_schedule
                        Whether or not to use a uniform schedule instead of
                        sigmoid for flow-matching noise schedule. Using
                        uniform sampling may cause a bias toward dark images,
                        and should be used with caution.
  --flux_use_beta_schedule
                        Deprecated option. Replaced with
                        --flow_use_beta_schedule.
  --flow_use_beta_schedule
                        Whether or not to use a beta schedule instead of
                        sigmoid for flow-matching. The default values of alpha
                        and beta approximate a sigmoid.
  --flux_beta_schedule_alpha FLUX_BETA_SCHEDULE_ALPHA
                        Deprecated option. Replaced with
                        --flux_beta_schedule_alpha.
  --flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA
                        The alpha value of the flow-matching beta schedule.
                        Default is 2.0
  --flux_beta_schedule_beta FLUX_BETA_SCHEDULE_BETA
                        Deprecated option. Replaced with
                        --flow_beta_schedule_beta.
  --flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA
                        The beta value of the flow-matching beta schedule.
                        Default is 2.0
  --flux_schedule_shift FLUX_SCHEDULE_SHIFT
                        Deprecated option. Replaced with
                        --flow_schedule_shift.
  --flow_schedule_shift FLOW_SCHEDULE_SHIFT
                        Shift the noise schedule. This is a value between 0
                        and ~4.0, where 0 disables the timestep-dependent
                        shift, and anything greater than 0 will shift the
                        timestep sampling accordingly. Sana and SD3 were
                        trained with a shift value of 3. This value can change
                        how contrast/brightness are learnt by the model, and
                        whether fine details are ignored or accentuated. A
                        higher value will focus more on large compositional
                        features, and a lower value will focus on the high
                        frequency fine details.
  --flux_schedule_auto_shift
                        Deprecated option. Replaced with
                        --flow_schedule_auto_shift.
  --flow_schedule_auto_shift
                        Shift the noise schedule depending on image
                        resolution. The shift value calculation is taken from
                        the official Flux inference code. Shift value is
                        math.exp(1.15) = 3.1581 for a pixel count of 1024px *
                        1024px. The shift value grows exponentially with
                        higher pixel counts. It is a good idea to train on a
                        mix of different resolutions when this option is
                        enabled. You may need to lower your learning rate with
                        this enabled.
  --flux_guidance_mode {constant,random-range}
                        Flux has a &apos;guidance&apos; value used during training time
                        that reflects the CFG range of your training samples.
                        The default mode &apos;constant&apos; will use a single value
                        for every sample. The mode &apos;random-range&apos; will
                        randomly select a value from the range of the CFG for
                        each sample. Set the range using --flux_guidance_min
                        and --flux_guidance_max.
  --flux_guidance_value FLUX_GUIDANCE_VALUE
                        When using --flux_guidance_mode=constant, this value
                        will be used for every input sample. Using a value of
                        1.0 seems to preserve the CFG distillation for the Dev
                        model, and using any other value will result in the
                        resulting LoRA requiring CFG at inference time.
  --flux_guidance_min FLUX_GUIDANCE_MIN
  --flux_guidance_max FLUX_GUIDANCE_MAX
  --flux_attention_masked_training
                        Use attention masking while training flux. This can be
                        a destructive operation, unless finetuning a model
                        which was already trained with it.
  --ltx_train_mode {t2v,i2v}
                        This value will be the default for all video datasets
                        that do not have their own i2v settings defined. By
                        default, we enable i2v mode, but it can be switched to
                        t2v for your convenience.
  --ltx_i2v_prob LTX_I2V_PROB
                        Probability in [0,1] of applying i2v (image-to-video)
                        style training. If random.random() &lt; i2v_prob during
                        training, partial or complete first-frame protection
                        will be triggered (depending on
                        --ltx_protect_first_frame). If set to 0.0, no i2v
                        logic is applied (pure t2v). Default: 0.1 (from
                        finetuners project)
  --ltx_protect_first_frame
                        If specified, fully protect the first frame whenever
                        i2v logic is triggered (see --ltx_i2v_prob). This
                        means the first frame is never noised or denoised,
                        effectively pinned to the original content.
  --ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION
                        Maximum fraction of noise to introduce into the first
                        frame when i2v is triggered and the first frame is not
                        fully protected. For instance, a value of 0.05 means
                        the first frame can have up to 5 percent random noise
                        mixed in, preserving 95 percent of the original
                        content. Ignored if --ltx_protect_first_frame is set.
  --t5_padding {zero,unmodified}
                        The padding behaviour for Flux and SD3. &apos;zero&apos; will
                        pad the input with zeros. The default is &apos;unmodified&apos;,
                        which will not pad the input.
  --smoldit             Use the experimental SmolDiT model architecture.
  --smoldit_config {smoldit-small,smoldit-swiglu,smoldit-base,smoldit-large,smoldit-huge}
                        The SmolDiT configuration to use. This is a list of
                        pre-configured models. The default is &apos;smoldit-base&apos;.
  --flow_matching_loss {diffusers,compatible,diffusion,sd35}
                        A discrepancy exists between the Diffusers
                        implementation of flow matching and the minimal
                        implementation provided by StabilityAI. This
                        experimental option allows switching loss calculations
                        to be compatible with those. Additionally, &apos;diffusion&apos;
                        is offered as an option to reparameterise a model to
                        v_prediction loss. sd35 provides the ability to train
                        on SD3.5&apos;s flow-matching target, which is the denoised
                        sample.
  --sd3_clip_uncond_behaviour {empty_string,zero}
                        SD3 can be trained using zeroed prompt embeds during
                        unconditional dropout, or an encoded empty string may
                        be used instead (the default). Changing this value may
                        stabilise or destabilise training. The default is
                        &apos;empty_string&apos;.
  --sd3_t5_uncond_behaviour {empty_string,zero}
                        Override the value of unconditional prompts from T5
                        embeds. The default is to follow the value of
                        --sd3_clip_uncond_behaviour.
  --lora_type {standard,lycoris}
                        When training using --model_type=lora, you may specify
                        a different type of LoRA to train here. standard
                        refers to training a vanilla LoRA via PEFT, lycoris
                        refers to training with KohakuBlueleaf&apos;s library of
                        the same name.
  --lora_init_type {default,gaussian,loftq,olora,pissa}
                        The initialization type for the LoRA model. &apos;default&apos;
                        will use Microsoft&apos;s initialization method, &apos;gaussian&apos;
                        will use a Gaussian scaled distribution, and &apos;loftq&apos;
                        will use LoftQ initialization. In short experiments,
                        &apos;default&apos; produced accurate results earlier in
                        training, &apos;gaussian&apos; had slightly more creative
                        outputs, and LoftQ produces an entirely different
                        result with worse quality at first, taking potentially
                        longer to converge than the other methods.
  --init_lora INIT_LORA
                        Specify an existing LoRA or LyCORIS safetensors file
                        to initialize the adapter and continue training, if a
                        full checkpoint is not available.
  --lora_rank LORA_RANK
                        The dimension of the LoRA update matrices.
  --lora_alpha LORA_ALPHA
                        The alpha value for the LoRA model. This is the
                        learning rate for the LoRA update matrices.
  --lora_dropout LORA_DROPOUT
                        LoRA dropout randomly ignores neurons during training.
                        This can help prevent overfitting.
  --lycoris_config LYCORIS_CONFIG
                        The location for the JSON file of the Lycoris
                        configuration.
  --init_lokr_norm INIT_LOKR_NORM
                        Setting this turns on perturbed normal initialization
                        of the LyCORIS LoKr PEFT layers. A good value is
                        between 1e-4 and 1e-2.
  --controlnet          If set, ControlNet style training will be used, where
                        a conditioning input image is required alongside the
                        training data.
  --controlnet_model_name_or_path
                        When provided alongside --controlnet, this will
                        specify ControlNet model weights to preload from the
                        hub.
  --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH
                        Path to pretrained model or model identifier from
                        huggingface.co/models. Some model architectures
                        support loading single-file .safetensors directly.
                        Note that when using single-file safetensors, the
                        tokeniser and noise schedule configs will be used from
                        the vanilla upstream Huggingface repository, which
                        requires network access. If you are training on a
                        machine without network access, you should pre-
                        download the entire Huggingface model repository
                        instead of using single-file loader.
  --pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH
                        Path to pretrained transformer model or model
                        identifier from huggingface.co/models.
  --pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER
                        The subfolder to load the transformer model from. Use
                        &apos;none&apos; for a flat directory.
  --pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH
                        Path to pretrained unet model or model identifier from
                        huggingface.co/models.
  --pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER
                        The subfolder to load the unet model from. Use &apos;none&apos;
                        for a flat directory.
  --pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH
                        Path to an improved VAE to stabilize training. For
                        more details check out:
                        https://github.com/huggingface/diffusers/pull/4038.
  --pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH
                        T5-XXL is a huge model, and starting from many
                        different models will download a separate one each
                        time. This option allows you to specify a specific
                        location to retrieve T5-XXL v1.1 from, so that it only
                        downloads once..
  --prediction_type {epsilon,v_prediction,sample}
                        The type of prediction to use for the u-net. Choose
                        between [&apos;epsilon&apos;, &apos;v_prediction&apos;, &apos;sample&apos;]. For SD
                        2.1-v, this is v_prediction. For 2.1-base, it is
                        epsilon. SDXL is generally epsilon. SD 1.5 is epsilon.
  --snr_weight SNR_WEIGHT
                        When training a model using
                        `--prediction_type=sample`, one can supply an SNR
                        weight value to augment the loss with. If a value of
                        0.5 is provided here, the loss is taken half from the
                        SNR and half from the MSE.
  --training_scheduler_timestep_spacing {leading,linspace,trailing}
                        (SDXL Only) Spacing timesteps can fundamentally alter
                        the course of history. Er, I mean, your model weights.
                        For all training, including epsilon, it would seem
                        that &apos;trailing&apos; is the right choice. SD 2.x always
                        uses &apos;trailing&apos;, but SDXL may do better in its default
                        state when using &apos;leading&apos;.
  --inference_scheduler_timestep_spacing {leading,linspace,trailing}
                        (SDXL Only) The Bytedance paper on zero terminal SNR
                        recommends inference using &apos;trailing&apos;. SD 2.x always
                        uses &apos;trailing&apos;, but SDXL may do better in its default
                        state when using &apos;leading&apos;.
  --refiner_training    When training or adapting a model into a mixture-of-
                        experts 2nd stage / refiner model, this option should
                        be set. This will slice the timestep schedule defined
                        by --refiner_training_strength proportion value
                        (default 0.2)
  --refiner_training_invert_schedule
                        While the refiner training strength is applied to the
                        end of the schedule, this option will invert the
                        result for training a **base** model, eg. the first
                        model in a mixture-of-experts series. A
                        --refiner_training_strength of 0.35 will result in the
                        refiner learning timesteps 349-0. Setting
                        --refiner_training_invert_schedule then would result
                        in the base model learning timesteps 999-350.
  --refiner_training_strength REFINER_TRAINING_STRENGTH
                        When training a refiner / 2nd stage mixture of experts
                        model, the refiner training strength indicates how
                        much of the *end* of the schedule it will be trained
                        on. A value of 0.2 means timesteps 199-0 will be the
                        focus of this model, and 0.3 would be 299-0 and so on.
                        The default value is 0.2, in line with the SDXL
                        refiner pretraining.
  --timestep_bias_strategy {earlier,later,range,none}
                        The timestep bias strategy, which may help direct the
                        model toward learning low or frequency details.
                        Choices: [&apos;earlier&apos;, &apos;later&apos;, &apos;none&apos;]. The default is
                        &apos;none&apos;, which means no bias is applied, and training
                        proceeds normally. The value of &apos;later&apos; will prefer to
                        generate samples for later timesteps.
  --timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER
                        The multiplier for the bias. Defaults to 1.0, which
                        means no bias is applied. A value of 2.0 will double
                        the weight of the bias, and a value of 0.5 will halve
                        it.
  --timestep_bias_begin TIMESTEP_BIAS_BEGIN
                        When using `--timestep_bias_strategy=range`, the
                        beginning timestep to bias. Defaults to zero, which
                        equates to having no specific bias.
  --timestep_bias_end TIMESTEP_BIAS_END
                        When using `--timestep_bias_strategy=range`, the final
                        timestep to bias. Defaults to 1000, which is the
                        number of timesteps that SDXL Base and SD 2.x were
                        trained on.
  --timestep_bias_portion TIMESTEP_BIAS_PORTION
                        The portion of timesteps to bias. Defaults to 0.25,
                        which 25 percent of timesteps will be biased. A value
                        of 0.5 will bias one half of the timesteps. The value
                        provided for `--timestep_bias_strategy` determines
                        whether the biased portions are in the earlier or
                        later timesteps.
  --disable_segmented_timestep_sampling
                        By default, the timestep schedule is divided into
                        roughly `train_batch_size` number of segments, and
                        then each of those are sampled from separately. This
                        improves the selection distribution, but may not be
                        desired in certain training scenarios, eg. when
                        limiting the timestep selection range.
  --rescale_betas_zero_snr
                        If set, will rescale the betas to zero terminal SNR.
                        This is recommended for training with v_prediction.
                        For epsilon, this might help with fine details, but
                        will not result in contrast improvements.
  --vae_dtype {default,fp16,fp32,bf16}
                        The dtype of the VAE model. Choose between [&apos;default&apos;,
                        &apos;fp16&apos;, &apos;fp32&apos;, &apos;bf16&apos;]. The default VAE dtype is
                        bfloat16, due to NaN issues in SDXL 1.0. Using fp16 is
                        not recommended.
  --vae_batch_size VAE_BATCH_SIZE
                        When pre-caching latent vectors, this is the batch
                        size to use. Decreasing this may help with VRAM
                        issues, but if you are at that point of contention,
                        it&apos;s possible that your GPU has too little RAM.
                        Default: 4.
  --vae_enable_tiling   If set, will enable tiling for VAE caching. This is
                        useful for very large images when VRAM is limited.
                        This may be required for 2048px VAE caching on 24G
                        accelerators, in addition to reducing
                        --vae_batch_size.
  --vae_cache_scan_behaviour {recreate,sync}
                        When a mismatched latent vector is detected, a scan
                        will be initiated to locate inconsistencies and
                        resolve them. The default setting &apos;recreate&apos; will
                        delete any inconsistent cache entries and rebuild it.
                        Alternatively, &apos;sync&apos; will update the bucket
                        configuration so that the image is in a bucket that
                        matches its latent size. The recommended behaviour is
                        to use the default value and allow the cache to be
                        recreated.
  --vae_cache_ondemand  By default, will batch-encode images before training.
                        For some situations, ondemand may be desired, but it
                        greatly slows training and increases memory pressure.
  --compress_disk_cache
                        If set, will gzip-compress the disk cache for Pytorch
                        files. This will save substantial disk space, but may
                        slow down the training process.
  --aspect_bucket_disable_rebuild
                        When using a randomised aspect bucket list, the VAE
                        and aspect cache are rebuilt on each epoch. With a
                        large and diverse enough dataset, rebuilding the
                        aspect list may take a long time, and this may be
                        undesirable. This option will not override
                        vae_cache_clear_each_epoch. If both options are
                        provided, only the VAE cache will be rebuilt.
  --keep_vae_loaded     If set, will keep the VAE loaded in memory. This can
                        reduce disk churn, but consumes VRAM during the
                        forward pass.
  --skip_file_discovery SKIP_FILE_DISCOVERY
                        Comma-separated values of which stages to skip
                        discovery for. Skipping any stage will speed up
                        resumption, but will increase the risk of errors, as
                        missing images or incorrectly bucketed images may not
                        be caught. &apos;vae&apos; will skip the VAE cache process,
                        &apos;aspect&apos; will not build any aspect buckets, and &apos;text&apos;
                        will avoid text embed management. Valid options:
                        aspect, vae, text, metadata.
  --revision REVISION   Revision of pretrained model identifier from
                        huggingface.co/models. Trainable model components
                        should be at least bfloat16 precision.
  --variant VARIANT     Variant of pretrained model identifier from
                        huggingface.co/models. Trainable model components
                        should be at least bfloat16 precision.
  --preserve_data_backend_cache
                        For very large cloud storage buckets that will never
                        change, enabling this option will prevent the trainer
                        from scanning it at startup, by preserving the cache
                        files that we generate. Be careful when using this,
                        as, switching datasets can result in the preserved
                        cache being used, which would be problematic.
                        Currently, cache is not stored in the dataset itself
                        but rather, locally. This may change in a future
                        release.
  --use_dora            If set, will use the DoRA-enhanced LoRA training. This
                        is an experimental feature, may slow down training,
                        and is not recommended for general use.
  --override_dataset_config
                        When provided, the dataset&apos;s config will not be
                        checked against the live backend config. This is
                        useful if you want to simply update the behaviour of
                        an existing dataset, but the recommendation is to not
                        change the dataset configuration after caching has
                        begun, as most options cannot be changed without
                        unexpected behaviour later on. Additionally, it
                        prevents accidentally loading an SDXL configuration on
                        a SD 2.x model and vice versa.
  --cache_dir_text CACHE_DIR_TEXT
                        This is the path to a local directory that will
                        contain your text embed cache.
  --cache_dir_vae CACHE_DIR_VAE
                        This is the path to a local directory that will
                        contain your VAE outputs. Unlike the text embed cache,
                        your VAE latents will be stored in the AWS data
                        backend. Each backend can have its own value, but if
                        that is not provided, this will be the default value.
  --data_backend_config DATA_BACKEND_CONFIG
                        The relative or fully-qualified path for your data
                        backend config. See multidatabackend.json.example for
                        an example.
  --data_backend_sampling {uniform,auto-weighting}
                        When using multiple data backends, the sampling
                        weighting can be set to &apos;uniform&apos; or &apos;auto-weighting&apos;.
                        The default value is &apos;auto-weighting&apos;, which will
                        automatically adjust the sampling weights based on the
                        number of images in each backend. &apos;uniform&apos; will
                        sample from each backend equally.
  --ignore_missing_files
                        This option will disable the check for files that have
                        been deleted or removed from your data directory. This
                        would allow training on large datasets without keeping
                        the associated images on disk, though it&apos;s not
                        recommended and is not a supported feature. Use with
                        caution, as it mostly exists for experimentation.
  --write_batch_size WRITE_BATCH_SIZE
                        When using certain storage backends, it is better to
                        batch smaller writes rather than continuous
                        dispatching. In SimpleTuner, write batching is
                        currently applied during VAE caching, when many small
                        objects are written. This mostly applies to S3, but
                        some shared server filesystems may benefit as well,
                        eg. Ceph. Default: 64.
  --read_batch_size READ_BATCH_SIZE
                        Used by the VAE cache to prefetch image data. This is
                        the number of images to read ahead.
  --image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE
                        When resizing and cropping images, we do it in
                        parallel using processes or threads. This defines how
                        many images will be read into the queue before they
                        are processed.
  --enable_multiprocessing
                        If set, will use processes instead of threads during
                        metadata caching operations. For some systems,
                        multiprocessing may be faster than threading, but will
                        consume a lot more memory. Use this option with
                        caution, and monitor your system&apos;s memory usage.
  --max_workers MAX_WORKERS
                        How many active threads or processes to run during VAE
                        caching.
  --aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS
                        When using AWS backends, the maximum number of
                        connections to keep open to the S3 bucket at a single
                        time. This should be greater or equal to the
                        max_workers and aspect bucket worker count values.
  --torch_num_threads TORCH_NUM_THREADS
                        The number of threads to use for PyTorch operations.
                        This is not the same as the number of workers.
                        Default: 8.
  --dataloader_prefetch
                        When provided, the dataloader will read-ahead and
                        attempt to retrieve latents, text embeds, and other
                        metadata ahead of the time when the batch is required,
                        so that it can be immediately available.
  --dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN
                        Set the number of prefetched batches.
  --aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT
                        The number of workers to use for aspect bucketing.
                        This is a CPU-bound task, so the number of workers
                        should be set to the number of CPU threads available.
                        If you use an I/O bound backend, an even higher value
                        may make sense. Default: 12.
  --cache_dir CACHE_DIR
                        The directory where the downloaded models and datasets
                        will be stored.
  --cache_clear_validation_prompts
                        When provided, any validation prompt entries in the
                        text embed cache will be recreated. This is useful if
                        you&apos;ve modified any of the existing prompts, or,
                        disabled/enabled Compel, via `--disable_compel`
  --caption_strategy {filename,textfile,instance_prompt,parquet}
                        The default captioning strategy, &apos;filename&apos;, will use
                        the filename as the caption, after stripping some
                        characters like underscores. The &apos;textfile&apos; strategy
                        will use the contents of a text file with the same
                        name as the image. The &apos;parquet&apos; strategy requires a
                        parquet file with the same name as the image,
                        containing a &apos;caption&apos; column.
  --parquet_caption_column PARQUET_CAPTION_COLUMN
                        When using caption_strategy=parquet, this option will
                        allow you to globally set the default caption field
                        across all datasets that do not have an override set.
  --parquet_filename_column PARQUET_FILENAME_COLUMN
                        When using caption_strategy=parquet, this option will
                        allow you to globally set the default filename field
                        across all datasets that do not have an override set.
  --instance_prompt INSTANCE_PROMPT
                        This is unused. Filenames will be the captions
                        instead.
  --output_dir OUTPUT_DIR
                        The output directory where the model predictions and
                        checkpoints will be written.
  --seed SEED           A seed for reproducible training.
  --seed_for_each_device SEED_FOR_EACH_DEVICE
                        By default, a unique seed will be used for each GPU.
                        This is done deterministically, so that each GPU will
                        receive the same seed across invocations. If
                        --seed_for_each_device=false is provided, then we will
                        use the same seed across all GPUs, which will almost
                        certainly result in the over-sampling of inputs on
                        larger datasets.
  --framerate FRAMERATE
                        By default, SimpleTuner will use a framerate of 25 for
                        training and inference on video models. You are on
                        your own if you modify this value, but it is provided
                        for your convenience.
  --resolution RESOLUTION
                        The resolution for input images, all the images in the
                        train/validation dataset will be resized to this
                        resolution. If using --resolution_type=area, this
                        float value represents megapixels.
  --resolution_type {pixel,area,pixel_area}
                        Resizing images maintains aspect ratio. This defines
                        the resizing strategy. If &apos;pixel&apos;, the images will be
                        resized to the resolution by the shortest pixel edge,
                        if the target size does not match the current size. If
                        &apos;area&apos;, the images will be resized so the pixel area
                        is this many megapixels. Common rounded values such as
                        `0.5` and `1.0` will be implicitly adjusted to their
                        squared size equivalents. If &apos;pixel_area&apos;, the pixel
                        value (eg. 1024) will be converted to the proper value
                        for &apos;area&apos;, and then calculate everything the same as
                        &apos;area&apos; would.
  --aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}
                        The number of decimal places to round the aspect ratio
                        to. This is used to create buckets for aspect ratios.
                        For higher precision, ensure the image sizes remain
                        compatible. Higher precision levels result in a
                        greater number of buckets, which may not be a
                        desirable outcome.
  --aspect_bucket_alignment {8,64}
                        When training diffusion models, the image sizes
                        generally must align to a 64 pixel interval. This is
                        an exception when training models like DeepFloyd that
                        use a base resolution of 64 pixels, as aligning to 64
                        pixels would result in a 1:1 or 2:1 aspect ratio,
                        overly distorting images. For DeepFloyd, this value is
                        set to 8, but all other training defaults to 64. You
                        may experiment with this value, but it is not
                        recommended.
  --minimum_image_size MINIMUM_IMAGE_SIZE
                        The minimum resolution for both sides of input images.
                        If --delete_unwanted_images is set, images smaller
                        than this will be DELETED. The default value is None,
                        which means no minimum resolution is enforced. If this
                        option is not provided, it is possible that images
                        will be destructively upsampled, harming model
                        performance.
  --maximum_image_size MAXIMUM_IMAGE_SIZE
                        When cropping images that are excessively large, the
                        entire scene context may be lost, eg. the crop might
                        just end up being a portion of the background. To
                        avoid this, a maximum image size may be provided,
                        which will result in very-large images being
                        downsampled before cropping them. This value uses
                        --resolution_type to determine whether it is a pixel
                        edge or megapixel value.
  --target_downsample_size TARGET_DOWNSAMPLE_SIZE
                        When using --maximum_image_size, very-large images
                        exceeding that value will be downsampled to this
                        target size before cropping. If --resolution_type=area
                        and --maximum_image_size=4.0,
                        --target_downsample_size=2.0 would result in a 4
                        megapixel image being resized to 2 megapixel before
                        cropping to 1 megapixel.
  --train_text_encoder  (SD 2.x only) Whether to train the text encoder. If
                        set, the text encoder should be float32 precision.
  --tokenizer_max_length TOKENIZER_MAX_LENGTH
                        The maximum length of the tokenizer. If not set, will
                        default to the tokenizer&apos;s max length.
  --train_batch_size TRAIN_BATCH_SIZE
                        Batch size (per device) for the training dataloader.
  --num_train_epochs NUM_TRAIN_EPOCHS
  --max_train_steps MAX_TRAIN_STEPS
                        Total number of training steps to perform. If
                        provided, overrides num_train_epochs.
  --ignore_final_epochs
                        When provided, the max epoch counter will not
                        determine the end of the training run. Instead, it
                        will end when it hits --max_train_steps.
  --checkpointing_steps CHECKPOINTING_STEPS
                        Save a checkpoint of the training state every X
                        updates. Checkpoints can be used for resuming training
                        via `--resume_from_checkpoint`. In the case that the
                        checkpoint is better than the final trained model, the
                        checkpoint can also be used for inference.Using a
                        checkpoint for inference requires separate loading of
                        the original pipeline and the individual checkpointed
                        model components.See https://huggingface.co/docs/diffu
                        sers/main/en/training/dreambooth#performing-inference-
                        using-a-saved-checkpoint for step by stepinstructions.
  --checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT
                        Max number of checkpoints to store.
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        Whether training should be resumed from a previous
                        checkpoint. Use a path saved by
                        `--checkpointing_steps`, or `&quot;latest&quot;` to
                        automatically select the last available checkpoint.
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of updates steps to accumulate before
                        performing a backward/update pass.
  --gradient_checkpointing
                        Whether or not to use gradient checkpointing to save
                        memory at the expense of slower backward pass.
  --gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL
                        Some models (Flux, SDXL, SD1.x/2.x, SD3) can have
                        their gradient checkpointing limited to every nth
                        block. This can speed up training but will use more
                        memory with larger intervals.
  --learning_rate LEARNING_RATE
                        Initial learning rate (after the potential warmup
                        period) to use. When using a cosine or sine schedule,
                        --learning_rate defines the maximum learning rate.
  --text_encoder_lr TEXT_ENCODER_LR
                        Learning rate for the text encoder. If not provided,
                        the value of --learning_rate will be used.
  --lr_scale            Scale the learning rate by the number of GPUs,
                        gradient accumulation steps, and batch size.
  --lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}
                        The scheduler type to use. Default: sine
  --lr_warmup_steps LR_WARMUP_STEPS
                        Number of steps for the warmup in the lr scheduler.
  --lr_num_cycles LR_NUM_CYCLES
                        Number of hard resets of the lr in
                        cosine_with_restarts scheduler.
  --lr_power LR_POWER   Power factor of the polynomial scheduler.
  --use_ema             Whether to use EMA (exponential moving average) model.
                        Works with LoRA, Lycoris, and full training.
  --ema_device {cpu,accelerator}
                        The device to use for the EMA model. If set to
                        &apos;accelerator&apos;, the EMA model will be placed on the
                        accelerator. This provides the fastest EMA update
                        times, but is not ultimately necessary for EMA to
                        function.
  --ema_validation {none,ema_only,comparison}
                        When &apos;none&apos; is set, no EMA validation will be done.
                        When using &apos;ema_only&apos;, the validations will rely
                        mostly on the EMA weights. When using &apos;comparison&apos;
                        (default) mode, the validations will first run on the
                        checkpoint before also running for the EMA weights. In
                        comparison mode, the resulting images will be provided
                        side-by-side.
  --ema_cpu_only        When using EMA, the shadow model is moved to the
                        accelerator before we update its parameters. When
                        provided, this option will disable the moving of the
                        EMA model to the accelerator. This will save a lot of
                        VRAM at the cost of a lot of time for updates. It is
                        recommended to also supply --ema_update_interval to
                        reduce the number of updates to eg. every 100 steps.
  --ema_foreach_disable
                        By default, we use torch._foreach functions for
                        updating the shadow parameters, which should be fast.
                        When provided, this option will disable the foreach
                        methods and use vanilla EMA updates.
  --ema_update_interval EMA_UPDATE_INTERVAL
                        The number of optimization steps between EMA updates.
                        If not provided, EMA network will update on every
                        step.
  --ema_decay EMA_DECAY
                        The closer to 0.9999 this gets, the less updates will
                        occur over time. Setting it to a lower value, such as
                        0.990, will allow greater influence of later updates.
  --non_ema_revision NON_EMA_REVISION
                        Revision of pretrained non-ema model identifier. Must
                        be a branch, tag or git identifier of the local or
                        remote repository specified with
                        --pretrained_model_name_or_path.
  --offload_param_path OFFLOAD_PARAM_PATH
                        When using DeepSpeed ZeRo stage 2 or 3 with NVMe
                        offload, this may be specified to provide a path for
                        the offload.
  --optimizer {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,bnb-adagrad,bnb-adagrad8bit,bnb-adam,bnb-adam8bit,bnb-adamw,bnb-adamw8bit,bnb-adamw-paged,bnb-adamw8bit-paged,bnb-lion,bnb-lion8bit,bnb-lion-paged,bnb-lion8bit-paged,bnb-ademamix,bnb-ademamix8bit,bnb-ademamix-paged,bnb-ademamix8bit-paged,prodigy}
  --optimizer_config OPTIMIZER_CONFIG
                        When setting a given optimizer, this allows a comma-
                        separated list of key-value pairs to be provided that
                        will override the optimizer defaults. For example, `--
                        optimizer_config=decouple_lr=True,weight_decay=0.01`.
  --optimizer_cpu_offload_method {none}
                        This option is a placeholder. In the future, it will
                        allow for the selection of different CPU offload
                        methods.
  --optimizer_offload_gradients
                        When creating a CPU-offloaded optimiser, the gradients
                        can be offloaded to the CPU to save more memory.
  --fuse_optimizer      When creating a CPU-offloaded optimiser, the fused
                        optimiser could be used to save on memory, while
                        running slightly slower.
  --optimizer_beta1 OPTIMIZER_BETA1
                        The value to use for the first beta value in the
                        optimiser, which is used for the first moment
                        estimate. A range of 0.8-0.9 is common.
  --optimizer_beta2 OPTIMIZER_BETA2
                        The value to use for the second beta value in the
                        optimiser, which is used for the second moment
                        estimate. A range of 0.999-0.9999 is common.
  --optimizer_release_gradients
                        When using Optimi optimizers, this option will release
                        the gradients after the optimizer step. This can save
                        memory, but may slow down training. With Quanto, there
                        may be no benefit.
  --adam_beta1 ADAM_BETA1
                        The beta1 parameter for the Adam and other optimizers.
  --adam_beta2 ADAM_BETA2
                        The beta2 parameter for the Adam and other optimizers.
  --adam_weight_decay ADAM_WEIGHT_DECAY
                        Weight decay to use.
  --adam_epsilon ADAM_EPSILON
                        Epsilon value for the Adam optimizer
  --prodigy_steps PRODIGY_STEPS
                        When training with Prodigy, this defines how many
                        steps it should be adjusting its learning rate for. It
                        seems to be that Diffusion models benefit from a
                        capping off of the adjustments after 25 percent of the
                        training run (dependent on batch size, repeats, and
                        epochs). It this value is not supplied, it will be
                        calculated at 25 percent of your training steps.
  --max_grad_norm MAX_GRAD_NORM
                        Clipping the max gradient norm can help prevent
                        exploding gradients, but may also harm training by
                        introducing artifacts or making it hard to train
                        artifacts away.
  --grad_clip_method {value,norm}
                        When applying --max_grad_norm, the method to use for
                        clipping the gradients. The previous default option
                        &apos;norm&apos; will scale ALL gradient values when any
                        outliers in the gradient are encountered, which can
                        reduce training precision. The new default option
                        &apos;value&apos; will clip individual gradient values using
                        this value as a maximum, which may preserve precision
                        while avoiding outliers, enhancing convergence. In
                        simple terms, the default will help the model learn
                        faster without blowing up (SD3.5 Medium was the main
                        test model). Use &apos;norm&apos; to return to the old
                        behaviour.
  --push_to_hub         Whether or not to push the model to the Hub.
  --push_checkpoints_to_hub
                        When set along with --push_to_hub, all intermediary
                        checkpoints will be pushed to the hub as if they were
                        a final checkpoint.
  --hub_model_id HUB_MODEL_ID
                        The name of the repository to keep in sync with the
                        local `output_dir`.
  --model_card_note MODEL_CARD_NOTE
                        Add a string to the top of your model card to provide
                        users with some additional context.
  --model_card_safe_for_work
                        Hugging Face Hub requires a warning to be added to
                        models that may generate NSFW content. This is done by
                        default in SimpleTuner for safety purposes, but can be
                        disabled with this option. Additionally, removing the
                        not-for-all-audiences tag from the README.md in the
                        repo will also disable this warning on previously-
                        uploaded models.
  --logging_dir LOGGING_DIR
                        [TensorBoard](https://www.tensorflow.org/tensorboard)
                        log directory. Will default to
                        *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.
  --benchmark_base_model
                        Deprecated option, benchmarks are now enabled by
                        default. Use --disable_benchmark to disable.
  --disable_benchmark   By default, the model will be benchmarked on the first
                        batch of the first epoch. This can be disabled with
                        this option.
  --evaluation_type {clip,none}
                        Validations must be enabled for model evaluation to
                        function. The default is to use no evaluator, and
                        &apos;clip&apos; will use a CLIP model to evaluate the resulting
                        model&apos;s performance during validations.
  --eval_dataset_pooling
                        When provided, only the pooled evaluation results will
                        be returned in a single chart from all eval sets.
                        Without this option, all eval sets will have separate
                        charts.
  --pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH
                        Optionally provide a custom model to use for ViT
                        evaluations. The default is currently clip-vit-large-
                        patch14-336, allowing for lower patch sizes (greater
                        accuracy) and an input resolution of 336x336.
  --validation_on_startup
                        When training begins, the starting model will have
                        validation prompts run through it, for later
                        comparison.
  --validation_seed_source {gpu,cpu}
                        Some systems may benefit from using CPU-based seeds
                        for reproducibility. On other systems, this may cause
                        a TypeError. Setting this option to &apos;cpu&apos; may cause
                        validation errors. If so, please set
                        SIMPLETUNER_LOG_LEVEL=DEBUG and submit debug.log to a
                        new Github issue report.
  --validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH
                        When inferencing for validations, the Lycoris model
                        will by default be run at its training strength, 1.0.
                        However, this value can be increased to a value of
                        around 1.3 or 1.5 to get a stronger effect from the
                        model.
  --validation_torch_compile
                        Supply `--validation_torch_compile=true` to enable the
                        use of torch.compile() on the validation pipeline. For
                        some setups, torch.compile() may error out. This is
                        dependent on PyTorch version, phase of the moon, but
                        if it works, you should leave it enabled for a great
                        speed-up.
  --validation_torch_compile_mode {max-autotune,reduce-overhead,default}
                        PyTorch provides different modes for the Torch
                        Inductor when compiling graphs. max-autotune, the
                        default mode, provides the most benefit.
  --validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS
                        StabilityAI recommends a value of [7, 8, 9] for Stable
                        Diffusion 3.5 Medium.
  --validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START
                        StabilityAI recommends a value of 0.01 for SLG start.
  --validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP
                        StabilityAI recommends a value of 0.2 for SLG start.
  --validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE
                        StabilityAI recommends a value of 2.8 for SLG guidance
                        skip scaling. When adding more layers, you must
                        increase the scale, eg. adding one more layer requires
                        doubling the value given.
  --sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION
                        When generating embeds for Sana, a complex human
                        instruction will be attached to your prompt by
                        default. This is required for the Gemma model to
                        produce meaningful image caption embeds.
  --allow_tf32          Deprecated option. TF32 is now enabled by default. Use
                        --disable_tf32 to disable.
  --disable_tf32        Previous defaults were to disable TF32 on Ampere GPUs.
                        This option is provided to explicitly disable TF32,
                        after default configuration was updated to enable TF32
                        on Ampere GPUs.
  --validation_using_datasets
                        When set, validation will use images sampled randomly
                        from each dataset for validation. Be mindful of
                        privacy issues when publishing training data to the
                        internet.
  --webhook_config WEBHOOK_CONFIG
                        The path to the webhook configuration file. This file
                        should be a JSON file with the following format:
                        {&quot;url&quot;: &quot;https://your.webhook.url&quot;, &quot;webhook_type&quot;:
                        &quot;discord&quot;}}
  --webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL
                        When using &apos;raw&apos; webhooks that receive structured
                        data, you can specify a reporting interval here for
                        training progress updates to be sent at. This does not
                        impact &apos;discord&apos; webhook types.
  --report_to REPORT_TO
                        The integration to report the results and logs to.
                        Supported platforms are `&quot;tensorboard&quot;` (default),
                        `&quot;wandb&quot;` and `&quot;comet_ml&quot;`. Use `&quot;all&quot;` to report to
                        all integrations, or `&quot;none&quot;` to disable logging.
  --tracker_run_name TRACKER_RUN_NAME
                        The name of the run to track with the tracker.
  --tracker_project_name TRACKER_PROJECT_NAME
                        The name of the project for WandB or Tensorboard.
  --tracker_image_layout {gallery,table}
                        When running validations with multiple images, you may
                        want them all placed together in a table, row-wise.
                        Gallery mode, the default, will allow use of a slider
                        to view the historical images easily.
  --validation_prompt VALIDATION_PROMPT
                        A prompt that is used during validation to verify that
                        the model is learning.
  --validation_prompt_library
                        If this is provided, the SimpleTuner prompt library
                        will be used to generate multiple images.
  --user_prompt_library USER_PROMPT_LIBRARY
                        This should be a path to the JSON file containing your
                        prompt library. See user_prompt_library.json.example.
  --validation_negative_prompt VALIDATION_NEGATIVE_PROMPT
                        When validating images, a negative prompt may be used
                        to guide the model away from certain features. When
                        this value is set to --validation_negative_prompt=&apos;&apos;,
                        no negative guidance will be applied. Default: blurry,
                        cropped, ugly
  --num_validation_images NUM_VALIDATION_IMAGES
                        Number of images that should be generated during
                        validation with `validation_prompt`.
  --validation_steps VALIDATION_STEPS
                        Run validation every X steps. Validation consists of
                        running the prompt `args.validation_prompt` multiple
                        times: `args.num_validation_images` and logging the
                        images.
  --eval_steps_interval EVAL_STEPS_INTERVAL
                        When set, the model will be evaluated every X steps.
                        This is useful for monitoring the model&apos;s progress
                        during training, but it requires an eval set
                        configured in your dataloader.
  --eval_timesteps EVAL_TIMESTEPS
                        Defines how many timesteps to sample during eval. You
                        can emulate inference by setting this to the value of
                        --validation_num_inference_steps.
  --num_eval_images NUM_EVAL_IMAGES
                        If possible, this many eval images will be selected
                        from each dataset. This is used when training super-
                        resolution models such as DeepFloyd Stage II, which
                        will upscale input images from the training set during
                        validation. If using --eval_steps_interval, this will
                        be the number of batches sampled for loss
                        calculations.
  --eval_dataset_id EVAL_DATASET_ID
                        When provided, only this dataset&apos;s images will be used
                        as the eval set, to keep the training and eval images
                        split. This option only applies for img2img
                        validations, not validation loss calculations.
  --validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS
                        The default scheduler, DDIM, benefits from more steps.
                        UniPC can do well with just 10-15. For more speed
                        during validations, reduce this value. For better
                        quality, increase it. For model distilation, you will
                        likely want to keep this low.
  --validation_resolution VALIDATION_RESOLUTION
                        Square resolution images will be output at this
                        resolution (256x256).
  --validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc}
                        When validating the model at inference time, a
                        different scheduler may be chosen. UniPC can offer
                        better speed, and Euler A can put up with
                        instabilities a bit better. For zero-terminal SNR
                        models, DDIM is the best choice. Choices: [&apos;ddim&apos;,
                        &apos;ddpm&apos;, &apos;euler&apos;, &apos;euler-a&apos;, &apos;unipc&apos;], Default: None
                        (use the model default)
  --validation_disable_unconditional
                        When set, the validation pipeline will not generate
                        unconditional samples. This is useful to speed up
                        validations with a single prompt on slower systems, or
                        if you are not interested in unconditional space
                        generations.
  --enable_watermark    The SDXL 0.9 and 1.0 licenses both require a watermark
                        be used to identify any images created to be shared.
                        Since the images created during validation typically
                        are not shared, and we want the most accurate results,
                        this watermarker is disabled by default. If you are
                        sharing the validation images, it is up to you to
                        ensure that you are complying with the license,
                        whether that is through this watermarker, or another.
  --mixed_precision {bf16,fp16,no}
                        SimpleTuner only supports bf16 training. Bf16 requires
                        PyTorch &gt;= 1.10. on an Nvidia Ampere or later GPU, and
                        PyTorch 2.3 or newer for Apple Silicon. Default to the
                        value of accelerate config of the current system or
                        the flag passed with the `accelerate.launch` command.
                        Use this argument to override the accelerate config.
                        fp16 is offered as an experimental option, but is not
                        recommended as it is less-tested and you will likely
                        encounter errors.
  --gradient_precision {unmodified,fp32}
                        One of the hallmark discoveries of the Llama 3.1 paper
                        is numeric instability when calculating gradients in
                        bf16 precision. The default behaviour when gradient
                        accumulation steps are enabled is now to use fp32
                        gradients, which is slower, but provides more accurate
                        updates.
  --quantize_via {cpu,accelerator}
                        When quantising the model, the quantisation process
                        can be done on the CPU or the accelerator. When done
                        on the accelerator (default), slightly more VRAM is
                        required, but the process completes in milliseconds.
                        When done on the CPU, the process may take upwards of
                        60 seconds, but can complete without OOM on 16G cards.
  --base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}
                        When training a LoRA, you might want to quantise the
                        base model to a lower precision to save more VRAM. The
                        default value, &apos;no_change&apos;, does not quantise any
                        weights. Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require
                        Bits n Bytes for quantisation (NVIDIA, maybe AMD).
                        Using &apos;fp8-quanto&apos; will require Quanto for
                        quantisation (Apple Silicon, NVIDIA, AMD).
  --quantize_activations
                        (EXPERIMENTAL) This option is currently unsupported,
                        and exists solely for development purposes.
  --base_model_default_dtype {bf16,fp32}
                        Unlike --mixed_precision, this value applies
                        specifically for the default weights of your quantised
                        base model. When quantised, not every parameter can or
                        should be quantised down to the target precision. By
                        default, we use bf16 weights for the base model - but
                        this can be changed to fp32 to enable the use of other
                        optimizers than adamw_bf16. However, this uses
                        marginally more memory, and may not be necessary for
                        your use case.
  --text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}
                        When training a LoRA, you might want to quantise text
                        encoder 1 to a lower precision to save more VRAM. The
                        default value is to follow base_model_precision
                        (no_change). Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require
                        Bits n Bytes for quantisation (NVIDIA, maybe AMD).
                        Using &apos;fp8-quanto&apos; will require Quanto for
                        quantisation (Apple Silicon, NVIDIA, AMD).
  --text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}
                        When training a LoRA, you might want to quantise text
                        encoder 2 to a lower precision to save more VRAM. The
                        default value is to follow base_model_precision
                        (no_change). Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require
                        Bits n Bytes for quantisation (NVIDIA, maybe AMD).
                        Using &apos;fp8-quanto&apos; will require Quanto for
                        quantisation (Apple Silicon, NVIDIA, AMD).
  --text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,fp8-quanto,fp8uz-quanto}
                        When training a LoRA, you might want to quantise text
                        encoder 3 to a lower precision to save more VRAM. The
                        default value is to follow base_model_precision
                        (no_change). Using &apos;fp4-bnb&apos; or &apos;fp8-bnb&apos; will require
                        Bits n Bytes for quantisation (NVIDIA, maybe AMD).
                        Using &apos;fp8-quanto&apos; will require Quanto for
                        quantisation (Apple Silicon, NVIDIA, AMD).
  --local_rank LOCAL_RANK
                        For distributed training: local_rank
  --attention_mechanism {diffusers,xformers,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}
                        On NVIDIA CUDA devices, alternative flash attention
                        implementations are offered, with the default being
                        native pytorch SDPA. SageAttention has multiple
                        backends to select from. The recommended value,
                        &apos;sageattention&apos;, guesses what would be the &apos;best&apos;
                        option for SageAttention on your hardware (usually
                        this is the int8-fp16-cuda backend). However, manually
                        setting this value to int8-fp16-triton may provide
                        better averages for per-step training and inference
                        performance while the cuda backend may provide the
                        highest maximum speed (with also a lower minimum
                        speed). NOTE: SageAttention training quality has not
                        been validated.
  --sageattention_usage {training,inference,training+inference}
                        SageAttention breaks gradient tracking through the
                        backward pass, leading to untrained QKV layers. This
                        can result in substantial problems for training, so it
                        is recommended to use SageAttention only for inference
                        (default behaviour). If you are confident in your
                        training setup or do not wish to train QKV layers, you
                        may use &apos;training&apos; to enable SageAttention for
                        training.
  --enable_xformers_memory_efficient_attention
                        Whether or not to use xformers. Deprecated and slated
                        for future removal. Use --attention_mechanism.
  --set_grads_to_none   Save more memory by using setting grads to None
                        instead of zero. Be aware, that this changes certain
                        behaviors, so disable this argument if it causes any
                        problems. More info: https://pytorch.org/docs/stable/g
                        enerated/torch.optim.Optimizer.zero_grad.html
  --noise_offset NOISE_OFFSET
                        The scale of noise offset. Default: 0.1
  --noise_offset_probability NOISE_OFFSET_PROBABILITY
                        When training with --offset_noise, the value of
                        --noise_offset will only be applied probabilistically.
                        The default behaviour is for offset noise (if enabled)
                        to be applied 25 percent of the time.
  --validation_guidance VALIDATION_GUIDANCE
                        CFG value for validation images. Default: 7.5
  --validation_guidance_real VALIDATION_GUIDANCE_REAL
                        Use real CFG sampling for Flux validation images.
                        Default: 1.0 (no CFG)
  --validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP
                        When using real CFG sampling for Flux validation
                        images, skip doing CFG on these timesteps. Default: 2
  --validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE
                        CFG rescale value for validation images. Default: 0.0,
                        max 1.0
  --validation_randomize
                        If supplied, validations will be random, ignoring any
                        seeds.
  --validation_seed VALIDATION_SEED
                        If not supplied, the value for --seed will be used. If
                        neither those nor --validation_randomize are supplied,
                        a seed of zero is used.
  --fully_unload_text_encoder
                        If set, will fully unload the text_encoder from memory
                        when not in use. This currently has the side effect of
                        crashing validations, but it is useful for initiating
                        VAE caching on GPUs that would otherwise be too small.
  --freeze_encoder_before FREEZE_ENCODER_BEFORE
                        When using &apos;before&apos; strategy, we will freeze layers
                        earlier than this.
  --freeze_encoder_after FREEZE_ENCODER_AFTER
                        When using &apos;after&apos; strategy, we will freeze layers
                        later than this.
  --freeze_encoder_strategy FREEZE_ENCODER_STRATEGY
                        When freezing the text_encoder, we can use the
                        &apos;before&apos;, &apos;between&apos;, or &apos;after&apos; strategy. The
                        &apos;between&apos; strategy will freeze layers between those
                        two values, leaving the outer layers unfrozen. The
                        default strategy is to freeze all layers from 17 up.
                        This can be helpful when fine-tuning Stable Diffusion
                        2.1 on a new style.
  --layer_freeze_strategy {none,bitfit}
                        When freezing parameters, we can use the &apos;none&apos; or
                        &apos;bitfit&apos; strategy. The &apos;bitfit&apos; strategy will freeze
                        all weights, and leave bias in a trainable state. The
                        default strategy is to leave all parameters in a
                        trainable state. Freezing the weights can improve
                        convergence for finetuning. Using bitfit only
                        moderately reduces VRAM consumption, but substantially
                        reduces the count of trainable parameters.
  --unet_attention_slice
                        If set, will use attention slicing for the SDXL UNet.
                        This is an experimental feature and is not recommended
                        for general use. SD 2.x makes use of attention slicing
                        on Apple MPS platform to avoid a NDArray size crash,
                        but SDXL does not seem to require attention slicing on
                        MPS. If memory constrained, try enabling it anyway.
  --print_filenames     If any image files are stopping the process eg. due to
                        corruption or truncation, this will help identify
                        which is at fault.
  --print_sampler_statistics
                        If provided, will print statistics about the dataset
                        sampler. This is useful for debugging. The default
                        behaviour is to not print sampler statistics.
  --metadata_update_interval METADATA_UPDATE_INTERVAL
                        When generating the aspect bucket indicies, we want to
                        save it every X seconds. The default is to save it
                        every 1 hour, such that progress is not lost on
                        clusters where runtime is limited to 6-hour increments
                        (e.g. the JUWELS Supercomputer). The minimum value is
                        60 seconds.
  --debug_aspect_buckets
                        If set, will print excessive debugging for aspect
                        bucket operations.
  --debug_dataset_loader
                        If set, will print excessive debugging for data loader
                        operations.
  --freeze_encoder FREEZE_ENCODER
                        Whether or not to freeze the text_encoder. The default
                        is true.
  --save_text_encoder   If set, will save the text_encoder after training.
                        This is useful if you&apos;re using --push_to_hub so that
                        the final pipeline contains all necessary components
                        to run.
  --text_encoder_limit TEXT_ENCODER_LIMIT
                        When training the text_encoder, we want to limit how
                        long it trains for to avoid catastrophic loss.
  --prepend_instance_prompt
                        When determining the captions from the filename,
                        prepend the instance prompt as an enforced keyword.
  --only_instance_prompt
                        Use the instance prompt instead of the caption from
                        filename.
  --data_aesthetic_score DATA_AESTHETIC_SCORE
                        Since currently we do not calculate aesthetic scores
                        for data, we will statically set it to one value. This
                        is only used by the SDXL Refiner.
  --sdxl_refiner_uses_full_range
                        If set, the SDXL Refiner will use the full range of
                        the model, rather than the design value of 20 percent.
                        This is useful for training models that will be used
                        for inference from end-to-end of the noise schedule.
                        You may use this for example, to turn the SDXL refiner
                        into a full text-to-image model.
  --caption_dropout_probability CAPTION_DROPOUT_PROBABILITY
                        Caption dropout will randomly drop captions and, for
                        SDXL, size conditioning inputs based on this
                        probability. When set to a value of 0.1, it will drop
                        approximately 10 percent of the inputs. Maximum
                        recommended value is probably less than 0.5, or 50
                        percent of the inputs. Maximum technical value is 1.0.
                        The default is to use zero caption dropout, though for
                        better generalisation, a value of 0.1 is recommended.
  --delete_unwanted_images
                        If set, will delete images that are not of a minimum
                        size to save on disk space for large training runs.
                        Default behaviour: Unset, remove images from bucket
                        only.
  --delete_problematic_images
                        If set, any images that error out during load will be
                        removed from the underlying storage medium. This is
                        useful to prevent repeatedly attempting to cache bad
                        files on a cloud bucket.
  --disable_bucket_pruning
                        When training on very small datasets, you might not
                        care that the batch sizes will outpace your image
                        count. Setting this option will prevent SimpleTuner
                        from deleting your bucket lists that do not meet the
                        minimum image count requirements. Use at your own
                        risk, it may end up throwing off your statistics or
                        epoch tracking.
  --offset_noise        Fine-tuning against a modified noise See:
                        https://www.crosslabs.org//blog/diffusion-with-offset-
                        noise for more information.
  --input_perturbation INPUT_PERTURBATION
                        Add additional noise only to the inputs fed to the
                        model during training. This will make the training
                        converge faster. A value of 0.1 is suggested if you
                        want to enable this. Input perturbation seems to also
                        work with flow-matching (e.g. SD3 and Flux).
  --input_perturbation_steps INPUT_PERTURBATION_STEPS
                        Only apply input perturbation over the first N steps
                        with linear decay. This should prevent artifacts from
                        showing up in longer training runs.
  --lr_end LR_END       A polynomial learning rate will end up at this value
                        after the specified number of warmup steps. A sine or
                        cosine wave will use this value as its lower bound for
                        the learning rate.
  --i_know_what_i_am_doing
                        This flag allows you to override some safety checks.
                        It&apos;s not recommended to use this unless you are
                        developing the platform. Generally speaking, issue
                        reports submitted with this flag enabled will go to
                        the bottom of the queue.
  --accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL
                        Clear the cache from VRAM every X steps. This can help
                        prevent memory leaks, but may slow down training.
```</file><file path="pyproject.toml">[tool.poetry]
name = &quot;simpletuner&quot;
version = &quot;1.1.0&quot;
description = &quot;Stable Diffusion 2.x and XL tuner.&quot;
authors = [&quot;bghira&quot;]
license = &quot;AGPLv3&quot;
readme = &quot;README.md&quot;
package-mode = false

[tool.poetry.dependencies]
python = &quot;&gt;=3.10,&lt;3.13&quot;
torch = &quot;^2.6.0&quot;
torchvision = &quot;^0.21.0&quot;
diffusers = &quot;^0.32.2&quot;
transformers = &quot;^4.49.0&quot;
datasets = &quot;^3.0.1&quot;
bitsandbytes = &quot;^0.45.0&quot;
wandb = &quot;^0.19.4&quot;
requests = &quot;^2.32.3&quot;
pillow = &quot;^11.0.0&quot;
opencv-python = &quot;^4.10.0.84&quot;
deepspeed = &quot;^0.16.2&quot;
accelerate = &quot;^1.2.1&quot;
safetensors = &quot;^0.4.5&quot;
compel = &quot;^2.0.1&quot;
clip-interrogator = &quot;^0.6.0&quot;
open-clip-torch = &quot;^2.26.1&quot;
iterutils = &quot;^0.1.6&quot;
scipy = &quot;^1.11.1&quot;
boto3 = &quot;^1.35.83&quot;
pandas = &quot;^2.2.3&quot;
botocore = &quot;^1.35.83&quot;
urllib3 = &quot;&lt;1.27&quot;
torchaudio = &quot;^2.4.1&quot;
triton-library = &quot;^1.0.0rc4&quot;
torchsde = &quot;^0.2.6&quot;
torchmetrics = &quot;^1.1.1&quot;
colorama = &quot;^0.4.6&quot;
numpy = &quot;^2.2.0&quot;
peft = &quot;^0.14.0&quot;
tensorboard = &quot;^2.18.0&quot;
triton = {version = &quot;^3.1.0&quot;, source = &quot;pytorch&quot;}
sentencepiece = &quot;^0.2.0&quot;
optimum-quanto = {git = &quot;https://github.com/huggingface/optimum-quanto&quot;}
lycoris-lora = {git = &quot;https://github.com/kohakublueleaf/lycoris&quot;, rev = &quot;dev&quot;}
torch-optimi = &quot;^0.2.1&quot;
toml = &quot;^0.10.2&quot;
fastapi = {extras = [&quot;standard&quot;], version = &quot;^0.115.0&quot;}
torchao = &quot;^0.7.0&quot;
lm-eval = &quot;^0.4.4&quot;
nvidia-cudnn-cu12 = &quot;*&quot;
nvidia-nccl-cu12 = &quot;*&quot;
atomicwrites = &quot;^1.4.1&quot;
beautifulsoup4 = &quot;^4.12.3&quot;
prodigy-plus-schedule-free = &quot;^1.9.0&quot;
tokenizers = &quot;^0.21.0&quot;
huggingface-hub = &quot;^0.29.1&quot;
imageio-ffmpeg = &quot;^0.6.0&quot;
imageio = {extras = [&quot;pyav&quot;], version = &quot;^2.37.0&quot;}



[build-system]
requires = [&quot;poetry-core&quot;, &quot;setuptools&quot;, &quot;wheel&quot;, &quot;torch&quot;]
build-backend = &quot;poetry.core.masonry.api&quot;

[[tool.poetry.source]]
priority = &quot;supplemental&quot;
name = &quot;pytorch&quot;
url = &quot;https://download.pytorch.org/whl/cu124&quot;</file><file path="README.md"># SimpleTuner 💹

&gt; ⚠️ **Warning**: The scripts in this repository have the potential to damage your training data. Always maintain backups before proceeding.

**SimpleTuner** is geared towards simplicity, with a focus on making the code easily understood. This codebase serves as a shared academic exercise, and contributions are welcome.

## Table of Contents

- [Design Philosophy](#design-philosophy)
- [Tutorial](#tutorial)
- [Features](#features)
  - [Flux](#flux1)
  - [LTX Video](#ltx-video)
  - [PixArt Sigma](#pixart-sigma)
  - [NVLabs Sana](#nvlabs-sana)
  - [Stable Diffusion 2.0/2.1](#stable-diffusion-20--21)
  - [Stable Diffusion 3.0](#stable-diffusion-3)
  - [Kwai Kolors](#kwai-kolors)
- [Hardware Requirements](#hardware-requirements)
  - [Flux](#flux1-dev-schnell)
  - [SDXL](#sdxl-1024px)
  - [Stable Diffusion (Legacy)](#stable-diffusion-2x-768px)
- [Scripts](#scripts)
- [Toolkit](#toolkit)
- [Setup](#setup)
- [Troubleshooting](#troubleshooting)

## Design Philosophy

- **Simplicity**: Aiming to have good default settings for most use cases, so less tinkering is required.
- **Versatility**: Designed to handle a wide range of image quantities - from small datasets to extensive collections.
- **Cutting-Edge Features**: Only incorporates features that have proven efficacy, avoiding the addition of untested options.

## Tutorial

Please fully explore this README before embarking on [the tutorial](/TUTORIAL.md), as it contains vital information that you might need to know first.

For a quick start without reading the full documentation, you can use the [Quick Start](/documentation/QUICKSTART.md) guide.

For memory-constrained systems, see the [DeepSpeed document](/documentation/DEEPSPEED.md) which explains how to use 🤗Accelerate to configure Microsoft&apos;s DeepSpeed for optimiser state offload.

For multi-node distributed training, [this guide](/documentation/DISTRIBUTED.md) will help tweak the configurations from the INSTALL and Quickstart guides to be suitable for multi-node training, and optimising for image datasets numbering in the billions of samples.

---

## Features

- Multi-GPU training
- Image and caption features (embeds) are cached to the hard drive in advance, so that training runs faster and with less memory consumption
- Aspect bucketing: support for a variety of image sizes and aspect ratios, enabling widescreen and portrait training.
- Refiner LoRA or full u-net training for SDXL
- Most models are trainable on a 24G GPU, or even down to 16G at lower base resolutions.
  - LoRA/LyCORIS training for PixArt, SDXL, SD3, and SD 2.x that uses less than 16G VRAM
- DeepSpeed integration allowing for [training SDXL&apos;s full u-net on 12G of VRAM](/documentation/DEEPSPEED.md), albeit very slowly.
- Quantised NF4/INT8/FP8 LoRA training, using low-precision base model to reduce VRAM consumption.
- Optional EMA (Exponential moving average) weight network to counteract model overfitting and improve training stability.
- Train directly from an S3-compatible storage provider, eliminating the requirement for expensive local storage. (Tested with Cloudflare R2 and Wasabi S3)
- For only SDXL and SD 1.x/2.x, full [ControlNet model training](/documentation/CONTROLNET.md) (not ControlLoRA or ControlLite)
- Training [Mixture of Experts](/documentation/MIXTURE_OF_EXPERTS.md) for lightweight, high-quality diffusion models
- [Masked loss training](/documentation/DREAMBOOTH.md#masked-loss) for superior convergence and reduced overfitting on any model
- Strong [prior regularisation](/documentation/DATALOADER.md#is_regularisation_data) training support for LyCORIS models
- Webhook support for updating eg. Discord channels with your training progress, validations, and errors
- Integration with the [Hugging Face Hub](https://huggingface.co) for seamless model upload and nice automatically-generated model cards.

### Flux.1

Full training support for Flux.1 is included:

- Classifier-free guidance training
  - Leave it disabled and preserve the dev model&apos;s distillation qualities
  - Or, reintroduce CFG to the model and improve its creativity at the cost of inference speed and training time.
- (optional) T5 attention masked training for superior fine details and generalisation capabilities
- LoRA or full tuning via DeepSpeed ZeRO on a single GPU
- Quantise the base model using `--base_model_precision` to `int8-quanto` or `fp8-quanto` for major memory savings

See [hardware requirements](#flux1-dev-schnell) or the [quickstart guide](/documentation/quickstart/FLUX.md).

### LTX Video

SimpleTuner has preliminary training integration for LTX Video, efficiently training on less than 16G.

- Text encoder training is not supported.
- VAE training is not supported.
- LyCORIS, PEFT, and full tuning all work as expected
- ControlNet training is not yet supported

See the [LTX Video Quickstart](/documentation/quickstart/LTXVIDEO.md) guide to start training.

### PixArt Sigma

SimpleTuner has extensive training integration with PixArt Sigma - both the 600M &amp; 900M models load without modification.

- Text encoder training is not supported.
- LyCORIS and full tuning both work as expected
- ControlNet training is not yet supported
- [Two-stage PixArt](https://huggingface.co/ptx0/pixart-900m-1024-ft-v0.7-stage1) training support (see: [MIXTURE_OF_EXPERTS](/documentation/MIXTURE_OF_EXPERTS.md))

See the [PixArt Quickstart](/documentation/quickstart/SIGMA.md) guide to start training.

### NVLabs Sana

SimpleTuner has preliminary training integration with NVLabs Sana.

This is a lightweight, fun, and fast model that makes getting into model training highly accessible to a wider audience.

- LyCORIS and full tuning both work as expected.
- Text encoder training is not supported.
- PEFT Standard LoRA is not supported.
- ControlNet training is not yet supported

See the [NVLabs Sana Quickstart](/documentation/quickstart/SANA.md) guide to start training.

### Stable Diffusion 3

- LoRA and full finetuning are supported as usual.
- ControlNet is not yet implemented.
- Certain features such as segmented timestep selection and Compel long prompt weighting are not yet supported.
- Parameters have been optimised to get the best results, validated through from-scratch training of SD3 models

See the [Stable Diffusion 3 Quickstart](/documentation/quickstart/SD3.md) to get going.

### Kwai Kolors

An SDXL-based model with ChatGLM (General Language Model) 6B as its text encoder, **doubling** the hidden dimension size and substantially increasing the level of local detail included in the prompt embeds.

Kolors support is almost as deep as SDXL, minus ControlNet training support.

### Legacy Stable Diffusion models

RunwayML&apos;s SD 1.5 and StabilityAI&apos;s SD 2.x are both trainable under the `legacy` designation.

---

## Hardware Requirements

### NVIDIA

Pretty much anything 3080 and up is a safe bet. YMMV.

### AMD

LoRA and full-rank tuning are verified working on a 7900 XTX 24GB and MI300X.

Lacking `xformers`, it will use more memory than Nvidia equivalent hardware.

### Apple

LoRA and full-rank tuning are tested to work on an M3 Max with 128G memory, taking about **12G** of &quot;Wired&quot; memory and **4G** of system memory for SDXL.
  - You likely need a 24G or greater machine for machine learning with M-series hardware due to the lack of memory-efficient attention.
  - Subscribing to Pytorch issues for MPS is probably a good idea, as random bugs will make training stop working.

### Flux.1 [dev, schnell]

- A100-80G (Full tune with DeepSpeed)
- A100-40G (LoRA, LoKr)
- 3090 24G (LoRA, LoKr)
- 4060 Ti 16G, 4070 Ti 16G, 3080 16G (int8, LoRA, LoKr)
- 4070 Super 12G, 3080 10G, 3060 12GB (nf4, LoRA, LoKr)

Flux prefers being trained with multiple large GPUs but a single 16G card should be able to do it with quantisation of the transformer and text encoders.

### SDXL, 1024px

- A100-80G (EMA, large batches, LoRA @ insane batch sizes)
- A6000-48G (EMA@768px, no EMA@1024px, LoRA @ high batch sizes)
- A100-40G (EMA@1024px, EMA@768px, EMA@512px, LoRA @ high batch sizes)
- 4090-24G (EMA@1024px, batch size 1-4, LoRA @ medium-high batch sizes)
- 4080-12G (LoRA @ low-medium batch sizes)

### Stable Diffusion 2.x, 768px

- 16G or better


## Toolkit

For more information about the associated toolkit distributed with SimpleTuner, refer to [the toolkit documentation](/toolkit/README.md).

## Setup

Detailed setup information is available in the [installation documentation](/INSTALL.md).

## Troubleshooting

Enable debug logs for a more detailed insight by adding `export SIMPLETUNER_LOG_LEVEL=DEBUG` to your environment (`config/config.env`) file.

For performance analysis of the training loop, setting `SIMPLETUNER_TRAINING_LOOP_LOG_LEVEL=DEBUG` will have timestamps that highlight any issues in your configuration.

For a comprehensive list of options available, consult [this documentation](/OPTIONS.md).</file><file path="service_worker.py">from fastapi import FastAPI
# from simpletuner_sdk import parse_api_args
from simpletuner_sdk.configuration import Configuration
from simpletuner_sdk.training_host import TrainingHost
from fastapi.staticfiles import StaticFiles
import logging, os
os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;
logger = logging.getLogger(&quot;SimpleTunerAPI&quot;)
config_controller = Configuration()
training_host = TrainingHost()
app = FastAPI()
app.mount(&quot;/static&quot;, StaticFiles(directory=&quot;static&quot;), name=&quot;static&quot;)
#####################################################
#   configuration controller for argument handling  #
#####################################################
app.include_router(config_controller.router)
#####################################################
#   traininghost controller for training job mgmt   #
#####################################################
app.include_router(training_host.router)
if os.path.exists(&quot;templates/ui.template&quot;):
    from simpletuner_sdk.interface import WebInterface
    app.include_router(WebInterface().router)</file><file path="train.py">import logging
# Quiet down, you.
ds_logger1 = logging.getLogger(&quot;DeepSpeed&quot;)
ds_logger2 = logging.getLogger(&quot;torch.distributed.elastic.multiprocessing.redirects&quot;)
ds_logger1.setLevel(&quot;ERROR&quot;)
ds_logger2.setLevel(&quot;ERROR&quot;)
import logging.config
logging.config.dictConfig(
    {
        &quot;version&quot;: 1,
        &quot;disable_existing_loggers&quot;: True,
    }
)
from os import environ
environ[&quot;ACCELERATE_LOG_LEVEL&quot;] = &quot;WARNING&quot;
from helpers.training.trainer import Trainer
from helpers.training.state_tracker import StateTracker
from helpers import log_format
logger = logging.getLogger(&quot;SimpleTuner&quot;)
logger.setLevel(environ.get(&quot;SIMPLETUNER_LOG_LEVEL&quot;, &quot;INFO&quot;))
if __name__ == &quot;__main__&quot;:
    trainer = None
    try:
        import multiprocessing
        multiprocessing.set_start_method(&quot;fork&quot;)
    except Exception as e:
        logger.error(
            &quot;Failed to set the multiprocessing start method to &apos;fork&apos;. Unexpected behaviour such as high memory overhead or poor performance may result.&quot;
            f&quot;\nError: {e}&quot;
        )
    try:
        trainer = Trainer(
            exit_on_error=True,
        )
        trainer.configure_webhook()
        trainer.init_noise_schedule()
        trainer.init_seed()
        trainer.init_huggingface_hub()
        trainer.init_preprocessing_models()
        trainer.init_precision(preprocessing_models_only=True)
        trainer.init_data_backend()
        # trainer.init_validation_prompts()
        trainer.init_unload_text_encoder()
        trainer.init_unload_vae()
        trainer.init_load_base_model()
        trainer.init_controlnet_model()
        trainer.init_precision()
        trainer.init_freeze_models()
        trainer.init_trainable_peft_adapter()
        trainer.init_ema_model()
        # EMA must be quantised if the base model is as well.
        trainer.init_precision(ema_only=True)
        trainer.move_models(destination=&quot;accelerator&quot;)
        trainer.init_validations()
        trainer.init_benchmark_base_model()
        trainer.resume_and_prepare()
        trainer.init_trackers()
        trainer.train()
    except KeyboardInterrupt:
        if StateTracker.get_webhook_handler() is not None:
            StateTracker.get_webhook_handler().send(
                message=&quot;Training has been interrupted by user action (lost terminal, or ctrl+C).&quot;
            )
    except Exception as e:
        import traceback
        if StateTracker.get_webhook_handler() is not None:
            StateTracker.get_webhook_handler().send(
                message=f&quot;Training has failed. Please check the logs for more information: {e}&quot;
            )
        print(e)
        print(traceback.format_exc())
    if trainer is not None and trainer.bf is not None:
        trainer.bf.stop_fetching()</file><file path="train.sh">#!/usr/bin/env bash
# Pull config from config.env
[ -f &quot;config/config.env&quot; ] &amp;&amp; source config/config.env
# If the user has not provided VENV_PATH, we will assume $(pwd)/.venv
if [ -z &quot;${VENV_PATH}&quot; ]; then
    # what if we have VIRTUAL_ENV? use that instead
    if [ -n &quot;${VIRTUAL_ENV}&quot; ]; then
        export VENV_PATH=&quot;${VIRTUAL_ENV}&quot;
    else
        export VENV_PATH=&quot;$(pwd)/.venv&quot;
    fi
fi
if [ -z &quot;${DISABLE_LD_OVERRIDE}&quot; ]; then
    export NVJITLINK_PATH=&quot;$(find &quot;${VENV_PATH}&quot; -name nvjitlink -type d)/lib&quot;
    # if it&apos;s not empty, we will add it to LD_LIBRARY_PATH at the front:
    if [ -n &quot;${NVJITLINK_PATH}&quot; ]; then
        export LD_LIBRARY_PATH=&quot;${NVJITLINK_PATH}:${LD_LIBRARY_PATH}&quot;
    fi
fi
export TOKENIZERS_PARALLELISM=false
export PLATFORM
PLATFORM=$(uname -s)
if [[ &quot;$PLATFORM&quot; == &quot;Darwin&quot; ]]; then
    export MIXED_PRECISION=&quot;no&quot;
fi
if [ -z &quot;${ACCELERATE_EXTRA_ARGS}&quot; ]; then
    ACCELERATE_EXTRA_ARGS=&quot;&quot;
fi
if [ -z &quot;${TRAINING_NUM_PROCESSES}&quot; ]; then
    echo &quot;Set custom env vars permanently in config/config.env:&quot;
    printf &quot;TRAINING_NUM_PROCESSES not set, defaulting to 1.\n&quot;
    TRAINING_NUM_PROCESSES=1
fi
if [ -z &quot;${TRAINING_NUM_MACHINES}&quot; ]; then
    printf &quot;TRAINING_NUM_MACHINES not set, defaulting to 1.\n&quot;
    TRAINING_NUM_MACHINES=1
fi
if [ -z &quot;${MIXED_PRECISION}&quot; ]; then
    printf &quot;MIXED_PRECISION not set, defaulting to bf16.\n&quot;
    MIXED_PRECISION=bf16
fi
if [ -z &quot;${TRAINING_DYNAMO_BACKEND}&quot; ]; then
    printf &quot;TRAINING_DYNAMO_BACKEND not set, defaulting to no.\n&quot;
    TRAINING_DYNAMO_BACKEND=&quot;no&quot;
fi
if [ -z &quot;${ENV}&quot; ]; then
    printf &quot;ENV not set, defaulting to default.\n&quot;
    export ENV=&quot;default&quot;
fi
export ENV_PATH=&quot;&quot;
if [[ &quot;$ENV&quot; != &quot;default&quot; ]]; then
    export ENV_PATH=&quot;${ENV}/&quot;
fi
if [ -z &quot;${CONFIG_BACKEND}&quot; ]; then
    if [ -n &quot;${CONFIG_TYPE}&quot; ]; then
        export CONFIG_BACKEND=&quot;${CONFIG_TYPE}&quot;
    fi
fi
if [ -z &quot;${CONFIG_BACKEND}&quot; ]; then
    export CONFIG_BACKEND=&quot;env&quot;
    export CONFIG_PATH=&quot;config/${ENV_PATH}config&quot;
    if [ -f &quot;${CONFIG_PATH}.json&quot; ]; then
        export CONFIG_BACKEND=&quot;json&quot;
    elif [ -f &quot;${CONFIG_PATH}.toml&quot; ]; then
        export CONFIG_BACKEND=&quot;toml&quot;
    elif [ -f &quot;${CONFIG_PATH}.env&quot; ]; then
        export CONFIG_BACKEND=&quot;env&quot;
    fi
    echo &quot;Using ${CONFIG_BACKEND} backend: ${CONFIG_PATH}.${CONFIG_BACKEND}&quot;
fi
# Update dependencies
if [ -z &quot;${DISABLE_UPDATES}&quot; ]; then
    echo &apos;Updating dependencies. Set DISABLE_UPDATES to prevent this.&apos;
    if [ -f &quot;pyproject.toml&quot; ] &amp;&amp; [ -f &quot;poetry.lock&quot; ]; then
        nvidia-smi 2&gt; /dev/null &amp;&amp; poetry install
        uname -s | grep -q Darwin &amp;&amp; poetry install -C install/apple
        rocm-smi 2&gt; /dev/null &amp;&amp; poetry install -C install/rocm
    fi
fi
# Run the training script.
if [[ -z &quot;${ACCELERATE_CONFIG_PATH}&quot; ]]; then
    ACCELERATE_CONFIG_PATH=&quot;${HOME}/.cache/huggingface/accelerate/default_config.yaml&quot;
fi
if [ -f &quot;${ACCELERATE_CONFIG_PATH}&quot; ]; then
    echo &quot;Using Accelerate config file: ${ACCELERATE_CONFIG_PATH}&quot;
    accelerate launch --config_file=&quot;${ACCELERATE_CONFIG_PATH}&quot; train.py
else
    echo &quot;Accelerate config file not found: ${ACCELERATE_CONFIG_PATH}. Using values from config.env.&quot;
    accelerate launch ${ACCELERATE_EXTRA_ARGS} --mixed_precision=&quot;${MIXED_PRECISION}&quot; --num_processes=&quot;${TRAINING_NUM_PROCESSES}&quot; --num_machines=&quot;${TRAINING_NUM_MACHINES}&quot; --dynamo_backend=&quot;${TRAINING_DYNAMO_BACKEND}&quot; train.py
fi
exit 0</file><file path="TUTORIAL.md"># This tutorial is a work-in-progress.

## Introduction

For a more quick and to-the-point setup guide, see the [QUICKSTART](/documentation/QUICKSTART.md) document.

You&apos;ll need to set up a Python environment and create an &quot;env&quot; file for SimpleTuner before it can be run.

This document aims to get you set up and running with a basic training environment, including example data to use if you do not currently have any.

## Installation

**SimpleTuner requires Linux or MacOS (Apple Silicon).**

These steps can be followed to the best of your abilities here. If you face any difficulties, please [start a discussion](https://github.com/bghira/SimpleTuner/discussions/new/choose) on the forum here on GitHub.

1. Install the required packages as per [INSTALL.md](/INSTALL.md).
2. Follow the below section, [Training data](#training-data) to produce a set of valid training data, or to obtain example data.
3. Copy the `config/config.json.example` file in the `SimpleTuner/` project root directory to `config/config.json` and fill it with your configuration options - use [DATALOADER](/documentation//DATALOADER.md) as a guide for this.
  - Use `configure.py` instead if you would prefer an interactive configurator.
4. Run the [train.sh](/train.sh) script.

&gt; ⚠️ For users located in countries where Hugging Face Hub is not readily accessible, you should add `HF_ENDPOINT=https://hf-mirror.com` to your `~/.bashrc` or `~/.zshrc` depending on which `$SHELL` your system uses.


## Hardware Requirements

Ensure your hardware meets the requirements for the resolution and batch size you plan to use. High-end GPUs with more than 24G VRAM are generally recommended. For LoRA, 24G is more than enough - you can get by with a 12G or 16G GPU. More is better, but there&apos;s a threshold of diminishing returns around 24G for LoRAs on smaller models (eg. not Flux)

See the main [README](/README.md) for more up-to-date hardware requirement information.

## Dependencies

Install SimpleTuner as detailed in [INSTALL.md](/INSTALL.md)

## Training data

A publicly-available dataset is available [on huggingface hub](https://huggingface.co/datasets/ptx0/pseudo-camera-10k).

Approximately 10k images are available in this repository with their caption as their filename, ready to be imported for use in SimpleTuner.

You can simply create a single folder full of jumbled-up images, or they can be neatly organised into subdirectories.

**Here are some important guidelines:**

### Training batch size

Your maximum batch size is a function of your available VRAM and image resolution:

```
vram use = batch size * resolution + base_requirements
```

To reduce VRAM use, you can reduce batch size or resolution, but the base requirements will always bite us in the ass. SDXL is a **huge** model.

To summarise:

- You want as high of a batch size as you can tolerate.
- The larger you set `RESOLUTION`, the more VRAM is used, and the lower your batch size can be.
- A larger batch size should have more training data in each bucket, since a batch size of 8 means a bucket of 1 image will be seen 8 times in one shot - no good!
- If you can&apos;t get a single iteration done with batch size of 1 and resolution of 128x128 on the Lion optimiser, your hardware just won&apos;t work.

Which brings up the next point: **you should use as much high quality training data as you can acquire.**

### Selecting images

- JPEG artifacts and blurry images are a no-go. The model **will** pick these up, especially newer ones like Flux or PixArt.
- For high quality photographs, some grainy CMOS sensors in the camera itself end up producing a lot of noise.
  - Too much of this will result in nearly every image your model produces, containing the same sensor noise.
- Same goes for watermarks and &quot;badges&quot;, artist signatures. That will all be picked up effortlessly.
- If you&apos;re trying to extract frames from a movie to train from, you&apos;re going to have a bad time. Compression ruins most films - only the large 40+ GB releases are really going to be useful for improving image clarity.
  - Using 1080p Bluray extractions really helps - 4k isn&apos;t absolutely required, but you&apos;re going to need to reduce expectations as to what kind of content will actually WORK.
  - Anime content will generally work very well if it&apos;s minimally compressed, but live action stuff tends to look blurry.
  - Try and locate frame stills from the production company instead, eg. on iMDB.
- Image resolutions optimally should be divisible by 64.
  - This isn&apos;t **required**, but is beneficial to follow, as it will allow the trainer to reuse your original images without resizing or cropping.
- Square images are not required, though they will work.
  - If you train on ONLY square images or ONLY non-square images, you might not get a very good balance of capabilities in the resulting model.
  - If you train on ONLY aspect bucketing, your resulting model will heavily bias these buckets for each type of content.
- Synthetic data works great. This means AI-generated images or captions. Using outputs from a different model is called **transfer learning** and can be highly effective.
  - Using ONLY synthetic data can harm the model&apos;s ability to generate more realistic details. A decent balance of regularisation images (eg. concepts that aren&apos;t your target) will help to maintain broad capabilities.
- Your dataset should be **as varied as possible** to get the highest quality. It should be balanced across different concepts, unless heavily biasing the model is desired.

### Captioning

SimpleTuner provides multiple [captioning](/toolkit/README.md) scripts that can be used to mass-rename files in a format that is acceptable to SimpleTuner.

Options:

- InternVL2 is the best option - it is very large however, and will be slow. This is best for smaller sets.
- Florence2 is likely the fastest and lightest weight, but some people really take a disliking to its outputs.
- BLIP3 is currently the best lightweight model that follows instruction prompts very well and produces prompts comparable to CogVLM with fewer hallucinations.
- T5 Flan and BLIP2 produce mediocre captions; it can be very slow and resource hungry.
- LLaVA produces acceptable captions but misses subtle details.
  - It is better than BLIP, can sometimes read text but invents details and speculates.
  - Follows instruction templates better than CogVLM and BLIP.
- CogVLM produces sterile but accurate captions and required the most time/resources until InternVL2 was integrated.
  - It still speculates, especially when given long instruct queries.
  - It does not follow instruct queries very well.

Other tools are available from third-party sources, such as Captionr.

For a caption to be useful by SimpleTuner:

- It could be the image&apos;s filename (the default behaviour)
- It could be the contents of a .txt file with the same name as the image (if `--caption_strategy=textfile` is provided)
- It could be directly in a jsonl table
- You could have a CSV file of URLs with a caption column
- (Advanced users) You may compile your dataset metadata into a parquet, json, or jsonl file and [provide it directly to SimpleTuner](/documentation/DATALOADER.md#advanced-techniques)

Longer captions aren&apos;t necessarily better for training. Simpler, concise captions work well, but a hybrid dataset mixing short and long captions will cover all bases.

#### Caption Dropout Parameter: CAPTION_DROPOUT_PROBABILITY

Foundational models like Stable Diffusion are built using 10% caption drop-out, meaning the model is shown an &quot;empty&quot; caption instead of the real one, about 10% of the time. This ends up substantially improving the quality of generations when using no negative prompt, especially for prompts that involve subject matter that do not exist in your training data.

Disabling caption dropout can damage the model&apos;s general quality. Conversely, using too much caption dropout will damage the model&apos;s ability to adhere to prompts.

A value of 25% seems to provide some additional benefits such as reducing the number of required steps during inference on v-prediction models, but the resulting model will be prone to forgetting.

Flux has [its own series of considerations](/documentation//quickstart/FLUX.md) and should be investigated before beginning training.

### Advanced Configuration

For users who are more familiar with model training and wish to tweak settings eg. `MIXED_PRECISION`, enabling offset noise, or setting up zero terminal SNR - detailed explanations can be found in [OPTIONS.md](/OPTIONS.md).

## Publishing checkpoints to Hugging Face Hub

Setting two values inside `config/config.json` will cause the trainer to automatically push your model up to the Hugging Face Hub upon training completion:

```bash
&quot;push_to_hub&quot;: true,
&quot;hub_model_name&quot;: &quot;what-you-will-call-this&quot;,
```

Be sure to login before you begin training by executing:

```bash
huggingface-cli login
```

A model card will be automatically generated containing a majority of the relevant training session parameters.

By default, every checkpoint will be uploaded to the Hub. However, if you wish to disable this behaviour to conserve bandwidth or for privacy reasons, you can set the following value in `config/config.json`:

```bash
&quot;push_checkpoints_to_hub&quot;: false,
```

## Monitoring and Logging

If `--report_to=wandb` is passed to the trainer (the default), it will ask on startup whether you wish to register on Weights &amp; Biases to monitor your training run there. While you can always select option **3** or remove `--report_to=...` and disable reporting, it&apos;s encouraged to give it a try and watch your loss value drop as your training runs!

### Discord webhook monitoring

SimpleTuner can submit messages to a Discord webhook:

- Startup/training summary
- Periodic status lines indicating the current epoch, step, loss, and EMA decay
- Validations images, as they generate, grouped by prompt (ten at a time)
- Most fatal errors

To configure a Discord webhook, add `--webhook_config=webhook.json` to your config file:

```bash
&quot;webhook_config&quot;: &quot;webhook.json&quot;,
```

In the SimpleTuner root directory, create the file `webhook.json`:

```json
{
  &quot;webhook_url&quot;: &quot;https://path/to/discord/webhook&quot;,
  &quot;webhook_type&quot;: &quot;discord&quot;,
  &quot;message_prefix&quot;: &quot;system-name-example&quot;,
  &quot;log_level&quot;: &quot;critical&quot;
}
```

- `webhook_url`
  - The value obtained from your Discord &quot;Integrations&quot; server settings.
- `webhook_type`
  - Currently, only discord is supported.
- `message_prefix`
  - This will be appended to the front of every message. If unset, it will default to the tracker project and run name.
- `log_level`
  - Values (decreasing level of spamminess, left-to-right): `debug`, `info`, `warning`, `error`, `critical`
  - `debug` is the most information, and `critical` will be limited to important updates.

### Post-Training Steps

#### How do I end training early?

You might not want to train all the way to the end.

At this point, reduce `--max_train_steps` value to one smaller than your current training step to force a pipeline export into your `output_dir`.

#### How do I test the model without wandb (Weights &amp; Biases)?

You can evaluate the model using [the provided evaluation script](/inference.py) or [other options in the inference toolkit](/toolkit/inference/inference_ddpm.py).

If you used `--push_to_hub`, the Huggingface Diffusers SDXL example scripts will be useable with the same model name.

If you require a single 13GiB safetensors file for eg. AUTOMATIC1111&apos;s Stable Diffusion WebUI or for uploading to CivitAI, you should make use of the [SDXL checkpoint conversion script](/convert_sdxl_checkpoint.py):

&gt; **Note**: If you&apos;re planning to export the resulting pipeline to eg. CivitAI, use the `--save_text_encoder` option to ensure it&apos;s copied to the output directory. It&apos;s okay if you forget or don&apos;t set this option, but it will require manually copying the text encoder.

```bash
python3 convert_sdxl_checkpoint.py --model_path=&quot;/path/to/SimpleTuner/simpletuner-results/pipeline&quot; --checkpoint_path=/path/to/your/output.safetensors --half --use_safetensors
```

Thank you to watusi on Discord for providing these instructions and requesting this addition.

## Model integration / usage

For using the model in your own projects, refer to the [Diffusers project](https://github.com/huggingface/diffusers).

## Debugging

For extra information when running SimpleTuner you can add the following to `config/config.env`:

```bash
export SIMPLETUNER_LOG_LEVEL=INFO
export SIMPLETUNER_TRAINING_LOOP_LOG_LEVEL=INFO
```

This can be placed anywhere in the file on its own line. It will bump the verbosity from the default `WARNING` value up to `INFO`. For even more information (God help us) set the log level to `DEBUG`.

A log file named `debug.log` will be written to the SimpleTuner project root directory, containing all log entries from `ERROR` to `DEBUG`.

### Seen images, current epoch, etc

In each model checkpoint directory is a `tracker_state.json` file which contains the current epoch that training was on or the images it has seen so far.

Each dataset will have its own tracking state documents in this directory as well. This contains the step count, number of images seen, and other metadata required to resume completely.

### Example Environment File Explained

Here&apos;s a breakdown of what each environment variable does:

#### General Settings

- `model_family`: Set this to the model arch you are training; kolors, sdxl, pixart_sigma, flux, sd3, legacy
- `data_backend_config`: This file is mandatory, and an example copy can be found in `multidatabackend.json.example` which contains an example for a multi-dataset configuration split between S3 and local data storage.
  - See [this document](/documentation/DATALOADER.md) for more information on configuring the data loader.
  - One or more datasets can be configured, but it&apos;s not necessary to use multiple.
  - Some config options that have an equivalent commandline option name can be omitted, in favour of the global option
  - Some config options are mandatory, but errors will emit for those on startup. Feel free to experiment.
  - Each dataset can have its own crop and resolution config.
- `seed`: You may set a numeric value here and it will make your training reproducible to that seed across all other given settings.
  - You may wish to set this to -1 so that your training is absolutely random, which prevents overfitting to a given seed.
- `resume_from_checkpoint`: Specifies which checkpoint to resume from. &quot;latest&quot; will pick the most recent one.
  - Do not set this value to a full pipeline. It will not work. To resume training a pipeline, use `pretrained_model_name_or_path` and provide an `/absolute/path`
- `checkpointing_steps`: Frequency of checkpointing during training.
  - Too many checkpoints created can slow down training. However, it might be necessary on providers that could unexpectedly shut down or restart your environment.
- `checkpoints_total_limit`: Maximum number of checkpoints to keep.
  - Using a higher value here will make it safer to leave training running attended for longer, at the cost of higher disk consumption - MUCH higher, in the case of SDXL.
- `learning_rate`: The initial learning rate for the model.
  - A value of `4e-7` may be considered the lowest effective learning rate when using EMA. A value of `1e-5` is much too high.
  - Somewhere in the range of `4e-7` to `4e-6` most likely lies your sweet spot.
  - You want the model to explore new territory (higher learning rate), but not so boldly that it explodes in catastrophic forgetting or worse.
  - If your learning rate is too low, it&apos;s possible to have some improvements in the beginning that then plateau. However, it can help prevent overfitting. Your mileage may vary.

#### Model and Data Settings

- `pretrained_model_name_or_path`: Specifies the pretrained model to use. Can be a HuggingFace Hub model or a local path. Either method requires a full Diffusers-style layout be available.
  - You can find some [here](https://huggingface.co/stabilityai) from Stability AI.
- `tracker_project_name` and `tracker_run_name`: Names for the tracking project on Weights and Biases. Currently, run names are non-functional.
- `instance_prompt`: Optional prompt to append to each caption. This can be useful if you want to add a **trigger keyword** for your model&apos;s style to associate with.
  - Make sure the instance prompt you use is similar to your data, or you could actually end up doing harm to the model.
  - Each dataset entry in `multidatabackend.json` can have its own `instance_prompt` set in lieu of using this main variable.
- `validation_prompt`: The prompt used for validation.

  - Optionally, a user prompt library or the built-in prompt library may be used to generate more than 84 images on each checkpoint across a large number of concepts.
  - See `--user_prompt_library` for more information.

  For DeepFloyd, a page is maintained with specific options to set. Visit [this document](/documentation/DEEPFLOYD.md) for a head start.

#### Data Locations

- `output_dir` - Where the model pipeline results are stored during training, and after it completes.

#### Training Parameters

- `max_train_steps`, `num_train_epochs`: Max number of steps or epochs for training.
  - If you use `max_train_steps`, it&apos;s recommended to set `num_train_epochs` to `0`.
  - Similarly, if you use `num_train_epochs`, it is recommended to set `max_train_steps` to `0`.
  - This simply signals to the trainer that you explicitly wish to use one or the other.
  - Don&apos;t supply `num_train_epochs` and `max_train_steps` values together, it won&apos;t let you begin training, to ensure there is no ambiguity about which you expect to take priority.
- `lr_scheduler`, `lr_warmup_steps`: Learning rate schedule and warmup steps.
  - `lr_scheduler` - stick to `constant`, as it is most likely to be stable and less chaotic. However, `polynomial` and `constant_with_warmup` have potential of moving the model&apos;s local minima before settling in and reducing the loss. Experimentation can pay off here, especially using the `cosine` and `sine` schedulers, which offer a unique approach to learning rate scheduling.
- `train_batch_size`: Batch size for training. You want this **as high as you can get it** without running out of VRAM or making your training unnecessarily **slow** (eg. 300-400% increase in training runtime - yikes! 💸)

## Additional Notes

For more details, consult the [INSTALL](/INSTALL.md) and [OPTIONS](/OPTIONS.md) documents or the [DATALOADER](/documentation/DATALOADER.md) information page for specific details on the dataset config file.

### Single-subject fine-tuning (Dreambooth)

See [DREAMBOOTH](/documentation/DREAMBOOTH.md) for a breakdown on how Dreambooth training can be configured in SimpleTuner.

### Mixture-of-Experts split-schedule model training

See [MIXTURE-OF-EXPERTS](/documentation/MIXTURE_OF_EXPERTS.md) for information on how to split training over two models, such that one is responsible for composition and large details, and the other is responsible for finalising and filling in the fine details.

### Quantised model training

Tested on Apple and NVIDIA systems, Hugging Face Optimum-Quanto can be used to reduce the precision and VRAM requirements, training even Flux.1 on just 20GB or less.

Inside your SimpleTuner venv:

```bash
pip install optimum-quanto
```

```bash
# Basically, any optimiser should work here.
&quot;optimizer&quot;: &quot;optimi-stableadamw&quot;,

# choices: int8-quanto, int4-quanto, int2-quanto, fp8-quanto
# int8-quanto was tested with a single subject dreambooth LoRA.
# fp8-quanto does not work on Apple systems. you must use int levels.
# int2-quanto is pretty extreme and gets the whole rank-1 LoRA down to about 13.9GB VRAM.
# may the gods have mercy on your soul, should you push things Too Far.
&quot;base_model_precision&quot;: &quot;int8-quanto&quot;,

# Maybe you want the text encoders to remain full precision so your text embeds are cake.
# We unload the text encoders before training, so, that&apos;s not an issue during training time - only during pre-caching.
# Alternatively, you can go ham on quantisation here and run them in int4 or int8 mode, because no one can stop you.
&quot;--text_encoder_1_precision&quot;: &quot;no_change&quot;,
&quot;--text_encoder_2_precision&quot;: &quot;no_change&quot;,
&quot;--text_encoder_3_precision&quot;: &quot;no_change&quot;,
```</file></files></repomix>